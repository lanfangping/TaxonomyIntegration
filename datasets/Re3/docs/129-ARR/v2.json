{
    "nodes": [
        {
            "ix": "129-ARR_v2_0",
            "content": "ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_2",
            "content": "While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training. Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average. Our implementation is available at https://github. com/yueyu1030/actune.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "129-ARR_v2_4",
            "content": "Fine-tuning pre-trained language models (PLMs) has achieved much success in natural language processing (NLP) (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020). One benefit of PLM fine-tuning is the promising performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021). However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many tasks. Besides, the performance of few-shot PLM finetuning can be sensitive to different sets of training data (Bragg et al., 2021). Therefore, there is a crucial need for approaches that make PLM finetuning more label-efficient and robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_5",
            "content": "Towards this goal, researchers have recently resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020). Nevertheless, they usually neglect unlabeled data, which can be useful for improving label efficiency for PLM fine-tuning (Du et al., 2021). To incorporate unlabeled data into active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Sim\u00e9oni et al., 2020). However, the query strategies proposed in these works can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency. Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and hurt model performance. This issue can be even more severe for PLMs, as the fine-tuning process is often sensitive to different weight initialization and data orderings (Dodge et al., 2020). Thus, it still remains open and challenging to design robust and label efficient method for active PLM fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_6",
            "content": "To tackle the above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning. Based on the estimated model uncertainty, AC-TUNE tightly couples active learning with selftraining in each learning round: (1) when the average uncertainty of a region is low, we trust the model's predictions and select its most certain predictions within the region for self-training; (2) when the average uncertainty of a region is high, indicating inadequate observations for parameter learning, we actively annotate its most uncertain samples within the region to improve model performance. Different from existing AL methods that only leverage uncertainty for querying labels, our uncertainty-driven self-training paradigm gradually leverages unlabeled data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mislabeled data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_7",
            "content": "To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB). Inspired by the fact that existing uncertainty-based AL methods often end up with choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design the region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs. Specifically, we first estimate the uncertainties of the unlabeled data with PLMs, then cluster the data using their PLM representations and weigh the data by the corresponding uncertainty. Such a clustering scheme partitions the embedding space into small sub-regions with an emphasis on highly-uncertain samples. Finally, by sampling over multiple highuncertainty regions, our strategy selects data with high uncertainty and low redundancy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_8",
            "content": "To rectify the erroneous pseudo labels derived by self-training, we design a simple but effective way to select low-uncertainty data for selftraining. Our method is motivated by the fact that fine-tuning PLMs suffer from instability issuesdifferent initializations and data orders can lead to large variance in model performance (Dodge et al., 2020;Zhang et al., 2020b;Mosbach et al., 2021). However, previous approaches only select pseudo-labeled data based on the prediction of the current round and are thus less reliable. In contrast, we maintain a dynamic memory bank to save the predictions of unlabeled samples for later use. We propose a momentum updating method to dynamically aggregate the predictions from preceding rounds (Laine and Aila, 2016) and select lowuncertainty samples based on aggregated prediction. As a result, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise. We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring little extra computational cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_9",
            "content": "Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates selftraining and active learning to minimize the labeling cost for fine-tuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to leverage the predictions in preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_10",
            "content": "2 Uncertainty-aware Active Self-training",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_11",
            "content": "Problem Formulation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "129-ARR_v2_12",
            "content": "We study active fine-tuning of pre-trained language models for text classification, formulated as follows: Given a small number of labeled samples X l = {(x i , y i )} L i=1 and unlabeled samples",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_13",
            "content": "X u = {x j } U j=1 (|X l | |X u |)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_14",
            "content": ", we aim to fine-tune a pre-trained language model f (x; \u03b8) : X \u2192 Y in an interactive way: we perform active self-training for T rounds with the total labeling budget b. In each round, we aim to query B = b/T samples denoted as B from X u to fine-tune a pre-trained language model f (x; \u03b8) with both X l , B and X u to maximize the performance on downstream text classification tasks. Here X = X l \u222a X u denotes all samples, and Y = {1, 2, \u2022 \u2022 \u2022 , C} is the label set where C is the number of classes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_15",
            "content": "Overview of ACTUNE Framework",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "129-ARR_v2_16",
            "content": "We now present our active self-training paradigm ACTUNE underpinned by estimated uncertainty. We begin the active self-training loop by finetuning a BERT f (\u03b8 (0) ) on the initial labeled data X L . Formally, we solve the following optimization problem",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_17",
            "content": "min \u03b8 1 |X L | (x i ,y i )\u2208X L CE f (x i ; \u03b8 (0) ), y i . (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_18",
            "content": "In round t (1 \u2264 t \u2264 T ) of active self-training, we first calculate the uncertainty score based on a given function a (t) i = a(x i , \u03b8 (t) ) 1 for all x i \u2208 X u . Then, we query labeled samples and generate pseudolabels for unlabeled data X u simultaneously to facilitate self-training. For each sample x i , the pseudo-label y is calculated based on the current 3), ( 4)) until convergence. 2. Select sample set Q (t) for AL and S (t) for self-training from Xu based on Eq. ( 11) or (13). 3. Augment the labeled set XL = XL \u222a Q (t) . 4. Obtain \u03b8 (t) by finetuning f (\u2022; \u03b8 t ) with LST ( Eq. ( 14)) using AdamW. 5. Update memory bank g(x; \u03b8 t ) with Eq. ( 10) or ( 12).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_19",
            "content": "Output: The final fine-tuned model f (\u2022; \u03b8 T ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_20",
            "content": "model's output:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_21",
            "content": "y = argmax j\u2208Y f (x; \u03b8 (t) ) j ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_22",
            "content": "where f (x; \u03b8 (t) ) \u2208 R C is a probability simplex and [f (x; \u03b8 (t) )] j is the j-th entry. The procedure of ACTUNE is summarized in Algorithm 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_23",
            "content": "Region-aware Sampling for Active Learning on High-uncertainty Data",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "129-ARR_v2_24",
            "content": "After obtaining the uncertainty for unlabeled data, we aim to query annotation for high-uncertainty samples. However, directly sampling the most uncertain samples gives suboptimal results as it tends to query repetitive data (Ein-Dor et al., 2020) that represent the overall data distribution poorly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_25",
            "content": "To tackle this issue, we propose region-aware sampling to capture both uncertainty and diversity during active self-training. Specifically, in the tth round, we first conduct the weighted K-means clustering (Huang et al., 2005), which weights samples based on their uncertainty. Denote by K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT. The weighted K-means process first initializes the center of each each cluster \u00b5 i (1 \u2264 i \u2264 K) via K-Means++ (Arthur and Vassilvitskii, 2007). Then, it jointly updates the centroid of each cluster and assigns each sample to cluster c i as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_26",
            "content": "c (t) i = argmin k=1,...,K v i \u2212 \u00b5 k 2 ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_27",
            "content": "\u00b5 (t) k = x i \u2208C (t) k a(x i , \u03b8 (t) ) \u2022 v (t) i x\u2208C (t) k a(x i , \u03b8 (t) )(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_28",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_29",
            "content": "C (t) k = {x (t) i |c (t) i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_30",
            "content": "= k}(k = 1, . . . , K) stands for the k-th cluster. The above two steps in Eq. ( 3), (4) are repeated until convergence. Compared with vanilla K-Means method, the weighting scheme increases the density of the samples with high uncertainty, thus enabling the K-Means methods to discover clusters with high uncertainty. After obtaining K regions with the corresponding data",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_31",
            "content": "C (t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_32",
            "content": "k , we calculate the uncertainty of each region as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_33",
            "content": "u (t) k = U (C (t) k ) + \u03b2I(C (t) k )(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_34",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_35",
            "content": "U (C (t) k ) = 1 |C (t) k | x i \u2208C (t) k a(x i , \u03b8 (t) ),(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_36",
            "content": "is the average uncertainty of samples and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_37",
            "content": "I(C (t) k ) = \u2212 j\u2208C f (t) j log f (t) j (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_38",
            "content": "is the inter-class diversity within cluster k and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_39",
            "content": "f (t) j = i 1{ y i =j} |C (t) k |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_40",
            "content": "is the frequency of class j on cluster k. Notably, the term U (C k ) assigns higher score for clusters with more uncertain samples, and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_41",
            "content": "I(C (t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_42",
            "content": "k ) grants higher scores for clusters containing samples with more diverse predicted classes from pseudo labels since such clusters would be closer to the decision boundary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_43",
            "content": "Then, we rank the clusters in an ascending order in u (t) k . A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the member instances to reduce uncertainty and improve the model's performance. We adopt a hierarchical sampling strategy: we first select the M clusters with the highest uncertainty, and then sample b = B M data with the highest uncertainty to form the batch",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_44",
            "content": "Q (t) . 2 K (t) a = top-M k\u2208{1,...,K} u (t) k , Q (t) = k\u2208K (t) a C (t) a,k where C (t) a,k = Top-b x i \u2208C (t) k a(xi, \u03b8 (t) ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_45",
            "content": "(8) We remark that such a hierarchical sampling strategy queries most uncertain samples from different regions, thus the uncertainty and diversity of queried samples can be both achieved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_46",
            "content": "Self-training over Confident Samples",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "129-ARR_v2_47",
            "content": "from Low-uncertainty Regions For self-training, we aim to select unlabeled samples which are most likely to have been correctly classified by the current model. This requires the sample to have low uncertainty. Therefore, we select the top k samples from the M lowest uncertainty regions to form the acquired batch S (t) :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_48",
            "content": "C (t) s = k\u2208K (t) s C (t) k",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_49",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_50",
            "content": "K (t) s = bottom-M k\u2208{1,...,K} u (t) k , S (t) = bottom-k x i \u2208C (t) s a(xi, \u03b8 (t) ).(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_51",
            "content": "Momentum-based Memory Bank for Selftraining. As PLMs are sensitive to the stochasticity involved in fine-tuning, the model suffers from the instability issue -different weight initialization and data orders may result in different predictions on the same dataset (Dodge et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_52",
            "content": "Additionally, if the model gives inconsistent predictions in different rounds for a specific sample, then it is potentially uncertain about the sample, and adding it to the training set may harm the active self-training process. For example, for a twoclass classification problem, suppose we obtain f (x; \u03b8 (t\u22121) ) = [0.65, 0.35] for sample x the round (t\u22121) and f (x; \u03b8 (t) ) = [0.05, 0.95] for the round t.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_53",
            "content": "Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is actually uncertain to which class x belongs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_54",
            "content": "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020;Laine and Aila, 2016) to aggregate the results from both the previous and current rounds. Then, during active self-training, we will select samples with the highest aggregated score. In this way, only those samples that the model is certain about over all previous rounds will be selected for self-training. We design two variants for the memory bank, namely prediction-based and value-based aggregation. Prediction based Momentum Update. We adopt an exponential moving average approach to aggregate the prediction g(x; \u03b8 (t) ) on round t as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_55",
            "content": "g(x; \u03b8 (t) ) = mtf (x; \u03b8 (t) ) + (1 \u2212 mt)g(x; \u03b8 (t\u22121) ), (10) where m t = (1 \u2212 t T )m L + t T m H (0 < m L \u2264 m H \u2264 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_56",
            "content": ") is a momentum coefficient. We gradually increase the weight for models on later rounds, since they are trained with more labeled data thus being able to provide more reliable predictions. Then, we calculate the uncertainty based on g(x; \u03b8 (t) ) and rewrite Eq. ( 9) and (2) as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_57",
            "content": "S (t) = bottom-k x i \u2208C (t) s a x i , g(x; \u03b8 (t) ), \u03b8 (t) y = argmax j\u2208Y g(x; \u03b8 (t) ) j ,(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_58",
            "content": "Value-based Momentum Update. For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as 12) Then, we use Eq. ( 12) to sample low-uncertainty data for self-training as 3 S (t) = bottom-k",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_59",
            "content": "g(x; \u03b8 (t) ) = mta(x; \u03b8 (t) ) + (1 \u2212 mt)g(x; \u03b8 (t\u22121) ). (",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_60",
            "content": "x i \u2208C (t) s g(x i , \u03b8 (t) ), y = argmax j\u2208Y f (x; \u03b8 (t) ) j . (13",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_61",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_62",
            "content": "By aggregating the prediction results over previous rounds, we filter the sample with inconsistent predictions to suppress noisy labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_63",
            "content": "Model Learning and Update",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "129-ARR_v2_64",
            "content": "After obtaining both the labeled data and pseudolabeled data, we fine-tune a new pre-trained BERT model \u03b8 (t+1) on them. Although we only include low-uncertainty samples during self-training, it is difficult to eliminate all the wrong pseudo-labels, and such mislabeled samples can still hurt model performance. To suppress such label noise, we use a threshold-based strategy to further remove noisy labels by selecting samples that agree with the corresponding pseudo labels. The loss objective of optimizing \u03b8 (t+1) is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_65",
            "content": "LST = 1 |L (t) | x i \u2208L (t) CE f (xi; \u03b8 (t+1) ), yi + \u03bb |S (t) | x i \u2208S (t) \u03c9i CE f ( xi; \u03b8 (t+1) ), yi ,(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_66",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_67",
            "content": "L (t) = X L \u222a Q (t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_68",
            "content": "is the labeled set, \u03bb is a hyper-parameter balancing the weight between clean and pseudo labels, and 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_69",
            "content": "\u03c9 i = 1{ f (x i ; \u03b8 (t+1) ) y i > \u03b3}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_70",
            "content": "Active Learning Setups. Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100. In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels. Since large development sets are impractical in low-resource settings (Kann et al., 2019), we keep the size of development set as 1000, which is the same as the labeling budget 4 . For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500. Implementation Details. We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora. In each round, we train from scratch to avoid overfitting the data collected in earlier rounds as observed by Hu et al. (2019) Note that when compared with active learning baselines, we do not augment the train set with pseudolabeled data (Eq. ( 9)) to ensure fair comparisons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_71",
            "content": "Main Result",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "129-ARR_v2_72",
            "content": "Figure 1 reports the performance of ACTUNE and the baselines on 4 benchmarks. From the results, we have the following observations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_73",
            "content": "\u2022 ACTUNE consistently outperforms baselines in most of the cases. Different from studies in the computer vision (CV) domain (Sim\u00e9oni et al., 2020) where the model does not perform well in the low-data regime, pre-trained LM has achieved competitive performance with only a few labeled data, which makes further improvements to the vanilla fine-tuning challenging. Nevertheless, AC-TUNE surpasses baselines in more than 90% of the rounds and achieves 0.4%-0.7% and 0.3%-1.5% absolute gain at the end of AL and SSAL respectively. Figure 3 quantitatively measures the number of labels needed for the most advanced active learning model and self-training model (UST) to outperform ACTUNE with 1000 labels. These baselines need >2000 clean labeled samples to reach the performance as ours. ACTUNE saves on average 56.2% and 57.0% of the labeled samples than most advanced active learning and selftraining baselines respectively, which justifies its promising performance under low-resource scenarios. Such improvements show the merits of two key designs under our active self-training framework: the region-aware sampling for active learning and the momentum-based memory bank for robust selftraining, which will be discussed in the section 3.5. \u2022 Compared with the previous AL baselines, AC-TUNE can bring consistent performance gain, while previous semi-supervised active learning methods cannot. For instance, BASS is based on BALD for active learning, but sometimes it performs even worse than BALD with the same number of labeled data (see Fig. 1(b) and Fig. 1(f)). This is mainly because previous methods simply combine noisy pseudo labels with clean labels for training without explicitly rectifying the wrongly-labeled data, which will cause the LM to overfit these hazardous labels. Moreover, previous methods do not exploit momentum updates to stabilize the learning process, as there are oscillations in the beginning rounds. In contrast, ACTUNE achieves a more stable learning process and enables an active selftraining process to benefit from more labeled data. \u2022 The self-training methods (ST & UST) achieve superior performance with limited labels. However, they mainly focus on leveraging unlabeled data for improving the performance, while our results demonstrate that adaptive selecting the most useful data for fine-tuning is also important for improving the performance. With a powerful querying policy, ACTUNE can improve these self-training baselines by 1.05% in terms of accuracy on average.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_74",
            "content": "Weakly-supervised Learning",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "129-ARR_v2_75",
            "content": "ACTUNE can be naturally used for weaklysupervised classification, where X l is a set of noisy labels derived from linguistic patterns or rules. Since the initial label set is noisy, the model trained with Eq. ( 1) can overfit the label noise (Zhang et al., 2022a), and we can actively query labeled data to refine the model. We conduct experiments on the TREC and Chemprot dataset 5 , where we first use Snorkel (Ratner et al., 2017) to obtain weak label set X l , then fine-tune the pre-trained LM f (\u03b8 (0) ) on X l . After that, we adopt ACTUNE for active self-training. Fig. 2 shows the results of these two datasets 6 . When combining ACTUNE with CAL, the performance is unsatisfactory. We believe it is because CAL requires clean labels to calculate uncertainties. When labels are inaccurate, it will prevent AC-TUNE from querying informative samples. In contrast, ACTUNE achieves the best performance over baselines when using Entropy as the uncertainty measure. The performance gain is more notable on the TREC dataset, where we achieve 96.68% accuracy, close to the fully supervised performance (96.80%) with only \u223c6% of clean labels. (e.g. BADGE), when using the entropy as an uncertainty measure to select pseudo-labeled data for self-training, ACTUNE can further boost the performance. This indicates that ACTUNE is a general active self-training approach, as it can serve as an efficient plug-in module for existing AL methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_76",
            "content": "Combination with Other AL Methods",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "129-ARR_v2_77",
            "content": "Ablation and Hyperparameter Study",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "129-ARR_v2_78",
            "content": "The Effect of Different Components in AC-TUNE. We inspect different components of ACTUNE, including the region-sampling (RS), momentum-based memory bank (MMB), and weighted clustering (WClus) 7 . Experimental results (Fig. 5(b)) shows that all the three components contribute to the final performance, as removing any of them hurts the classification accuracy. Also, we find that when removing MMB, the performance hurts most in the beginning rounds, which indicates that MMB effectively suppresses label noise when the model's capacity is weak. Conversely, removing WClus hurts the performance on later rounds, as it enables the model to select most informative samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_79",
            "content": "Hyperparameter Study. We study two hyperparameters, namely \u03b2 and K used in querying labels. Figure 4(a) and 4(b) show the results. In general, the model is insensitive to \u03b2 as the performance difference is less than 0.6%. The model cannot perform well with smaller K since it cannot pinpoint to high-uncertainty regions. For larger K, the performance also drops as some of the high-uncertainty regions can be outliers and sampling from them would hurt the model performance (Karamcheti et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_80",
            "content": "A Closer Look at the Momentum-based Memory Bank. To examine the role of MMB, we show the overall accuracy of pseudo-labels on AG News dataset in Fig. 4(c). From the result, it is clear that the momentum-based memory bank can stabilize the active self-training process, as the accuracy of pseudo labels increases around 1%, especially in Value,mL=0.7,mH=0.9 Value,mL=0.9,mH=0.9 Prob,m=0.7,mH=0.9 Prob,m=0.9,mH=0.9 No Momentum (d) Entropy ,mL=0.7,mH=0.9 Value,mL=0.9,mH=0.9 Prob,m=0.7,mH=0.9 Prob,m=0.9,mH=0.9 No Momentum (e) CAL Overall, we find that our model is robust to different choices as ACTUNE outperform the baseline without momentum update consistently. Moreover, we find that the larger m H will generally lead to better performance in later rounds. This is mainly because in later rounds, the model's prediction is more reliable. Conversely, at the beginning of the training, the model's prediction might be oscillating on unlabeled data. In this case, using a smaller m L will favor samples with consistent predictions to improve the robustness of active self-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_81",
            "content": "Another finding is that for different AL methods, the optimal memory bank can be different. For Entropy, probability-based memory bank leads to a better result, while for CAL, simple aggregating over uncertainty score achieves better performance. This is mainly because the method used in CAL is more complicated, and using probability-based memory bank may hurt the uncertainty calculation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_82",
            "content": "Case Study",
            "ntype": "title",
            "meta": {
                "section": "3.6"
            }
        },
        {
            "ix": "129-ARR_v2_83",
            "content": "We give an example of our querying strategy on AG News dataset for the 1st round of active selftraining process in figure 6. Note that we use t-SNE algorithm (Van der Maaten and Hinton, 2008) for dimension reduction, and the black triangle stands for the queried samples while other circles stands for the unlabeled data. We can see that the existing uncertainty-based methods such as Entropy and CAL, are suffered from the issue of limited diversity. However, when combined with ACTUNE, the diversity is much improved. Such results, compared with the main results in figure 1, demonstrate the efficacy of ACTUNE empirically.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_84",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "129-ARR_v2_85",
            "content": "Active Learning. Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021). So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversity-based methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020). Ein-Dor et al. ( 2020) offer an empirical study of active learning with PLMs. Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b). In our study, we leverage the power of unlabeled instances via self-training to further promote the performance of AL. Semi-supervised Active Learning (SSAL). 2020) exploit the most-certain samples from the unlabeled with pseudo-labeling to augment the training set. So far, most of the SSAL approaches are designed for CV domain and it remains unknown how this paradigm performs with PLMs on NLP tasks. In contrast, we propose ACTUNE to effectively leverage unlabeled data during finetuing PLMs for NLP tasks. Self-training. Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013). It first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability. However, it is known to be vulnerable to error propagation (Arazo et al., 2020;Rizve et al., 2021;. To alleviate this, we adopt a simple momentum-based method to select high confidence samples, effectively reducing 1429",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_86",
            "content": "Conclusion and Discussion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "129-ARR_v2_87",
            "content": "In this paper, we develop ACTUNE, a general active self-training framework for enhancing both label efficiency and model performance in fine-tuning pre-trained language models (PLMs). We propose a region-aware sampling approach to guarantee both the uncertainty the diversity for querying labels. To combat the label noise propagation issue, we design a momentum-based memory bank to effectively utilize the model predictions for preceding AL rounds. Empirical results on 6 public text classification benchmarks suggest the superiority of ACTUNE to conventional active learning and semi-supervised active learning methods for fine-tuning PLMs with limited resources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_88",
            "content": "There are several directions to improve ACTUNE. First, since our focus is on fine-tuning pre-trained language models, we use the representation of [CLS] token for classification. In the future work, we can consider using prompt tuning (Gao et al., 2021;Schick and Sch\u00fctze, 2021), a more dataefficient method for adopting pre-trained language models on classification tasks to further promote the efficiency. Also, due to the computational resource constraints, we do not use larger pre-trained language models such as RoBERTa-large (Liu et al., 2019) which shown even better performance with only a few labels (Du et al., 2021). Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance. Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference (NLI). (Holub et al., 2008) 461s 646s BALD (Gal et al., 2017) 4595s 6451s ALPS (Yuan et al., 2020) 488s 677s BADGE (Ash et al., 2020) 554s 1140s CAL (Margatina et al., 2021b) 493s 688s REVIVAL (Guo et al., based aggregation by default.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_89",
            "content": "Table 3 shows the time in one active learning round of ACTUNE and baselines. Here we highlight that the additional time for region-aware sampling and momentum-based memory bank is rather small compared with the inference time. Also, we find that BALD and REVIVAL are not so efficient. For BALD, it needs to infer the uncertainty of the model by passing the data to model with multitple times. Such an operation will make the total inference time for PLMs very long. For REVIVAL, we find that calculating the adversarial gradient needs extra forward passes and backward passes, which could be time-consuming for PLMs with millions of parameters 8 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "129-ARR_v2_90",
            "content": "Eric Arazo, Diego Ortego, Paul Albert, E O' Noel, Kevin Connor,  Mcguinness, Pseudolabeling and confirmation bias in deep semisupervised learning, 2020, 2020 International Joint Conference on Neural Networks (IJCNN), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Eric Arazo",
                    "Diego Ortego",
                    "Paul Albert",
                    "E O' Noel",
                    "Kevin Connor",
                    " Mcguinness"
                ],
                "title": "Pseudolabeling and confirmation bias in deep semisupervised learning",
                "pub_date": "2020",
                "pub_title": "2020 International Joint Conference on Neural Networks (IJCNN)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v2_91",
            "content": "David Arthur, Sergei Vassilvitskii, K-means++: The advantages of careful seeding, 2007, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "David Arthur",
                    "Sergei Vassilvitskii"
                ],
                "title": "K-means++: The advantages of careful seeding",
                "pub_date": "2007",
                "pub_title": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_92",
            "content": "Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal, Deep batch active learning by diverse, uncertain gradient lower bounds, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jordan Ash",
                    "Chicheng Zhang",
                    "Akshay Krishnamurthy",
                    "John Langford",
                    "Alekh Agarwal"
                ],
                "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_93",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Trapit Bansal",
                    "Rishikesh Jha",
                    "Tsendsuren Munkhdalai",
                    "Andrew Mccallum"
                ],
                "title": "Self-supervised meta-learning for few-shot natural language classification tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_94",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Iz Beltagy",
                    "Kyle Lo",
                    "Arman Cohan"
                ],
                "title": "SciB-ERT: A pretrained language model for scientific text",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_95",
            "content": "UNKNOWN, None, 2020, Interactive weak supervision: Learning useful heuristics for data labeling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Interactive weak supervision: Learning useful heuristics for data labeling",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_96",
            "content": "Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy, Flex: Unifying evaluation for few-shot nlp, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jonathan Bragg",
                    "Arman Cohan",
                    "Kyle Lo",
                    "Iz Beltagy"
                ],
                "title": "Flex: Unifying evaluation for few-shot nlp",
                "pub_date": "2021",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_97",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Language models are fewshot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tom Brown",
                    "Benjamin Mann",
                    "Nick Ryder",
                    "Melanie Subbiah",
                    "Jared Kaplan",
                    "Prafulla Dhariwal",
                    "Arvind Neelakantan"
                ],
                "title": "Language models are fewshot learners",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_98",
            "content": "Franck Dernoncourt, Ji Lee, PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Franck Dernoncourt",
                    "Ji Lee"
                ],
                "title": "PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "129-ARR_v2_99",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "129-ARR_v2_100",
            "content": "UNKNOWN, None, 2002, 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_101",
            "content": "Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart, The hitchhiker's guide to testing statistical significance in natural language processing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Rotem Dror",
                    "Gili Baumer",
                    "Segev Shlomov",
                    "Roi Reichart"
                ],
                "title": "The hitchhiker's guide to testing statistical significance in natural language processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v2_102",
            "content": "Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, Alexis Conneau, Self-training improves pre-training for natural language understanding, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jingfei Du",
                    "Edouard Grave",
                    "Beliz Gunel",
                    "Vishrav Chaudhary",
                    "Onur Celebi",
                    "Michael Auli",
                    "Veselin Stoyanov",
                    "Alexis Conneau"
                ],
                "title": "Self-training improves pre-training for natural language understanding",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_103",
            "content": "Alon Liat Ein-Dor, Ariel Halfon, Eyal Gera, Lena Shnarch, Leshem Dankin, Marina Choshen, Ranit Danilevsky, Yoav Aharonov, Noam Katz,  Slonim, Active Learning for BERT: An Empirical Study, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Alon Liat Ein-Dor",
                    "Ariel Halfon",
                    "Eyal Gera",
                    "Lena Shnarch",
                    "Leshem Dankin",
                    "Marina Choshen",
                    "Ranit Danilevsky",
                    "Yoav Aharonov",
                    "Noam Katz",
                    " Slonim"
                ],
                "title": "Active Learning for BERT: An Empirical Study",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_104",
            "content": "UNKNOWN, None, 2015, Bayesian convolutional neural networks with bernoulli approximate variational inference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Bayesian convolutional neural networks with bernoulli approximate variational inference",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_105",
            "content": "Yarin Gal, Riashat Islam, Zoubin Ghahramani, Deep bayesian active learning with image data, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Yarin Gal",
                    "Riashat Islam",
                    "Zoubin Ghahramani"
                ],
                "title": "Deep bayesian active learning with image data",
                "pub_date": "2017",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "129-ARR_v2_106",
            "content": "Mingfei Gao, Zizhao Zhang, Guo Yu, \u00d6 Sercan,  Ar\u0131k, S Larry, Tomas Davis,  Pfister, Consistency-based semi-supervised active learning: Towards minimizing labeling cost, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Mingfei Gao",
                    "Zizhao Zhang",
                    "Guo Yu",
                    "\u00d6 Sercan",
                    " Ar\u0131k",
                    "S Larry",
                    "Tomas Davis",
                    " Pfister"
                ],
                "title": "Consistency-based semi-supervised active learning: Towards minimizing labeling cost",
                "pub_date": "2020",
                "pub_title": "European Conference on Computer Vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "129-ARR_v2_107",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v2_108",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_109",
            "content": "Fei Sun, Yueting Wu,  Zhuang, Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Fei Sun",
                    "Yueting Wu",
                    " Zhuang"
                ],
                "title": "Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_110",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kaiming He",
                    "Haoqi Fan",
                    "Yuxin Wu",
                    "Saining Xie",
                    "Ross Girshick"
                ],
                "title": "Momentum contrast for unsupervised visual representation learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_111",
            "content": "Alex Holub, Pietro Perona, Michael C Burl, Entropy-based active learning for object recognition, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Alex Holub",
                    "Pietro Perona",
                    "Michael C Burl"
                ],
                "title": "Entropy-based active learning for object recognition",
                "pub_date": "2008",
                "pub_title": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v2_112",
            "content": "UNKNOWN, None, 2022, Nemo: Guiding and contextualizing weak supervision for interactive data programming, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2022",
                "pub_title": "Nemo: Guiding and contextualizing weak supervision for interactive data programming",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_113",
            "content": "Peiyun Hu, Zack Lipton, Anima Anandkumar, Deva Ramanan, Active learning with partial feedback, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Peiyun Hu",
                    "Zack Lipton",
                    "Anima Anandkumar",
                    "Deva Ramanan"
                ],
                "title": "Active learning with partial feedback",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_114",
            "content": "UNKNOWN, None, 2005, Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_115",
            "content": "Zhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang, Changlong Sun, Qiong Zhang, Xiaozhong Liu, Camouflaged Chinese spam content detection with semi-supervised generative active learning, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Zhuoren Jiang",
                    "Zhe Gao",
                    "Yu Duan",
                    "Yangyang Kang",
                    "Changlong Sun",
                    "Qiong Zhang",
                    "Xiaozhong Liu"
                ],
                "title": "Camouflaged Chinese spam content detection with semi-supervised generative active learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_116",
            "content": "Katharina Kann, Kyunghyun Cho, Samuel , Towards realistic practices in lowresource natural language processing: The development set, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Katharina Kann",
                    "Kyunghyun Cho",
                    "Samuel "
                ],
                "title": "Towards realistic practices in lowresource natural language processing: The development set",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_117",
            "content": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher Manning, Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Siddharth Karamcheti",
                    "Ranjay Krishna",
                    "Li Fei-Fei",
                    "Christopher Manning"
                ],
                "title": "Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "129-ARR_v2_118",
            "content": "Lingkai Kong, Jimeng Sun, Chao Zhang, Sde-net: Equipping deep neural networks with uncertainty estimates, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Lingkai Kong",
                    "Jimeng Sun",
                    "Chao Zhang"
                ],
                "title": "Sde-net: Equipping deep neural networks with uncertainty estimates",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "129-ARR_v2_119",
            "content": "Martin Krallinger, Obdulia Rabal, A Saber,  Akhondi, Overview of the biocreative VI chemical-protein interaction track, 2017, BioCreative evaluation Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Martin Krallinger",
                    "Obdulia Rabal",
                    "A Saber",
                    " Akhondi"
                ],
                "title": "Overview of the biocreative VI chemical-protein interaction track",
                "pub_date": "2017",
                "pub_title": "BioCreative evaluation Workshop",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_120",
            "content": "UNKNOWN, None, 2016, Temporal ensembling for semi-supervised learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Temporal ensembling for semi-supervised learning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_121",
            "content": "Dong-Hyun Lee, Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks, 2013, ICML Workshop on challenges in representation learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Dong-Hyun Lee"
                ],
                "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
                "pub_date": "2013",
                "pub_title": "ICML Workshop on challenges in representation learning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_122",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, The 19th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Xin Li",
                    "Dan Roth"
                ],
                "title": "Learning question classifiers",
                "pub_date": "2002",
                "pub_title": "The 19th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_123",
            "content": "Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, Chao Zhang, Bond: Bert-assisted open-domain named entity recognition with distant supervision, 2020, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Chen Liang",
                    "Yue Yu",
                    "Haoming Jiang",
                    "Siawpeng Er",
                    "Ruijia Wang",
                    "Tuo Zhao",
                    "Chao Zhang"
                ],
                "title": "Bond: Bert-assisted open-domain named entity recognition with distant supervision",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_124",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_125",
            "content": "UNKNOWN, None, , Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_126",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Katerina Margatina",
                    "Giorgos Vernikos"
                ],
                "title": "Active learning by acquiring contrastive examples",
                "pub_date": null,
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_127",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Marius Mosbach",
                    "Maksym Andriushchenko",
                    "Dietrich Klakow"
                ],
                "title": "On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_128",
            "content": "Subhabrata Mukherjee, Ahmed Awadallah, Uncertainty-aware self-training for few-shot text classification, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Subhabrata Mukherjee",
                    "Ahmed Awadallah"
                ],
                "title": "Uncertainty-aware self-training for few-shot text classification",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_129",
            "content": "Alexander Ratner, H Stephen, Henry Bach, Jason Ehrenberg, Sen Fries, Christopher Wu,  R\u00e9, Snorkel: Rapid training data creation with weak supervision, 2017, Proceedings of the VLDB Endowment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Alexander Ratner",
                    "H Stephen",
                    "Henry Bach",
                    "Jason Ehrenberg",
                    "Sen Fries",
                    "Christopher Wu",
                    " R\u00e9"
                ],
                "title": "Snorkel: Rapid training data creation with weak supervision",
                "pub_date": "2017",
                "pub_title": "Proceedings of the VLDB Endowment",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_130",
            "content": "Kevin Mamshad Nayeem Rizve,  Duarte, S Yogesh, Mubarak Rawat,  Shah, In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Kevin Mamshad Nayeem Rizve",
                    " Duarte",
                    "S Yogesh",
                    "Mubarak Rawat",
                    " Shah"
                ],
                "title": "In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_131",
            "content": "Matthias Rottmann, Karsten Kahl, Hanno Gottschalk, Deep bayesian active semisupervised learning, 2018, 17th IEEE International Conference on Machine Learning and Applications (ICMLA), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Matthias Rottmann",
                    "Karsten Kahl",
                    "Hanno Gottschalk"
                ],
                "title": "Deep bayesian active semisupervised learning",
                "pub_date": "2018",
                "pub_title": "17th IEEE International Conference on Machine Learning and Applications (ICMLA)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v2_132",
            "content": "Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan Zhang, Yong Yu, Lei Li, Active sentence learning by adversarial uncertainty sampling in discrete space, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Dongyu Ru",
                    "Jiangtao Feng",
                    "Lin Qiu",
                    "Hao Zhou",
                    "Mingxuan Wang",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Active sentence learning by adversarial uncertainty sampling in discrete space",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_133",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_134",
            "content": "Ozan Sener, Silvio Savarese, Active learning for convolutional neural networks: A core-set approach, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Ozan Sener",
                    "Silvio Savarese"
                ],
                "title": "Active learning for convolutional neural networks: A core-set approach",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_135",
            "content": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, V Dmitry, Alexander Dylov,  Panchenko, Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Artem Shelmanov",
                    "Dmitri Puzyrev",
                    "Lyubov Kupriyanova",
                    "Denis Belyakov",
                    "Daniil Larionov",
                    "Nikita Khromov",
                    "Olga Kozlova",
                    "Ekaterina Artemova",
                    "V Dmitry",
                    "Alexander Dylov",
                    " Panchenko"
                ],
                "title": "Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_136",
            "content": "Oriane Sim\u00e9oni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier, Rethinking deep active learning: Using unlabeled data at model training, 2020, the 25th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Oriane Sim\u00e9oni",
                    "Mateusz Budnik",
                    "Yannis Avrithis",
                    "Guillaume Gravier"
                ],
                "title": "Rethinking deep active learning: Using unlabeled data at model training",
                "pub_date": "2020",
                "pub_title": "the 25th International Conference on Pattern Recognition (ICPR)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "129-ARR_v2_137",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_138",
            "content": "Katrin Tomanek, Udo Hahn, Semisupervised active learning for sequence labeling, 2009, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Katrin Tomanek",
                    "Udo Hahn"
                ],
                "title": "Semisupervised active learning for sequence labeling",
                "pub_date": "2009",
                "pub_title": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_139",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Laurens Van Der Maaten",
                    "Geoffrey Hinton"
                ],
                "title": "Visualizing data using t-sne",
                "pub_date": "2008",
                "pub_title": "Journal of machine learning research",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_140",
            "content": "Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin, Cost-effective active learning for deep image classification, 2016, IEEE Transactions on Circuits and Systems for Video Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Keze Wang",
                    "Dongyu Zhang",
                    "Ya Li",
                    "Ruimao Zhang",
                    "Liang Lin"
                ],
                "title": "Cost-effective active learning for deep image classification",
                "pub_date": "2016",
                "pub_title": "IEEE Transactions on Circuits and Systems for Video Technology",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_141",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Others. 2020. Transformers: State-of-theart natural language processing, , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf"
                ],
                "title": "Morgan Funtowicz, and Others. 2020. Transformers: State-of-theart natural language processing",
                "pub_date": null,
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_142",
            "content": "Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, Quoc Le, Unsupervised data augmentation for consistency training, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Qizhe Xie",
                    "Zihang Dai",
                    "Eduard Hovy",
                    "Thang Luong",
                    "Quoc Le"
                ],
                "title": "Unsupervised data augmentation for consistency training",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_143",
            "content": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang, Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Yue Yu",
                    "Simiao Zuo",
                    "Haoming Jiang",
                    "Wendi Ren",
                    "Tuo Zhao",
                    "Chao Zhang"
                ],
                "title": "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_144",
            "content": "Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber, Cold-start active learning through self-supervised language modeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Michelle Yuan",
                    "Hsuan-Tien Lin",
                    "Jordan Boyd-Graber"
                ],
                "title": "Cold-start active learning through self-supervised language modeling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "129-ARR_v2_145",
            "content": "UNKNOWN, None, , 2022a. A survey on programmatic weak supervision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2022a. A survey on programmatic weak supervision",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_146",
            "content": "Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, Alexander Ratner, WRENCH: A comprehensive benchmark for weak supervision, 2021, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Jieyu Zhang",
                    "Yue Yu",
                    "Yinghao Li",
                    "Yujing Wang",
                    "Yaming Yang",
                    "Mao Yang",
                    "Alexander Ratner"
                ],
                "title": "WRENCH: A comprehensive benchmark for weak supervision",
                "pub_date": "2021",
                "pub_title": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_147",
            "content": "UNKNOWN, None, 2022, Prboost: Promptbased rule discovery and boosting for interactive weakly-supervised learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": null,
                "title": null,
                "pub_date": "2022",
                "pub_title": "Prboost: Promptbased rule discovery and boosting for interactive weakly-supervised learning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_148",
            "content": "Rongzhi Zhang, Yue Yu, Chao Zhang, Se-qMix: Augmenting active sequence labeling via sequence mixup, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Rongzhi Zhang",
                    "Yue Yu",
                    "Chao Zhang"
                ],
                "title": "Se-qMix: Augmenting active sequence labeling via sequence mixup",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_149",
            "content": "UNKNOWN, None, 2020, Revisiting few-sample bert fine-tuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Revisiting few-sample bert fine-tuning",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_150",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "129-ARR_v2_151",
            "content": "UNKNOWN, None, 2021, Self-training with differentiable teacher, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Self-training with differentiable teacher",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "129-ARR_v2_0@0",
            "content": "ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_0",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@0",
            "content": "While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@1",
            "content": "Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 163,
            "end": 324,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@2",
            "content": "We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 326,
            "end": 448,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@3",
            "content": "ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 450,
            "end": 651,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@4",
            "content": "Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 653,
            "end": 915,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@5",
            "content": "Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 917,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@6",
            "content": "Our implementation is available at https://github.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 1123,
            "end": 1172,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_2@7",
            "content": "com/yueyu1030/actune.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_2",
            "start": 1174,
            "end": 1194,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_4@0",
            "content": "Fine-tuning pre-trained language models (PLMs) has achieved much success in natural language processing (NLP) (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_4",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_4@1",
            "content": "One benefit of PLM fine-tuning is the promising performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_4",
            "start": 169,
            "end": 316,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_4@2",
            "content": "However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_4",
            "start": 318,
            "end": 427,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_4@3",
            "content": "Besides, the performance of few-shot PLM finetuning can be sensitive to different sets of training data (Bragg et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_4",
            "start": 429,
            "end": 553,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_4@4",
            "content": "Therefore, there is a crucial need for approaches that make PLM finetuning more label-efficient and robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_4",
            "start": 555,
            "end": 774,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@0",
            "content": "Towards this goal, researchers have recently resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@1",
            "content": "Nevertheless, they usually neglect unlabeled data, which can be useful for improving label efficiency for PLM fine-tuning (Du et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 249,
            "end": 388,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@2",
            "content": "To incorporate unlabeled data into active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Sim\u00e9oni et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 390,
            "end": 577,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@3",
            "content": "However, the query strategies proposed in these works can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 579,
            "end": 746,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@4",
            "content": "Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and hurt model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 748,
            "end": 963,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@5",
            "content": "This issue can be even more severe for PLMs, as the fine-tuning process is often sensitive to different weight initialization and data orderings (Dodge et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 965,
            "end": 1130,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_5@6",
            "content": "Thus, it still remains open and challenging to design robust and label efficient method for active PLM fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_5",
            "start": 1132,
            "end": 1246,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_6@0",
            "content": "To tackle the above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_6",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_6@1",
            "content": "Based on the estimated model uncertainty, AC-TUNE tightly couples active learning with selftraining in each learning round: (1) when the average uncertainty of a region is low, we trust the model's predictions and select its most certain predictions within the region for self-training; (2) when the average uncertainty of a region is high, indicating inadequate observations for parameter learning, we actively annotate its most uncertain samples within the region to improve model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_6",
            "start": 142,
            "end": 636,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_6@2",
            "content": "Different from existing AL methods that only leverage uncertainty for querying labels, our uncertainty-driven self-training paradigm gradually leverages unlabeled data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mislabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_6",
            "start": 638,
            "end": 938,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_7@0",
            "content": "To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_7",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_7@1",
            "content": "Inspired by the fact that existing uncertainty-based AL methods often end up with choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design the region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_7",
            "start": 201,
            "end": 511,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_7@2",
            "content": "Specifically, we first estimate the uncertainties of the unlabeled data with PLMs, then cluster the data using their PLM representations and weigh the data by the corresponding uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_7",
            "start": 513,
            "end": 701,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_7@3",
            "content": "Such a clustering scheme partitions the embedding space into small sub-regions with an emphasis on highly-uncertain samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_7",
            "start": 703,
            "end": 826,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_7@4",
            "content": "Finally, by sampling over multiple highuncertainty regions, our strategy selects data with high uncertainty and low redundancy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_7",
            "start": 828,
            "end": 954,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@0",
            "content": "To rectify the erroneous pseudo labels derived by self-training, we design a simple but effective way to select low-uncertainty data for selftraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@1",
            "content": "Our method is motivated by the fact that fine-tuning PLMs suffer from instability issuesdifferent initializations and data orders can lead to large variance in model performance (Dodge et al., 2020;Zhang et al., 2020b;Mosbach et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 151,
            "end": 390,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@2",
            "content": "However, previous approaches only select pseudo-labeled data based on the prediction of the current round and are thus less reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 392,
            "end": 524,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@3",
            "content": "In contrast, we maintain a dynamic memory bank to save the predictions of unlabeled samples for later use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 526,
            "end": 631,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@4",
            "content": "We propose a momentum updating method to dynamically aggregate the predictions from preceding rounds (Laine and Aila, 2016) and select lowuncertainty samples based on aggregated prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 633,
            "end": 821,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@5",
            "content": "As a result, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 823,
            "end": 978,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_8@6",
            "content": "We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring little extra computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_8",
            "start": 980,
            "end": 1126,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_9@0",
            "content": "Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates selftraining and active learning to minimize the labeling cost for fine-tuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to leverage the predictions in preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_9",
            "start": 0,
            "end": 588,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_10@0",
            "content": "2 Uncertainty-aware Active Self-training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_10",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_11@0",
            "content": "Problem Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_11",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_12@0",
            "content": "We study active fine-tuning of pre-trained language models for text classification, formulated as follows: Given a small number of labeled samples X l = {(x i , y i )} L i=1 and unlabeled samples",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_12",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_13@0",
            "content": "X u = {x j } U j=1 (|X l | |X u |)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_13",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_14@0",
            "content": ", we aim to fine-tune a pre-trained language model f (x; \u03b8) : X \u2192 Y in an interactive way: we perform active self-training for T rounds with the total labeling budget b. In each round, we aim to query B = b/T samples denoted as B from X u to fine-tune a pre-trained language model f (x; \u03b8) with both X l , B and X u to maximize the performance on downstream text classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_14",
            "start": 0,
            "end": 383,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_14@1",
            "content": "Here X = X l \u222a X u denotes all samples, and Y = {1, 2, \u2022 \u2022 \u2022 , C} is the label set where C is the number of classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_14",
            "start": 385,
            "end": 500,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_15@0",
            "content": "Overview of ACTUNE Framework",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_15",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_16@0",
            "content": "We now present our active self-training paradigm ACTUNE underpinned by estimated uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_16",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_16@1",
            "content": "We begin the active self-training loop by finetuning a BERT f (\u03b8 (0) ) on the initial labeled data X L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_16",
            "start": 94,
            "end": 197,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_16@2",
            "content": "Formally, we solve the following optimization problem",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_16",
            "start": 199,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_17@0",
            "content": "min \u03b8 1 |X L | (x i ,y i )\u2208X L CE f (x i ; \u03b8 (0) ), y i . (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_17",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@0",
            "content": "In round t (1 \u2264 t \u2264 T ) of active self-training, we first calculate the uncertainty score based on a given function a (t) i = a(x i , \u03b8 (t) ) 1 for all x i \u2208 X u .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@1",
            "content": "Then, we query labeled samples and generate pseudolabels for unlabeled data X u simultaneously to facilitate self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 164,
            "end": 286,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@2",
            "content": "For each sample x i , the pseudo-label y is calculated based on the current 3), ( 4)) until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 288,
            "end": 391,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@3",
            "content": "2. Select sample set Q (t) for AL and S (t) for self-training from Xu based on Eq. ( 11) or (13).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 393,
            "end": 489,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@4",
            "content": "3. Augment the labeled set XL = XL \u222a Q (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 491,
            "end": 532,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@5",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 534,
            "end": 534,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@6",
            "content": "4. Obtain \u03b8 (t) by finetuning f (\u2022; \u03b8 t ) with LST ( Eq. ( 14)) using AdamW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 536,
            "end": 611,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_18@7",
            "content": "5. Update memory bank g(x; \u03b8 t ) with Eq. ( 10) or ( 12).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_18",
            "start": 613,
            "end": 669,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_19@0",
            "content": "Output: The final fine-tuned model f (\u2022; \u03b8 T ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_19",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_20@0",
            "content": "model's output:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_20",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_21@0",
            "content": "y = argmax j\u2208Y f (x; \u03b8 (t) ) j ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_21",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_22@0",
            "content": "where f (x; \u03b8 (t) ) \u2208 R C is a probability simplex and [f (x; \u03b8 (t) )] j is the j-th entry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_22",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_22@1",
            "content": "The procedure of ACTUNE is summarized in Algorithm 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_22",
            "start": 92,
            "end": 144,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_23@0",
            "content": "Region-aware Sampling for Active Learning on High-uncertainty Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_23",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_24@0",
            "content": "After obtaining the uncertainty for unlabeled data, we aim to query annotation for high-uncertainty samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_24",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_24@1",
            "content": "However, directly sampling the most uncertain samples gives suboptimal results as it tends to query repetitive data (Ein-Dor et al., 2020) that represent the overall data distribution poorly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_24",
            "start": 109,
            "end": 299,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_25@0",
            "content": "To tackle this issue, we propose region-aware sampling to capture both uncertainty and diversity during active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_25",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_25@1",
            "content": "Specifically, in the tth round, we first conduct the weighted K-means clustering (Huang et al., 2005), which weights samples based on their uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_25",
            "start": 126,
            "end": 277,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_25@2",
            "content": "Denote by K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_25",
            "start": 279,
            "end": 399,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_25@3",
            "content": "The weighted K-means process first initializes the center of each each cluster \u00b5 i (1 \u2264 i \u2264 K) via K-Means++ (Arthur and Vassilvitskii, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_25",
            "start": 401,
            "end": 542,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_25@4",
            "content": "Then, it jointly updates the centroid of each cluster and assigns each sample to cluster c i as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_25",
            "start": 544,
            "end": 638,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_26@0",
            "content": "c (t) i = argmin k=1,...,K v i \u2212 \u00b5 k 2 ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_26",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_27@0",
            "content": "\u00b5 (t) k = x i \u2208C (t) k a(x i , \u03b8 (t) ) \u2022 v (t) i x\u2208C (t) k a(x i , \u03b8 (t) )(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_27",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_28@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_28",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_29@0",
            "content": "C (t) k = {x (t) i |c (t) i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_29",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_30@0",
            "content": "= k}(k = 1, . . . , K) stands for the k-th cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_30",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_30@1",
            "content": "The above two steps in Eq. ( 3), (4) are repeated until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_30",
            "start": 52,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_30@2",
            "content": "Compared with vanilla K-Means method, the weighting scheme increases the density of the samples with high uncertainty, thus enabling the K-Means methods to discover clusters with high uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_30",
            "start": 121,
            "end": 316,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_30@3",
            "content": "After obtaining K regions with the corresponding data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_30",
            "start": 318,
            "end": 370,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_31@0",
            "content": "C (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_31",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_32@0",
            "content": "k , we calculate the uncertainty of each region as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_32",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_33@0",
            "content": "u (t) k = U (C (t) k ) + \u03b2I(C (t) k )(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_33",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_34@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_34",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_35@0",
            "content": "U (C (t) k ) = 1 |C (t) k | x i \u2208C (t) k a(x i , \u03b8 (t) ),(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_35",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_36@0",
            "content": "is the average uncertainty of samples and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_36",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_37@0",
            "content": "I(C (t) k ) = \u2212 j\u2208C f (t) j log f (t) j (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_37",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_38@0",
            "content": "is the inter-class diversity within cluster k and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_38",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_39@0",
            "content": "f (t) j = i 1{ y i =j} |C (t) k |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_39",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_40@0",
            "content": "is the frequency of class j on cluster k. Notably, the term U (C k ) assigns higher score for clusters with more uncertain samples, and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_40",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_41@0",
            "content": "I(C (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_41",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_42@0",
            "content": "k ) grants higher scores for clusters containing samples with more diverse predicted classes from pseudo labels since such clusters would be closer to the decision boundary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_42",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_43@0",
            "content": "Then, we rank the clusters in an ascending order in u (t) k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_43",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_43@1",
            "content": "A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the member instances to reduce uncertainty and improve the model's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_43",
            "start": 62,
            "end": 244,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_43@2",
            "content": "We adopt a hierarchical sampling strategy: we first select the M clusters with the highest uncertainty, and then sample b = B M data with the highest uncertainty to form the batch",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_43",
            "start": 246,
            "end": 424,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_44@0",
            "content": "Q (t) . 2 K (t) a = top-M k\u2208{1,...,K} u (t) k , Q (t) = k\u2208K (t) a C (t) a,k where C (t) a,k = Top-b x i \u2208C (t) k a(xi, \u03b8 (t) ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_44",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_45@0",
            "content": "(8) We remark that such a hierarchical sampling strategy queries most uncertain samples from different regions, thus the uncertainty and diversity of queried samples can be both achieved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_45",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_46@0",
            "content": "Self-training over Confident Samples",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_46",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_47@0",
            "content": "from Low-uncertainty Regions For self-training, we aim to select unlabeled samples which are most likely to have been correctly classified by the current model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_47",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_47@1",
            "content": "This requires the sample to have low uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_47",
            "start": 161,
            "end": 209,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_47@2",
            "content": "Therefore, we select the top k samples from the M lowest uncertainty regions to form the acquired batch S (t) :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_47",
            "start": 211,
            "end": 321,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_48@0",
            "content": "C (t) s = k\u2208K (t) s C (t) k",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_48",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_49@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_49",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_50@0",
            "content": "K (t) s = bottom-M k\u2208{1,...,K} u (t) k , S (t) = bottom-k x i \u2208C (t) s a(xi, \u03b8 (t) ).(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_50",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_51@0",
            "content": "Momentum-based Memory Bank for Selftraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_51",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_51@1",
            "content": "As PLMs are sensitive to the stochasticity involved in fine-tuning, the model suffers from the instability issue -different weight initialization and data orders may result in different predictions on the same dataset (Dodge et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_51",
            "start": 45,
            "end": 283,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_52@0",
            "content": "Additionally, if the model gives inconsistent predictions in different rounds for a specific sample, then it is potentially uncertain about the sample, and adding it to the training set may harm the active self-training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_52",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_52@1",
            "content": "For example, for a twoclass classification problem, suppose we obtain f (x; \u03b8 (t\u22121) ) = [0.65, 0.35] for sample x the round (t\u22121) and f (x; \u03b8 (t) ) = [0.05, 0.95] for the round t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_52",
            "start": 229,
            "end": 407,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_53@0",
            "content": "Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is actually uncertain to which class x belongs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_53",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@0",
            "content": "To effectively mitigate the noise and stabilize the active self-training process, we maintain a dynamic memory bank to save the results from previous rounds, and use momentum update (He et al., 2020;Laine and Aila, 2016) to aggregate the results from both the previous and current rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@1",
            "content": "Then, during active self-training, we will select samples with the highest aggregated score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 289,
            "end": 380,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@2",
            "content": "In this way, only those samples that the model is certain about over all previous rounds will be selected for self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 382,
            "end": 505,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@3",
            "content": "We design two variants for the memory bank, namely prediction-based and value-based aggregation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 507,
            "end": 602,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@4",
            "content": "Prediction based Momentum Update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 604,
            "end": 636,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_54@5",
            "content": "We adopt an exponential moving average approach to aggregate the prediction g(x; \u03b8 (t) ) on round t as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_54",
            "start": 638,
            "end": 739,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_55@0",
            "content": "g(x; \u03b8 (t) ) = mtf (x; \u03b8 (t) ) + (1 \u2212 mt)g(x; \u03b8 (t\u22121) ), (10) where m t = (1 \u2212 t T )m L + t T m H (0 < m L \u2264 m H \u2264 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_55",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_56@0",
            "content": ") is a momentum coefficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_56",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_56@1",
            "content": "We gradually increase the weight for models on later rounds, since they are trained with more labeled data thus being able to provide more reliable predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_56",
            "start": 29,
            "end": 188,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_56@2",
            "content": "Then, we calculate the uncertainty based on g(x; \u03b8 (t) ) and rewrite Eq. ( 9) and (2) as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_56",
            "start": 190,
            "end": 277,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_57@0",
            "content": "S (t) = bottom-k x i \u2208C (t) s a x i , g(x; \u03b8 (t) ), \u03b8 (t) y = argmax j\u2208Y g(x; \u03b8 (t) ) j ,(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_57",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_58@0",
            "content": "Value-based Momentum Update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_58",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_58@1",
            "content": "For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as 12) Then, we use Eq. ( 12) to sample low-uncertainty data for self-training as 3 S (t) = bottom-k",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_58",
            "start": 29,
            "end": 239,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_59@0",
            "content": "g(x; \u03b8 (t) ) = mta(x; \u03b8 (t) ) + (1 \u2212 mt)g(x; \u03b8 (t\u22121) ). (",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_59",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_60@0",
            "content": "x i \u2208C (t) s g(x i , \u03b8 (t) ), y = argmax j\u2208Y f (x; \u03b8 (t) ) j . (13",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_60",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_61@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_61",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_62@0",
            "content": "By aggregating the prediction results over previous rounds, we filter the sample with inconsistent predictions to suppress noisy labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_62",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_63@0",
            "content": "Model Learning and Update",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_63",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_64@0",
            "content": "After obtaining both the labeled data and pseudolabeled data, we fine-tune a new pre-trained BERT model \u03b8 (t+1) on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_64",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_64@1",
            "content": "Although we only include low-uncertainty samples during self-training, it is difficult to eliminate all the wrong pseudo-labels, and such mislabeled samples can still hurt model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_64",
            "start": 121,
            "end": 310,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_64@2",
            "content": "To suppress such label noise, we use a threshold-based strategy to further remove noisy labels by selecting samples that agree with the corresponding pseudo labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_64",
            "start": 312,
            "end": 475,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_64@3",
            "content": "The loss objective of optimizing \u03b8 (t+1) is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_64",
            "start": 477,
            "end": 519,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_65@0",
            "content": "LST = 1 |L (t) | x i \u2208L (t) CE f (xi; \u03b8 (t+1) ), yi + \u03bb |S (t) | x i \u2208S (t) \u03c9i CE f ( xi; \u03b8 (t+1) ), yi ,(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_65",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_66@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_66",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_67@0",
            "content": "L (t) = X L \u222a Q (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_67",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_68@0",
            "content": "is the labeled set, \u03bb is a hyper-parameter balancing the weight between clean and pseudo labels, and 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_68",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_69@0",
            "content": "\u03c9 i = 1{ f (x i ; \u03b8 (t+1) ) y i > \u03b3}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_69",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@0",
            "content": "Active Learning Setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@1",
            "content": "Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 24,
            "end": 192,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@2",
            "content": "In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 194,
            "end": 294,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@3",
            "content": "Since large development sets are impractical in low-resource settings (Kann et al., 2019), we keep the size of development set as 1000, which is the same as the labeling budget 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 296,
            "end": 475,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@4",
            "content": "For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 477,
            "end": 627,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@5",
            "content": "Implementation Details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 629,
            "end": 651,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@6",
            "content": "We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 653,
            "end": 914,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_70@7",
            "content": "In each round, we train from scratch to avoid overfitting the data collected in earlier rounds as observed by Hu et al. (2019) Note that when compared with active learning baselines, we do not augment the train set with pseudolabeled data (Eq. ( 9)) to ensure fair comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_70",
            "start": 916,
            "end": 1192,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_71@0",
            "content": "Main Result",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_71",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_72@0",
            "content": "Figure 1 reports the performance of ACTUNE and the baselines on 4 benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_72",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_72@1",
            "content": "From the results, we have the following observations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_72",
            "start": 78,
            "end": 130,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_73@0",
            "content": "\u2022 ACTUNE consistently outperforms baselines in most of the cases. Different from studies in the computer vision (CV) domain (Sim\u00e9oni et al., 2020) where the model does not perform well in the low-data regime, pre-trained LM has achieved competitive performance with only a few labeled data, which makes further improvements to the vanilla fine-tuning challenging. Nevertheless, AC-TUNE surpasses baselines in more than 90% of the rounds and achieves 0.4%-0.7% and 0.3%-1.5% absolute gain at the end of AL and SSAL respectively. Figure 3 quantitatively measures the number of labels needed for the most advanced active learning model and self-training model (UST) to outperform ACTUNE with 1000 labels. These baselines need >2000 clean labeled samples to reach the performance as ours. ACTUNE saves on average 56.2% and 57.0% of the labeled samples than most advanced active learning and selftraining baselines respectively, which justifies its promising performance under low-resource scenarios. Such improvements show the merits of two key designs under our active self-training framework: the region-aware sampling for active learning and the momentum-based memory bank for robust selftraining, which will be discussed in the section 3.5. \u2022 Compared with the previous AL baselines, AC-TUNE can bring consistent performance gain, while previous semi-supervised active learning methods cannot. For instance, BASS is based on BALD for active learning, but sometimes it performs even worse than BALD with the same number of labeled data (see Fig. 1(b) and Fig. 1(f)). This is mainly because previous methods simply combine noisy pseudo labels with clean labels for training without explicitly rectifying the wrongly-labeled data, which will cause the LM to overfit these hazardous labels. Moreover, previous methods do not exploit momentum updates to stabilize the learning process, as there are oscillations in the beginning rounds. In contrast, ACTUNE achieves a more stable learning process and enables an active selftraining process to benefit from more labeled data. \u2022 The self-training methods (ST & UST) achieve superior performance with limited labels. However, they mainly focus on leveraging unlabeled data for improving the performance, while our results demonstrate that adaptive selecting the most useful data for fine-tuning is also important for improving the performance. With a powerful querying policy, ACTUNE can improve these self-training baselines by 1.05% in terms of accuracy on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_73",
            "start": 0,
            "end": 2508,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_74@0",
            "content": "Weakly-supervised Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_74",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@0",
            "content": "ACTUNE can be naturally used for weaklysupervised classification, where X l is a set of noisy labels derived from linguistic patterns or rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@1",
            "content": "Since the initial label set is noisy, the model trained with Eq. ( 1) can overfit the label noise (Zhang et al., 2022a), and we can actively query labeled data to refine the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 144,
            "end": 323,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@2",
            "content": "We conduct experiments on the TREC and Chemprot dataset 5 , where we first use Snorkel (Ratner et al., 2017) to obtain weak label set X l , then fine-tune the pre-trained LM f (\u03b8 (0) ) on X l .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 325,
            "end": 517,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@3",
            "content": "After that, we adopt ACTUNE for active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 519,
            "end": 571,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@4",
            "content": "Fig. 2 shows the results of these two datasets 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 573,
            "end": 622,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@5",
            "content": "When combining ACTUNE with CAL, the performance is unsatisfactory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 624,
            "end": 689,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@6",
            "content": "We believe it is because CAL requires clean labels to calculate uncertainties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 691,
            "end": 768,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@7",
            "content": "When labels are inaccurate, it will prevent AC-TUNE from querying informative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 770,
            "end": 855,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@8",
            "content": "In contrast, ACTUNE achieves the best performance over baselines when using Entropy as the uncertainty measure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 857,
            "end": 967,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@9",
            "content": "The performance gain is more notable on the TREC dataset, where we achieve 96.68% accuracy, close to the fully supervised performance (96.80%) with only \u223c6% of clean labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 969,
            "end": 1141,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@10",
            "content": "(e.g. BADGE), when using the entropy as an uncertainty measure to select pseudo-labeled data for self-training, ACTUNE can further boost the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 1143,
            "end": 1295,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_75@11",
            "content": "This indicates that ACTUNE is a general active self-training approach, as it can serve as an efficient plug-in module for existing AL methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_75",
            "start": 1297,
            "end": 1438,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_76@0",
            "content": "Combination with Other AL Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_76",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_77@0",
            "content": "Ablation and Hyperparameter Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_77",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_78@0",
            "content": "The Effect of Different Components in AC-TUNE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_78",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_78@1",
            "content": "We inspect different components of ACTUNE, including the region-sampling (RS), momentum-based memory bank (MMB), and weighted clustering (WClus) 7 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_78",
            "start": 47,
            "end": 194,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_78@2",
            "content": "Experimental results (Fig. 5(b)) shows that all the three components contribute to the final performance, as removing any of them hurts the classification accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_78",
            "start": 196,
            "end": 359,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_78@3",
            "content": "Also, we find that when removing MMB, the performance hurts most in the beginning rounds, which indicates that MMB effectively suppresses label noise when the model's capacity is weak.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_78",
            "start": 361,
            "end": 544,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_78@4",
            "content": "Conversely, removing WClus hurts the performance on later rounds, as it enables the model to select most informative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_78",
            "start": 546,
            "end": 670,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_79@0",
            "content": "Hyperparameter Study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_79",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_79@1",
            "content": "We study two hyperparameters, namely \u03b2 and K used in querying labels. Figure 4(a) and 4(b) show the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_79",
            "start": 22,
            "end": 129,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_79@2",
            "content": "In general, the model is insensitive to \u03b2 as the performance difference is less than 0.6%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_79",
            "start": 131,
            "end": 220,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_79@3",
            "content": "The model cannot perform well with smaller K since it cannot pinpoint to high-uncertainty regions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_79",
            "start": 222,
            "end": 319,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_79@4",
            "content": "For larger K, the performance also drops as some of the high-uncertainty regions can be outliers and sampling from them would hurt the model performance (Karamcheti et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_79",
            "start": 321,
            "end": 499,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@0",
            "content": "A Closer Look at the Momentum-based Memory Bank. To examine the role of MMB, we show the overall accuracy of pseudo-labels on AG News dataset in Fig. 4(c).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@1",
            "content": "From the result, it is clear that the momentum-based memory bank can stabilize the active self-training process, as the accuracy of pseudo labels increases around 1%, especially in Value,mL=0.7,mH=0.9 Value,mL=0.9,mH=0.9 Prob,m=0.7,mH=0.9 Prob,m=0.9,mH=0.9 No Momentum",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 156,
            "end": 423,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@2",
            "content": "(d) Entropy ,mL=0.7,mH=0.9 Value,mL=0.9,mH=0.9 Prob,m=0.7,mH=0.9 Prob,m=0.9,mH=0.9 No Momentum",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 425,
            "end": 518,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@3",
            "content": "(e) CAL Overall, we find that our model is robust to different choices as ACTUNE outperform the baseline without momentum update consistently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 520,
            "end": 661,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@4",
            "content": "Moreover, we find that the larger m H will generally lead to better performance in later rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 663,
            "end": 758,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@5",
            "content": "This is mainly because in later rounds, the model's prediction is more reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 760,
            "end": 839,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@6",
            "content": "Conversely, at the beginning of the training, the model's prediction might be oscillating on unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 841,
            "end": 948,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_80@7",
            "content": "In this case, using a smaller m L will favor samples with consistent predictions to improve the robustness of active self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_80",
            "start": 950,
            "end": 1080,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_81@0",
            "content": "Another finding is that for different AL methods, the optimal memory bank can be different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_81",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_81@1",
            "content": "For Entropy, probability-based memory bank leads to a better result, while for CAL, simple aggregating over uncertainty score achieves better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_81",
            "start": 92,
            "end": 245,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_81@2",
            "content": "This is mainly because the method used in CAL is more complicated, and using probability-based memory bank may hurt the uncertainty calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_81",
            "start": 247,
            "end": 390,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_82@0",
            "content": "Case Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_82",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_83@0",
            "content": "We give an example of our querying strategy on AG News dataset for the 1st round of active selftraining process in figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_83",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_83@1",
            "content": "Note that we use t-SNE algorithm (Van der Maaten and Hinton, 2008) for dimension reduction, and the black triangle stands for the queried samples while other circles stands for the unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_83",
            "start": 125,
            "end": 320,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_83@2",
            "content": "We can see that the existing uncertainty-based methods such as Entropy and CAL, are suffered from the issue of limited diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_83",
            "start": 322,
            "end": 450,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_83@3",
            "content": "However, when combined with ACTUNE, the diversity is much improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_83",
            "start": 452,
            "end": 518,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_83@4",
            "content": "Such results, compared with the main results in figure 1, demonstrate the efficacy of ACTUNE empirically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_83",
            "start": 520,
            "end": 624,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_84@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_84",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@0",
            "content": "Active Learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@1",
            "content": "Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 17,
            "end": 144,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@2",
            "content": "So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversity-based methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 146,
            "end": 383,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@3",
            "content": "Ein-Dor et al. ( 2020) offer an empirical study of active learning with PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 385,
            "end": 461,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@4",
            "content": "Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 463,
            "end": 638,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@5",
            "content": "In our study, we leverage the power of unlabeled instances via self-training to further promote the performance of AL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 640,
            "end": 757,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@6",
            "content": "Semi-supervised Active Learning (SSAL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 759,
            "end": 797,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@7",
            "content": "2020) exploit the most-certain samples from the unlabeled with pseudo-labeling to augment the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 799,
            "end": 905,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@8",
            "content": "So far, most of the SSAL approaches are designed for CV domain and it remains unknown how this paradigm performs with PLMs on NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 907,
            "end": 1042,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@9",
            "content": "In contrast, we propose ACTUNE to effectively leverage unlabeled data during finetuing PLMs for NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 1044,
            "end": 1149,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@10",
            "content": "Self-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 1151,
            "end": 1164,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@11",
            "content": "Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 1166,
            "end": 1266,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@12",
            "content": "It first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 1268,
            "end": 1412,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_85@13",
            "content": "However, it is known to be vulnerable to error propagation (Arazo et al., 2020;Rizve et al., 2021;. To alleviate this, we adopt a simple momentum-based method to select high confidence samples, effectively reducing 1429",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_85",
            "start": 1414,
            "end": 1632,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_86@0",
            "content": "Conclusion and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_86",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_87@0",
            "content": "In this paper, we develop ACTUNE, a general active self-training framework for enhancing both label efficiency and model performance in fine-tuning pre-trained language models (PLMs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_87",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_87@1",
            "content": "We propose a region-aware sampling approach to guarantee both the uncertainty the diversity for querying labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_87",
            "start": 184,
            "end": 295,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_87@2",
            "content": "To combat the label noise propagation issue, we design a momentum-based memory bank to effectively utilize the model predictions for preceding AL rounds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_87",
            "start": 297,
            "end": 449,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_87@3",
            "content": "Empirical results on 6 public text classification benchmarks suggest the superiority of ACTUNE to conventional active learning and semi-supervised active learning methods for fine-tuning PLMs with limited resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_87",
            "start": 451,
            "end": 665,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@0",
            "content": "There are several directions to improve ACTUNE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@1",
            "content": "First, since our focus is on fine-tuning pre-trained language models, we use the representation of [CLS] token for classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 48,
            "end": 177,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@2",
            "content": "In the future work, we can consider using prompt tuning (Gao et al., 2021;Schick and Sch\u00fctze, 2021), a more dataefficient method for adopting pre-trained language models on classification tasks to further promote the efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 179,
            "end": 406,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@3",
            "content": "Also, due to the computational resource constraints, we do not use larger pre-trained language models such as RoBERTa-large (Liu et al., 2019) which shown even better performance with only a few labels (Du et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 408,
            "end": 627,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@4",
            "content": "Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 629,
            "end": 766,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@5",
            "content": "Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference (NLI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 768,
            "end": 922,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_88@6",
            "content": "(Holub et al., 2008) 461s 646s BALD (Gal et al., 2017) 4595s 6451s ALPS (Yuan et al., 2020) 488s 677s BADGE (Ash et al., 2020) 554s 1140s CAL (Margatina et al., 2021b) 493s 688s REVIVAL (Guo et al., based aggregation by default.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_88",
            "start": 924,
            "end": 1151,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_89@0",
            "content": "Table 3 shows the time in one active learning round of ACTUNE and baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_89",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_89@1",
            "content": "Here we highlight that the additional time for region-aware sampling and momentum-based memory bank is rather small compared with the inference time. Also, we find that BALD and REVIVAL are not so efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_89",
            "start": 77,
            "end": 283,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_89@2",
            "content": "For BALD, it needs to infer the uncertainty of the model by passing the data to model with multitple times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_89",
            "start": 285,
            "end": 391,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_89@3",
            "content": "Such an operation will make the total inference time for PLMs very long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_89",
            "start": 393,
            "end": 464,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_89@4",
            "content": "For REVIVAL, we find that calculating the adversarial gradient needs extra forward passes and backward passes, which could be time-consuming for PLMs with millions of parameters 8 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_89",
            "start": 466,
            "end": 646,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_90@0",
            "content": "Eric Arazo, Diego Ortego, Paul Albert, E O' Noel, Kevin Connor,  Mcguinness, Pseudolabeling and confirmation bias in deep semisupervised learning, 2020, 2020 International Joint Conference on Neural Networks (IJCNN), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_90",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_91@0",
            "content": "David Arthur, Sergei Vassilvitskii, K-means++: The advantages of careful seeding, 2007, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_91",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_92@0",
            "content": "Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal, Deep batch active learning by diverse, uncertain gradient lower bounds, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_92",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_93@0",
            "content": "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew Mccallum, Self-supervised meta-learning for few-shot natural language classification tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_93",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_94@0",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_94",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_95@0",
            "content": "UNKNOWN, None, 2020, Interactive weak supervision: Learning useful heuristics for data labeling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_95",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_96@0",
            "content": "Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy, Flex: Unifying evaluation for few-shot nlp, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_96",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_97@0",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Language models are fewshot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_97",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_98@0",
            "content": "Franck Dernoncourt, Ji Lee, PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_98",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_99@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_99",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_100@0",
            "content": "UNKNOWN, None, 2002, 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_100",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_101@0",
            "content": "Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart, The hitchhiker's guide to testing statistical significance in natural language processing, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_101",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_102@0",
            "content": "Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Veselin Stoyanov, Alexis Conneau, Self-training improves pre-training for natural language understanding, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_102",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_103@0",
            "content": "Alon Liat Ein-Dor, Ariel Halfon, Eyal Gera, Lena Shnarch, Leshem Dankin, Marina Choshen, Ranit Danilevsky, Yoav Aharonov, Noam Katz,  Slonim, Active Learning for BERT: An Empirical Study, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_103",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2015, Bayesian convolutional neural networks with bernoulli approximate variational inference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_104",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_105@0",
            "content": "Yarin Gal, Riashat Islam, Zoubin Ghahramani, Deep bayesian active learning with image data, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_105",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_106@0",
            "content": "Mingfei Gao, Zizhao Zhang, Guo Yu, \u00d6 Sercan,  Ar\u0131k, S Larry, Tomas Davis,  Pfister, Consistency-based semi-supervised active learning: Towards minimizing labeling cost, 2020, European Conference on Computer Vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_106",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_107@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_107",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_108@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_108",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_109@0",
            "content": "Fei Sun, Yueting Wu,  Zhuang, Semisupervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_109",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_110@0",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_110",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_111@0",
            "content": "Alex Holub, Pietro Perona, Michael C Burl, Entropy-based active learning for object recognition, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_111",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2022, Nemo: Guiding and contextualizing weak supervision for interactive data programming, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_112",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_113@0",
            "content": "Peiyun Hu, Zack Lipton, Anima Anandkumar, Deva Ramanan, Active learning with partial feedback, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_113",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2005, Automated variable weighting in k-means type clustering. IEEE transactions on pattern analysis and machine intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_114",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_115@0",
            "content": "Zhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang, Changlong Sun, Qiong Zhang, Xiaozhong Liu, Camouflaged Chinese spam content detection with semi-supervised generative active learning, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_115",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_116@0",
            "content": "Katharina Kann, Kyunghyun Cho, Samuel , Towards realistic practices in lowresource natural language processing: The development set, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_116",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_117@0",
            "content": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher Manning, Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_117",
            "start": 0,
            "end": 368,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_118@0",
            "content": "Lingkai Kong, Jimeng Sun, Chao Zhang, Sde-net: Equipping deep neural networks with uncertainty estimates, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_118",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_119@0",
            "content": "Martin Krallinger, Obdulia Rabal, A Saber,  Akhondi, Overview of the biocreative VI chemical-protein interaction track, 2017, BioCreative evaluation Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_119",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_120@0",
            "content": "UNKNOWN, None, 2016, Temporal ensembling for semi-supervised learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_120",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_121@0",
            "content": "Dong-Hyun Lee, Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks, 2013, ICML Workshop on challenges in representation learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_121",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_122@0",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, The 19th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_122",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_123@0",
            "content": "Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, Chao Zhang, Bond: Bert-assisted open-domain named entity recognition with distant supervision, 2020, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_123",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_124",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_125@0",
            "content": "UNKNOWN, None, , Loic Barrault, and Nikolaos Aletras. 2021a. Bayesian active learning with pretrained language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_125",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_126@0",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_126",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_127@0",
            "content": "Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_127",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_128@0",
            "content": "Subhabrata Mukherjee, Ahmed Awadallah, Uncertainty-aware self-training for few-shot text classification, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_128",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_129@0",
            "content": "Alexander Ratner, H Stephen, Henry Bach, Jason Ehrenberg, Sen Fries, Christopher Wu,  R\u00e9, Snorkel: Rapid training data creation with weak supervision, 2017, Proceedings of the VLDB Endowment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_129",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_130@0",
            "content": "Kevin Mamshad Nayeem Rizve,  Duarte, S Yogesh, Mubarak Rawat,  Shah, In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_130",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_131@0",
            "content": "Matthias Rottmann, Karsten Kahl, Hanno Gottschalk, Deep bayesian active semisupervised learning, 2018, 17th IEEE International Conference on Machine Learning and Applications (ICMLA), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_131",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_132@0",
            "content": "Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan Zhang, Yong Yu, Lei Li, Active sentence learning by adversarial uncertainty sampling in discrete space, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_132",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_133@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_133",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_134@0",
            "content": "Ozan Sener, Silvio Savarese, Active learning for convolutional neural networks: A core-set approach, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_134",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_135@0",
            "content": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, V Dmitry, Alexander Dylov,  Panchenko, Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_135",
            "start": 0,
            "end": 405,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_136@0",
            "content": "Oriane Sim\u00e9oni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier, Rethinking deep active learning: Using unlabeled data at model training, 2020, the 25th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_136",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_137@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_137",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_138@0",
            "content": "Katrin Tomanek, Udo Hahn, Semisupervised active learning for sequence labeling, 2009, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_138",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_139@0",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008, Journal of machine learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_139",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_140@0",
            "content": "Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin, Cost-effective active learning for deep image classification, 2016, IEEE Transactions on Circuits and Systems for Video Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_140",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_141@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Others. 2020. Transformers: State-of-theart natural language processing, , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_141",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_142@0",
            "content": "Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, Quoc Le, Unsupervised data augmentation for consistency training, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_142",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_143@0",
            "content": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang, Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_143",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_144@0",
            "content": "Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber, Cold-start active learning through self-supervised language modeling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_144",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_145@0",
            "content": "UNKNOWN, None, , 2022a. A survey on programmatic weak supervision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_145",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_146@0",
            "content": "Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, Alexander Ratner, WRENCH: A comprehensive benchmark for weak supervision, 2021, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_146",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_147@0",
            "content": "UNKNOWN, None, 2022, Prboost: Promptbased rule discovery and boosting for interactive weakly-supervised learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_147",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_148@0",
            "content": "Rongzhi Zhang, Yue Yu, Chao Zhang, Se-qMix: Augmenting active sequence labeling via sequence mixup, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_148",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_149@0",
            "content": "UNKNOWN, None, 2020, Revisiting few-sample bert fine-tuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_149",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_150@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_150",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "129-ARR_v2_151@0",
            "content": "UNKNOWN, None, 2021, Self-training with differentiable teacher, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "129-ARR_v2_151",
            "start": 0,
            "end": 64,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_1",
            "tgt_ix": "129-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_1",
            "tgt_ix": "129-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_6",
            "tgt_ix": "129-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_9",
            "tgt_ix": "129-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_10",
            "tgt_ix": "129-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_12",
            "tgt_ix": "129-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_13",
            "tgt_ix": "129-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_11",
            "tgt_ix": "129-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_11",
            "tgt_ix": "129-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_11",
            "tgt_ix": "129-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_11",
            "tgt_ix": "129-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_14",
            "tgt_ix": "129-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_16",
            "tgt_ix": "129-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_17",
            "tgt_ix": "129-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_19",
            "tgt_ix": "129-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_20",
            "tgt_ix": "129-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_21",
            "tgt_ix": "129-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_22",
            "tgt_ix": "129-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_24",
            "tgt_ix": "129-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_26",
            "tgt_ix": "129-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_27",
            "tgt_ix": "129-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_28",
            "tgt_ix": "129-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_29",
            "tgt_ix": "129-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_30",
            "tgt_ix": "129-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_31",
            "tgt_ix": "129-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_32",
            "tgt_ix": "129-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_33",
            "tgt_ix": "129-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_34",
            "tgt_ix": "129-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_35",
            "tgt_ix": "129-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_36",
            "tgt_ix": "129-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_37",
            "tgt_ix": "129-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_38",
            "tgt_ix": "129-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_40",
            "tgt_ix": "129-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_41",
            "tgt_ix": "129-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_42",
            "tgt_ix": "129-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_43",
            "tgt_ix": "129-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_44",
            "tgt_ix": "129-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_45",
            "tgt_ix": "129-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_47",
            "tgt_ix": "129-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_48",
            "tgt_ix": "129-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_49",
            "tgt_ix": "129-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_50",
            "tgt_ix": "129-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_51",
            "tgt_ix": "129-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_52",
            "tgt_ix": "129-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_53",
            "tgt_ix": "129-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_55",
            "tgt_ix": "129-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_56",
            "tgt_ix": "129-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_57",
            "tgt_ix": "129-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_58",
            "tgt_ix": "129-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_59",
            "tgt_ix": "129-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_60",
            "tgt_ix": "129-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_61",
            "tgt_ix": "129-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_62",
            "tgt_ix": "129-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_64",
            "tgt_ix": "129-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_65",
            "tgt_ix": "129-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_66",
            "tgt_ix": "129-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_67",
            "tgt_ix": "129-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_68",
            "tgt_ix": "129-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_69",
            "tgt_ix": "129-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_72",
            "tgt_ix": "129-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_71",
            "tgt_ix": "129-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_71",
            "tgt_ix": "129-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_71",
            "tgt_ix": "129-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_74",
            "tgt_ix": "129-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_74",
            "tgt_ix": "129-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_76",
            "tgt_ix": "129-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_81",
            "tgt_ix": "129-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_82",
            "tgt_ix": "129-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_82",
            "tgt_ix": "129-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_84",
            "tgt_ix": "129-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_84",
            "tgt_ix": "129-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_87",
            "tgt_ix": "129-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_86",
            "tgt_ix": "129-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_86",
            "tgt_ix": "129-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_86",
            "tgt_ix": "129-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_86",
            "tgt_ix": "129-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "129-ARR_v2_0",
            "tgt_ix": "129-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_1",
            "tgt_ix": "129-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_2",
            "tgt_ix": "129-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_3",
            "tgt_ix": "129-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_4",
            "tgt_ix": "129-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_5",
            "tgt_ix": "129-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_6",
            "tgt_ix": "129-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_6",
            "tgt_ix": "129-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_6",
            "tgt_ix": "129-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_7",
            "tgt_ix": "129-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_8",
            "tgt_ix": "129-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_9",
            "tgt_ix": "129-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_10",
            "tgt_ix": "129-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_11",
            "tgt_ix": "129-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_12",
            "tgt_ix": "129-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_13",
            "tgt_ix": "129-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_14",
            "tgt_ix": "129-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_14",
            "tgt_ix": "129-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_15",
            "tgt_ix": "129-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_16",
            "tgt_ix": "129-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_16",
            "tgt_ix": "129-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_16",
            "tgt_ix": "129-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_17",
            "tgt_ix": "129-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_18",
            "tgt_ix": "129-ARR_v2_18@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_19",
            "tgt_ix": "129-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_20",
            "tgt_ix": "129-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_21",
            "tgt_ix": "129-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_22",
            "tgt_ix": "129-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_22",
            "tgt_ix": "129-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_23",
            "tgt_ix": "129-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_24",
            "tgt_ix": "129-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_24",
            "tgt_ix": "129-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_25",
            "tgt_ix": "129-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_26",
            "tgt_ix": "129-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_27",
            "tgt_ix": "129-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_28",
            "tgt_ix": "129-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_29",
            "tgt_ix": "129-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_30",
            "tgt_ix": "129-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_30",
            "tgt_ix": "129-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_30",
            "tgt_ix": "129-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_30",
            "tgt_ix": "129-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_31",
            "tgt_ix": "129-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_32",
            "tgt_ix": "129-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_33",
            "tgt_ix": "129-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_34",
            "tgt_ix": "129-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_35",
            "tgt_ix": "129-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_36",
            "tgt_ix": "129-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_37",
            "tgt_ix": "129-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_38",
            "tgt_ix": "129-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_39",
            "tgt_ix": "129-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_40",
            "tgt_ix": "129-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_41",
            "tgt_ix": "129-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_42",
            "tgt_ix": "129-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_43",
            "tgt_ix": "129-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_43",
            "tgt_ix": "129-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_43",
            "tgt_ix": "129-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_44",
            "tgt_ix": "129-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_45",
            "tgt_ix": "129-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_46",
            "tgt_ix": "129-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_47",
            "tgt_ix": "129-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_47",
            "tgt_ix": "129-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_47",
            "tgt_ix": "129-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_48",
            "tgt_ix": "129-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_49",
            "tgt_ix": "129-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_50",
            "tgt_ix": "129-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_51",
            "tgt_ix": "129-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_51",
            "tgt_ix": "129-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_52",
            "tgt_ix": "129-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_52",
            "tgt_ix": "129-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_53",
            "tgt_ix": "129-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_54",
            "tgt_ix": "129-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_55",
            "tgt_ix": "129-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_56",
            "tgt_ix": "129-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_56",
            "tgt_ix": "129-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_56",
            "tgt_ix": "129-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_57",
            "tgt_ix": "129-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_58",
            "tgt_ix": "129-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_58",
            "tgt_ix": "129-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_59",
            "tgt_ix": "129-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_60",
            "tgt_ix": "129-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_61",
            "tgt_ix": "129-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_62",
            "tgt_ix": "129-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_63",
            "tgt_ix": "129-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_64",
            "tgt_ix": "129-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_64",
            "tgt_ix": "129-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_64",
            "tgt_ix": "129-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_64",
            "tgt_ix": "129-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_65",
            "tgt_ix": "129-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_66",
            "tgt_ix": "129-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_67",
            "tgt_ix": "129-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_68",
            "tgt_ix": "129-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_69",
            "tgt_ix": "129-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_70",
            "tgt_ix": "129-ARR_v2_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_71",
            "tgt_ix": "129-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_72",
            "tgt_ix": "129-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_72",
            "tgt_ix": "129-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_73",
            "tgt_ix": "129-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_74",
            "tgt_ix": "129-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_75",
            "tgt_ix": "129-ARR_v2_75@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_76",
            "tgt_ix": "129-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_77",
            "tgt_ix": "129-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_78",
            "tgt_ix": "129-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_79",
            "tgt_ix": "129-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_80",
            "tgt_ix": "129-ARR_v2_80@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_81",
            "tgt_ix": "129-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_81",
            "tgt_ix": "129-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_81",
            "tgt_ix": "129-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_82",
            "tgt_ix": "129-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_83",
            "tgt_ix": "129-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_84",
            "tgt_ix": "129-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_85",
            "tgt_ix": "129-ARR_v2_85@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_86",
            "tgt_ix": "129-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_87",
            "tgt_ix": "129-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_87",
            "tgt_ix": "129-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_87",
            "tgt_ix": "129-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_87",
            "tgt_ix": "129-ARR_v2_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_88",
            "tgt_ix": "129-ARR_v2_88@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_89",
            "tgt_ix": "129-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_89",
            "tgt_ix": "129-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_89",
            "tgt_ix": "129-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_89",
            "tgt_ix": "129-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_89",
            "tgt_ix": "129-ARR_v2_89@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_90",
            "tgt_ix": "129-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_91",
            "tgt_ix": "129-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_92",
            "tgt_ix": "129-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_93",
            "tgt_ix": "129-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_94",
            "tgt_ix": "129-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_95",
            "tgt_ix": "129-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_96",
            "tgt_ix": "129-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_97",
            "tgt_ix": "129-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_98",
            "tgt_ix": "129-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_99",
            "tgt_ix": "129-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_100",
            "tgt_ix": "129-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_101",
            "tgt_ix": "129-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_102",
            "tgt_ix": "129-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_103",
            "tgt_ix": "129-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_104",
            "tgt_ix": "129-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_105",
            "tgt_ix": "129-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_106",
            "tgt_ix": "129-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_107",
            "tgt_ix": "129-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_108",
            "tgt_ix": "129-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_109",
            "tgt_ix": "129-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_110",
            "tgt_ix": "129-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_111",
            "tgt_ix": "129-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_112",
            "tgt_ix": "129-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_113",
            "tgt_ix": "129-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_114",
            "tgt_ix": "129-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_115",
            "tgt_ix": "129-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_116",
            "tgt_ix": "129-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_117",
            "tgt_ix": "129-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_118",
            "tgt_ix": "129-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_119",
            "tgt_ix": "129-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_120",
            "tgt_ix": "129-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_121",
            "tgt_ix": "129-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_122",
            "tgt_ix": "129-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_123",
            "tgt_ix": "129-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_124",
            "tgt_ix": "129-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_125",
            "tgt_ix": "129-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_126",
            "tgt_ix": "129-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_127",
            "tgt_ix": "129-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_128",
            "tgt_ix": "129-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_129",
            "tgt_ix": "129-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_130",
            "tgt_ix": "129-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_131",
            "tgt_ix": "129-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_132",
            "tgt_ix": "129-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_133",
            "tgt_ix": "129-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_134",
            "tgt_ix": "129-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_135",
            "tgt_ix": "129-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_136",
            "tgt_ix": "129-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_137",
            "tgt_ix": "129-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_138",
            "tgt_ix": "129-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_139",
            "tgt_ix": "129-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_140",
            "tgt_ix": "129-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_141",
            "tgt_ix": "129-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_142",
            "tgt_ix": "129-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_143",
            "tgt_ix": "129-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_144",
            "tgt_ix": "129-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_145",
            "tgt_ix": "129-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_146",
            "tgt_ix": "129-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_147",
            "tgt_ix": "129-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_148",
            "tgt_ix": "129-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_149",
            "tgt_ix": "129-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_150",
            "tgt_ix": "129-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "129-ARR_v2_151",
            "tgt_ix": "129-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 909,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "129-ARR",
        "version": 2
    }
}