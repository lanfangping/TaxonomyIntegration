{
    "nodes": [
        {
            "ix": "465-ARR_v2_0",
            "content": "KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_2",
            "content": "Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic Knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "465-ARR_v2_4",
            "content": "The PubMed 1 database is a resource that provides access to the MEDLINE bibliographic database of references and abstracts together with the full text articles of some of these citations which are available in the PubMed Central 2 (PMC) repository. MEDLINE 3 contains more than 28 million references (as of Feb. 2021) to journal articles in the biomedical, health, and related disciplines. Journal articles in MEDLINE are indexed according to Medical Subject Headings (MeSH) 4 , an hierarchically organized vocabulary that has been developed and maintained by the National Library of Medicine (NLM) 5 . Currently, there are 29,369 main MeSH headings, and each MEDLINE citation has 13 MeSH indices, on average. MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008;Huang et al., 2011;Gu et al., 2013), being recognized as important tools for research (e.g., knowledge discovery and hypothesis generation).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_5",
            "content": "Currently, MeSH indexing is done by human annotators who examine full articles and assign MeSH terms to each article according to rules set by NLM 6 . Human annotation is time consuming and costly -the average cost of annotating one article in MEDLINE is about $9.40 . Nearly 1 million citations were added to MEDLINE in 2020 (approximately 2,600 on a daily basis) 7 . The rate of articles being added to the MEDLINE database is constantly increasing, so there is a huge financial and time-consuming cost for the status quo. Therefore, it is imperative to develop an automatic annotation system that can assist MeSH indexing of large-scale biomedical articles efficiently and accurately.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_6",
            "content": "Automatic MeSH indexing can be regarded as an extreme multi-label text classification (XMC) problem, where each article can be labeled with multiple MeSH terms. Compared with standard multilabel problems, XMC finds relevant labels from an enormous set of candidate labels. The challenge of large-scale MeSH indexing comes from both the label and article sides. Currently, there are more than 29,000 distinct MeSH terms, and new MeSH terms are updated to the vocabulary every year. The frequency of different MeSH terms appearing in documents are quite imbalanced. For instance, the most frequent MeSH term, 'humans', appears in more than 8 million citations; 'Pandanaceae', on the other hand, appears in only 31 documents . In addition, the MeSH terms that have been assigned to each article varies greatly, ranging from more than 30 to fewer than 5. Furthermore, semantic features of the biomedical literature are complicated to capture, as they contain many domain-specific concepts, phrases, and abbreviations. The aforementioned difficulties make the task more complicated to generate an effective and efficient prediction model for MeSH indexing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_7",
            "content": "In this work, inspired by the rapid development of deep learning, we propose a novel neural architecture called KenMeSH (Knowledge-enhanced MeSH labelling) which is suitable for handling XMC problems where the labels are arrayed hierarchically and could capture useful information as a directed graph. Our method uses a dynamic knowledge-enhanced mask attention mechanism and incorporates document features together with label features to index biomedical articles. Our major contributions are:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_8",
            "content": "1. We design a multi-channel document representation module to extract document features from the title and the abstract using a bidirectional LSTM. We use multi-level dilated convolution to capture semantic units in the abstract channel. This module combines a hybrid of information, at the levels of words and the latent representations of the semantic units, to capture local correlations and longterm dependencies from text. 2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate information from the complete MeSH hierarchy to map label representations. 3. We propose a novel dynamic knowledgeenhanced mask attention mechanism which incorporates external journal-MeSH cooccurrence information and document similarity in the PubMed database to constrain the large universe of possible labels in the MeSH indexing task. 4. We evaluate our model on a corpus of PMC articles. Our proposed method consistently achieves superior performance over previous approaches on a number of measures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_9",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_10",
            "content": "Automatic MeSH Indexing",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "465-ARR_v2_11",
            "content": "To address the MeSH indexing task mentioned in above section, the National Library of Medicine developed Medical Text Indexer (MTI) -software that automatically recommends MeSH terms to each MEDLINE article using the abstract and title as input (Aronson et al., 2004). It first generates the candidate MeSH terms for given articles, and then ranks the candidates to provide the final predictions. There are two modules in MTI -MetaMap Indexing (MMI) and PubMed-Related Citations (PRC) (Lin and Wilbur, 2007;Aronson and Lang, 2010). , DeepMeSH (Peng et al., 2016), AttentionMeSH (Jin et al., 2018), and MeSHProbeNet (Xun et al., 2019). MeSHLabeler introduced a Learning-to-Rank (LTR) framework, which is a two-step strategy, first predicting the candidate MeSH terms and then ranking them to obtain the final suggestions. MeSHLabeler first trained an independent binary classifier for each MeSH term and then used various evidence, including similar publications and term frequencies, to rank candidate MeSH terms. DeepMeSH is an improved version of MeSHLabeler, which also uses the LTR strategy. It first generates MeSH predictions by incorporating deep semantics in the word embedding space, and then ranks the candidates. AttentionMeSH and MeSH-ProbeNet are based on bidirectional recurrent neural networks (RNNs) and attention mechanisms. The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of information from the input articles.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_12",
            "content": "Studies in MeSH indexing with full texts are very limited because of restrictions on full text ac-cess. randomly selected 1413 articles from the PMC Open Access Subset and used automatically-generated summaries from these full texts as input to MTI for MeSH indexing. Demner-Fushman and Mork (2015) collected 14,828 full text articles from PMC Open Access Subset and developed a rule-based string-matching algorithm to extract a subject of MeSH terms called 'check tags' that are used to describe the characteristics of the subjects. Wang and Mercer (2019) randomly selected 257,590 full text articles from PMC Open Access Subset and developed a multichannel model using CNN-based feature selection to extract important information from different sections of the articles. HGCN4MeSH (Yu et al., 2020) used the PMC dataset generated by Wang and Mercer (2019) and employed graph convolutional neural network to learn the co-occurrences between MeSH terms. FullMeSH (Dai et al., 2019) and BERTMeSH (You et al., 2020) used all available full text articles in PMC Open Access Subset. FullMeSH applied an attention-based CNN to predict the MeSH terms and LTR to get the final MeSH candidates; BERTMeSH incorporated pre-trained BERT and an attention mechanism to improve the performance of MeSH indexing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_13",
            "content": "Graph Convolutional Networks in Natural Language Processing",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "465-ARR_v2_14",
            "content": "Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention and achieved remarkable success in natural language processing recently. Some text classification systems introduce GCN by formulating their problems as graph-structural tasks. For instance, TextGCN (Yao et al., 2019) built a single text graph for a corpus based on word co-occurrence and document word relations to infer labels. Zhang et al. (2019a) built a GCN-based dependency tree of a sentence to exploit syntactical information and word dependencies for sentiment analysis. Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multi-label text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels, and Rios and Kavuluru (2018) built a multilabel classifier that was learned from a 2-layer GCN over the label hierarchy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_15",
            "content": "GCN also provides a powerful toolkit for embedding the taxonomies into low dimension representations that could be utilized for specific tasks. For instance, Pujary et al. (2020) used GCN to learn an undirected graph derived from disease names in the MeSH taxonomy in order to detect and normalize disease mentions in biomedical texts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_16",
            "content": "Proposed Model",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "465-ARR_v2_17",
            "content": "MeSH indexing can be regarded as a multi-label text classification problem in which, given a set of biomedical documents X = {x 1 , x 2 , ..., x n } and a set of MeSH labels Y = {y 1 , y 2 , ..., y L }, multi-label classification learns the function f :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_18",
            "content": "X \u2192 [0, 1] Y using the training set D = (x i , Y i ), i = 1, ..., n,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_19",
            "content": "where n is the number of documents in the set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_20",
            "content": "Figure 1 illustrates our overall architecture. Our model is composed of a multi-channel document representation module, a label features learning module, a dynamic semantic mask attention module, and a classifier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_21",
            "content": "Multi-channel Document Representation Module",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "465-ARR_v2_22",
            "content": "The multi-channel document representation module has two input channels -the title channel and the abstract channel, for each type of text. These two texts are represented by two embedding matrices, namely E title \u2208 R d , the word embedding matrix for the title, and E abstract \u2208 R d , the word embedding matrix for the abstract. We first apply a bidirectional Long Short-Term Memory (biLSTM) network (Hochreiter and Schmidhuber, 1997)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_23",
            "content": "\u2212 \u2192 h t = LST M (x t , \u2212 \u2212 \u2192 h t\u22121 , c t\u22121 ) \u2190 \u2212 h t = LST M (x t , \u2190 \u2212 \u2212 h t\u22121 , c t\u22121 )(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_24",
            "content": "We then obtain the final representation for each word by concatenating the hidden states from both directions, namely",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_25",
            "content": "h t = [ \u2212 \u2192 h t : \u2190 \u2212 h t ] and h t \u2208 R l\u00d72d h",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_26",
            "content": ", where l is the number of words in the text and d h is the hidden dimensions. The biLSTM returns context-aware representations H title and H abstract for the title and abstract channels, respectively: In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel. The concept of dilated convolution was originally developed for wavelet decomposition (Holschneider et al., 1990), and has been applied to NLP tasks such as neural machine translation (Kalchbrenner et al., 2017) and text classification (Lin et al., 2018). The main idea of DCNN is to insert 'holes' in convolutional kernels, which extract the longer-term dependencies and generate higher-level representations, such as phases and sentences. Following Lin et al. (2018), we apply a multi-level DCNN with different dilation rates on top of the hidden representations generated by the biLSTM on the abstract channel. Small dilation rates capture phrase-level information, and large ones capture sentence-level information. The DCNN returns the semantic features of the abstract channel D abstract \u2208 R (l\u2212s+1)\u00d72d h , where s is the width of the convolution kernels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_27",
            "content": "H title = biLST M (E title ) H abstract = biLST M (E abstract ) (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_28",
            "content": "Label Features Learning Module",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "465-ARR_v2_29",
            "content": "MeSH taxonomies are organized in 16 categories, and each is further divided into subcategories. Within each subcategory, MeSH terms are ordered hierarchically from most general to most specific, up to 13 hierarchical levels. As the MeSH hierarchy is important to our task, we use a two-layer GCN to incorporate the hierarchical parent and child information among labels. We first use the MeSH descriptors to generate a label feature vector for each MeSH term. Each label vector is calculated by averaging the word embedding of each word in its descriptors:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_30",
            "content": "v i = 1 N j\u2208N w j , i = 1, 2, ..., L,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_31",
            "content": "where v i \u2208 R d , N is the number of words in its descriptor, and L is the number of labels. In the graph structure, we formulate each node as a MeSH label, and edges represent relationships in the MeSH hierarchy. The edge types of a node include edges from its parent, from its children, and from itself.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_32",
            "content": "At each GCN layer, the node feature is aggregated by its parent and children to form the new label feature for the next layer:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_33",
            "content": "h l+1 = \u03c3(A \u2022 h l \u2022 W l ),(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_34",
            "content": "where h l and h l+1 \u2208 R L\u00d7d indicate the node presentation of the l th and (l + 1) th layers, \u03c3(\u2022) denotes an activation function, A is the adjacency matrix of the MeSH hierarchical graph, and W l is a layer-specific trainable weight matrix. We then concatenate the label feature vectors from descriptors in Equation 3 with GCN label vectors to form:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_35",
            "content": "H label = [v : h l+1 ],(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_36",
            "content": "where H label \u2208 R L\u00d72d is the final label vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_37",
            "content": "Dynamic Knowledge-enhanced Mask Attention Module",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "465-ARR_v2_38",
            "content": "In the dynamic knowledge-enhanced mask attention module, we integrate external knowledge from outside sources to generate a unique mask for each article dynamically. We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons. First, the MeSH terms are numerous and have widely varying occurrence frequencies. Therefore, for each MeSH label, there are far more negative examples than positive ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_39",
            "content": "For each article, selecting a subset of MeSH labels, namely a MeSH mask, downsamples the negative examples, which forces the classifier to concentrate on the candidate labels. Second, the issue with the original attention mechanism (Bahdanau et al., 2015) is that the classifier focuses on spotting relevant information for all predicted labels, which is a lack of pertinence. Using a masked label-wise attention allows the classifier to find relevant information for each label inside the MeSH mask.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_40",
            "content": "The dynamic ensures that the module generates a unique MeSH mask for each article, specifically. To generate the MeSH masks, we consider two external knowledge sources: journal information and document similarity. The journal information refers to the name of the journal in which an article was published, which usually defines a specific research domain. We expect that articles published in the same journal tend to be indexed with MeSH terms that are relevant to the journal's research focus. We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_41",
            "content": "P (L i | J j ) = C L i \u2229J j C J j ,(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_42",
            "content": "where C L i \u2229J j denotes the number of cooccurrences of L i and J j , and C J j is the number of occurrences of J j in the training set. To avoid the noise of rare co-occurrences, a threshold \u03c4 filters noisy correlations. M j denotes the MeSH label set for journal j.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_43",
            "content": "M j = {L k |P (L k |J j ) > \u03c4, k = 1, ..., L}(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_44",
            "content": "We then use k-nearest neighbors (KNN) to choose a subset of specific MeSH terms for each article by referring to document similarity. We represent each article by the IDF-weighted sum of word embeddings in the abstract:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_45",
            "content": "D idf = n i=1 IDF i \u00d7 e i n i=1 IDF i ,(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_46",
            "content": "where e i is the word embedding, and IDF i is the inverse document frequency of the word. Next, we use KNN based on cosine similarity between abstracts to find the K nearest neighbours for each article in the training set. To form the unique MeSH mask for article a, we collect MeSH terms M a from the neighbours of a:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_47",
            "content": "M a = T 1 \u222a T 2 \u222a ... \u222a T K ,(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_48",
            "content": "where T i is the MeSH label set from the i th neighbour of article a. We then join the MeSH labels generated from journal-MeSH co-occurrence for the journal that article a has been published in together with the MeSH terms obtained from the neighbours of article a to form the final MeSH mask label set M :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_49",
            "content": "M = M j \u222a M a(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_50",
            "content": "Then we assign a value to each label in Y to form",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_51",
            "content": "M vec \u2208 [0, 1] Y .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_52",
            "content": "If the label appears in M , we assign 1, 0 otherwise. The label order of M vec is the same as H label .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_53",
            "content": "We calculate the similarity between MeSH terms and the texts in two channels by applying masked label-wise attention.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_54",
            "content": "H masked = H label M vec \u03b1 title = Softmax(H title \u2022 H masked ) \u03b1 abstract = Softmax(D abstract \u2022 H masked ),(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_55",
            "content": "where denotes element-wise multiplication, H masked denotes the masked label features, and \u03b1 title and \u03b1 abstract measure how informative each text fragment is for each label in the title and abstract channels, respectively. We then generate the label-specific title and abstract representations, respectively: such that c title \u2208 R L\u00d72d , and c abstract \u2208 R L\u00d72d . We sum up the representations in the title and abstract channels to form the document vector for each article:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_56",
            "content": "c title = \u03b1 T title \u2022 H title c abstract = \u03b1 T abstract \u2022 D abstract ,(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_57",
            "content": "D = c title + c abstract (13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_58",
            "content": "Classifier",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "465-ARR_v2_59",
            "content": "We gain scores for each MeSH term i:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_60",
            "content": "\u0177i = \u03c3(D H label ), i = 1, 2, ..., L,(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_61",
            "content": "where \u03c3(\u2022) represents the sigmoid function. We train our model using the multi-label binary crossentropy loss (Nam et al., 2014):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_62",
            "content": "L = L i=1 [\u2212y i \u2022 log( \u0177i ) \u2212 (1 \u2212 y i ) \u2022 log(1 \u2212 \u0177i ))],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_63",
            "content": "(15) where y i \u2208 [0, 1] is the ground truth of label i, and \u0177i \u2208 [0, 1] denotes the prediction of label i obtained from the proposed model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_64",
            "content": "Experiment",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "465-ARR_v2_65",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "465-ARR_v2_66",
            "content": "We follow Dai et al. (2019) and You et al. (2020) by using the PMC FTP service 9 (Comeau et al., 2019) and downloading PMC Open Access Subset (as of Sep. 2021), totalling 3,601,092 citations. We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts. In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a 'curated' or 'auto' modes in MEDLINE). We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations. Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set. In total, 28,415 distinct MeSH terms are covered in the training dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_67",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "465-ARR_v2_68",
            "content": "We implement our model in PyTorch (Paszke et al., 2019). For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words to lowercase. Titles longer than 100 characters and abstracts longer than 400 characters are truncated. We use pre-trained biomedical word embeddings (BioWordVec) (Zhang et al., 2019b), and the embedding dimension is 200. To avoid overfitting, we use dropout directly after the embedding layer with a rate of 0.2. The number of units in hidden layers are 200 in all three modules. We use a three-level dilated convolution with dilation rate [1, 2, 3] and select 1000 nearest documents to generate MeSH masks for each article. We use FAISS (Johnson et al., 2019) to find similar documents for each citation among the training set, and the whole process takes 10 hours. We use Adam optimizer (Kingma and Ba, 2015) and early stopping strategies. The learning rate is initialized to 0.0003, and the decay rate is 0.9 in every epoch. The gradient clip is applied to the maximum norm of 5. The batch size is 32. The model trained for 50 hours on a single NVIDIA V100 GPU. The detailed hyper-parameter settings are shown in Table 3. The code for our method is available at https://github.com/xdwang0726/KenMeSH.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_69",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "465-ARR_v2_70",
            "content": "We use three main evaluation metrics to test the performance of MeSH indexing systems: Microaverage measure (MiM), example-based measure (EBM), and ranking-based measure (RBM), where MiM and EBM are commonly used in MeSH indexing tasks and RBM is commonly used in evaluating multi-label classification. Micro-average Fmeasure (MiF) aggregate the global contributions of all MeSH labels and then calculate the harmonic mean of micro-average precision (MiP) and microaverage recall (MiR), which are heavily influenced by frequent MeSH terms. Example-based measures are computed per data point, which computes the harmonic mean of standard precision (EBP) and recall (EBR) for each data point. In the ranking-based measure, precision at k (P @k) shows the number of relevant MeSH terms that are suggested in the top-k recommendations of the MeSH indexing system, and recall at k (R@k) indicates the proportion of relevant items that are suggested in the top-k recommendations. The detailed computations of evaluation metrics can be found in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_71",
            "content": "The threshold has a large influence on MiF and EBF, see Appendix B. We select final MeSH labels whose predicted probability is larger than a tuned threshold t i :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_72",
            "content": "MeSH i = \u0177i \u2265 t i , 1 \u0177i < t i , 0(16)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_73",
            "content": "where t i is the threshold for MeSH term i. We compute optimal threshold for each MeSH term on the validation set following Pillai et al. (2013) that tunes t i by maximizing MiF:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_74",
            "content": "t i = argmax T MiF(T),(17)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_75",
            "content": "where T denotes all possible threshold values for label i.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_76",
            "content": "Results and Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "465-ARR_v2_77",
            "content": "We evaluate our proposed model with five state-of-the-art models: MTI, DeepMeSH, FullMeSH, BERTMeSH and HGCN4MeSH. Among these, MTI, DeepMeSH, BERTMeSH, and HGCN4MeSH are trained with abstracts and titles only; FullMeSH (Full) and BERTMeSH (Full) are trained with full PMC articles. Our proposed model is trained on titles and abstracts, and is tested using 20,000 of the latest articles. We mainly focus on MiF, which is the main evaluation metric in MeSH indexing task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_78",
            "content": "We compare our model against previous related systems on micro-average measure and examplebases measure in Table 1. Each row in the table shows all evaluation metrics on a specific method, where the best score for each metric is indicated. As reported, our model achieves the best performance on most evaluation metrics, expect MiR and EBR, on which BERTMeSH (Full) achieves the best performance. This is because that BERTMeSH (Full) is trained on full text articles, which uses much more content information in the articles than ours. Our model outperforms the subset of systems that were trained only on the abstract and the title -MTI, HGCN4MeSH, DeepMeSH and BERTMeSH in all metrics. Most importantly, there is improvement in precision without a decrease in recall. Comparing with systems trained on full articles indicates that our model achieves the best MiF, and is only slightly below BERTMeSH (Full) on MiR (0.4 percentage points). Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems. Furthermore, we compare the performance of our model with HGCN4MeSH on ranking-based measures that do not require a specific threshold. The results, summarized in Table 2, show that our model always performs better than HGCN4MeSH with up to almost 18% improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_79",
            "content": "As the frequency of different MeSH terms are imbalanced, we are interested in examining the efficiency of our model on infrequent MeSH terms. We divide MeSH terms into four groups based on the number of occurrences in the training set: (0, 100), [100, 1000), [1000, 5000), and [5000, ). Figure 2a shows the distribution of MeSH terms and percent of occurrence among the four divided groups in the training set, which indicates that the distribution of MeSH frequency is highly biased and it falls into a long-tail distribution. Figure 2b and 2c show the performance of our model comparing to MTI baseline in the four MeSH groups on MiF and EBF respectively. Our model obtains substantial improvements among frequent and infrequent labels on both MiF and EBF. We are interested in studying how the effectiveness and robustness of our model are due to the various modules, such as the multi-channel mechanism, the dilated CNN, the label graph, and masked attention. To further understand the impacts of these factors, we conduct controlled experiments with four different settings: (a) examining a single channel architecture by concatenating the title and abstract as input into the abstract channel; (b) removing the dilated CNN; (c) replacing the label feature learning module with a fully connected layer; and (d) removing the masked attention module. The influence of each of these modules can then be evaluated individually. The results are summarized in Table 4. 4, the multi-channel setting outperforms the single channel one. The reason for this could be that the single channel model misses some important features in titles and abstracts in the LSTM layer. LSTM has the capability to learn and remember over long sequences of inputs, but it can be challenging to use when facing very long input sequences. Concatenating the title and abstract into one longer sequence may hurt the performance of LSTM. To be more explicit, the single channel model may be remembering insignificant features in the LSTM layer when dealing with longer sequences. Therefore, extracting information from the title and the abstract separately is better than directly concatenating the information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_80",
            "content": "Impacts on Dilated Semantic Feature Extractions As reported in Table 4, the performance drops when removing the dilated CNN layer. The reason for this seems to be that multi-level dilated CNNs can extract high-level semantic information from the semantic units that are often wrapped in phrases or sentences, and then capture local correlation together with longer-term dependencies from the text. Compared with word-level information extracted from the biLSTM layer, high-level information extracted from the semantic units seems to provide better understanding of the text, at least for the purposes of labelling. 4, not learning the label features has the largest negative impacts on performance especially for recall (and subsequently F-measure). By removing the label features, the model pays more attention to the frequent MeSH terms and misclassifies infrequent labels as negative. This indicates that label features learned through GCN can capture the hierarchical information between MeSH terms, and MeSH indexing for infrequent terms can benefit from this hierarchical information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_81",
            "content": "Impacts on Dynamic Knowledge-enhanced Mask Attention Table 4 shows a performance drop when removing the masked attention layer, suggesting that the attention mechanism has positive impacts on performance. This result further suggest that the masked attention takes advantage of incorporating external knowledge to alleviate the extremely large pool of possible labels. To select the proper mask for each article, two hyperparameters are used: threshold \u03c4 for journal-MeSH occurrence and the number of nearest articles K. With \u03c4 = 0.5 and K = 1000, all of the gold-standard MeSH labels are guaranteed to be in the mask.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_82",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "465-ARR_v2_83",
            "content": "We propose a novel end-to-end model integrating document features and label hierarchical features for MeSH indexing. We use a novel dynamic knowledge-enhanced mask attention mechanism to handle the large universe of candidate MeSH terms and employ GCN in extracting label correlations. Experimental results demonstrate that our proposed model significantly outperforms the baseline models and provides especially large improvements on infrequent MeSH labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_84",
            "content": "In the future, we believe two important research directions will lead to further improvements. First, we plan to explore full text articles, which contain more information, to see whether our model takes advantage of the full text to improve the performance of large-scale MeSH indexing. Second, we are interested in integrating knowledge from the Unified Medical Language System (UMLS) (Bodenreider, 2004), a comprehensive ontology of biomedical concepts, in our model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "465-ARR_v2_85",
            "content": "A Aronson, James Mork, Clifford Gay, S Humphrey, Willie Rogers, The NLM Indexing Initiative's Medical Text Indexer, 2004, Studies in health technology and informatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "A Aronson",
                    "James Mork",
                    "Clifford Gay",
                    "S Humphrey",
                    "Willie Rogers"
                ],
                "title": "The NLM Indexing Initiative's Medical Text Indexer",
                "pub_date": "2004",
                "pub_title": "Studies in health technology and informatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_86",
            "content": "Fran\u00e7ois-Michel Alan R Aronson,  Lang, An overview of MetaMap: Historical perspective and recent advances, 2010, Journal of the American Medical Informatics Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Fran\u00e7ois-Michel Alan R Aronson",
                    " Lang"
                ],
                "title": "An overview of MetaMap: Historical perspective and recent advances",
                "pub_date": "2010",
                "pub_title": "Journal of the American Medical Informatics Association",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_87",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": "CoRR"
            }
        },
        {
            "ix": "465-ARR_v2_88",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Olivier Bodenreider"
                ],
                "title": "The unified medical language system (umls): integrating biomedical terminology",
                "pub_date": "2004",
                "pub_title": "Nucleic acids research",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_89",
            "content": "C Donald, Chih-Hsuan Comeau, R Wei, Zhiyong Dogan,  Lu, PMC text mining subset in BioC: about three million full-text articles and growing, 2019, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "C Donald",
                    "Chih-Hsuan Comeau",
                    "R Wei",
                    "Zhiyong Dogan",
                    " Lu"
                ],
                "title": "PMC text mining subset in BioC: about three million full-text articles and growing",
                "pub_date": "2019",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_90",
            "content": "Suyang Dai, Ronghui You, Zhiyong Lu, Xiaodi Huang, Hiroshi Mamitsuka, Shanfeng Zhu, FullMeSH: improving large-scale MeSH indexing with full text, 2019, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Suyang Dai",
                    "Ronghui You",
                    "Zhiyong Lu",
                    "Xiaodi Huang",
                    "Hiroshi Mamitsuka",
                    "Shanfeng Zhu"
                ],
                "title": "FullMeSH: improving large-scale MeSH indexing with full text",
                "pub_date": "2019",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_91",
            "content": "Dina Demner-Fushman, James Mork, Extracting characteristics of the study subjects from full-text articles, 2015, Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Dina Demner-Fushman",
                    "James Mork"
                ],
                "title": "Extracting characteristics of the study subjects from full-text articles",
                "pub_date": "2015",
                "pub_title": "Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_92",
            "content": "Jun Gu, Wei Feng, Jia Zeng, Hiroshi Mamitsuka, Shanfeng Zhu, Efficient Semisupervised MEDLINE Document Clustering With MeSH-Semantic and Global-Content Constraints, 2013, IEEE Transactions on Cybernetics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jun Gu",
                    "Wei Feng",
                    "Jia Zeng",
                    "Hiroshi Mamitsuka",
                    "Shanfeng Zhu"
                ],
                "title": "Efficient Semisupervised MEDLINE Document Clustering With MeSH-Semantic and Global-Content Constraints",
                "pub_date": "2013",
                "pub_title": "IEEE Transactions on Cybernetics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_93",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Sepp Hochreiter",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Long short-term memory",
                "pub_date": "1997",
                "pub_title": "Neural Computation",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_94",
            "content": "M Holschneider, R Kronland-Martinet, J Morlet, Ph Tchamitchian, A real-time algorithm for signal analysis with the help of the wavelet transform, 1990, Wavelets, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "M Holschneider",
                    "R Kronland-Martinet",
                    "J Morlet",
                    "Ph Tchamitchian"
                ],
                "title": "A real-time algorithm for signal analysis with the help of the wavelet transform",
                "pub_date": "1990",
                "pub_title": "Wavelets",
                "pub": "Springer"
            }
        },
        {
            "ix": "465-ARR_v2_95",
            "content": "Minlie Huang, Aur\u00e9lie N\u00e9v\u00e9ol, Zhiyong Lu, Recommending MeSH terms for annotating biomedical articles, 2011, Journal of the American Medical Informatics Association : JAMIA, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Minlie Huang",
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Zhiyong Lu"
                ],
                "title": "Recommending MeSH terms for annotating biomedical articles",
                "pub_date": "2011",
                "pub_title": "Journal of the American Medical Informatics Association : JAMIA",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_96",
            "content": "Antonio Jimeno-Yepes, James Mork, Dina Demner-Fushman, Alan Aronson, Comparison and combination of several MeSH indexing approaches, 2013, Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Antonio Jimeno-Yepes",
                    "James Mork",
                    "Dina Demner-Fushman",
                    "Alan Aronson"
                ],
                "title": "Comparison and combination of several MeSH indexing approaches",
                "pub_date": "2013",
                "pub_title": "Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_97",
            "content": "Qiao Jin, Bhuwan Dhingra, William Cohen, AttentionMeSH: Simple, effective and interpretable automatic MeSH indexer, 2018, Proceedings of the 2018 EMNLP Workshop BioASQ: Largescale Biomedical Semantic Indexing and Question Answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Qiao Jin",
                    "Bhuwan Dhingra",
                    "William Cohen"
                ],
                "title": "AttentionMeSH: Simple, effective and interpretable automatic MeSH indexer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 EMNLP Workshop BioASQ: Largescale Biomedical Semantic Indexing and Question Answering",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_98",
            "content": "Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, Billion-scale similarity search with GPUs, 2019, IEEE Transactions on Big Data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jeff Johnson",
                    "Matthijs Douze",
                    "Herv\u00e9 J\u00e9gou"
                ],
                "title": "Billion-scale similarity search with GPUs",
                "pub_date": "2019",
                "pub_title": "IEEE Transactions on Big Data",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_99",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_100",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_101",
            "content": "Thomas Kipf, Max Welling, Semisupervised classification with graph convolutional networks, 2017, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Thomas Kipf",
                    "Max Welling"
                ],
                "title": "Semisupervised classification with graph convolutional networks",
                "pub_date": "2017",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_102",
            "content": "Jimmy Lin, W. John Wilbur, PubMed related articles: A probabilistic topic-based model for content similarity, 2007, BMC Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Jimmy Lin",
                    "W. John Wilbur"
                ],
                "title": "PubMed related articles: A probabilistic topic-based model for content similarity",
                "pub_date": "2007",
                "pub_title": "BMC Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_103",
            "content": "Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun, Semantic-unit-based dilated convolution for multi-label text classification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Junyang Lin",
                    "Qi Su",
                    "Pengcheng Yang",
                    "Shuming Ma",
                    "Xu Sun"
                ],
                "title": "Semantic-unit-based dilated convolution for multi-label text classification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_104",
            "content": "Ke Liu, Shengwen Peng, Junqiu Wu, Chengxiang Zhai, Hiroshi Mamitsuka, Shanfeng Zhu, MeSHLabeler: Improving the accuracy of largescale mesh indexing by integrating diverse evidence, 2015, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Ke Liu",
                    "Shengwen Peng",
                    "Junqiu Wu",
                    "Chengxiang Zhai",
                    "Hiroshi Mamitsuka",
                    "Shanfeng Zhu"
                ],
                "title": "MeSHLabeler: Improving the accuracy of largescale mesh indexing by integrating diverse evidence",
                "pub_date": "2015",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_105",
            "content": "Zhiyong Lu, W Kim, W Wilbur, Evaluation of query expansion using, 2008, MeSH in PubMed. Information Retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Zhiyong Lu",
                    "W Kim",
                    "W Wilbur"
                ],
                "title": "Evaluation of query expansion using",
                "pub_date": "2008",
                "pub_title": "MeSH in PubMed. Information Retrieval",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_106",
            "content": "James Mork, Antonio Jimeno-Yepes, Alan Aronson, The NLM Medical Text Indexer system for indexing biomedical literature, 2013, Proceedings of the first Workshop on Bio-Medical Semantic Indexing and Question Answering (BioASQ), .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "James Mork",
                    "Antonio Jimeno-Yepes",
                    "Alan Aronson"
                ],
                "title": "The NLM Medical Text Indexer system for indexing biomedical literature",
                "pub_date": "2013",
                "pub_title": "Proceedings of the first Workshop on Bio-Medical Semantic Indexing and Question Answering (BioASQ)",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_107",
            "content": "Jinseok Nam, Jungi Kim, Eneldo Loza Menc\u00eda, Iryna Gurevych, Johannes F\u00fcrnkranz, Largescale multi-label text classification -revisiting neural networks, 2014, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Jinseok Nam",
                    "Jungi Kim",
                    "Eneldo Loza Menc\u00eda",
                    "Iryna Gurevych",
                    "Johannes F\u00fcrnkranz"
                ],
                "title": "Largescale multi-label text classification -revisiting neural networks",
                "pub_date": "2014",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_108",
            "content": "UNKNOWN, None, 2020, Multi-label text classification using attention-based graph neural network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Multi-label text classification using attention-based graph neural network",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_109",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Py-Torch: An imperative style, high-performance deep learning library, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Adam Paszke",
                    "Sam Gross",
                    "Francisco Massa",
                    "Adam Lerer",
                    "James Bradbury",
                    "Gregory Chanan",
                    "Trevor Killeen",
                    "Zeming Lin",
                    "Natalia Gimelshein",
                    "Luca Antiga",
                    "Alban Desmaison",
                    "Andreas Kopf",
                    "Edward Yang",
                    "Zachary Devito",
                    "Martin Raison",
                    "Alykhan Tejani",
                    "Sasank Chilamkurthy",
                    "Benoit Steiner",
                    "Lu Fang",
                    "Junjie Bai",
                    "Soumith Chintala"
                ],
                "title": "Py-Torch: An imperative style, high-performance deep learning library",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "465-ARR_v2_110",
            "content": "Shengwen Peng, Ronghui You, Hongning Wang, Chengxiang Zhai, Hiroshi Mamitsuka, Shanfeng Zhu, DeepMeSH: deep semantic representation for improving large-scale MeSH indexing, 2016, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Shengwen Peng",
                    "Ronghui You",
                    "Hongning Wang",
                    "Chengxiang Zhai",
                    "Hiroshi Mamitsuka",
                    "Shanfeng Zhu"
                ],
                "title": "DeepMeSH: deep semantic representation for improving large-scale MeSH indexing",
                "pub_date": "2016",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_111",
            "content": "Ignazio Pillai, Giorgio Fumera, Fabio Roli, Threshold optimisation for multi-label classifiers, 2013, Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Ignazio Pillai",
                    "Giorgio Fumera",
                    "Fabio Roli"
                ],
                "title": "Threshold optimisation for multi-label classifiers",
                "pub_date": "2013",
                "pub_title": "Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_112",
            "content": "UNKNOWN, None, 2020, Disease normalization with graph embeddings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Disease normalization with graph embeddings",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_113",
            "content": "Anthony Rios, Ramakanth Kavuluru, Fewshot and zero-shot multi-label learning for structured label spaces, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Anthony Rios",
                    "Ramakanth Kavuluru"
                ],
                "title": "Fewshot and zero-shot multi-label learning for structured label spaces",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_114",
            "content": "Xindi Wang, Robert Mercer, Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing, 2019, BioNLP@ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Xindi Wang",
                    "Robert Mercer"
                ],
                "title": "Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing",
                "pub_date": "2019",
                "pub_title": "BioNLP@ACL",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_115",
            "content": "UNKNOWN, None, 2019, MeSHProbeNet: A selfattentive probe net for MeSH indexing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "MeSHProbeNet: A selfattentive probe net for MeSH indexing",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_116",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, Graph convolutional networks for text classification, 2019, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Liang Yao",
                    "Chengsheng Mao",
                    "Yuan Luo"
                ],
                "title": "Graph convolutional networks for text classification",
                "pub_date": "2019",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_117",
            "content": "R You, Yuxuan Liu, Hiroshi Mamitsuka, Shanfeng Zhu, Bertmesh: Deep contextual representation learning for large-scale high-performance mesh indexing with full text, 2020, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "R You",
                    "Yuxuan Liu",
                    "Hiroshi Mamitsuka",
                    "Shanfeng Zhu"
                ],
                "title": "Bertmesh: Deep contextual representation learning for large-scale high-performance mesh indexing with full text",
                "pub_date": "2020",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_118",
            "content": "Miaomiao Yu, Yujiu Yang, Chenhui Li, HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Miaomiao Yu",
                    "Yujiu Yang",
                    "Chenhui Li"
                ],
                "title": "HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_119",
            "content": "Chengxiang Zhai, Hiroshi Mamitsuka, Junqiu Wu, Ke Liu, Shanfeng Zhu, Shengwen Peng, MeSHLabeler: Improving the accuracy of largescale MeSH indexing by integrating diverse evidence, 2015, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Chengxiang Zhai",
                    "Hiroshi Mamitsuka",
                    "Junqiu Wu",
                    "Ke Liu",
                    "Shanfeng Zhu",
                    "Shengwen Peng"
                ],
                "title": "MeSHLabeler: Improving the accuracy of largescale MeSH indexing by integrating diverse evidence",
                "pub_date": "2015",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_120",
            "content": "Chen Zhang, Qiuchi Li, Dawei Song, Aspect-based sentiment classification with aspectspecific graph convolutional networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Chen Zhang",
                    "Qiuchi Li",
                    "Dawei Song"
                ],
                "title": "Aspect-based sentiment classification with aspectspecific graph convolutional networks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "465-ARR_v2_121",
            "content": "UNKNOWN, None, 2019, BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "465-ARR_v2_0@0",
            "content": "KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_0",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_2@0",
            "content": "Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_2",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_2@1",
            "content": "With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_2",
            "start": 199,
            "end": 316,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_2@2",
            "content": "MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_2",
            "start": 318,
            "end": 488,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_2@3",
            "content": "To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic Knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_2",
            "start": 490,
            "end": 749,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_2@4",
            "content": "Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_2",
            "start": 751,
            "end": 858,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_4@0",
            "content": "The PubMed 1 database is a resource that provides access to the MEDLINE bibliographic database of references and abstracts together with the full text articles of some of these citations which are available in the PubMed Central 2 (PMC) repository.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_4",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_4@1",
            "content": "MEDLINE 3 contains more than 28 million references (as of Feb. 2021) to journal articles in the biomedical, health, and related disciplines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_4",
            "start": 249,
            "end": 388,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_4@2",
            "content": "Journal articles in MEDLINE are indexed according to Medical Subject Headings (MeSH) 4 , an hierarchically organized vocabulary that has been developed and maintained by the National Library of Medicine (NLM) 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_4",
            "start": 390,
            "end": 601,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_4@3",
            "content": "Currently, there are 29,369 main MeSH headings, and each MEDLINE citation has 13 MeSH indices, on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_4",
            "start": 603,
            "end": 708,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_4@4",
            "content": "MeSH terms are distinctive features of MEDLINE and can be used in many applications in biomedical text mining and information retrieval (Lu et al., 2008;Huang et al., 2011;Gu et al., 2013), being recognized as important tools for research (e.g., knowledge discovery and hypothesis generation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_4",
            "start": 710,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_5@0",
            "content": "Currently, MeSH indexing is done by human annotators who examine full articles and assign MeSH terms to each article according to rules set by NLM 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_5",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_5@1",
            "content": "Human annotation is time consuming and costly -the average cost of annotating one article in MEDLINE is about $9.40 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_5",
            "start": 151,
            "end": 267,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_5@2",
            "content": "Nearly 1 million citations were added to MEDLINE in 2020 (approximately 2,600 on a daily basis) 7 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_5",
            "start": 269,
            "end": 367,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_5@3",
            "content": "The rate of articles being added to the MEDLINE database is constantly increasing, so there is a huge financial and time-consuming cost for the status quo.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_5",
            "start": 369,
            "end": 523,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_5@4",
            "content": "Therefore, it is imperative to develop an automatic annotation system that can assist MeSH indexing of large-scale biomedical articles efficiently and accurately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_5",
            "start": 525,
            "end": 686,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@0",
            "content": "Automatic MeSH indexing can be regarded as an extreme multi-label text classification (XMC) problem, where each article can be labeled with multiple MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@1",
            "content": "Compared with standard multilabel problems, XMC finds relevant labels from an enormous set of candidate labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 161,
            "end": 271,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@2",
            "content": "The challenge of large-scale MeSH indexing comes from both the label and article sides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 273,
            "end": 359,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@3",
            "content": "Currently, there are more than 29,000 distinct MeSH terms, and new MeSH terms are updated to the vocabulary every year.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 361,
            "end": 479,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@4",
            "content": "The frequency of different MeSH terms appearing in documents are quite imbalanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 481,
            "end": 562,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@5",
            "content": "For instance, the most frequent MeSH term, 'humans', appears in more than 8 million citations; 'Pandanaceae', on the other hand, appears in only 31 documents .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 564,
            "end": 722,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@6",
            "content": "In addition, the MeSH terms that have been assigned to each article varies greatly, ranging from more than 30 to fewer than 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 724,
            "end": 849,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@7",
            "content": "Furthermore, semantic features of the biomedical literature are complicated to capture, as they contain many domain-specific concepts, phrases, and abbreviations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 851,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_6@8",
            "content": "The aforementioned difficulties make the task more complicated to generate an effective and efficient prediction model for MeSH indexing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_6",
            "start": 1014,
            "end": 1150,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_7@0",
            "content": "In this work, inspired by the rapid development of deep learning, we propose a novel neural architecture called KenMeSH (Knowledge-enhanced MeSH labelling) which is suitable for handling XMC problems where the labels are arrayed hierarchically and could capture useful information as a directed graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_7",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_7@1",
            "content": "Our method uses a dynamic knowledge-enhanced mask attention mechanism and incorporates document features together with label features to index biomedical articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_7",
            "start": 302,
            "end": 464,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_7@2",
            "content": "Our major contributions are:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_7",
            "start": 466,
            "end": 493,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@0",
            "content": "1. We design a multi-channel document representation module to extract document features from the title and the abstract using a bidirectional LSTM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@1",
            "content": "We use multi-level dilated convolution to capture semantic units in the abstract channel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 149,
            "end": 237,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@2",
            "content": "This module combines a hybrid of information, at the levels of words and the latent representations of the semantic units, to capture local correlations and longterm dependencies from text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 239,
            "end": 427,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@3",
            "content": "2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate information from the complete MeSH hierarchy to map label representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 429,
            "end": 610,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@4",
            "content": "3. We propose a novel dynamic knowledgeenhanced mask attention mechanism which incorporates external journal-MeSH cooccurrence information and document similarity in the PubMed database to constrain the large universe of possible labels in the MeSH indexing task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 612,
            "end": 874,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@5",
            "content": "4. We evaluate our model on a corpus of PMC articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 876,
            "end": 928,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_8@6",
            "content": "Our proposed method consistently achieves superior performance over previous approaches on a number of measures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_8",
            "start": 930,
            "end": 1041,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_9@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_9",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_10@0",
            "content": "Automatic MeSH Indexing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_10",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@0",
            "content": "To address the MeSH indexing task mentioned in above section, the National Library of Medicine developed Medical Text Indexer (MTI) -software that automatically recommends MeSH terms to each MEDLINE article using the abstract and title as input (Aronson et al., 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@1",
            "content": "It first generates the candidate MeSH terms for given articles, and then ranks the candidates to provide the final predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 269,
            "end": 395,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@2",
            "content": "There are two modules in MTI -MetaMap Indexing (MMI) and PubMed-Related Citations (PRC) (Lin and Wilbur, 2007;Aronson and Lang, 2010). , DeepMeSH (Peng et al., 2016), AttentionMeSH (Jin et al., 2018), and MeSHProbeNet (Xun et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 397,
            "end": 633,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@3",
            "content": "MeSHLabeler introduced a Learning-to-Rank (LTR) framework, which is a two-step strategy, first predicting the candidate MeSH terms and then ranking them to obtain the final suggestions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 635,
            "end": 819,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@4",
            "content": "MeSHLabeler first trained an independent binary classifier for each MeSH term and then used various evidence, including similar publications and term frequencies, to rank candidate MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 821,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@5",
            "content": "DeepMeSH is an improved version of MeSHLabeler, which also uses the LTR strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 1014,
            "end": 1094,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@6",
            "content": "It first generates MeSH predictions by incorporating deep semantics in the word embedding space, and then ranks the candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 1096,
            "end": 1222,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@7",
            "content": "AttentionMeSH and MeSH-ProbeNet are based on bidirectional recurrent neural networks (RNNs) and attention mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 1224,
            "end": 1340,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_11@8",
            "content": "The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of information from the input articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_11",
            "start": 1342,
            "end": 1581,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@0",
            "content": "Studies in MeSH indexing with full texts are very limited because of restrictions on full text ac-cess.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@1",
            "content": "randomly selected 1413 articles from the PMC Open Access Subset and used automatically-generated summaries from these full texts as input to MTI for MeSH indexing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 104,
            "end": 266,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@2",
            "content": "Demner-Fushman and Mork (2015) collected 14,828 full text articles from PMC Open Access Subset and developed a rule-based string-matching algorithm to extract a subject of MeSH terms called 'check tags' that are used to describe the characteristics of the subjects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 268,
            "end": 532,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@3",
            "content": "Wang and Mercer (2019) randomly selected 257,590 full text articles from PMC Open Access Subset and developed a multichannel model using CNN-based feature selection to extract important information from different sections of the articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 534,
            "end": 771,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@4",
            "content": "HGCN4MeSH (Yu et al., 2020) used the PMC dataset generated by Wang and Mercer (2019) and employed graph convolutional neural network to learn the co-occurrences between MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 773,
            "end": 952,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@5",
            "content": "FullMeSH (Dai et al., 2019) and BERTMeSH (You et al., 2020) used all available full text articles in PMC Open Access Subset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 954,
            "end": 1077,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_12@6",
            "content": "FullMeSH applied an attention-based CNN to predict the MeSH terms and LTR to get the final MeSH candidates; BERTMeSH incorporated pre-trained BERT and an attention mechanism to improve the performance of MeSH indexing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_12",
            "start": 1079,
            "end": 1296,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_13@0",
            "content": "Graph Convolutional Networks in Natural Language Processing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_13",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_14@0",
            "content": "Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention and achieved remarkable success in natural language processing recently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_14",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_14@1",
            "content": "Some text classification systems introduce GCN by formulating their problems as graph-structural tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_14",
            "start": 178,
            "end": 280,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_14@2",
            "content": "For instance, TextGCN (Yao et al., 2019) built a single text graph for a corpus based on word co-occurrence and document word relations to infer labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_14",
            "start": 282,
            "end": 433,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_14@3",
            "content": "Zhang et al. (2019a) built a GCN-based dependency tree of a sentence to exploit syntactical information and word dependencies for sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_14",
            "start": 435,
            "end": 583,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_14@4",
            "content": "Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multi-label text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels, and Rios and Kavuluru (2018) built a multilabel classifier that was learned from a 2-layer GCN over the label hierarchy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_14",
            "start": 585,
            "end": 953,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_15@0",
            "content": "GCN also provides a powerful toolkit for embedding the taxonomies into low dimension representations that could be utilized for specific tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_15",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_15@1",
            "content": "For instance, Pujary et al. (2020) used GCN to learn an undirected graph derived from disease names in the MeSH taxonomy in order to detect and normalize disease mentions in biomedical texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_15",
            "start": 144,
            "end": 334,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_16@0",
            "content": "Proposed Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_16",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_17@0",
            "content": "MeSH indexing can be regarded as a multi-label text classification problem in which, given a set of biomedical documents X = {x 1 , x 2 , ..., x n } and a set of MeSH labels Y = {y 1 , y 2 , ..., y L }, multi-label classification learns the function f :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_17",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_18@0",
            "content": "X \u2192 [0, 1] Y using the training set D = (x i , Y i ), i = 1, ..., n,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_18",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_19@0",
            "content": "where n is the number of documents in the set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_19",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_20@0",
            "content": "Figure 1 illustrates our overall architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_20",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_20@1",
            "content": "Our model is composed of a multi-channel document representation module, a label features learning module, a dynamic semantic mask attention module, and a classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_20",
            "start": 47,
            "end": 212,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_21@0",
            "content": "Multi-channel Document Representation Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_21",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_22@0",
            "content": "The multi-channel document representation module has two input channels -the title channel and the abstract channel, for each type of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_22",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_22@1",
            "content": "These two texts are represented by two embedding matrices, namely E title \u2208 R d , the word embedding matrix for the title, and E abstract \u2208 R d , the word embedding matrix for the abstract.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_22",
            "start": 140,
            "end": 328,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_22@2",
            "content": "We first apply a bidirectional Long Short-Term Memory (biLSTM) network (Hochreiter and Schmidhuber, 1997)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_22",
            "start": 330,
            "end": 434,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_23@0",
            "content": "\u2212 \u2192 h t = LST M (x t , \u2212 \u2212 \u2192 h t\u22121 , c t\u22121 ) \u2190 \u2212 h t = LST M (x t , \u2190 \u2212 \u2212 h t\u22121 , c t\u22121 )(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_23",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_24@0",
            "content": "We then obtain the final representation for each word by concatenating the hidden states from both directions, namely",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_24",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_25@0",
            "content": "h t = [ \u2212 \u2192 h t : \u2190 \u2212 h t ] and h t \u2208 R l\u00d72d h",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_25",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@0",
            "content": ", where l is the number of words in the text and d h is the hidden dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@1",
            "content": "The biLSTM returns context-aware representations H title and H abstract for the title and abstract channels, respectively: In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 79,
            "end": 355,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@2",
            "content": "The concept of dilated convolution was originally developed for wavelet decomposition (Holschneider et al., 1990), and has been applied to NLP tasks such as neural machine translation (Kalchbrenner et al., 2017) and text classification (Lin et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 357,
            "end": 611,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@3",
            "content": "The main idea of DCNN is to insert 'holes' in convolutional kernels, which extract the longer-term dependencies and generate higher-level representations, such as phases and sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 613,
            "end": 796,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@4",
            "content": "Following Lin et al. (2018), we apply a multi-level DCNN with different dilation rates on top of the hidden representations generated by the biLSTM on the abstract channel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 798,
            "end": 969,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@5",
            "content": "Small dilation rates capture phrase-level information, and large ones capture sentence-level information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 971,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_26@6",
            "content": "The DCNN returns the semantic features of the abstract channel D abstract \u2208 R (l\u2212s+1)\u00d72d h , where s is the width of the convolution kernels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_26",
            "start": 1077,
            "end": 1217,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_27@0",
            "content": "H title = biLST M (E title ) H abstract = biLST M (E abstract ) (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_27",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_28@0",
            "content": "Label Features Learning Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_28",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_29@0",
            "content": "MeSH taxonomies are organized in 16 categories, and each is further divided into subcategories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_29",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_29@1",
            "content": "Within each subcategory, MeSH terms are ordered hierarchically from most general to most specific, up to 13 hierarchical levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_29",
            "start": 96,
            "end": 223,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_29@2",
            "content": "As the MeSH hierarchy is important to our task, we use a two-layer GCN to incorporate the hierarchical parent and child information among labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_29",
            "start": 225,
            "end": 369,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_29@3",
            "content": "We first use the MeSH descriptors to generate a label feature vector for each MeSH term.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_29",
            "start": 371,
            "end": 458,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_29@4",
            "content": "Each label vector is calculated by averaging the word embedding of each word in its descriptors:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_29",
            "start": 460,
            "end": 555,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_30@0",
            "content": "v i = 1 N j\u2208N w j , i = 1, 2, ..., L,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_30",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_31@0",
            "content": "where v i \u2208 R d , N is the number of words in its descriptor, and L is the number of labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_31",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_31@1",
            "content": "In the graph structure, we formulate each node as a MeSH label, and edges represent relationships in the MeSH hierarchy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_31",
            "start": 93,
            "end": 212,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_31@2",
            "content": "The edge types of a node include edges from its parent, from its children, and from itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_31",
            "start": 214,
            "end": 304,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_32@0",
            "content": "At each GCN layer, the node feature is aggregated by its parent and children to form the new label feature for the next layer:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_32",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_33@0",
            "content": "h l+1 = \u03c3(A \u2022 h l \u2022 W l ),(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_33",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_34@0",
            "content": "where h l and h l+1 \u2208 R L\u00d7d indicate the node presentation of the l th and (l + 1) th layers, \u03c3(\u2022) denotes an activation function, A is the adjacency matrix of the MeSH hierarchical graph, and W l is a layer-specific trainable weight matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_34",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_34@1",
            "content": "We then concatenate the label feature vectors from descriptors in Equation 3 with GCN label vectors to form:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_34",
            "start": 242,
            "end": 349,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_35@0",
            "content": "H label = [v : h l+1 ],(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_35",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_36@0",
            "content": "where H label \u2208 R L\u00d72d is the final label vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_36",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_37@0",
            "content": "Dynamic Knowledge-enhanced Mask Attention Module",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_37",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_38@0",
            "content": "In the dynamic knowledge-enhanced mask attention module, we integrate external knowledge from outside sources to generate a unique mask for each article dynamically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_38",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_38@1",
            "content": "We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_38",
            "start": 166,
            "end": 353,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_38@2",
            "content": "First, the MeSH terms are numerous and have widely varying occurrence frequencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_38",
            "start": 355,
            "end": 436,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_38@3",
            "content": "Therefore, for each MeSH label, there are far more negative examples than positive ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_38",
            "start": 438,
            "end": 525,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_39@0",
            "content": "For each article, selecting a subset of MeSH labels, namely a MeSH mask, downsamples the negative examples, which forces the classifier to concentrate on the candidate labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_39",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_39@1",
            "content": "Second, the issue with the original attention mechanism (Bahdanau et al., 2015) is that the classifier focuses on spotting relevant information for all predicted labels, which is a lack of pertinence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_39",
            "start": 176,
            "end": 375,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_39@2",
            "content": "Using a masked label-wise attention allows the classifier to find relevant information for each label inside the MeSH mask.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_39",
            "start": 377,
            "end": 499,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_40@0",
            "content": "The dynamic ensures that the module generates a unique MeSH mask for each article, specifically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_40",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_40@1",
            "content": "To generate the MeSH masks, we consider two external knowledge sources: journal information and document similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_40",
            "start": 97,
            "end": 212,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_40@2",
            "content": "The journal information refers to the name of the journal in which an article was published, which usually defines a specific research domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_40",
            "start": 214,
            "end": 355,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_40@3",
            "content": "We expect that articles published in the same journal tend to be indexed with MeSH terms that are relevant to the journal's research focus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_40",
            "start": 357,
            "end": 495,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_40@4",
            "content": "We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_40",
            "start": 497,
            "end": 684,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_41@0",
            "content": "P (L i | J j ) = C L i \u2229J j C J j ,(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_41",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_42@0",
            "content": "where C L i \u2229J j denotes the number of cooccurrences of L i and J j , and C J j is the number of occurrences of J j in the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_42",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_42@1",
            "content": "To avoid the noise of rare co-occurrences, a threshold \u03c4 filters noisy correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_42",
            "start": 137,
            "end": 220,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_42@2",
            "content": "M j denotes the MeSH label set for journal j.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_42",
            "start": 222,
            "end": 266,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_43@0",
            "content": "M j = {L k |P (L k |J j ) > \u03c4, k = 1, ..., L}(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_43",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_44@0",
            "content": "We then use k-nearest neighbors (KNN) to choose a subset of specific MeSH terms for each article by referring to document similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_44",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_44@1",
            "content": "We represent each article by the IDF-weighted sum of word embeddings in the abstract:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_44",
            "start": 134,
            "end": 218,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_45@0",
            "content": "D idf = n i=1 IDF i \u00d7 e i n i=1 IDF i ,(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_45",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_46@0",
            "content": "where e i is the word embedding, and IDF i is the inverse document frequency of the word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_46",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_46@1",
            "content": "Next, we use KNN based on cosine similarity between abstracts to find the K nearest neighbours for each article in the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_46",
            "start": 90,
            "end": 221,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_46@2",
            "content": "To form the unique MeSH mask for article a, we collect MeSH terms M a from the neighbours of a:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_46",
            "start": 223,
            "end": 317,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_47@0",
            "content": "M a = T 1 \u222a T 2 \u222a ... \u222a T K ,(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_47",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_48@0",
            "content": "where T i is the MeSH label set from the i th neighbour of article a. We then join the MeSH labels generated from journal-MeSH co-occurrence for the journal that article a has been published in together with the MeSH terms obtained from the neighbours of article a to form the final MeSH mask label set M :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_48",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_49@0",
            "content": "M = M j \u222a M a(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_49",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_50@0",
            "content": "Then we assign a value to each label in Y to form",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_50",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_51@0",
            "content": "M vec \u2208 [0, 1] Y .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_51",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_52@0",
            "content": "If the label appears in M , we assign 1, 0 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_52",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_52@1",
            "content": "The label order of M vec is the same as H label .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_52",
            "start": 54,
            "end": 102,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_53@0",
            "content": "We calculate the similarity between MeSH terms and the texts in two channels by applying masked label-wise attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_53",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_54@0",
            "content": "H masked = H label M vec \u03b1 title = Softmax(H title \u2022 H masked ) \u03b1 abstract = Softmax(D abstract \u2022 H masked ),(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_54",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_55@0",
            "content": "where denotes element-wise multiplication, H masked denotes the masked label features, and \u03b1 title and \u03b1 abstract measure how informative each text fragment is for each label in the title and abstract channels, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_55",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_55@1",
            "content": "We then generate the label-specific title and abstract representations, respectively: such that c title \u2208 R L\u00d72d , and c abstract \u2208 R L\u00d72d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_55",
            "start": 225,
            "end": 364,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_55@2",
            "content": "We sum up the representations in the title and abstract channels to form the document vector for each article:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_55",
            "start": 366,
            "end": 475,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_56@0",
            "content": "c title = \u03b1 T title \u2022 H title c abstract = \u03b1 T abstract \u2022 D abstract ,(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_56",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_57@0",
            "content": "D = c title + c abstract (13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_57",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_58@0",
            "content": "Classifier",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_58",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_59@0",
            "content": "We gain scores for each MeSH term i:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_59",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_60@0",
            "content": "\u0177i = \u03c3(D H label ), i = 1, 2, ..., L,(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_60",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_61@0",
            "content": "where \u03c3(\u2022) represents the sigmoid function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_61",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_61@1",
            "content": "We train our model using the multi-label binary crossentropy loss (Nam et al., 2014):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_61",
            "start": 44,
            "end": 128,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_62@0",
            "content": "L = L i=1 [\u2212y i \u2022 log( \u0177i ) \u2212 (1 \u2212 y i ) \u2022 log(1 \u2212 \u0177i ))],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_62",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_63@0",
            "content": "(15) where y i \u2208 [0, 1] is the ground truth of label i, and \u0177i \u2208 [0, 1] denotes the prediction of label i obtained from the proposed model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_63",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_64@0",
            "content": "Experiment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_64",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_65@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_65",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@0",
            "content": "We follow Dai et al. (2019) and You et al. (2020) by using the PMC FTP service 9 (Comeau et al., 2019) and downloading PMC Open Access Subset (as of Sep. 2021), totalling 3,601,092 citations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@1",
            "content": "We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 192,
            "end": 361,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@2",
            "content": "In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a 'curated' or 'auto' modes in MEDLINE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 363,
            "end": 508,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@3",
            "content": "We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 510,
            "end": 613,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@4",
            "content": "Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 615,
            "end": 806,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_66@5",
            "content": "In total, 28,415 distinct MeSH terms are covered in the training dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_66",
            "start": 808,
            "end": 880,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_67@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_67",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@0",
            "content": "We implement our model in PyTorch (Paszke et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@1",
            "content": "For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words to lowercase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 57,
            "end": 208,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@2",
            "content": "Titles longer than 100 characters and abstracts longer than 400 characters are truncated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 210,
            "end": 298,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@3",
            "content": "We use pre-trained biomedical word embeddings (BioWordVec) (Zhang et al., 2019b), and the embedding dimension is 200.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 300,
            "end": 416,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@4",
            "content": "To avoid overfitting, we use dropout directly after the embedding layer with a rate of 0.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 418,
            "end": 508,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@5",
            "content": "The number of units in hidden layers are 200 in all three modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 510,
            "end": 575,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@6",
            "content": "We use a three-level dilated convolution with dilation rate [1, 2, 3] and select 1000 nearest documents to generate MeSH masks for each article.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 577,
            "end": 720,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@7",
            "content": "We use FAISS (Johnson et al., 2019) to find similar documents for each citation among the training set, and the whole process takes 10 hours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 722,
            "end": 862,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@8",
            "content": "We use Adam optimizer (Kingma and Ba, 2015) and early stopping strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 864,
            "end": 937,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@9",
            "content": "The learning rate is initialized to 0.0003, and the decay rate is 0.9 in every epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 939,
            "end": 1023,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@10",
            "content": "The gradient clip is applied to the maximum norm of 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 1025,
            "end": 1078,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@11",
            "content": "The batch size is 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 1080,
            "end": 1100,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@12",
            "content": "The model trained for 50 hours on a single NVIDIA V100 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 1102,
            "end": 1160,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@13",
            "content": "The detailed hyper-parameter settings are shown in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 1162,
            "end": 1220,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_68@14",
            "content": "The code for our method is available at https://github.com/xdwang0726/KenMeSH.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_68",
            "start": 1222,
            "end": 1299,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_69@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_69",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_70@0",
            "content": "We use three main evaluation metrics to test the performance of MeSH indexing systems: Microaverage measure (MiM), example-based measure (EBM), and ranking-based measure (RBM), where MiM and EBM are commonly used in MeSH indexing tasks and RBM is commonly used in evaluating multi-label classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_70",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_70@1",
            "content": "Micro-average Fmeasure (MiF) aggregate the global contributions of all MeSH labels and then calculate the harmonic mean of micro-average precision (MiP) and microaverage recall (MiR), which are heavily influenced by frequent MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_70",
            "start": 303,
            "end": 538,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_70@2",
            "content": "Example-based measures are computed per data point, which computes the harmonic mean of standard precision (EBP) and recall (EBR) for each data point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_70",
            "start": 540,
            "end": 689,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_70@3",
            "content": "In the ranking-based measure, precision at k (P @k) shows the number of relevant MeSH terms that are suggested in the top-k recommendations of the MeSH indexing system, and recall at k (R@k) indicates the proportion of relevant items that are suggested in the top-k recommendations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_70",
            "start": 691,
            "end": 972,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_70@4",
            "content": "The detailed computations of evaluation metrics can be found in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_70",
            "start": 974,
            "end": 1048,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_71@0",
            "content": "The threshold has a large influence on MiF and EBF, see Appendix B. We select final MeSH labels whose predicted probability is larger than a tuned threshold t i :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_71",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_72@0",
            "content": "MeSH i = \u0177i \u2265 t i , 1 \u0177i < t i , 0(16)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_72",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_73@0",
            "content": "where t i is the threshold for MeSH term i. We compute optimal threshold for each MeSH term on the validation set following Pillai et al. (2013) that tunes t i by maximizing MiF:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_73",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_74@0",
            "content": "t i = argmax T MiF(T),(17)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_74",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_75@0",
            "content": "where T denotes all possible threshold values for label i.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_75",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_76@0",
            "content": "Results and Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_76",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_77@0",
            "content": "We evaluate our proposed model with five state-of-the-art models: MTI, DeepMeSH, FullMeSH, BERTMeSH and HGCN4MeSH.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_77",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_77@1",
            "content": "Among these, MTI, DeepMeSH, BERTMeSH, and HGCN4MeSH are trained with abstracts and titles only; FullMeSH (Full) and BERTMeSH (Full) are trained with full PMC articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_77",
            "start": 115,
            "end": 281,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_77@2",
            "content": "Our proposed model is trained on titles and abstracts, and is tested using 20,000 of the latest articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_77",
            "start": 283,
            "end": 387,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_77@3",
            "content": "We mainly focus on MiF, which is the main evaluation metric in MeSH indexing task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_77",
            "start": 389,
            "end": 470,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@0",
            "content": "We compare our model against previous related systems on micro-average measure and examplebases measure in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@1",
            "content": "Each row in the table shows all evaluation metrics on a specific method, where the best score for each metric is indicated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 116,
            "end": 238,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@2",
            "content": "As reported, our model achieves the best performance on most evaluation metrics, expect MiR and EBR, on which BERTMeSH (Full) achieves the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 240,
            "end": 395,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@3",
            "content": "This is because that BERTMeSH (Full) is trained on full text articles, which uses much more content information in the articles than ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 397,
            "end": 534,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@4",
            "content": "Our model outperforms the subset of systems that were trained only on the abstract and the title -MTI, HGCN4MeSH, DeepMeSH and BERTMeSH in all metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 536,
            "end": 686,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@5",
            "content": "Most importantly, there is improvement in precision without a decrease in recall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 688,
            "end": 768,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@6",
            "content": "Comparing with systems trained on full articles indicates that our model achieves the best MiF, and is only slightly below BERTMeSH (Full) on MiR (0.4 percentage points).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 770,
            "end": 939,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@7",
            "content": "Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 941,
            "end": 1113,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@8",
            "content": "Furthermore, we compare the performance of our model with HGCN4MeSH on ranking-based measures that do not require a specific threshold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 1115,
            "end": 1249,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_78@9",
            "content": "The results, summarized in Table 2, show that our model always performs better than HGCN4MeSH with up to almost 18% improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_78",
            "start": 1251,
            "end": 1378,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@0",
            "content": "As the frequency of different MeSH terms are imbalanced, we are interested in examining the efficiency of our model on infrequent MeSH terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@1",
            "content": "We divide MeSH terms into four groups based on the number of occurrences in the training set: (0, 100), [100, 1000), [1000, 5000), and [5000, ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 142,
            "end": 285,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@2",
            "content": "Figure 2a shows the distribution of MeSH terms and percent of occurrence among the four divided groups in the training set, which indicates that the distribution of MeSH frequency is highly biased and it falls into a long-tail distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 287,
            "end": 526,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@3",
            "content": "Figure 2b and 2c show the performance of our model comparing to MTI baseline in the four MeSH groups on MiF and EBF respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 528,
            "end": 656,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@4",
            "content": "Our model obtains substantial improvements among frequent and infrequent labels on both MiF and EBF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 658,
            "end": 757,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@5",
            "content": "We are interested in studying how the effectiveness and robustness of our model are due to the various modules, such as the multi-channel mechanism, the dilated CNN, the label graph, and masked attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 759,
            "end": 962,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@6",
            "content": "To further understand the impacts of these factors, we conduct controlled experiments with four different settings: (a) examining a single channel architecture by concatenating the title and abstract as input into the abstract channel; (b) removing the dilated CNN; (c) replacing the label feature learning module with a fully connected layer; and (d) removing the masked attention module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 964,
            "end": 1352,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@7",
            "content": "The influence of each of these modules can then be evaluated individually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1354,
            "end": 1427,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@8",
            "content": "The results are summarized in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1429,
            "end": 1466,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@9",
            "content": "4, the multi-channel setting outperforms the single channel one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1468,
            "end": 1531,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@10",
            "content": "The reason for this could be that the single channel model misses some important features in titles and abstracts in the LSTM layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1533,
            "end": 1664,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@11",
            "content": "LSTM has the capability to learn and remember over long sequences of inputs, but it can be challenging to use when facing very long input sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1666,
            "end": 1813,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@12",
            "content": "Concatenating the title and abstract into one longer sequence may hurt the performance of LSTM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1815,
            "end": 1909,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@13",
            "content": "To be more explicit, the single channel model may be remembering insignificant features in the LSTM layer when dealing with longer sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 1911,
            "end": 2051,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_79@14",
            "content": "Therefore, extracting information from the title and the abstract separately is better than directly concatenating the information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_79",
            "start": 2053,
            "end": 2183,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@0",
            "content": "Impacts on Dilated Semantic Feature Extractions As reported in Table 4, the performance drops when removing the dilated CNN layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@1",
            "content": "The reason for this seems to be that multi-level dilated CNNs can extract high-level semantic information from the semantic units that are often wrapped in phrases or sentences, and then capture local correlation together with longer-term dependencies from the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 131,
            "end": 396,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@2",
            "content": "Compared with word-level information extracted from the biLSTM layer, high-level information extracted from the semantic units seems to provide better understanding of the text, at least for the purposes of labelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 398,
            "end": 614,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@3",
            "content": "4, not learning the label features has the largest negative impacts on performance especially for recall (and subsequently F-measure).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 616,
            "end": 749,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@4",
            "content": "By removing the label features, the model pays more attention to the frequent MeSH terms and misclassifies infrequent labels as negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 751,
            "end": 887,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_80@5",
            "content": "This indicates that label features learned through GCN can capture the hierarchical information between MeSH terms, and MeSH indexing for infrequent terms can benefit from this hierarchical information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_80",
            "start": 889,
            "end": 1090,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_81@0",
            "content": "Impacts on Dynamic Knowledge-enhanced Mask Attention Table 4 shows a performance drop when removing the masked attention layer, suggesting that the attention mechanism has positive impacts on performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_81",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_81@1",
            "content": "This result further suggest that the masked attention takes advantage of incorporating external knowledge to alleviate the extremely large pool of possible labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_81",
            "start": 205,
            "end": 367,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_81@2",
            "content": "To select the proper mask for each article, two hyperparameters are used: threshold \u03c4 for journal-MeSH occurrence and the number of nearest articles K. With \u03c4 = 0.5 and K = 1000, all of the gold-standard MeSH labels are guaranteed to be in the mask.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_81",
            "start": 369,
            "end": 617,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_82@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_82",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_83@0",
            "content": "We propose a novel end-to-end model integrating document features and label hierarchical features for MeSH indexing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_83",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_83@1",
            "content": "We use a novel dynamic knowledge-enhanced mask attention mechanism to handle the large universe of candidate MeSH terms and employ GCN in extracting label correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_83",
            "start": 117,
            "end": 284,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_83@2",
            "content": "Experimental results demonstrate that our proposed model significantly outperforms the baseline models and provides especially large improvements on infrequent MeSH labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_83",
            "start": 286,
            "end": 457,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_84@0",
            "content": "In the future, we believe two important research directions will lead to further improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_84",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_84@1",
            "content": "First, we plan to explore full text articles, which contain more information, to see whether our model takes advantage of the full text to improve the performance of large-scale MeSH indexing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_84",
            "start": 95,
            "end": 286,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_84@2",
            "content": "Second, we are interested in integrating knowledge from the Unified Medical Language System (UMLS) (Bodenreider, 2004), a comprehensive ontology of biomedical concepts, in our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_84",
            "start": 288,
            "end": 469,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_85@0",
            "content": "A Aronson, James Mork, Clifford Gay, S Humphrey, Willie Rogers, The NLM Indexing Initiative's Medical Text Indexer, 2004, Studies in health technology and informatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_85",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_86@0",
            "content": "Fran\u00e7ois-Michel Alan R Aronson,  Lang, An overview of MetaMap: Historical perspective and recent advances, 2010, Journal of the American Medical Informatics Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_86",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_87@0",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_87",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_88@0",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_88",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_89@0",
            "content": "C Donald, Chih-Hsuan Comeau, R Wei, Zhiyong Dogan,  Lu, PMC text mining subset in BioC: about three million full-text articles and growing, 2019, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_89",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_90@0",
            "content": "Suyang Dai, Ronghui You, Zhiyong Lu, Xiaodi Huang, Hiroshi Mamitsuka, Shanfeng Zhu, FullMeSH: improving large-scale MeSH indexing with full text, 2019, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_90",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_91@0",
            "content": "Dina Demner-Fushman, James Mork, Extracting characteristics of the study subjects from full-text articles, 2015, Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_91",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_92@0",
            "content": "Jun Gu, Wei Feng, Jia Zeng, Hiroshi Mamitsuka, Shanfeng Zhu, Efficient Semisupervised MEDLINE Document Clustering With MeSH-Semantic and Global-Content Constraints, 2013, IEEE Transactions on Cybernetics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_92",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_93@0",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_93",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_94@0",
            "content": "M Holschneider, R Kronland-Martinet, J Morlet, Ph Tchamitchian, A real-time algorithm for signal analysis with the help of the wavelet transform, 1990, Wavelets, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_94",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_95@0",
            "content": "Minlie Huang, Aur\u00e9lie N\u00e9v\u00e9ol, Zhiyong Lu, Recommending MeSH terms for annotating biomedical articles, 2011, Journal of the American Medical Informatics Association : JAMIA, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_95",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_96@0",
            "content": "Antonio Jimeno-Yepes, James Mork, Dina Demner-Fushman, Alan Aronson, Comparison and combination of several MeSH indexing approaches, 2013, Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_96",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_97@0",
            "content": "Qiao Jin, Bhuwan Dhingra, William Cohen, AttentionMeSH: Simple, effective and interpretable automatic MeSH indexer, 2018, Proceedings of the 2018 EMNLP Workshop BioASQ: Largescale Biomedical Semantic Indexing and Question Answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_97",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_98@0",
            "content": "Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, Billion-scale similarity search with GPUs, 2019, IEEE Transactions on Big Data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_98",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_99",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_100@0",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_100",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_101@0",
            "content": "Thomas Kipf, Max Welling, Semisupervised classification with graph convolutional networks, 2017, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_101",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_102@0",
            "content": "Jimmy Lin, W. John Wilbur, PubMed related articles: A probabilistic topic-based model for content similarity, 2007, BMC Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_102",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_103@0",
            "content": "Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun, Semantic-unit-based dilated convolution for multi-label text classification, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_103",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_104@0",
            "content": "Ke Liu, Shengwen Peng, Junqiu Wu, Chengxiang Zhai, Hiroshi Mamitsuka, Shanfeng Zhu, MeSHLabeler: Improving the accuracy of largescale mesh indexing by integrating diverse evidence, 2015, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_104",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_105@0",
            "content": "Zhiyong Lu, W Kim, W Wilbur, Evaluation of query expansion using, 2008, MeSH in PubMed. Information Retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_105",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_106@0",
            "content": "James Mork, Antonio Jimeno-Yepes, Alan Aronson, The NLM Medical Text Indexer system for indexing biomedical literature, 2013, Proceedings of the first Workshop on Bio-Medical Semantic Indexing and Question Answering (BioASQ), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_106",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_107@0",
            "content": "Jinseok Nam, Jungi Kim, Eneldo Loza Menc\u00eda, Iryna Gurevych, Johannes F\u00fcrnkranz, Largescale multi-label text classification -revisiting neural networks, 2014, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_107",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_108@0",
            "content": "UNKNOWN, None, 2020, Multi-label text classification using attention-based graph neural network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_108",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_109@0",
            "content": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Py-Torch: An imperative style, high-performance deep learning library, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_109",
            "start": 0,
            "end": 463,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_110@0",
            "content": "Shengwen Peng, Ronghui You, Hongning Wang, Chengxiang Zhai, Hiroshi Mamitsuka, Shanfeng Zhu, DeepMeSH: deep semantic representation for improving large-scale MeSH indexing, 2016, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_110",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_111@0",
            "content": "Ignazio Pillai, Giorgio Fumera, Fabio Roli, Threshold optimisation for multi-label classifiers, 2013, Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_111",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2020, Disease normalization with graph embeddings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_112",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_113@0",
            "content": "Anthony Rios, Ramakanth Kavuluru, Fewshot and zero-shot multi-label learning for structured label spaces, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_113",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_114@0",
            "content": "Xindi Wang, Robert Mercer, Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing, 2019, BioNLP@ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_114",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_115@0",
            "content": "UNKNOWN, None, 2019, MeSHProbeNet: A selfattentive probe net for MeSH indexing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_115",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_116@0",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, Graph convolutional networks for text classification, 2019, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_116",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_117@0",
            "content": "R You, Yuxuan Liu, Hiroshi Mamitsuka, Shanfeng Zhu, Bertmesh: Deep contextual representation learning for large-scale high-performance mesh indexing with full text, 2020, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_117",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_118@0",
            "content": "Miaomiao Yu, Yujiu Yang, Chenhui Li, HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_118",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_119@0",
            "content": "Chengxiang Zhai, Hiroshi Mamitsuka, Junqiu Wu, Ke Liu, Shanfeng Zhu, Shengwen Peng, MeSHLabeler: Improving the accuracy of largescale MeSH indexing by integrating diverse evidence, 2015, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_119",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_120@0",
            "content": "Chen Zhang, Qiuchi Li, Dawei Song, Aspect-based sentiment classification with aspectspecific graph convolutional networks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_120",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "465-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2019, BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "465-ARR_v2_121",
            "start": 0,
            "end": 122,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_1",
            "tgt_ix": "465-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_1",
            "tgt_ix": "465-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_7",
            "tgt_ix": "465-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_9",
            "tgt_ix": "465-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_10",
            "tgt_ix": "465-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_10",
            "tgt_ix": "465-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_10",
            "tgt_ix": "465-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_13",
            "tgt_ix": "465-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_13",
            "tgt_ix": "465-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_13",
            "tgt_ix": "465-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_15",
            "tgt_ix": "465-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_17",
            "tgt_ix": "465-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_18",
            "tgt_ix": "465-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_19",
            "tgt_ix": "465-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_20",
            "tgt_ix": "465-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_22",
            "tgt_ix": "465-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_23",
            "tgt_ix": "465-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_24",
            "tgt_ix": "465-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_25",
            "tgt_ix": "465-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_27",
            "tgt_ix": "465-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_30",
            "tgt_ix": "465-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_31",
            "tgt_ix": "465-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_32",
            "tgt_ix": "465-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_33",
            "tgt_ix": "465-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_34",
            "tgt_ix": "465-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_35",
            "tgt_ix": "465-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_36",
            "tgt_ix": "465-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_38",
            "tgt_ix": "465-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_39",
            "tgt_ix": "465-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_41",
            "tgt_ix": "465-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_42",
            "tgt_ix": "465-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_43",
            "tgt_ix": "465-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_44",
            "tgt_ix": "465-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_45",
            "tgt_ix": "465-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_46",
            "tgt_ix": "465-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_47",
            "tgt_ix": "465-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_48",
            "tgt_ix": "465-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_49",
            "tgt_ix": "465-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_50",
            "tgt_ix": "465-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_51",
            "tgt_ix": "465-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_52",
            "tgt_ix": "465-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_53",
            "tgt_ix": "465-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_54",
            "tgt_ix": "465-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_55",
            "tgt_ix": "465-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_56",
            "tgt_ix": "465-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_57",
            "tgt_ix": "465-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_59",
            "tgt_ix": "465-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_60",
            "tgt_ix": "465-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_61",
            "tgt_ix": "465-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_62",
            "tgt_ix": "465-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_63",
            "tgt_ix": "465-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_64",
            "tgt_ix": "465-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_64",
            "tgt_ix": "465-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_65",
            "tgt_ix": "465-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_65",
            "tgt_ix": "465-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_64",
            "tgt_ix": "465-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_67",
            "tgt_ix": "465-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_67",
            "tgt_ix": "465-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_64",
            "tgt_ix": "465-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_71",
            "tgt_ix": "465-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_72",
            "tgt_ix": "465-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_73",
            "tgt_ix": "465-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_74",
            "tgt_ix": "465-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_75",
            "tgt_ix": "465-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_77",
            "tgt_ix": "465-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_81",
            "tgt_ix": "465-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_83",
            "tgt_ix": "465-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_82",
            "tgt_ix": "465-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_82",
            "tgt_ix": "465-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_82",
            "tgt_ix": "465-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "465-ARR_v2_0",
            "tgt_ix": "465-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_1",
            "tgt_ix": "465-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_2",
            "tgt_ix": "465-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_3",
            "tgt_ix": "465-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_4",
            "tgt_ix": "465-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_5",
            "tgt_ix": "465-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_6",
            "tgt_ix": "465-ARR_v2_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_7",
            "tgt_ix": "465-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_7",
            "tgt_ix": "465-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_7",
            "tgt_ix": "465-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_8",
            "tgt_ix": "465-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_9",
            "tgt_ix": "465-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_10",
            "tgt_ix": "465-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_11",
            "tgt_ix": "465-ARR_v2_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_12",
            "tgt_ix": "465-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_13",
            "tgt_ix": "465-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_14",
            "tgt_ix": "465-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_15",
            "tgt_ix": "465-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_15",
            "tgt_ix": "465-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_16",
            "tgt_ix": "465-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_17",
            "tgt_ix": "465-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_18",
            "tgt_ix": "465-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_19",
            "tgt_ix": "465-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_20",
            "tgt_ix": "465-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_20",
            "tgt_ix": "465-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_21",
            "tgt_ix": "465-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_22",
            "tgt_ix": "465-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_22",
            "tgt_ix": "465-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_22",
            "tgt_ix": "465-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_23",
            "tgt_ix": "465-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_24",
            "tgt_ix": "465-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_25",
            "tgt_ix": "465-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_26",
            "tgt_ix": "465-ARR_v2_26@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_27",
            "tgt_ix": "465-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_28",
            "tgt_ix": "465-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_29",
            "tgt_ix": "465-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_30",
            "tgt_ix": "465-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_31",
            "tgt_ix": "465-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_31",
            "tgt_ix": "465-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_31",
            "tgt_ix": "465-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_32",
            "tgt_ix": "465-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_33",
            "tgt_ix": "465-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_34",
            "tgt_ix": "465-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_34",
            "tgt_ix": "465-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_35",
            "tgt_ix": "465-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_36",
            "tgt_ix": "465-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_37",
            "tgt_ix": "465-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_38",
            "tgt_ix": "465-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_38",
            "tgt_ix": "465-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_38",
            "tgt_ix": "465-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_38",
            "tgt_ix": "465-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_39",
            "tgt_ix": "465-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_39",
            "tgt_ix": "465-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_39",
            "tgt_ix": "465-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_40",
            "tgt_ix": "465-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_41",
            "tgt_ix": "465-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_42",
            "tgt_ix": "465-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_42",
            "tgt_ix": "465-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_42",
            "tgt_ix": "465-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_43",
            "tgt_ix": "465-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_44",
            "tgt_ix": "465-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_44",
            "tgt_ix": "465-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_45",
            "tgt_ix": "465-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_46",
            "tgt_ix": "465-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_46",
            "tgt_ix": "465-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_46",
            "tgt_ix": "465-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_47",
            "tgt_ix": "465-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_48",
            "tgt_ix": "465-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_49",
            "tgt_ix": "465-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_50",
            "tgt_ix": "465-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_51",
            "tgt_ix": "465-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_52",
            "tgt_ix": "465-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_52",
            "tgt_ix": "465-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_53",
            "tgt_ix": "465-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_54",
            "tgt_ix": "465-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_55",
            "tgt_ix": "465-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_55",
            "tgt_ix": "465-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_55",
            "tgt_ix": "465-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_56",
            "tgt_ix": "465-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_57",
            "tgt_ix": "465-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_58",
            "tgt_ix": "465-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_59",
            "tgt_ix": "465-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_60",
            "tgt_ix": "465-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_61",
            "tgt_ix": "465-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_61",
            "tgt_ix": "465-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_62",
            "tgt_ix": "465-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_63",
            "tgt_ix": "465-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_64",
            "tgt_ix": "465-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_65",
            "tgt_ix": "465-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_66",
            "tgt_ix": "465-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_67",
            "tgt_ix": "465-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_68",
            "tgt_ix": "465-ARR_v2_68@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_69",
            "tgt_ix": "465-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_70",
            "tgt_ix": "465-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_71",
            "tgt_ix": "465-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_72",
            "tgt_ix": "465-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_73",
            "tgt_ix": "465-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_74",
            "tgt_ix": "465-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_75",
            "tgt_ix": "465-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_76",
            "tgt_ix": "465-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_77",
            "tgt_ix": "465-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_77",
            "tgt_ix": "465-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_77",
            "tgt_ix": "465-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_77",
            "tgt_ix": "465-ARR_v2_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_78",
            "tgt_ix": "465-ARR_v2_78@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_79",
            "tgt_ix": "465-ARR_v2_79@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_80",
            "tgt_ix": "465-ARR_v2_80@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_81",
            "tgt_ix": "465-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_81",
            "tgt_ix": "465-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_81",
            "tgt_ix": "465-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_82",
            "tgt_ix": "465-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_83",
            "tgt_ix": "465-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_83",
            "tgt_ix": "465-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_83",
            "tgt_ix": "465-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_84",
            "tgt_ix": "465-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_84",
            "tgt_ix": "465-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_84",
            "tgt_ix": "465-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_85",
            "tgt_ix": "465-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_86",
            "tgt_ix": "465-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_87",
            "tgt_ix": "465-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_88",
            "tgt_ix": "465-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_89",
            "tgt_ix": "465-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_90",
            "tgt_ix": "465-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_91",
            "tgt_ix": "465-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_92",
            "tgt_ix": "465-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_93",
            "tgt_ix": "465-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_94",
            "tgt_ix": "465-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_95",
            "tgt_ix": "465-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_96",
            "tgt_ix": "465-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_97",
            "tgt_ix": "465-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_98",
            "tgt_ix": "465-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_99",
            "tgt_ix": "465-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_100",
            "tgt_ix": "465-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_101",
            "tgt_ix": "465-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_102",
            "tgt_ix": "465-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_103",
            "tgt_ix": "465-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_104",
            "tgt_ix": "465-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_105",
            "tgt_ix": "465-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_106",
            "tgt_ix": "465-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_107",
            "tgt_ix": "465-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_108",
            "tgt_ix": "465-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_109",
            "tgt_ix": "465-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_110",
            "tgt_ix": "465-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_111",
            "tgt_ix": "465-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_112",
            "tgt_ix": "465-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_113",
            "tgt_ix": "465-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_114",
            "tgt_ix": "465-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_115",
            "tgt_ix": "465-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_116",
            "tgt_ix": "465-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_117",
            "tgt_ix": "465-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_118",
            "tgt_ix": "465-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_119",
            "tgt_ix": "465-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_120",
            "tgt_ix": "465-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "465-ARR_v2_121",
            "tgt_ix": "465-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 783,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "465-ARR",
        "version": 2
    }
}