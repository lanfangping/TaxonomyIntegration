{
    "nodes": [
        {
            "ix": "24-ARR_v1_0",
            "content": "Parallel Decoding Sequences by Glancing Discrete Latent Variables",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_2",
            "content": "Recently, parallel decoding of sentences has received widespread attention due to its success in decoding efficiency. Many advanced techniques are proposed to improve its inferior quality, such as curriculum learning or introducing latent variables. However, we observe that these techniques still need an autoregressive teacher to help the model overcome the one-to-many multi-modal phenomenon in the dataset, which enlarges the training costs. In this paper, we propose mix-GLT, which well combines the power of latent variable models and an advanced glancing training technique to alleviate the multi-modality problem. Experiments results on several representative text generation tasks show that our method is much better than the strong baselines and further improving the generality of the parallel decoding paradigm.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "24-ARR_v1_4",
            "content": "Non-autoregressive Transformer (NAT, Gu et al., 2018) introduce a parallel decoding paradigm with higher decoding efficiency (> 10\u00d7) than autoregressive models (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017). Unlike autoregressive models, NAT models impose conditional independence assumptions in words to support parallel decoding of sentences during inference. It attracts many researchers to explore NAT in machine translation (Gu et al., 2018;Lee et al., 2018;Kaiser et al., 2018) and text-to-speech tasks (Chen et al., 2019;Peng et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_5",
            "content": "Amount of researchers devoted themselves to improve the NATs' generation quality. Such as modeling word inter-dependencies by curriculum learning (Guo et al., 2020a;Liu et al., 2020) or iterative refinements mechanism (Ghazvininejad et al., 2019;Guo et al., 2020b), introducing latent variables serve as the springboard for parallel decoding (Shu et al., 2019;Ma et al., 2019;Bao et al., 2021), or introduce inductive bias for NAT models' training (Wei et al., 2019;. The most successful method is the glancing transformer (GLAT, Qian et al., 2021a), which trains the NAT model by sampling partial target words as inputs to predict the remaining words, explicitly building dependencies between the observed and unobserved words. Qian et al. (2021b) employ GLAT to achieve a fantastic result, even outperforming many strong autoregressive translation systems in BLEU score (Papineni et al., 2002) on the German-English translation task of WMT21 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_6",
            "content": "Although they achieve competitive results compared to autoregressive models in translation tasks, we observe that they still need the help of an autoregressive Transformer (AT, Vaswani et al., 2017) as a teacher for training, i.e., sequence-level knowledge distillation (Kim and Rush, 2016). ; Sun and Yang (2020) point out that the multi-modality phenomenon, namely, each input may have multiple valid outputs in datasets, preventing NAT models from learning to organize consistent outputs. Training with the outputs of an AT can alleviate the multi-modality problem by filtering the training set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_7",
            "content": "However, training NAT models with an AT model is quite limited. Including knowledge distillation via an AT model significantly enlarges the training cost due to its extra training time. Besides, the AT models may not be accurate enough in other text generation settings except machine translation, which will become the bottleneck for its student NAT model. Therefore, training a model from scratch without an AT model is an open and exciting challenge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_8",
            "content": "In this paper, we propose mix-GLT, which can directly learn from the raw dataset. To overcome the multi-modality problem, we follow a divideand-conquer spirit, introducing a small set of discrete latent variables to divide the origin goal into latent variable modeling and sentence modeling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_9",
            "content": "Each latent variable sequence discretized from sentence are information to determine the mode of the sentence, which effectively reduces the multimodality problem at the sentence level. Also, these latent variables will have fewer modes than origin sentences, namely, fewer multi-modality phenomena, which can be modeled with a glancing transformer (Qian et al., 2021a) directly. Finally, we extend glancing training with the latent variables to model the sentence, encouraging the model to build dependencies on latent variables rather than specific words, which works more robust.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_10",
            "content": "We conduct experiments on various text generation tasks, including machine translation, paraphrase generation, and dialog generation. Experiments results show that mix-GLT achieves remarkable improvements over several strong baselines, verify the effectiveness of mix-GLT. In-depth analyses and ablation studies indicate that the introduced latent variables and glancing training are necessary for performance improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_11",
            "content": "Parallel Decoding in Sequence Models",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "24-ARR_v1_12",
            "content": "A sequence-to-sequence model aims to parameterize p(Y|X ), which defines the probability of target sentence",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_13",
            "content": "Y = (y 1 , y 2 , \u2022 \u2022 \u2022 , y m ) given input sen- tence X = (x 1 , x 2 , \u2022 \u2022 \u2022 , x n ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_14",
            "content": "where n and m are the sentence lengths. Dozens of researches (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017) factorize p(Y|X ) with a series of conditional probability:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_15",
            "content": "p AT (Y|X ) = m t=1 p(y t |y <t , X ),(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_16",
            "content": "where y <t = (y 1 , y 2 , \u2022 \u2022 \u2022 , y t\u22121 ). Such autoregressive factorization predicts words 2 based on its history predictions, which exists the risk of exposure bias and slow decoding during inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_17",
            "content": "Non-autoregressive Transformer. To tackle the above problems, Gu et al. (2018) firstly propose non-autoregressive Transformer (NAT), removing y <t and factorizing p(Y|X ) as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_18",
            "content": "p NAT (Y|X ) = m t=1 p(y t |X ),(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_19",
            "content": "where each word y t are modeled independently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_20",
            "content": "During inference, NAT model decodes a sentence by arg max yt p(y t |X ) for each position t, remarkably improving the efficiency (15\u00d7 speedups). However, the efficiency improvements of NAT are at the cost of its quality, e.g., the performance degradation by more than 10.0 BLEU (Papineni et al., 2002) points in machine translation tasks (Gu et al., 2018). The independence assumption prevents the NAT model from leveraging the inherent word dependencies to organize consistent outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_21",
            "content": "Glancing Transformer. Qian et al. (2021a) propose Glancing Transformer (GLAT), which introduces glancing training (GLT) and trains NAT predictions with:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_22",
            "content": "L GLT = \u2212 log p(Y obs |Y obs , X ) = \u2212 y i \u2208Y obs log p(y i |Y obs , X ),(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_23",
            "content": "where Y obs is the partial target tokens, and Y obs is its complements set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_24",
            "content": "During training, GLT samples Y obs according to the model's prediction: sample fewer target words for well-predicted cases and sample more words for worse-predicted cases. Moreover, it progressively decreases the sampling ratio and obtains better performances in machine translation tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_25",
            "content": "Nevertheless, we experimentally find that GLAT still has a multi-modal problem 3 : First, the sampling rate cannot be reduced too small during training; otherwise, the performance will be close to the NAT model. Second, it still needs a teacher model for further improvements (Qian et al., 2021a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_26",
            "content": "Latent Transformer. Kaiser et al. (2018); Shu et al. (2019); Ma et al. (2019); Bao et al. (2021) propose Latent Transformer (LT), introducing latent variables z for NAT predictions as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_27",
            "content": "p LT (Y|X ) = z p(z|X ) \u2022 m t=1 p(y t |z, X ). (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_28",
            "content": "As observed, LT models first predict the latent variables with p(z|X ), then simultaneously decode the sentence by arg max yt p(y t |z, X ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_29",
            "content": "In practice, LT models are always trained by variational inference (Shu et al., 2019;Ma et al., 2019) or discretization techniques (Kaiser et al., 2018;Bao et al., 2021). Such latent variables are decomposed from the target sentence, which is informative to determine the mode of the sentence and alleviates the multi-modality problems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_30",
            "content": "Although LT models improve performance in terms of BLEU score, the used autoregressive predictor (Kaiser et al., 2018;Bao et al., 2021) or deep iterative transformation (Shu et al., 2019;Ma et al., 2019) for predicting latent variables unavoidable sacrifice the overall decoding efficiency. In addition, they do not explicitly build the interdependencies among the outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_31",
            "content": "3 Proposed Method: mix-GLT",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_32",
            "content": "In this section, we present mix-GLT in detail. mix-GLT follows Latent Transformer models (Kaiser et al., 2018;Bao et al., 2021) but introduces glancing training (Qian et al., 2021a) with the discrete latent variables. Our intuitions are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_33",
            "content": "As aforementioned in Section \u00a72, we can decompose the original multi-modal targets with a small set of latent variables. The decomposed latent variables are informative to determine the mode of the sentence and alleviate its multi-modality issues. In such a case, we can incorporate glancing training to directly model the reduced-modality target sentences based on the latent variables.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_34",
            "content": "Model Structure",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "24-ARR_v1_35",
            "content": "In this part, we state the structure of mix-GLT, which introduces a small set of discrete latent variables for a NAT model, basically following Kaiser et al. (2018); ; Bao et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_36",
            "content": "Let K be the size of the discrete latent space and let [K] denote the set {1, 2, \u2022 \u2022 \u2022 , K}. For each target sentence Y = (y 1 , y 2 , \u2022 \u2022 \u2022 , y m ), we use a same-length latent variable sequence for modeling it as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_37",
            "content": "p(Y|X ) = z p \u03b8 (z|X ) \u2022 m t=1 p \u03b8 (y t |z, X ), (5) where z = (z 1 , z 2 , \u2022 \u2022 \u2022 , z m ) and z i \u2208 [K], \u03b8 is the model parameters.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_38",
            "content": "Maximizing the log-likelihood of Eqn. ( 5) is unpractical due to its exponential searching spaces and complex decoding process. Therefore, we follow Kaiser et al. (2018) and adopt a discretization technique to train the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_39",
            "content": "For discretizing target sentences to latent variables, we use vector quantization , which works by dividing a large set of origin vector representations into small groups. Specifically, we assign each token y i with a group j \u2208 [K] that has the nearest distance to its representation:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_40",
            "content": "z i = arg min j\u2208[K] || repr(y i ) \u2212 q j || 2 ,(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_41",
            "content": "Mix. Decoder",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_42",
            "content": "e 1 e 2 e 3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_43",
            "content": "y 1 y 3 y 4 y 2 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention + + + + Predict Length & Softcopy Vector Quantization z 1 z + + + + Mix. Decoder y 1 y 3 h h 2 y 3 y 2 y 1 y Attention Glancing h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (a) mix-GLT Architecture NAT Encoder e 1 e 2 e 3 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention Predict Length & Softcopy Qu z 1 z 4 + + + + y 1 y 3 h 4 h 2 G h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (b) LLP for Latent Predictor Mix. Decoder NAT Encoder e 1 e 2 e 3 y 1 y 3 y 4 y 2 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention + + + + Predict Length & Softcopy Vector Quantization z 1 z 4 + + + + Mix. Decoder y 1 y 3 h 4 h 2 y 3 y 2 y 1 y 4 Attention Glancing h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (c) LMIX for Mixture Decoder",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_44",
            "content": "Figure 1: Model architecture of mix-GLT, and our introduced losses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_45",
            "content": ": a gated network to positionwise combine q zi and h i for the Mix. Decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_46",
            "content": "where q \u2208 R K\u00d7D is the maintained representations and D is its dimension, and we use the embedding as repr(y i ), refer to Bao et al. (2021). Then, the model is trained to minimize",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_47",
            "content": "L = L rec + \u03b2 repr(y i ) \u2212 sg(q z i )(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_48",
            "content": "where L rec is the prediction loss for Y and z, and sg(\u2022) is the stop gradient operator.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_49",
            "content": "The maintained representations q are updated with an exponential moving average over a minibatch of target tokens {y",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_50",
            "content": "1 , \u2022 \u2022 \u2022 , y i , \u2022 \u2022 \u2022 }: c j \u2190 \u03bbc j + (1 \u2212 \u03bb) i 1[z i = j], q j \u2190 \u03bbq j + (1 \u2212 \u03bb) i 1[z i = j] repr(y i ) c j (8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_51",
            "content": "where c j is assigned count for group j, and we set decay parameter \u03bb = 0.999 in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_52",
            "content": "Architecture. As shown in Figure 1a, mix-GLT mainly consists of an encoder F enc (NAT Encoder), a latent predictor F LP (NAT Predictor), and a decoder F dec (Mix. Decoder). We parameterize them with the multi-head attention-based encoder or decoder, similar to Transformer (Vaswani et al., 2017). We formalize their functions as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_53",
            "content": "(e 1 , e 2 , \u2022 \u2022 \u2022 , e n ) \u2190 F enc (x 1 , x 2 , \u2022 \u2022 \u2022 , x n ), (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) \u2190 softcopy(e 1:n ), p \u03b8 (z|X ) \u2190 F LP (h 1:m , e 1:n ), p \u03b8 (Y|z, X ) \u2190 F dec (z 1:m , h 1:m , e 1:n ),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_54",
            "content": "where we use an extra module F len to predict the target length m and initialize the decoder inputs (Wei et al., 2019) mechanism.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_55",
            "content": "H = (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) with the softcopy",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_56",
            "content": "Training by Glancing Latent Variables",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "24-ARR_v1_57",
            "content": "As our design, the discretized latent variables will have fewer modes than raw sentences, which can be trained directly without the help of distillation. Notably, we model the latent variable sequence in a non-autoregressive fashion, in which we use a glancing training technique (Qian et al., 2021a) for optimizing it, as shown in Figure 1b:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_58",
            "content": "L LP = \u2212 log p \u03b8 (z obs |z obs , X )(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_59",
            "content": "where z obs is uniformly sampled from z, refer to Qian et al. (2021a). We provide more details for glancing training with mix-GLT in Appendix B Our next insight is modeling the sentence using the sampled latent variables z obs rather than z, as we cannot predict the z exactly during inference:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_60",
            "content": "L GSZ = \u2212 log p \u03b8 (Y|z obs , X ).(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_61",
            "content": "We experimentally find Eqn. ( 11) works robustly and analyze it in Section ( \u00a7 4.3). As shown in Figure 1c, we eventually employ glancing training with target token for optimizing L GSZ , namely we optimize the Mix. Decoder by minimizing",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_62",
            "content": "L MIX = \u2212 log p \u03b8 (Y obs |z obs , Y obs , X ),(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_63",
            "content": "where Y obs and z obs are the sampled target tokens and discrete latent variables.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_64",
            "content": "Overall Training Loss. Our full-fledged loss includes latent variable prediction, sentence reconstruction, and length prediction losses:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_65",
            "content": "L = L MIX + L LP + \u03b1L LEN ,(13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_66",
            "content": "where \u03b1 = 0.1 are the hyperparameters to adjust the importance of length prediction loss L LEN .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_67",
            "content": "Inference",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "24-ARR_v1_68",
            "content": "The inference process of mix-GLT includes length prediction, latent variables and sentence prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_69",
            "content": "For determining the target length, mix-GLT first predicts the target target length m with the length predictor. To avoid the length prediction errors during inference, mix-GLT expands the length m to a ranges (we use",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_70",
            "content": "[m \u2212 3, \u2022 \u2022 \u2022 , m + 2], total six candidates in our experiments).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_71",
            "content": "Then, mix-GLT predicts the latent variables \u1e91 with arg max z p \u03b8 (z|X ) and sentence \u0176 with arg max Y p \u03b8 (Y| \u1e91, X ) for each candidate length.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_72",
            "content": "Like Ma et al. (2019), mix-GLT ranks them by itself and chooses the highest score output with:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_73",
            "content": "\u0176 = arg max Y p \u03b8 (Y| \u1e91, X ) \u2022 \u03b3 |Y| (14",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_74",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_75",
            "content": "where \u03b3 is the length penalty ratio to avoid the length bias, and |Y| denotes the length of Y.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_76",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "24-ARR_v1_77",
            "content": "We conduct experiments on several generation tasks, including machine translation, paraphrase, and dialog generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_78",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "24-ARR_v1_79",
            "content": "Dataset. We chose the most popular benchmarks for each task:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_80",
            "content": "\u2022 Machine Translation (MT): We follow previous practices in NAT models and use the WMT14 English (EN) \u2194 German (DE) corpus (4.5M sentence pairs) and the IWSLT14 German (DE) \u2192 English (EN) corpus (160K sentence pairs) to validate our proposed model. We obtain the datasets following the instruction open-sourced in fairseq 4 . In detail, we first tokenize the datasets with Moses script. Then, we use 37,000 and 10,000 operations to split the words into byte-pair encodings (BPE, Sennrich et al., 2016) in WMT14 and IWSLT14 datasets, respectively. We also share subword embeddings between the source and target language for each dataset. \u2022 Paraphrase Generation (PG): We use the Quora 5 dataset to evaluate the paraphrase generation task. The Quora dataset contains around 135K labeled paraphrases pairs. Following the standard dataset split, we sample 100K sentence pairs from the labeled paraphrases as training data and hold out 30K pairs for testing, the remaining about 5K pairs for validation. Like the MT tasks, we tokenize the corpus with Moses scripts and split the words into BPE units with total 32K operations. \u2022 Dialog Generation (DG): We conduct the dialog generation experiments on the DailyDialog dataset (Li et al., 2017). We obtain the The task of MT aims to transfer bilingual sentences with semantically invariant conditions. The PG task differs from machine translation and works on mode transformation in the same language, whose goal is to synthesize a sentence different from the original input but conveys the same meaning. The DG task is most challenging due to the limited corpus and complex background.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_81",
            "content": "Implementations. We mainly compare mix-GLT with Transformer (Vaswani et al., 2017), vanilla NAT (Gu et al., 2018), and GLAT (Qian et al., 2021a) models. We implement them based on the open-source framework fairseq (Ott et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_82",
            "content": "For machine translation tasks, we use the base setting (d model = 512, d hidden = 2048, dropout = 0.1, n head = 8, and n layer = 6) of Transformer (Vaswani et al., 2017) for WMT14 dataset and a smaller setting (d model = 512, d hidden = 1024, dropout = 0.3, n head = 4, and n layer = 6) for IWSLT14 dataset. The number of layers in mix-GLT decoder and latent predictor are both set to 4 in experiments. We use inverse square root learning rate scheduling for WMT14 and a linear annealing learning rate from 3.0 \u00d7 10 \u22124 to 1.0 \u00d7 10 \u22125 in 250K steps for IWSLT14. The models are optimized with Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 = 0.9, \u03b2 2 = 0.999) in 300K steps for WMT14 and 250K steps for IWSLT14. As for the ratio \u03c4 that used in glancing sampling, we linear anneal the ratio from 0.5 to 0.3 in whole training steps. The mini-batch in each step consists of 2K MultiTurnDialogZoo tokens for IWSLT14 and 64K tokens for WMT14.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_83",
            "content": "Since the scale of the Quora and DailyDialog datasets are close to the IWSLT14, we keep the same setting to the IWSLT14, such as the Adam, learning rate (linear annealing from 3.0 \u00d7 10 \u22124 to 1.0 \u00d7 10 \u22125 ), and batch size (2K tokens).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_84",
            "content": "Evaluation. To validate the effectiveness of our proposed method, we evaluate it in terms of quality and efficiency. We use tokenized and cased BLEU scores (Papineni et al., 2002) 7 to evaluate the generation quality of MT and PG tasks. For dialog generation, we also include BLEU-1 and BLEU-2 scores for analysis. Following the common practices (Gu et al., 2018;Qian et al., 2021a), we measure the decoding latency of each model by decoding sentence by sentence and compute the speedup compared with the autoregressive Transformer (AT) model to reflect its decoding efficiency. We highlight the best NAT result.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_85",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "24-ARR_v1_86",
            "content": "We can see from Table 1 that our mix-GLT almost outperforms all the NAT baselines (NAT and GLAT) in generation quality on all tasks while keeping a competitive decoding speedup to the autoregressive counterpart.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_87",
            "content": "Machine Translation. As seen, without using an AT model as a teacher for training NAT models, the vanilla NAT and advanced GLAT model only obtain inferior generation quality. In contrast, mix-GLT achieves competitive generation quality in machine translation tasks, indicating that the introduced latent variables effectively reduce the multi-modality issue and support glancing training well. It narrows the performance gap between nonautoregressive decoding and autoregressive decoding from 11.46 (GLAT vs. AT) to 2.34 (mix-GLT vs. AT) BLEU points on WMT14 EN\u2192DE task while keeping a high-speed decoding efficiency. Paraphrasing. Unlike the translation task, the performance gap between non-autoregressive and autoregressive decoding on the paraphrase generation task is minor (NAT vs. AT, \u22123.32 BLEU points, GLAT vs. AT, \u22120.96 BLEU points ). Nevertheless, our introduced discrete latent variables still are helpful to obtain a better performance. mix-GLT realizes a non-autoregressive model with better performance than the autoregressive model on Quora dataset (mix-GLT vs. AT, +1.14 BLEU points).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_88",
            "content": "Dialog Generation. We can see a different trend on the DailyDialog dataset -an AT model performs poorly than NAT models. Both GLAT and mix-GLT outperform the AT model in BLEU-1, BLEU-2, and BLEU scores, indicating that these models recall more reference tokens and organize the tokens well. We conjecture that the weak and indirect association between the inputs and outputs of the dialogue encourages the AT model to predict the tokens by paying more attention to their history outputs. Finally, it may collapse into a target-side language model. In contrast, the NAT models do not have this fast track, pushing them to pay more attention to the inputs and recall more target tokens. We further find that there are so-called safe response (Li et al., 2016) in AT's outputs, which verify our conjecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_89",
            "content": "More Comparisons. we further compare the advanced NAT models that builds upon latent vari-Figure 2: BLEU scores and their relative decoding speedups of different models on WMT14 EN\u2192DE test set. Note that we evaluate the speedups with a single GTX 1080-Ti GPU and include the results with the same evaluating hardware for fair comparisons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_90",
            "content": "ables or iterative refinement in machine translation tasks:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_91",
            "content": "\u2022 NATs w/ latent variables: LV-NAR (Shu et al., 2019), SynST (Akoury et al., 2019), Flowseq (Ma et al., 2019), and CNAT (Bao et al., 2021). \u2022 Iterative NATs: CMLM (Ghazvininejad et al., 2019) and LevT (Gu et al., 2019).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_92",
            "content": "It is shown in Table 2 that introducing latent variables (LV-NAR, Flowseq, and CNAT) or decoding with multiple iterations (CMLM and LevT) both improve non-autoregressive decoding in translation quality. However, iterative refinements or deep transformations always sacrifice decoding efficiency. In contrast, the proposed mix-GLT outperforms all NAT models with a relatively low cost, keeping a competitive speedup over autoregressive Transformer (AT). Specifically, mix-GLT with onepass decoding narrows the performance gap to the AT from 5.87 BLEU points to 2.34 BLEU points on the WMT14 EN\u2192DE test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_93",
            "content": "Decoding efficiency. We can see there is a tradeoff between the translation quality and decoding efficiency in Table 2. We thus present the scatter plot of different models in Figure 2, showing the trend of translation quality and decoding efficiency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_94",
            "content": "As seen, mix-GLT is located on the top-right of the baselines. It outperforms the baselines in the BLEU score if decoding speedup is fixed and in decoding speedup if the BLEU score is fixed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_95",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "24-ARR_v1_96",
            "content": "mix-GLT largely alleviates the sentence-level multi-modal problem. Previous researches (Gu et al., 2018;Ma et al., 2019;Qian et al., 2021a;Bao et al., 2021) always utilize a Transformer model as a teacher for training NAT models, namely sequence-level knowledge distillation (Kim and Rush, 2016), which can directly reduces the sentence-level multi-modal phenomenon in datasets. Therefore, we use the average gains from the knowledge distillation to reflect the ability of the NAT models to overcome this issue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_97",
            "content": "As seen in Table 3, the pure NAT models heavily rely on knowledge distillation. By introducing the target information with the latent variables (Flowseq and CNAT) or sampled tokens (GLAT), the NAT models improve its' ability to overcome the multi-modality issue. Our proposed mix-GLT well combines the above two techniques. It obtains only 0.95 BLEU points average gains and validates our motivation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_98",
            "content": "Discrete latent variables have fewer modes than raw sentences. To validate our intuition that the introduced latent variables are easier to predict than tokens, we refer to the complexity metrics on each dataset according to alignment relations. Specifically, we use the fast_align 8 toolkit to align source input X and target outputs Y or discretized latent variable sequences z. Then, we compute the token-level complexity C TOK (d) and the sentence-level complexity C SEN (d) according to . These metrics can trivially understand as the number of valid candidates for each input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_99",
            "content": "As shown in Table 4, the latent variables have the lowest complexity in both token-level complexity and sentence-level complexity. In other words, predicting the latent variable sequences is effortless than predicting others, which is consistent with our intuition. Although we obtain a lower complexity dataset by filtering the datasets with an autoregressive model (AT outputs versus Raw outputs), they may introduce model error and need extra training for AT model. In contrast, the discrete latent variables are simple and informative enough to serve as a springboard for modeling target sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_100",
            "content": "Glancing with latent variables improves the performance with a large margin. We can see in Effects of K and \u03b3. As shown in Figure 3 and Table 6, we search the hyper-parameter of mix-GLT that the number of discrete latent variables and the length penalty ratio \u03b3 according to the validation performance. We notice that using more latent codes causes performance degradation during inference, in which the latent variables may degenerate to tokens and contains more prediction error during inference. The mix-GLT implemented with 64 latent variables and \u03b3 = 1.1 obtains the best result.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_101",
            "content": "5 Related Work Gu et al. (2018) first propose a non-autoregressive Transformer (NAT) model for neural machine translation (NMT) and begin to explore parallel decoding. It abandons explicitly modeling word interdependencies to decode the tokens in parallel, significantly improving the inference speed. However, its translation quality is inferior to the Transformer (Vaswani et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_102",
            "content": "To alleviate this performance degradation, many researchers work to enhance word dependency modeling, including imitation learning (Wei et al., 2019;, curriculum learning (Guo et al., 2020a;Liu et al., 2020), iterative refinements (Lee et al., 2018;Ghazvininejad et al., 2019;Gu et al., 2019;Guo et al., 2020b), and a simplified autoregressive process . The most representative method is the glancing transformer model (Qian et al., 2021a), which adaptively and progressively samples partial tokens as inputs and predicts the remaining tokens, effectively establishing the dependencies between the sampled tokens and the remaining tokens. However, these models still rely on a teacher for training, which cannot directly learn the raw dataset that contains one-tomany multi-modality phenomenon.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_103",
            "content": "Our method introduces latent variables to NAT models, which are close to Kaiser et al. (2018) Bao et al. (2021). These methods decompose the latent variables (hints) from the target sentence and divide the origin goal into two parts: modeling latent variables and modeling the target sentences based on latent variables. It implicitly overcomes the multi-modality phenomenon of target sentences because the latent variables can largely determine the mode of the sentence. However, these methods always model the latent variables with an autoregressive predictor, which naturally sacrifices the decoding efficiency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_104",
            "content": "Unlike them, our approach models the discrete latent variables in a non-autoregressive fashion and extends glancing training with the discrete latent variables. As a result, mix-GLT accomplishes a competitive performance both in decoding efficiency and quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_105",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "24-ARR_v1_106",
            "content": "We propose mix-GLT, which can be directly trained without the help of knowledge distillation. Specifically, we employ discrete latent variables to divide the NAT prediction to the latent variables modeling and sentence reconstruction tasks. Then, we learn each task with the glancing training and encourages the model to build dependencies on the latent variables, which have few modes and are informative to modeling the target sentences. Experiments results on several representative text generation tasks validate the effectiveness of that our mix-GLT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_107",
            "content": "We can see in Figure 1a that the performance of a GLAT model will degenerate to that of a NAT model while the sampling ratio annealing too small. In addition, introducing the knowledge distillation for training the GLAT model alleviates this issue (Figure 1b). B Structure Details of mix-GLT Softcopy Inputs. mix-GLT uses Softcopy (Wei et al., 2019; mechanism to obtain the inputs H = (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_108",
            "content": "h i = n i \u03b1 ij \u2022 e i , \u03b1 ij \u221d exp [\u2212(i \u2212 j \u2022 n m ) 2 ],(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_109",
            "content": "where E = (e 1 , e 2 , \u2022 \u2022 \u2022 , e n ) is the contextual representation of X encoded by the NAT encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_110",
            "content": "Glancing Training for Latent Predictor. With the decoder input H = h 1:m and the discretized latent variable sequence z = z 1:m , we adopt the glancing sampling technique for training the latent predictor in the following steps:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_111",
            "content": "\u2022 Predicting \u1e91: mix-GLT predicts the latent variable sequence with its latent predictor: \u1e91 \u2190 F LP (h 1:m , e 1:n ). \u2022 Determining sample number N z : Given z and \u1e91, we compute the sampling number as:",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_112",
            "content": "N z = \u03c4 \u2022 Hamming(z, \u1e91)(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_113",
            "content": "where \u03c4 is the sampling ratio decreasing in the training steps, and we use Hamming distance (Hamming, 1950) for measuring the prediction quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_114",
            "content": "\u2022 Glancing reference: The glancing reference z obs is obtained by random selecting N z elements from reference sequence z. \u2022 Re-constructing inputs H : We construct H by position-wise replacing the decoder input h 1:m with z obs . \u2022 Updating Latent Predictor: With the H as inputs, We then train the latent predictor to predict the unobserved references z obs .",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_115",
            "content": "Glancing Training for Mix. Decoder. Glancing training for Mix. Decoder is largely follow the Qian et al. (2021a), except using extra latent variables as inputs. With the decoder input H = h 1:m , the reference sentence Y, and the glancing latent variable sequence z obs , we train it in the following steps:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_116",
            "content": "\u2022 Predicting \u0176: mix-GLT predicts the target sentences: \u0176 \u2190 F dec (z obs , h 1:m , e 1:n ). \u2022 Determining sample number N y : Given Y and \u0176, we compute the sampling number N y = \u03c4 \u2022 Hamming(Y, \u0176). \u2022 Glancing reference: The glancing reference Y obs is obtained by random selecting N y tokens from reference sequence Y. \u2022 Re-constructing inputs H : H is constructed by position-wise replacing the decoder input H with embedding of Y obs . \u2022 Updating Mix. Decoder: We then train the Mix. Decoder to predict the unobserved references Y obs , with the H and z obs as inputs.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "24-ARR_v1_117",
            "content": "Nader Akoury, Kalpesh Krishna, Mohit Iyyer, Syntactically supervised transformers for faster neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Nader Akoury",
                    "Kalpesh Krishna",
                    "Mohit Iyyer"
                ],
                "title": "Syntactically supervised transformers for faster neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_118",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Dzmitry Bahdanau",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "Neural machine translation by jointly learning to align and translate",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_119",
            "content": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, PLATO: Pre-trained dialogue generation model with discrete latent variable, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Siqi Bao",
                    "Huang He",
                    "Fan Wang",
                    "Hua Wu",
                    "Haifeng Wang"
                ],
                "title": "PLATO: Pre-trained dialogue generation model with discrete latent variable",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_120",
            "content": "UNKNOWN, None, 2021, Nonautoregressive translation by learning target categorical codes, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Nonautoregressive translation by learning target categorical codes",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_121",
            "content": "UNKNOWN, None, 2019, Listen and fill in the missing letters: Non-autoregressive transformer for speech recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Listen and fill in the missing letters: Non-autoregressive transformer for speech recognition",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_122",
            "content": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin, Convolutional sequence to sequence learning, 2017-08-11, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jonas Gehring",
                    "Michael Auli",
                    "David Grangier",
                    "Denis Yarats",
                    "Yann Dauphin"
                ],
                "title": "Convolutional sequence to sequence learning",
                "pub_date": "2017-08-11",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "24-ARR_v1_123",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Omer Levy",
                    "Yinhan Liu",
                    "Luke Zettlemoyer"
                ],
                "title": "Mask-predict: Parallel decoding of conditional masked language models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_124",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Nonautoregressive neural machine translation, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jiatao Gu",
                    "James Bradbury",
                    "Caiming Xiong",
                    "O Victor",
                    "Richard Li",
                    " Socher"
                ],
                "title": "Nonautoregressive neural machine translation",
                "pub_date": "2018-04-30",
                "pub_title": "6th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_125",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, Vancouver.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jiatao Gu",
                    "Changhan Wang",
                    "Junbo Zhao"
                ],
                "title": "Levenshtein transformer",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": "Vancouver"
            }
        },
        {
            "ix": "24-ARR_v1_126",
            "content": "Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong Chen, Tie-Yan Liu, Fine-tuning by curriculum learning for non-autoregressive neural machine translation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Junliang Guo",
                    "Xu Tan",
                    "Linli Xu",
                    "Tao Qin",
                    "Enhong Chen",
                    "Tie-Yan Liu"
                ],
                "title": "Fine-tuning by curriculum learning for non-autoregressive neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_127",
            "content": "Junliang Guo, Linli Xu, Enhong Chen, Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Junliang Guo",
                    "Linli Xu",
                    "Enhong Chen"
                ],
                "title": "Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_128",
            "content": "UNKNOWN, None, 1950, Error detecting and error correcting codes. The Bell system technical journal, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "1950",
                "pub_title": "Error detecting and error correcting codes. The Bell system technical journal",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_129",
            "content": "Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer, Fast decoding in sequence models using discrete latent variables, 2018-07-10, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Lukasz Kaiser",
                    "Samy Bengio",
                    "Aurko Roy",
                    "Ashish Vaswani",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Noam Shazeer"
                ],
                "title": "Fast decoding in sequence models using discrete latent variables",
                "pub_date": "2018-07-10",
                "pub_title": "Proceedings of the 35th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "24-ARR_v1_130",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Yoon Kim",
                    "Alexander Rush"
                ],
                "title": "Sequencelevel knowledge distillation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_131",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_132",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Jason Lee",
                    "Elman Mansimov",
                    "Kyunghyun Cho"
                ],
                "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_133",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jiwei Li",
                    "Michel Galley",
                    "Chris Brockett",
                    "Jianfeng Gao",
                    "William B Dolan"
                ],
                "title": "A diversity-promoting objective function for neural conversation models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_134",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, DailyDialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Yanran Li",
                    "Hui Su",
                    "Xiaoyu Shen",
                    "Wenjie Li",
                    "Ziqiang Cao",
                    "Shuzi Niu"
                ],
                "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_135",
            "content": "Zhuohan Li, Di He, Fei Tian, Tao Qin, Liwei Wang, Tie-Yan Liu, Hint-based training for non-autoregressive translation, 2019, NeuralIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Zhuohan Li",
                    "Di He",
                    "Fei Tian",
                    "Tao Qin",
                    "Liwei Wang",
                    "Tie-Yan Liu"
                ],
                "title": "Hint-based training for non-autoregressive translation",
                "pub_date": "2019",
                "pub_title": "NeuralIPS",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_136",
            "content": "UNKNOWN, None, 2020, Task-level curriculum learning for non-autoregressive neural machine translation, AAAI.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Task-level curriculum learning for non-autoregressive neural machine translation",
                "pub": "AAAI"
            }
        },
        {
            "ix": "24-ARR_v1_137",
            "content": "Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, Eduard Hovy, FlowSeq: Nonautoregressive conditional sequence generation with generative flow, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Xuezhe Ma",
                    "Chunting Zhou",
                    "Xian Li",
                    "Graham Neubig",
                    "Eduard Hovy"
                ],
                "title": "FlowSeq: Nonautoregressive conditional sequence generation with generative flow",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_138",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Myle Ott",
                    "Sergey Edunov",
                    "Alexei Baevski",
                    "Angela Fan",
                    "Sam Gross",
                    "Nathan Ng",
                    "David Grangier",
                    "Michael Auli"
                ],
                "title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_139",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_140",
            "content": "Kainan Peng, Wei Ping, Zhao Song, Kexin Zhao, Non-autoregressive neural text-to-speech, 2020-07, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Kainan Peng",
                    "Wei Ping",
                    "Zhao Song",
                    "Kexin Zhao"
                ],
                "title": "Non-autoregressive neural text-to-speech",
                "pub_date": "2020-07",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "24-ARR_v1_141",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Lihua Qian",
                    "Hao Zhou",
                    "Yu Bao",
                    "Mingxuan Wang",
                    "Lin Qiu",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Glancing transformer for non-autoregressive neural machine translation",
                "pub_date": "2021",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_142",
            "content": "UNKNOWN, None, 2021, The volctrans glat system: Nonautoregressive translation meets wmt21, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "The volctrans glat system: Nonautoregressive translation meets wmt21",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_143",
            "content": "UNKNOWN, None, 2018, Towards a better understanding of vector quantized autoencoders. arXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Towards a better understanding of vector quantized autoencoders. arXiv",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_144",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "24-ARR_v1_145",
            "content": "UNKNOWN, None, 2019, Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_146",
            "content": "Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, Zhi-Hong Deng, Fast structured decoding for sequence models, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Zhiqing Sun",
                    "Zhuohan Li",
                    "Haoqing Wang",
                    "Di He",
                    "Zi Lin",
                    "Zhi-Hong Deng"
                ],
                "title": "Fast structured decoding for sequence models",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_147",
            "content": "Zhiqing Sun, Yiming Yang, An em approach to non-autoregressive conditional sequence generation, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Zhiqing Sun",
                    "Yiming Yang"
                ],
                "title": "An em approach to non-autoregressive conditional sequence generation",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "24-ARR_v1_148",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "24-ARR_v1_149",
            "content": "Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Xu Sun, Imitation learning for nonautoregressive neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Bingzhen Wei",
                    "Mingxuan Wang",
                    "Hao Zhou",
                    "Junyang Lin",
                    "Xu Sun"
                ],
                "title": "Imitation learning for nonautoregressive neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "24-ARR_v1_150",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Chunting Zhou",
                    "Jiatao Gu",
                    "Graham Neubig"
                ],
                "title": "Understanding knowledge distillation in nonautoregressive machine translation",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "24-ARR_v1_0@0",
            "content": "Parallel Decoding Sequences by Glancing Discrete Latent Variables",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_2@0",
            "content": "Recently, parallel decoding of sentences has received widespread attention due to its success in decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_2",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_2@1",
            "content": "Many advanced techniques are proposed to improve its inferior quality, such as curriculum learning or introducing latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_2",
            "start": 118,
            "end": 248,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_2@2",
            "content": "However, we observe that these techniques still need an autoregressive teacher to help the model overcome the one-to-many multi-modal phenomenon in the dataset, which enlarges the training costs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_2",
            "start": 250,
            "end": 444,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_2@3",
            "content": "In this paper, we propose mix-GLT, which well combines the power of latent variable models and an advanced glancing training technique to alleviate the multi-modality problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_2",
            "start": 446,
            "end": 620,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_2@4",
            "content": "Experiments results on several representative text generation tasks show that our method is much better than the strong baselines and further improving the generality of the parallel decoding paradigm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_2",
            "start": 622,
            "end": 822,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_4@0",
            "content": "Non-autoregressive Transformer (NAT, Gu et al., 2018) introduce a parallel decoding paradigm with higher decoding efficiency (> 10\u00d7) than autoregressive models (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_4",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_4@1",
            "content": "Unlike autoregressive models, NAT models impose conditional independence assumptions in words to support parallel decoding of sentences during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_4",
            "start": 227,
            "end": 379,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_4@2",
            "content": "It attracts many researchers to explore NAT in machine translation (Gu et al., 2018;Lee et al., 2018;Kaiser et al., 2018) and text-to-speech tasks (Chen et al., 2019;Peng et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_4",
            "start": 381,
            "end": 565,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_5@0",
            "content": "Amount of researchers devoted themselves to improve the NATs' generation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_5",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_5@1",
            "content": "Such as modeling word inter-dependencies by curriculum learning (Guo et al., 2020a;Liu et al., 2020) or iterative refinements mechanism (Ghazvininejad et al., 2019;Guo et al., 2020b), introducing latent variables serve as the springboard for parallel decoding (Shu et al., 2019;Ma et al., 2019;Bao et al., 2021), or introduce inductive bias for NAT models' training (Wei et al., 2019;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_5",
            "start": 82,
            "end": 466,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_5@2",
            "content": "The most successful method is the glancing transformer (GLAT, Qian et al., 2021a), which trains the NAT model by sampling partial target words as inputs to predict the remaining words, explicitly building dependencies between the observed and unobserved words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_5",
            "start": 468,
            "end": 727,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_5@3",
            "content": "Qian et al. (2021b) employ GLAT to achieve a fantastic result, even outperforming many strong autoregressive translation systems in BLEU score (Papineni et al., 2002) on the German-English translation task of WMT21 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_5",
            "start": 729,
            "end": 946,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_6@0",
            "content": "Although they achieve competitive results compared to autoregressive models in translation tasks, we observe that they still need the help of an autoregressive Transformer (AT, Vaswani et al., 2017) as a teacher for training, i.e., sequence-level knowledge distillation (Kim and Rush, 2016). ; Sun and Yang (2020) point out that the multi-modality phenomenon, namely, each input may have multiple valid outputs in datasets, preventing NAT models from learning to organize consistent outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_6",
            "start": 0,
            "end": 490,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_6@1",
            "content": "Training with the outputs of an AT can alleviate the multi-modality problem by filtering the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_6",
            "start": 492,
            "end": 597,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_7@0",
            "content": "However, training NAT models with an AT model is quite limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_7",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_7@1",
            "content": "Including knowledge distillation via an AT model significantly enlarges the training cost due to its extra training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_7",
            "start": 64,
            "end": 184,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_7@2",
            "content": "Besides, the AT models may not be accurate enough in other text generation settings except machine translation, which will become the bottleneck for its student NAT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_7",
            "start": 186,
            "end": 356,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_7@3",
            "content": "Therefore, training a model from scratch without an AT model is an open and exciting challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_7",
            "start": 358,
            "end": 452,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_8@0",
            "content": "In this paper, we propose mix-GLT, which can directly learn from the raw dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_8",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_8@1",
            "content": "To overcome the multi-modality problem, we follow a divideand-conquer spirit, introducing a small set of discrete latent variables to divide the origin goal into latent variable modeling and sentence modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_8",
            "start": 82,
            "end": 290,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_9@0",
            "content": "Each latent variable sequence discretized from sentence are information to determine the mode of the sentence, which effectively reduces the multimodality problem at the sentence level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_9",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_9@1",
            "content": "Also, these latent variables will have fewer modes than origin sentences, namely, fewer multi-modality phenomena, which can be modeled with a glancing transformer (Qian et al., 2021a) directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_9",
            "start": 186,
            "end": 378,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_9@2",
            "content": "Finally, we extend glancing training with the latent variables to model the sentence, encouraging the model to build dependencies on latent variables rather than specific words, which works more robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_9",
            "start": 380,
            "end": 581,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_10@0",
            "content": "We conduct experiments on various text generation tasks, including machine translation, paraphrase generation, and dialog generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_10",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_10@1",
            "content": "Experiments results show that mix-GLT achieves remarkable improvements over several strong baselines, verify the effectiveness of mix-GLT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_10",
            "start": 134,
            "end": 271,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_10@2",
            "content": "In-depth analyses and ablation studies indicate that the introduced latent variables and glancing training are necessary for performance improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_10",
            "start": 273,
            "end": 421,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_11@0",
            "content": "Parallel Decoding in Sequence Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_11",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_12@0",
            "content": "A sequence-to-sequence model aims to parameterize p(Y|X ), which defines the probability of target sentence",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_12",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_13@0",
            "content": "Y = (y 1 , y 2 , \u2022 \u2022 \u2022 , y m ) given input sen- tence X = (x 1 , x 2 , \u2022 \u2022 \u2022 , x n ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_13",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_14@0",
            "content": "where n and m are the sentence lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_14",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_14@1",
            "content": "Dozens of researches (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017) factorize p(Y|X ) with a series of conditional probability:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_14",
            "start": 40,
            "end": 185,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_15@0",
            "content": "p AT (Y|X ) = m t=1 p(y t |y <t , X ),(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_15",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_16@0",
            "content": "where y <t = (y 1 , y 2 , \u2022 \u2022 \u2022 , y t\u22121 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_16",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_16@1",
            "content": "Such autoregressive factorization predicts words 2 based on its history predictions, which exists the risk of exposure bias and slow decoding during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_16",
            "start": 43,
            "end": 201,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_17@0",
            "content": "Non-autoregressive Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_17",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_17@1",
            "content": "To tackle the above problems, Gu et al. (2018) firstly propose non-autoregressive Transformer (NAT), removing y <t and factorizing p(Y|X ) as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_17",
            "start": 32,
            "end": 173,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_18@0",
            "content": "p NAT (Y|X ) = m t=1 p(y t |X ),(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_18",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_19@0",
            "content": "where each word y t are modeled independently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_19",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_20@0",
            "content": "During inference, NAT model decodes a sentence by arg max yt p(y t |X ) for each position t, remarkably improving the efficiency (15\u00d7 speedups).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_20",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_20@1",
            "content": "However, the efficiency improvements of NAT are at the cost of its quality, e.g., the performance degradation by more than 10.0 BLEU (Papineni et al., 2002) points in machine translation tasks (Gu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_20",
            "start": 145,
            "end": 355,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_20@2",
            "content": "The independence assumption prevents the NAT model from leveraging the inherent word dependencies to organize consistent outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_20",
            "start": 357,
            "end": 485,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_21@0",
            "content": "Glancing Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_21",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_21@1",
            "content": "Qian et al. (2021a) propose Glancing Transformer (GLAT), which introduces glancing training (GLT) and trains NAT predictions with:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_21",
            "start": 22,
            "end": 151,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_22@0",
            "content": "L GLT = \u2212 log p(Y obs |Y obs , X ) = \u2212 y i \u2208Y obs log p(y i |Y obs , X ),(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_22",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_23@0",
            "content": "where Y obs is the partial target tokens, and Y obs is its complements set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_23",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_24@0",
            "content": "During training, GLT samples Y obs according to the model's prediction: sample fewer target words for well-predicted cases and sample more words for worse-predicted cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_24",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_24@1",
            "content": "Moreover, it progressively decreases the sampling ratio and obtains better performances in machine translation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_24",
            "start": 172,
            "end": 288,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_25@0",
            "content": "Nevertheless, we experimentally find that GLAT still has a multi-modal problem 3 : First, the sampling rate cannot be reduced too small during training; otherwise, the performance will be close to the NAT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_25",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_25@1",
            "content": "Second, it still needs a teacher model for further improvements (Qian et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_25",
            "start": 212,
            "end": 296,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_26@0",
            "content": "Latent Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_26",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_26@1",
            "content": "Kaiser et al. (2018); Shu et al. (2019); Ma et al. (2019); Bao et al. (2021) propose Latent Transformer (LT), introducing latent variables z for NAT predictions as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_26",
            "start": 20,
            "end": 183,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_27@0",
            "content": "p LT (Y|X ) = z p(z|X ) \u2022 m t=1 p(y t |z, X ). (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_27",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_28@0",
            "content": "As observed, LT models first predict the latent variables with p(z|X ), then simultaneously decode the sentence by arg max yt p(y t |z, X ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_28",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_29@0",
            "content": "In practice, LT models are always trained by variational inference (Shu et al., 2019;Ma et al., 2019) or discretization techniques (Kaiser et al., 2018;Bao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_29",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_29@1",
            "content": "Such latent variables are decomposed from the target sentence, which is informative to determine the mode of the sentence and alleviates the multi-modality problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_29",
            "start": 171,
            "end": 335,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_30@0",
            "content": "Although LT models improve performance in terms of BLEU score, the used autoregressive predictor (Kaiser et al., 2018;Bao et al., 2021) or deep iterative transformation (Shu et al., 2019;Ma et al., 2019) for predicting latent variables unavoidable sacrifice the overall decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_30",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_30@1",
            "content": "In addition, they do not explicitly build the interdependencies among the outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_30",
            "start": 291,
            "end": 372,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_31@0",
            "content": "3 Proposed Method: mix-GLT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_31",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_32@0",
            "content": "In this section, we present mix-GLT in detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_32",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_32@1",
            "content": "mix-GLT follows Latent Transformer models (Kaiser et al., 2018;Bao et al., 2021) but introduces glancing training (Qian et al., 2021a) with the discrete latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_32",
            "start": 47,
            "end": 216,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_32@2",
            "content": "Our intuitions are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_32",
            "start": 218,
            "end": 247,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_33@0",
            "content": "As aforementioned in Section \u00a72, we can decompose the original multi-modal targets with a small set of latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_33",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_33@1",
            "content": "The decomposed latent variables are informative to determine the mode of the sentence and alleviate its multi-modality issues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_33",
            "start": 121,
            "end": 246,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_33@2",
            "content": "In such a case, we can incorporate glancing training to directly model the reduced-modality target sentences based on the latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_33",
            "start": 248,
            "end": 386,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_34@0",
            "content": "Model Structure",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_34",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_35@0",
            "content": "In this part, we state the structure of mix-GLT, which introduces a small set of discrete latent variables for a NAT model, basically following Kaiser et al. (2018); ; Bao et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_35",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_36@0",
            "content": "Let K be the size of the discrete latent space and let [K] denote the set {1, 2, \u2022 \u2022 \u2022 , K}. For each target sentence Y = (y 1 , y 2 , \u2022 \u2022 \u2022 , y m ), we use a same-length latent variable sequence for modeling it as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_36",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_37@0",
            "content": "p(Y|X ) = z p \u03b8 (z|X ) \u2022 m t=1 p \u03b8 (y t |z, X ), (5) where z = (z 1 , z 2 , \u2022 \u2022 \u2022 , z m ) and z i \u2208 [K], \u03b8 is the model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_37",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_38@0",
            "content": "Maximizing the log-likelihood of Eqn. ( 5) is unpractical due to its exponential searching spaces and complex decoding process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_38",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_38@1",
            "content": "Therefore, we follow Kaiser et al. (2018) and adopt a discretization technique to train the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_38",
            "start": 128,
            "end": 225,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_39@0",
            "content": "For discretizing target sentences to latent variables, we use vector quantization , which works by dividing a large set of origin vector representations into small groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_39",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_39@1",
            "content": "Specifically, we assign each token y i with a group j \u2208 [K] that has the nearest distance to its representation:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_39",
            "start": 172,
            "end": 283,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_40@0",
            "content": "z i = arg min j\u2208[K] || repr(y i ) \u2212 q j || 2 ,(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_40",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_41@0",
            "content": "Mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_41",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_41@1",
            "content": "Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_41",
            "start": 5,
            "end": 11,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_42@0",
            "content": "e 1 e 2 e 3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_42",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_43@0",
            "content": "y 1 y 3 y 4 y 2 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention + + + + Predict Length & Softcopy Vector Quantization z 1 z + + + + Mix. Decoder y 1 y 3 h h 2 y 3 y 2 y 1 y Attention Glancing h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (a) mix-GLT Architecture NAT Encoder e 1 e 2 e 3 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention Predict Length & Softcopy Qu z 1 z 4 + + + + y 1 y 3 h 4 h 2 G h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (b) LLP for Latent Predictor Mix. Decoder NAT Encoder e 1 e 2 e 3 y 1 y 3 y 4 y 2 x 1 x 3 x 2 h 1 h 3 h 4 h 2 NAT Predictor z 1 z 3 z 4 z 2 Attention + + + + Predict Length & Softcopy Vector Quantization z 1 z 4 + + + + Mix. Decoder y 1 y 3 h 4 h 2 y 3 y 2 y 1 y 4 Attention Glancing h 3 h 2 z 1 z 4 NAT Predictor z 3 z 2 z 1 z 4 Glancing Attention (c) LMIX for Mixture Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_43",
            "start": 0,
            "end": 898,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_44@0",
            "content": "Figure 1: Model architecture of mix-GLT, and our introduced losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_44",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_45@0",
            "content": ": a gated network to positionwise combine q zi and h i for the Mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_45",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_45@1",
            "content": "Decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_45",
            "start": 68,
            "end": 75,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_46@0",
            "content": "where q \u2208 R K\u00d7D is the maintained representations and D is its dimension, and we use the embedding as repr(y i ), refer to Bao et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_46",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_46@1",
            "content": "Then, the model is trained to minimize",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_46",
            "start": 142,
            "end": 179,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_47@0",
            "content": "L = L rec + \u03b2 repr(y i ) \u2212 sg(q z i )(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_47",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_48@0",
            "content": "where L rec is the prediction loss for Y and z, and sg(\u2022) is the stop gradient operator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_48",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_49@0",
            "content": "The maintained representations q are updated with an exponential moving average over a minibatch of target tokens {y",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_49",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_50@0",
            "content": "1 , \u2022 \u2022 \u2022 , y i , \u2022 \u2022 \u2022 }: c j \u2190 \u03bbc j + (1 \u2212 \u03bb) i 1[z i = j], q j \u2190 \u03bbq j + (1 \u2212 \u03bb) i 1[z i = j] repr(y i ) c j (8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_50",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_51@0",
            "content": "where c j is assigned count for group j, and we set decay parameter \u03bb = 0.999 in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_51",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_52@0",
            "content": "Architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_52",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_52@1",
            "content": "As shown in Figure 1a, mix-GLT mainly consists of an encoder F enc (NAT Encoder), a latent predictor F LP (NAT Predictor), and a decoder F dec (Mix. Decoder).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_52",
            "start": 14,
            "end": 171,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_52@2",
            "content": "We parameterize them with the multi-head attention-based encoder or decoder, similar to Transformer (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_52",
            "start": 173,
            "end": 295,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_52@3",
            "content": "We formalize their functions as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_52",
            "start": 297,
            "end": 328,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_53@0",
            "content": "(e 1 , e 2 , \u2022 \u2022 \u2022 , e n ) \u2190 F enc (x 1 , x 2 , \u2022 \u2022 \u2022 , x n ), (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) \u2190 softcopy(e 1:n ), p \u03b8 (z|X ) \u2190 F LP (h 1:m , e 1:n ), p \u03b8 (Y|z, X ) \u2190 F dec (z 1:m , h 1:m , e 1:n ),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_53",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_54@0",
            "content": "where we use an extra module F len to predict the target length m and initialize the decoder inputs (Wei et al., 2019) mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_54",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_55@0",
            "content": "H = (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) with the softcopy",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_55",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_56@0",
            "content": "Training by Glancing Latent Variables",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_56",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_57@0",
            "content": "As our design, the discretized latent variables will have fewer modes than raw sentences, which can be trained directly without the help of distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_57",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_57@1",
            "content": "Notably, we model the latent variable sequence in a non-autoregressive fashion, in which we use a glancing training technique (Qian et al., 2021a) for optimizing it, as shown in Figure 1b:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_57",
            "start": 154,
            "end": 341,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_58@0",
            "content": "L LP = \u2212 log p \u03b8 (z obs |z obs , X )(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_58",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_59@0",
            "content": "where z obs is uniformly sampled from z, refer to Qian et al. (2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_59",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_59@1",
            "content": "We provide more details for glancing training with mix-GLT in Appendix B Our next insight is modeling the sentence using the sampled latent variables z obs rather than z, as we cannot predict the z exactly during inference:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_59",
            "start": 71,
            "end": 293,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_60@0",
            "content": "L GSZ = \u2212 log p \u03b8 (Y|z obs , X ).(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_60",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_61@0",
            "content": "We experimentally find Eqn. ( 11) works robustly and analyze it in Section ( \u00a7 4.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_61",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_61@1",
            "content": "As shown in Figure 1c, we eventually employ glancing training with target token for optimizing L GSZ , namely we optimize the Mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_61",
            "start": 85,
            "end": 214,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_61@2",
            "content": "Decoder by minimizing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_61",
            "start": 216,
            "end": 236,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_62@0",
            "content": "L MIX = \u2212 log p \u03b8 (Y obs |z obs , Y obs , X ),(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_62",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_63@0",
            "content": "where Y obs and z obs are the sampled target tokens and discrete latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_63",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_64@0",
            "content": "Overall Training Loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_64",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_64@1",
            "content": "Our full-fledged loss includes latent variable prediction, sentence reconstruction, and length prediction losses:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_64",
            "start": 23,
            "end": 135,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_65@0",
            "content": "L = L MIX + L LP + \u03b1L LEN ,(13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_65",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_66@0",
            "content": "where \u03b1 = 0.1 are the hyperparameters to adjust the importance of length prediction loss L LEN .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_66",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_67@0",
            "content": "Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_67",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_68@0",
            "content": "The inference process of mix-GLT includes length prediction, latent variables and sentence prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_68",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_69@0",
            "content": "For determining the target length, mix-GLT first predicts the target target length m with the length predictor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_69",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_69@1",
            "content": "To avoid the length prediction errors during inference, mix-GLT expands the length m to a ranges (we use",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_69",
            "start": 112,
            "end": 215,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_70@0",
            "content": "[m \u2212 3, \u2022 \u2022 \u2022 , m + 2], total six candidates in our experiments).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_70",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_71@0",
            "content": "Then, mix-GLT predicts the latent variables \u1e91 with arg max z p \u03b8 (z|X ) and sentence \u0176 with arg max Y p \u03b8 (Y| \u1e91, X ) for each candidate length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_71",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_72@0",
            "content": "Like Ma et al. (2019), mix-GLT ranks them by itself and chooses the highest score output with:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_72",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_73@0",
            "content": "\u0176 = arg max Y p \u03b8 (Y| \u1e91, X ) \u2022 \u03b3 |Y| (14",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_73",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_74@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_74",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_75@0",
            "content": "where \u03b3 is the length penalty ratio to avoid the length bias, and |Y| denotes the length of Y.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_75",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_76@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_76",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_77@0",
            "content": "We conduct experiments on several generation tasks, including machine translation, paraphrase, and dialog generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_77",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_78@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_78",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_79@0",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_79",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_79@1",
            "content": "We chose the most popular benchmarks for each task:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_79",
            "start": 9,
            "end": 59,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_80@0",
            "content": "\u2022 Machine Translation (MT): We follow previous practices in NAT models and use the WMT14 English (EN) \u2194 German (DE) corpus (4.5M sentence pairs) and the IWSLT14 German (DE) \u2192 English (EN) corpus (160K sentence pairs) to validate our proposed model. We obtain the datasets following the instruction open-sourced in fairseq 4 . In detail, we first tokenize the datasets with Moses script. Then, we use 37,000 and 10,000 operations to split the words into byte-pair encodings (BPE, Sennrich et al., 2016) in WMT14 and IWSLT14 datasets, respectively. We also share subword embeddings between the source and target language for each dataset. \u2022 Paraphrase Generation (PG): We use the Quora 5 dataset to evaluate the paraphrase generation task. The Quora dataset contains around 135K labeled paraphrases pairs. Following the standard dataset split, we sample 100K sentence pairs from the labeled paraphrases as training data and hold out 30K pairs for testing, the remaining about 5K pairs for validation. Like the MT tasks, we tokenize the corpus with Moses scripts and split the words into BPE units with total 32K operations. \u2022 Dialog Generation (DG): We conduct the dialog generation experiments on the DailyDialog dataset (Li et al., 2017). We obtain the The task of MT aims to transfer bilingual sentences with semantically invariant conditions. The PG task differs from machine translation and works on mode transformation in the same language, whose goal is to synthesize a sentence different from the original input but conveys the same meaning. The DG task is most challenging due to the limited corpus and complex background.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_80",
            "start": 0,
            "end": 1628,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_81@0",
            "content": "Implementations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_81",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_81@1",
            "content": "We mainly compare mix-GLT with Transformer (Vaswani et al., 2017), vanilla NAT (Gu et al., 2018), and GLAT (Qian et al., 2021a) models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_81",
            "start": 17,
            "end": 151,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_81@2",
            "content": "We implement them based on the open-source framework fairseq (Ott et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_81",
            "start": 153,
            "end": 232,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@0",
            "content": "For machine translation tasks, we use the base setting (d model = 512, d hidden = 2048, dropout = 0.1, n head = 8, and n layer = 6) of Transformer (Vaswani et al., 2017) for WMT14 dataset and a smaller setting (d model = 512, d hidden = 1024, dropout = 0.3, n head = 4, and n layer = 6) for IWSLT14 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@1",
            "content": "The number of layers in mix-GLT decoder and latent predictor are both set to 4 in experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 308,
            "end": 401,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@2",
            "content": "We use inverse square root learning rate scheduling for WMT14 and a linear annealing learning rate from 3.0 \u00d7 10 \u22124 to 1.0 \u00d7 10 \u22125 in 250K steps for IWSLT14.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 403,
            "end": 559,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@3",
            "content": "The models are optimized with Adam (Kingma and Ba, 2015) optimizer (\u03b2 1 = 0.9, \u03b2 2 = 0.999) in 300K steps for WMT14 and 250K steps for IWSLT14.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 561,
            "end": 703,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@4",
            "content": "As for the ratio \u03c4 that used in glancing sampling, we linear anneal the ratio from 0.5 to 0.3 in whole training steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 705,
            "end": 822,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_82@5",
            "content": "The mini-batch in each step consists of 2K MultiTurnDialogZoo tokens for IWSLT14 and 64K tokens for WMT14.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_82",
            "start": 824,
            "end": 929,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_83@0",
            "content": "Since the scale of the Quora and DailyDialog datasets are close to the IWSLT14, we keep the same setting to the IWSLT14, such as the Adam, learning rate (linear annealing from 3.0 \u00d7 10 \u22124 to 1.0 \u00d7 10 \u22125 ), and batch size (2K tokens).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_83",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@0",
            "content": "Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@1",
            "content": "To validate the effectiveness of our proposed method, we evaluate it in terms of quality and efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 12,
            "end": 115,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@2",
            "content": "We use tokenized and cased BLEU scores (Papineni et al., 2002) 7 to evaluate the generation quality of MT and PG tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 117,
            "end": 235,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@3",
            "content": "For dialog generation, we also include BLEU-1 and BLEU-2 scores for analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 237,
            "end": 313,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@4",
            "content": "Following the common practices (Gu et al., 2018;Qian et al., 2021a), we measure the decoding latency of each model by decoding sentence by sentence and compute the speedup compared with the autoregressive Transformer (AT) model to reflect its decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 315,
            "end": 577,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_84@5",
            "content": "We highlight the best NAT result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_84",
            "start": 579,
            "end": 611,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_85@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_85",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_86@0",
            "content": "We can see from Table 1 that our mix-GLT almost outperforms all the NAT baselines (NAT and GLAT) in generation quality on all tasks while keeping a competitive decoding speedup to the autoregressive counterpart.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_86",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@0",
            "content": "Machine Translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@1",
            "content": "As seen, without using an AT model as a teacher for training NAT models, the vanilla NAT and advanced GLAT model only obtain inferior generation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 21,
            "end": 173,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@2",
            "content": "In contrast, mix-GLT achieves competitive generation quality in machine translation tasks, indicating that the introduced latent variables effectively reduce the multi-modality issue and support glancing training well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 175,
            "end": 392,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@3",
            "content": "It narrows the performance gap between nonautoregressive decoding and autoregressive decoding from 11.46 (GLAT vs. AT) to 2.34 (mix-GLT vs. AT) BLEU points on WMT14 EN\u2192DE task while keeping a high-speed decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 394,
            "end": 616,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@4",
            "content": "Paraphrasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 618,
            "end": 630,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@5",
            "content": "Unlike the translation task, the performance gap between non-autoregressive and autoregressive decoding on the paraphrase generation task is minor (NAT vs. AT, \u22123.32 BLEU points, GLAT vs. AT, \u22120.96 BLEU points ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 632,
            "end": 843,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@6",
            "content": "Nevertheless, our introduced discrete latent variables still are helpful to obtain a better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 845,
            "end": 948,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_87@7",
            "content": "mix-GLT realizes a non-autoregressive model with better performance than the autoregressive model on Quora dataset (mix-GLT vs. AT, +1.14 BLEU points).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_87",
            "start": 950,
            "end": 1100,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@0",
            "content": "Dialog Generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@1",
            "content": "We can see a different trend on the DailyDialog dataset -an AT model performs poorly than NAT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 19,
            "end": 119,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@2",
            "content": "Both GLAT and mix-GLT outperform the AT model in BLEU-1, BLEU-2, and BLEU scores, indicating that these models recall more reference tokens and organize the tokens well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 121,
            "end": 289,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@3",
            "content": "We conjecture that the weak and indirect association between the inputs and outputs of the dialogue encourages the AT model to predict the tokens by paying more attention to their history outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 291,
            "end": 486,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@4",
            "content": "Finally, it may collapse into a target-side language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 488,
            "end": 546,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@5",
            "content": "In contrast, the NAT models do not have this fast track, pushing them to pay more attention to the inputs and recall more target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 548,
            "end": 683,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_88@6",
            "content": "We further find that there are so-called safe response (Li et al., 2016) in AT's outputs, which verify our conjecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_88",
            "start": 685,
            "end": 802,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_89@0",
            "content": "More Comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_89",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_89@1",
            "content": "we further compare the advanced NAT models that builds upon latent vari-Figure 2: BLEU scores and their relative decoding speedups of different models on WMT14 EN\u2192DE test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_89",
            "start": 18,
            "end": 192,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_89@2",
            "content": "Note that we evaluate the speedups with a single GTX 1080-Ti GPU and include the results with the same evaluating hardware for fair comparisons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_89",
            "start": 194,
            "end": 337,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_90@0",
            "content": "ables or iterative refinement in machine translation tasks:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_90",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_91@0",
            "content": "\u2022 NATs w/ latent variables: LV-NAR (Shu et al., 2019), SynST (Akoury et al., 2019), Flowseq (Ma et al., 2019), and CNAT (Bao et al., 2021). \u2022 Iterative NATs: CMLM (Ghazvininejad et al., 2019) and LevT (Gu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_91",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_92@0",
            "content": "It is shown in Table 2 that introducing latent variables (LV-NAR, Flowseq, and CNAT) or decoding with multiple iterations (CMLM and LevT) both improve non-autoregressive decoding in translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_92",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_92@1",
            "content": "However, iterative refinements or deep transformations always sacrifice decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_92",
            "start": 203,
            "end": 294,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_92@2",
            "content": "In contrast, the proposed mix-GLT outperforms all NAT models with a relatively low cost, keeping a competitive speedup over autoregressive Transformer (AT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_92",
            "start": 296,
            "end": 451,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_92@3",
            "content": "Specifically, mix-GLT with onepass decoding narrows the performance gap to the AT from 5.87 BLEU points to 2.34 BLEU points on the WMT14 EN\u2192DE test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_92",
            "start": 453,
            "end": 604,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_93@0",
            "content": "Decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_93",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_93@1",
            "content": "We can see there is a tradeoff between the translation quality and decoding efficiency in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_93",
            "start": 21,
            "end": 118,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_93@2",
            "content": "We thus present the scatter plot of different models in Figure 2, showing the trend of translation quality and decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_93",
            "start": 120,
            "end": 250,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_94@0",
            "content": "As seen, mix-GLT is located on the top-right of the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_94",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_94@1",
            "content": "It outperforms the baselines in the BLEU score if decoding speedup is fixed and in decoding speedup if the BLEU score is fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_94",
            "start": 63,
            "end": 189,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_95@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_95",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_96@0",
            "content": "mix-GLT largely alleviates the sentence-level multi-modal problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_96",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_96@1",
            "content": "Previous researches (Gu et al., 2018;Ma et al., 2019;Qian et al., 2021a;Bao et al., 2021) always utilize a Transformer model as a teacher for training NAT models, namely sequence-level knowledge distillation (Kim and Rush, 2016), which can directly reduces the sentence-level multi-modal phenomenon in datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_96",
            "start": 67,
            "end": 377,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_96@2",
            "content": "Therefore, we use the average gains from the knowledge distillation to reflect the ability of the NAT models to overcome this issue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_96",
            "start": 379,
            "end": 510,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_97@0",
            "content": "As seen in Table 3, the pure NAT models heavily rely on knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_97",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_97@1",
            "content": "By introducing the target information with the latent variables (Flowseq and CNAT) or sampled tokens (GLAT), the NAT models improve its' ability to overcome the multi-modality issue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_97",
            "start": 80,
            "end": 261,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_97@2",
            "content": "Our proposed mix-GLT well combines the above two techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_97",
            "start": 263,
            "end": 322,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_97@3",
            "content": "It obtains only 0.95 BLEU points average gains and validates our motivation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_97",
            "start": 324,
            "end": 399,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_98@0",
            "content": "Discrete latent variables have fewer modes than raw sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_98",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_98@1",
            "content": "To validate our intuition that the introduced latent variables are easier to predict than tokens, we refer to the complexity metrics on each dataset according to alignment relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_98",
            "start": 63,
            "end": 244,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_98@2",
            "content": "Specifically, we use the fast_align 8 toolkit to align source input X and target outputs Y or discretized latent variable sequences z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_98",
            "start": 246,
            "end": 379,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_98@3",
            "content": "Then, we compute the token-level complexity C TOK (d) and the sentence-level complexity C SEN (d) according to .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_98",
            "start": 381,
            "end": 492,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_98@4",
            "content": "These metrics can trivially understand as the number of valid candidates for each input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_98",
            "start": 494,
            "end": 581,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_99@0",
            "content": "As shown in Table 4, the latent variables have the lowest complexity in both token-level complexity and sentence-level complexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_99",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_99@1",
            "content": "In other words, predicting the latent variable sequences is effortless than predicting others, which is consistent with our intuition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_99",
            "start": 131,
            "end": 264,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_99@2",
            "content": "Although we obtain a lower complexity dataset by filtering the datasets with an autoregressive model (AT outputs versus Raw outputs), they may introduce model error and need extra training for AT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_99",
            "start": 266,
            "end": 467,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_99@3",
            "content": "In contrast, the discrete latent variables are simple and informative enough to serve as a springboard for modeling target sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_99",
            "start": 469,
            "end": 601,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_100@0",
            "content": "Glancing with latent variables improves the performance with a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_100",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_100@1",
            "content": "We can see in Effects of K and \u03b3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_100",
            "start": 77,
            "end": 109,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_100@2",
            "content": "As shown in Figure 3 and Table 6, we search the hyper-parameter of mix-GLT that the number of discrete latent variables and the length penalty ratio \u03b3 according to the validation performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_100",
            "start": 111,
            "end": 301,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_100@3",
            "content": "We notice that using more latent codes causes performance degradation during inference, in which the latent variables may degenerate to tokens and contains more prediction error during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_100",
            "start": 303,
            "end": 497,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_100@4",
            "content": "The mix-GLT implemented with 64 latent variables and \u03b3 = 1.1 obtains the best result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_100",
            "start": 499,
            "end": 583,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_101@0",
            "content": "5 Related Work Gu et al. (2018) first propose a non-autoregressive Transformer (NAT) model for neural machine translation (NMT) and begin to explore parallel decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_101",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_101@1",
            "content": "It abandons explicitly modeling word interdependencies to decode the tokens in parallel, significantly improving the inference speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_101",
            "start": 168,
            "end": 300,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_101@2",
            "content": "However, its translation quality is inferior to the Transformer (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_101",
            "start": 302,
            "end": 388,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_102@0",
            "content": "To alleviate this performance degradation, many researchers work to enhance word dependency modeling, including imitation learning (Wei et al., 2019;, curriculum learning (Guo et al., 2020a;Liu et al., 2020), iterative refinements (Lee et al., 2018;Ghazvininejad et al., 2019;Gu et al., 2019;Guo et al., 2020b), and a simplified autoregressive process .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_102",
            "start": 0,
            "end": 352,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_102@1",
            "content": "The most representative method is the glancing transformer model (Qian et al., 2021a), which adaptively and progressively samples partial tokens as inputs and predicts the remaining tokens, effectively establishing the dependencies between the sampled tokens and the remaining tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_102",
            "start": 354,
            "end": 637,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_102@2",
            "content": "However, these models still rely on a teacher for training, which cannot directly learn the raw dataset that contains one-tomany multi-modality phenomenon.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_102",
            "start": 639,
            "end": 793,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_103@0",
            "content": "Our method introduces latent variables to NAT models, which are close to Kaiser et al. (2018) Bao et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_103",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_103@1",
            "content": "These methods decompose the latent variables (hints) from the target sentence and divide the origin goal into two parts: modeling latent variables and modeling the target sentences based on latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_103",
            "start": 113,
            "end": 319,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_103@2",
            "content": "It implicitly overcomes the multi-modality phenomenon of target sentences because the latent variables can largely determine the mode of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_103",
            "start": 321,
            "end": 470,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_103@3",
            "content": "However, these methods always model the latent variables with an autoregressive predictor, which naturally sacrifices the decoding efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_103",
            "start": 472,
            "end": 613,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_104@0",
            "content": "Unlike them, our approach models the discrete latent variables in a non-autoregressive fashion and extends glancing training with the discrete latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_104",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_104@1",
            "content": "As a result, mix-GLT accomplishes a competitive performance both in decoding efficiency and quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_104",
            "start": 161,
            "end": 260,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_105@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_105",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_106@0",
            "content": "We propose mix-GLT, which can be directly trained without the help of knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_106",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_106@1",
            "content": "Specifically, we employ discrete latent variables to divide the NAT prediction to the latent variables modeling and sentence reconstruction tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_106",
            "start": 94,
            "end": 239,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_106@2",
            "content": "Then, we learn each task with the glancing training and encourages the model to build dependencies on the latent variables, which have few modes and are informative to modeling the target sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_106",
            "start": 241,
            "end": 438,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_106@3",
            "content": "Experiments results on several representative text generation tasks validate the effectiveness of that our mix-GLT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_106",
            "start": 440,
            "end": 554,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_107@0",
            "content": "We can see in Figure 1a that the performance of a GLAT model will degenerate to that of a NAT model while the sampling ratio annealing too small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_107",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_107@1",
            "content": "In addition, introducing the knowledge distillation for training the GLAT model alleviates this issue (Figure 1b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_107",
            "start": 146,
            "end": 259,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_107@2",
            "content": "B Structure Details of mix-GLT Softcopy Inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_107",
            "start": 261,
            "end": 307,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_107@3",
            "content": "mix-GLT uses Softcopy (Wei et al., 2019; mechanism to obtain the inputs H = (h 1 , h 2 , \u2022 \u2022 \u2022 , h m ) as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_107",
            "start": 309,
            "end": 414,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_108@0",
            "content": "h i = n i \u03b1 ij \u2022 e i , \u03b1 ij \u221d exp [\u2212(i \u2212 j \u2022 n m ) 2 ],(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_108",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_109@0",
            "content": "where E = (e 1 , e 2 , \u2022 \u2022 \u2022 , e n ) is the contextual representation of X encoded by the NAT encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_109",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_110@0",
            "content": "Glancing Training for Latent Predictor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_110",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_110@1",
            "content": "With the decoder input H = h 1:m and the discretized latent variable sequence z = z 1:m , we adopt the glancing sampling technique for training the latent predictor in the following steps:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_110",
            "start": 40,
            "end": 227,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_111@0",
            "content": "\u2022 Predicting \u1e91: mix-GLT predicts the latent variable sequence with its latent predictor: \u1e91 \u2190 F LP (h 1:m , e 1:n ). \u2022 Determining sample number N z : Given z and \u1e91, we compute the sampling number as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_111",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_112@0",
            "content": "N z = \u03c4 \u2022 Hamming(z, \u1e91)(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_112",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_113@0",
            "content": "where \u03c4 is the sampling ratio decreasing in the training steps, and we use Hamming distance (Hamming, 1950) for measuring the prediction quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_113",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_114@0",
            "content": "\u2022 Glancing reference: The glancing reference z obs is obtained by random selecting N z elements from reference sequence z. \u2022 Re-constructing inputs H : We construct H by position-wise replacing the decoder input h 1:m with z obs . \u2022 Updating Latent Predictor: With the H as inputs, We then train the latent predictor to predict the unobserved references z obs .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_114",
            "start": 0,
            "end": 360,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_115@0",
            "content": "Glancing Training for Mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_115",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_115@1",
            "content": "Decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_115",
            "start": 27,
            "end": 34,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_115@2",
            "content": "Glancing training for Mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_115",
            "start": 36,
            "end": 61,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_115@3",
            "content": "Decoder is largely follow the Qian et al. (2021a), except using extra latent variables as inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_115",
            "start": 63,
            "end": 159,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_115@4",
            "content": "With the decoder input H = h 1:m , the reference sentence Y, and the glancing latent variable sequence z obs , we train it in the following steps:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_115",
            "start": 161,
            "end": 306,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_116@0",
            "content": "\u2022 Predicting \u0176: mix-GLT predicts the target sentences: \u0176 \u2190 F dec (z obs , h 1:m , e 1:n ). \u2022 Determining sample number N y : Given Y and \u0176, we compute the sampling number N y = \u03c4 \u2022 Hamming(Y, \u0176). \u2022 Glancing reference: The glancing reference Y obs is obtained by random selecting N y tokens from reference sequence Y. \u2022 Re-constructing inputs H : H is constructed by position-wise replacing the decoder input H with embedding of Y obs . \u2022 Updating Mix. Decoder: We then train the Mix. Decoder to predict the unobserved references Y obs , with the H and z obs as inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_116",
            "start": 0,
            "end": 567,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_117@0",
            "content": "Nader Akoury, Kalpesh Krishna, Mohit Iyyer, Syntactically supervised transformers for faster neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_117",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_118@0",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_118",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_119@0",
            "content": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, PLATO: Pre-trained dialogue generation model with discrete latent variable, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_119",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_120@0",
            "content": "UNKNOWN, None, 2021, Nonautoregressive translation by learning target categorical codes, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_120",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_121@0",
            "content": "UNKNOWN, None, 2019, Listen and fill in the missing letters: Non-autoregressive transformer for speech recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_121",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_122@0",
            "content": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin, Convolutional sequence to sequence learning, 2017-08-11, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_122",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_123@0",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_123",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_124@0",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Nonautoregressive neural machine translation, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_124",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_125@0",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, Vancouver.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_125",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_126@0",
            "content": "Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong Chen, Tie-Yan Liu, Fine-tuning by curriculum learning for non-autoregressive neural machine translation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_126",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_127@0",
            "content": "Junliang Guo, Linli Xu, Enhong Chen, Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_127",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_128@0",
            "content": "UNKNOWN, None, 1950, Error detecting and error correcting codes. The Bell system technical journal, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_128",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_129@0",
            "content": "Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer, Fast decoding in sequence models using discrete latent variables, 2018-07-10, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_129",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_130@0",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_130",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_131@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_131",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_132@0",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_132",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_133@0",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_133",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_134@0",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, DailyDialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_134",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_135@0",
            "content": "Zhuohan Li, Di He, Fei Tian, Tao Qin, Liwei Wang, Tie-Yan Liu, Hint-based training for non-autoregressive translation, 2019, NeuralIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_135",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_136@0",
            "content": "UNKNOWN, None, 2020, Task-level curriculum learning for non-autoregressive neural machine translation, AAAI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_136",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_137@0",
            "content": "Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, Eduard Hovy, FlowSeq: Nonautoregressive conditional sequence generation with generative flow, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_137",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_138@0",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_138",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_139@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_139",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_140@0",
            "content": "Kainan Peng, Wei Ping, Zhao Song, Kexin Zhao, Non-autoregressive neural text-to-speech, 2020-07, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_140",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_141@0",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_141",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_142@0",
            "content": "UNKNOWN, None, 2021, The volctrans glat system: Nonautoregressive translation meets wmt21, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_142",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_143@0",
            "content": "UNKNOWN, None, 2018, Towards a better understanding of vector quantized autoencoders. arXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_143",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_144@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_144",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_145@0",
            "content": "UNKNOWN, None, 2019, Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_145",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_146@0",
            "content": "Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, Zhi-Hong Deng, Fast structured decoding for sequence models, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_146",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_147@0",
            "content": "Zhiqing Sun, Yiming Yang, An em approach to non-autoregressive conditional sequence generation, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_147",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_148@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_148",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_149@0",
            "content": "Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Xu Sun, Imitation learning for nonautoregressive neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_149",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "24-ARR_v1_150@0",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "24-ARR_v1_150",
            "start": 0,
            "end": 190,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_1",
            "tgt_ix": "24-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_1",
            "tgt_ix": "24-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_4",
            "tgt_ix": "24-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_5",
            "tgt_ix": "24-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_6",
            "tgt_ix": "24-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_7",
            "tgt_ix": "24-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_8",
            "tgt_ix": "24-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_9",
            "tgt_ix": "24-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_10",
            "tgt_ix": "24-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_12",
            "tgt_ix": "24-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_13",
            "tgt_ix": "24-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_14",
            "tgt_ix": "24-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_15",
            "tgt_ix": "24-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_16",
            "tgt_ix": "24-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_17",
            "tgt_ix": "24-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_18",
            "tgt_ix": "24-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_19",
            "tgt_ix": "24-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_20",
            "tgt_ix": "24-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_21",
            "tgt_ix": "24-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_22",
            "tgt_ix": "24-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_23",
            "tgt_ix": "24-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_24",
            "tgt_ix": "24-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_25",
            "tgt_ix": "24-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_26",
            "tgt_ix": "24-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_27",
            "tgt_ix": "24-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_28",
            "tgt_ix": "24-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_29",
            "tgt_ix": "24-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_30",
            "tgt_ix": "24-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_31",
            "tgt_ix": "24-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_32",
            "tgt_ix": "24-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_33",
            "tgt_ix": "24-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_35",
            "tgt_ix": "24-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_36",
            "tgt_ix": "24-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_37",
            "tgt_ix": "24-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_38",
            "tgt_ix": "24-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_39",
            "tgt_ix": "24-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_40",
            "tgt_ix": "24-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_42",
            "tgt_ix": "24-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_43",
            "tgt_ix": "24-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_44",
            "tgt_ix": "24-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_45",
            "tgt_ix": "24-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_46",
            "tgt_ix": "24-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_47",
            "tgt_ix": "24-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_48",
            "tgt_ix": "24-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_49",
            "tgt_ix": "24-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_50",
            "tgt_ix": "24-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_51",
            "tgt_ix": "24-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_52",
            "tgt_ix": "24-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_53",
            "tgt_ix": "24-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_54",
            "tgt_ix": "24-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_41",
            "tgt_ix": "24-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_55",
            "tgt_ix": "24-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_57",
            "tgt_ix": "24-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_58",
            "tgt_ix": "24-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_59",
            "tgt_ix": "24-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_60",
            "tgt_ix": "24-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_61",
            "tgt_ix": "24-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_62",
            "tgt_ix": "24-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_63",
            "tgt_ix": "24-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_64",
            "tgt_ix": "24-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_65",
            "tgt_ix": "24-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_66",
            "tgt_ix": "24-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_68",
            "tgt_ix": "24-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_69",
            "tgt_ix": "24-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_70",
            "tgt_ix": "24-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_71",
            "tgt_ix": "24-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_72",
            "tgt_ix": "24-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_73",
            "tgt_ix": "24-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_74",
            "tgt_ix": "24-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_75",
            "tgt_ix": "24-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_77",
            "tgt_ix": "24-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_79",
            "tgt_ix": "24-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_81",
            "tgt_ix": "24-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_83",
            "tgt_ix": "24-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_86",
            "tgt_ix": "24-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_89",
            "tgt_ix": "24-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_90",
            "tgt_ix": "24-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_92",
            "tgt_ix": "24-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_93",
            "tgt_ix": "24-ARR_v1_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_94",
            "tgt_ix": "24-ARR_v1_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_96",
            "tgt_ix": "24-ARR_v1_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_97",
            "tgt_ix": "24-ARR_v1_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_99",
            "tgt_ix": "24-ARR_v1_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_101",
            "tgt_ix": "24-ARR_v1_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_102",
            "tgt_ix": "24-ARR_v1_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_103",
            "tgt_ix": "24-ARR_v1_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_104",
            "tgt_ix": "24-ARR_v1_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_107",
            "tgt_ix": "24-ARR_v1_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_108",
            "tgt_ix": "24-ARR_v1_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_109",
            "tgt_ix": "24-ARR_v1_110",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_110",
            "tgt_ix": "24-ARR_v1_111",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_112",
            "tgt_ix": "24-ARR_v1_113",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_113",
            "tgt_ix": "24-ARR_v1_114",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_116",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_110",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_111",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_112",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_113",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_114",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_115",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_116",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_106",
            "tgt_ix": "24-ARR_v1_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "24-ARR_v1_0",
            "tgt_ix": "24-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_1",
            "tgt_ix": "24-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_2",
            "tgt_ix": "24-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_3",
            "tgt_ix": "24-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_4",
            "tgt_ix": "24-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_4",
            "tgt_ix": "24-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_4",
            "tgt_ix": "24-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_5",
            "tgt_ix": "24-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_5",
            "tgt_ix": "24-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_5",
            "tgt_ix": "24-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_5",
            "tgt_ix": "24-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_6",
            "tgt_ix": "24-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_6",
            "tgt_ix": "24-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_7",
            "tgt_ix": "24-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_7",
            "tgt_ix": "24-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_7",
            "tgt_ix": "24-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_7",
            "tgt_ix": "24-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_8",
            "tgt_ix": "24-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_8",
            "tgt_ix": "24-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_9",
            "tgt_ix": "24-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_9",
            "tgt_ix": "24-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_9",
            "tgt_ix": "24-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_10",
            "tgt_ix": "24-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_10",
            "tgt_ix": "24-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_10",
            "tgt_ix": "24-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_11",
            "tgt_ix": "24-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_12",
            "tgt_ix": "24-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_13",
            "tgt_ix": "24-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_14",
            "tgt_ix": "24-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_14",
            "tgt_ix": "24-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_15",
            "tgt_ix": "24-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_16",
            "tgt_ix": "24-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_16",
            "tgt_ix": "24-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_17",
            "tgt_ix": "24-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_17",
            "tgt_ix": "24-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_18",
            "tgt_ix": "24-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_19",
            "tgt_ix": "24-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_20",
            "tgt_ix": "24-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_20",
            "tgt_ix": "24-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_20",
            "tgt_ix": "24-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_21",
            "tgt_ix": "24-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_21",
            "tgt_ix": "24-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_22",
            "tgt_ix": "24-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_23",
            "tgt_ix": "24-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_24",
            "tgt_ix": "24-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_24",
            "tgt_ix": "24-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_25",
            "tgt_ix": "24-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_25",
            "tgt_ix": "24-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_26",
            "tgt_ix": "24-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_26",
            "tgt_ix": "24-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_27",
            "tgt_ix": "24-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_28",
            "tgt_ix": "24-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_29",
            "tgt_ix": "24-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_29",
            "tgt_ix": "24-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_30",
            "tgt_ix": "24-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_30",
            "tgt_ix": "24-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_31",
            "tgt_ix": "24-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_32",
            "tgt_ix": "24-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_32",
            "tgt_ix": "24-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_32",
            "tgt_ix": "24-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_33",
            "tgt_ix": "24-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_33",
            "tgt_ix": "24-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_33",
            "tgt_ix": "24-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_34",
            "tgt_ix": "24-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_35",
            "tgt_ix": "24-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_36",
            "tgt_ix": "24-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_37",
            "tgt_ix": "24-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_38",
            "tgt_ix": "24-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_38",
            "tgt_ix": "24-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_39",
            "tgt_ix": "24-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_39",
            "tgt_ix": "24-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_40",
            "tgt_ix": "24-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_41",
            "tgt_ix": "24-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_41",
            "tgt_ix": "24-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_42",
            "tgt_ix": "24-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_43",
            "tgt_ix": "24-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_44",
            "tgt_ix": "24-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_45",
            "tgt_ix": "24-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_45",
            "tgt_ix": "24-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_46",
            "tgt_ix": "24-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_46",
            "tgt_ix": "24-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_47",
            "tgt_ix": "24-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_48",
            "tgt_ix": "24-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_49",
            "tgt_ix": "24-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_50",
            "tgt_ix": "24-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_51",
            "tgt_ix": "24-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_52",
            "tgt_ix": "24-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_52",
            "tgt_ix": "24-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_52",
            "tgt_ix": "24-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_52",
            "tgt_ix": "24-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_53",
            "tgt_ix": "24-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_54",
            "tgt_ix": "24-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_55",
            "tgt_ix": "24-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_56",
            "tgt_ix": "24-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_57",
            "tgt_ix": "24-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_57",
            "tgt_ix": "24-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_58",
            "tgt_ix": "24-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_59",
            "tgt_ix": "24-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_59",
            "tgt_ix": "24-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_60",
            "tgt_ix": "24-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_61",
            "tgt_ix": "24-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_61",
            "tgt_ix": "24-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_61",
            "tgt_ix": "24-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_62",
            "tgt_ix": "24-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_63",
            "tgt_ix": "24-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_64",
            "tgt_ix": "24-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_64",
            "tgt_ix": "24-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_65",
            "tgt_ix": "24-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_66",
            "tgt_ix": "24-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_67",
            "tgt_ix": "24-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_68",
            "tgt_ix": "24-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_69",
            "tgt_ix": "24-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_69",
            "tgt_ix": "24-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_70",
            "tgt_ix": "24-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_71",
            "tgt_ix": "24-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_72",
            "tgt_ix": "24-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_73",
            "tgt_ix": "24-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_74",
            "tgt_ix": "24-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_75",
            "tgt_ix": "24-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_76",
            "tgt_ix": "24-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_77",
            "tgt_ix": "24-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_78",
            "tgt_ix": "24-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_79",
            "tgt_ix": "24-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_79",
            "tgt_ix": "24-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_80",
            "tgt_ix": "24-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_81",
            "tgt_ix": "24-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_81",
            "tgt_ix": "24-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_81",
            "tgt_ix": "24-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_82",
            "tgt_ix": "24-ARR_v1_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_83",
            "tgt_ix": "24-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_84",
            "tgt_ix": "24-ARR_v1_84@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_85",
            "tgt_ix": "24-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_86",
            "tgt_ix": "24-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_87",
            "tgt_ix": "24-ARR_v1_87@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_88",
            "tgt_ix": "24-ARR_v1_88@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_89",
            "tgt_ix": "24-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_89",
            "tgt_ix": "24-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_89",
            "tgt_ix": "24-ARR_v1_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_90",
            "tgt_ix": "24-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_91",
            "tgt_ix": "24-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_92",
            "tgt_ix": "24-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_92",
            "tgt_ix": "24-ARR_v1_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_92",
            "tgt_ix": "24-ARR_v1_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_92",
            "tgt_ix": "24-ARR_v1_92@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_93",
            "tgt_ix": "24-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_93",
            "tgt_ix": "24-ARR_v1_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_93",
            "tgt_ix": "24-ARR_v1_93@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_94",
            "tgt_ix": "24-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_94",
            "tgt_ix": "24-ARR_v1_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_95",
            "tgt_ix": "24-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_96",
            "tgt_ix": "24-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_96",
            "tgt_ix": "24-ARR_v1_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_96",
            "tgt_ix": "24-ARR_v1_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_97",
            "tgt_ix": "24-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_97",
            "tgt_ix": "24-ARR_v1_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_97",
            "tgt_ix": "24-ARR_v1_97@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_97",
            "tgt_ix": "24-ARR_v1_97@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_98@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_98@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_98@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_98",
            "tgt_ix": "24-ARR_v1_98@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_99",
            "tgt_ix": "24-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_99",
            "tgt_ix": "24-ARR_v1_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_99",
            "tgt_ix": "24-ARR_v1_99@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_99",
            "tgt_ix": "24-ARR_v1_99@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_100@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_100@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_100@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_100",
            "tgt_ix": "24-ARR_v1_100@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_101",
            "tgt_ix": "24-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_101",
            "tgt_ix": "24-ARR_v1_101@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_101",
            "tgt_ix": "24-ARR_v1_101@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_102",
            "tgt_ix": "24-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_102",
            "tgt_ix": "24-ARR_v1_102@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_102",
            "tgt_ix": "24-ARR_v1_102@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_103",
            "tgt_ix": "24-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_103",
            "tgt_ix": "24-ARR_v1_103@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_103",
            "tgt_ix": "24-ARR_v1_103@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_103",
            "tgt_ix": "24-ARR_v1_103@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_104",
            "tgt_ix": "24-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_104",
            "tgt_ix": "24-ARR_v1_104@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_105",
            "tgt_ix": "24-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_106",
            "tgt_ix": "24-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_106",
            "tgt_ix": "24-ARR_v1_106@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_106",
            "tgt_ix": "24-ARR_v1_106@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_106",
            "tgt_ix": "24-ARR_v1_106@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_107",
            "tgt_ix": "24-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_107",
            "tgt_ix": "24-ARR_v1_107@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_107",
            "tgt_ix": "24-ARR_v1_107@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_107",
            "tgt_ix": "24-ARR_v1_107@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_108",
            "tgt_ix": "24-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_109",
            "tgt_ix": "24-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_110",
            "tgt_ix": "24-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_110",
            "tgt_ix": "24-ARR_v1_110@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_111",
            "tgt_ix": "24-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_112",
            "tgt_ix": "24-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_113",
            "tgt_ix": "24-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_114",
            "tgt_ix": "24-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_115@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_115@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_115@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_115",
            "tgt_ix": "24-ARR_v1_115@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_116",
            "tgt_ix": "24-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_117",
            "tgt_ix": "24-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_118",
            "tgt_ix": "24-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_119",
            "tgt_ix": "24-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_120",
            "tgt_ix": "24-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_121",
            "tgt_ix": "24-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_122",
            "tgt_ix": "24-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_123",
            "tgt_ix": "24-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_124",
            "tgt_ix": "24-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_125",
            "tgt_ix": "24-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_126",
            "tgt_ix": "24-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_127",
            "tgt_ix": "24-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_128",
            "tgt_ix": "24-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_129",
            "tgt_ix": "24-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_130",
            "tgt_ix": "24-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_131",
            "tgt_ix": "24-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_132",
            "tgt_ix": "24-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_133",
            "tgt_ix": "24-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_134",
            "tgt_ix": "24-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_135",
            "tgt_ix": "24-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_136",
            "tgt_ix": "24-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_137",
            "tgt_ix": "24-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_138",
            "tgt_ix": "24-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_139",
            "tgt_ix": "24-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_140",
            "tgt_ix": "24-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_141",
            "tgt_ix": "24-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_142",
            "tgt_ix": "24-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_143",
            "tgt_ix": "24-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_144",
            "tgt_ix": "24-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_145",
            "tgt_ix": "24-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_146",
            "tgt_ix": "24-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_147",
            "tgt_ix": "24-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_148",
            "tgt_ix": "24-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_149",
            "tgt_ix": "24-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "24-ARR_v1_150",
            "tgt_ix": "24-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1324,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "24-ARR",
        "version": 1
    }
}