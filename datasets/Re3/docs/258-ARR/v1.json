{
    "nodes": [
        {
            "ix": "258-ARR_v1_0",
            "content": "Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_2",
            "content": "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "258-ARR_v1_4",
            "content": "Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from. Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_5",
            "content": "In this paper, we propose a strategy to construct open cloze exercises using transformer models (Vaswani et al., 2017). Our transformer-based architecture employs two objectives to predict the words that should be gapped in a text passage. One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not. The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap. Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_6",
            "content": "Our output aims to mimic the style of open cloze tests in the First Certificate in English (FCE) exam 1 , which is targeted at learners of English at the B2 proficiency level of the Common European Framework of Reference (CEFR) for languages (Council of Europe, 2001). Unlike other tests, the FCE open cloze task aims to simultaneously test many aspects of grammar and vocabulary that students are expected to know at this level. Since the tests are created from a text passage, they must be skilfully designed in order to ensure an optimal distribution of gaps that adheres to guidelines. A shortened example is shown in Figure 1. Our system is evaluated under two settings: 1) automatic evaluation, where the generated gaps are compared to goldstandard gaps proposed by test experts, and 2) human evaluation, where the quality of the generated gaps is judged by test experts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_7",
            "content": "The main contributions of our work are as follows: 1) we are the first to employ transformer models for open cloze test generation, 2) unlike previous studies, we work at the paragraph level, which is a much more challenging task, 3) we propose a multi-task learning approach with two objectives: one is to classify tokens into gaps/non-gaps and the other to minimise the error of re-generating the gapped word, 4) we report state-of-the-art results, outperforming previous work and a strong baseline, 5) we propose additional components to control the structure of the final cloze tests as human experts do, 6) we perform both automatic and human evaluation and 7) make our test data, system output and human annotations available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "258-ARR_v1_9",
            "content": "While research into automatic cloze test generation is vast (Mostow et al., 2017;Kurdi et al., 2020;Yang et al., 2021), work on open cloze tests for language learning is scarce. Pino et al. (2008) generate open cloze questions using sample sen- I first saw some guys doing motorbike stunts. I'd never seen anyone riding a motorbike using just the back wheel before and I was (3) impressed I went straight home and taught (4) to do the same. tences from a learners' dictionary based on four linguistic criteria: (grammatical) complexity, welldefined context (collocations), grammaticality and length. A later version of their system adds hints for gapped words (Pino and Eskenazi, 2009). Exercise Maker (Malafeev, 2014) is a rule-based open source system that attempts to emulate exercises in Cambridge English examinations based on the most frequently tested words. Most of the gaps it proposes were found to be useful and the automated exercises were hard to differentiate from authentic tests. Chinkina et al. (2017) generate open cloze exercises for phrasal verbs by extracting sentences from news articles and generating a pair of questions and answers where the identified particle verbs are gapped. Similarly, Soonklang et al. (2017) gap words in sentences according to their part of speech in order to practise articles, prepositions, etc. Finally, Marrese-Taylor et al. (2018) use LSTMs to build sequence labelling and classification models that decide where to insert a single gap in a single sentence. Automatic evaluation against goldstandard gaps showed the method was effective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_10",
            "content": "Other work has focused on creating automated cloze tests by controlling aspects of the proposed gaps so that they correlate with a target proficiency level. , for example, manipulate the difficulty of C-tests (open cloze tests with hints, Grotjahn et al. (2002)) by varying the position and word length of the gaps. A similar concept is presented by Settles et al. (2020) and McCarthy et al. (2021), although difficulty is predicted using a machine-learning model that correlates with CEFR levels. In these cases, tests are dynamically adapted to the examinee's proficiency level during the test session. From a different perspective, Felice and Buttery (2019) show that controlling gap entropy can be useful for designing open cloze tests at different CEFR levels. The work we present in this paper, however, aims to model the more complex task of predicting a full set of gaps at the paragraph level that comply with design and testing principles and is, to the best of our knowledge, the first to employ and adapt transformer-based models for this task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_11",
            "content": "System evaluation is also challenging, since there is usually more than one potential word in the text that could constitute a good gap. While previous work often made a choice between automatic (Marrese-Taylor et al., 2018) or human evaluation (Malafeev, 2014;Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_12",
            "content": "Model",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "258-ARR_v1_13",
            "content": "We define open cloze generation as the task of predicting a set of tokens that should be gapped in the text. Unlike previous approaches that work at the sentence level, our models work at the paragraph level (i.e. take the full text as input), since we believe the interactions between gaps can only be optimally captured when the text is processed as a whole rather than sentence by sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_14",
            "content": "Given a text passage, we aim to predict the words that should be gapped in order to create a cloze test that would reliably assess student ability. The task is modelled as a supervised sequence tagging problem where each token is classified as being a good potential gap or not. We employ ELECTRA (Clark et al., 2020), one of the state-of-the-art pre-trained transformer-based language representation models (Wolf et al., 2020). ELECTRA is an extension of BERT (Devlin et al., 2019) with a different pretraining task which is a discriminator (rather than a generator) and aims to detect replaced tokens (rather than generating words for the masks). We fine-tune this model using two training objectives (see Figure 2): 1 A token classification objective which aims to minimise the error of classifying each token as a potential gap or not. 2 A language modelling objective that aims to minimise the negative log-likelihood of regenerating the words that have been gapped. The first objective is typical of any standard token classification model. In particular, we use ELECTRA's discriminator head with softmax to tag each word in the input sequence as a 'good' Architecture of our multi-objective ELECTRA-based system. The model is simultaneously trained on two objectives: 1) token classification and 2) LM prediction of gapped words. gap or not. All the gaps in our training data are replaced with the first intended target answer and labelled positive, while the remaining tokens are labelled negative ( A ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_15",
            "content": "The second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context. This is to avoid generating gaps that are too 'open' and therefore ineffective, such as a gap that accepts any noun or adjective. Specifically, we mask the words in the positions that are predicted as gaps by the discriminator and use ELECTRA's generative head to generate the expected words in the blanks ( B ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_16",
            "content": "While the input layers are shared between the discriminator and the generator model, the two branches of the system leading to the two objectives are fine-tuned in parallel in a multi-task setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_17",
            "content": "Extensions",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "258-ARR_v1_18",
            "content": "Our neural transformer-based sequence tagging model can be very effective at proposing potentially good gaps, but the task becomes more challenging when we expect the output to meet additional requirements such as no repetitions, no gap interdependence, a minimum distance between gaps and a varied selection of lexico-grammatical items. We address these issues using two complementary strategies: a manipulation of the loss function and a post-processing module.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_19",
            "content": "Loss manipulation",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "258-ARR_v1_20",
            "content": "In order to spread gaps evenly throughout the text, we modify the token-level loss function of our tagging model by imposing a higher penalty on tokens that are in close proximity to a gap. Let g be the position of a gap in the sequence, then for each token in position i in the proximity of g, i.e. |g \u2212 i| < D, the loss function l i for the token in position i is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_21",
            "content": "l i = l i * W |g \u2212 i| (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_22",
            "content": "where W represents the penalty and D is the maximum distance scope for penalisation. 2 Equation 1 thus gives more weight to tokens closer to gaps, which results in higher penalisation of their cost functions whenever they are misclassified.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_23",
            "content": "Post-processing",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "258-ARR_v1_24",
            "content": "We also employ a post-processing strategy where we replace the gaps that are repeated in the text with better options. We optimise the choice of these alternative gaps by considering the distance between them and the resulting distribution of gaps with different part-of-speech (PoS) tags.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_25",
            "content": "Our post-processing step can be seen as a reranking function. The gap candidates that are originally ranked based on the model's confidence scores change their ranking to match other desirable requirements of a well-structured cloze test. If the selected n-best gaps include repetitions, our post-processing algorithm randomly chooses one of them at a time and attempts to replace it with a better alternative. An alternative gap is deemed better if 1) its answer is not a repetition of another gapped word, 2) its distance to other selected gaps meets the minimum required distance or is higher than the pairwise distances of the originally selected gaps, and 3) it improves the PoS distribution of the gapped words. The PoS distribution of each new selection of gaps is compared to the average gapped PoS distribution of the cloze tests in the training data using Kullback-Leibler (KL) divergence. A combination of gaps that yields lower KL divergence is assumed to be a better selection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_26",
            "content": "These extensions to the base model bring our final cloze tests closer to those created by human experts by automatically controlling variables that would otherwise need to be adjusted manually. This makes our solution a fully-automated system that can produce ready-to-use cloze tests from an input text passage.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_27",
            "content": "Data",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "258-ARR_v1_28",
            "content": "To the best of our knowledge, there are no public datasets of full-text open cloze tests that could be used for our task. The CLOTH dataset (Xie et al., 2018), for example, contains gapped passages designed for language learners, but it is primarily focused on reasoning and reading comprehension and uses multiple choice questions where distractors play a major role, making it substantially different to the task we aim to model. For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes. Each task consists of a text passage of no more than 300 tokens, a variable number of gaps (between 8 and 16) and a list of valid answers for each gap (between 1 and 7). During the design process, the tasks undergo extensive quality control and pretesting, so their gaps are guaranteed to be very effective at assessing student ability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_29",
            "content": "For training, we reconstruct the texts by replacing each gap with its first answer and we split the whole collection into train, dev and test. Details of our dataset are shown in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_30",
            "content": "Given the lack of publicly available data, we make our test set available with this paper 3 so as to provide a common benchmark for the task and to encourage further research in this area. All the texts were tokenised and parsed using spaCy v2.3 4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_31",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "258-ARR_v1_32",
            "content": "Setup",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "258-ARR_v1_33",
            "content": "We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers. Along with all the tokens in the sequences, we also input dependency parsing information to the system. More specifically, we concatenate the ELECTRA representation of each token with the representation of its head in the dependency graph. 6 On top of the encoding layers, we have two branches that are being learned simultaneously.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_34",
            "content": "The first branch is a simple linear layer that aims to classify each token as a gap or non-gap. For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary. We are only interested in predicting the answer words for the gaps. Therefore, we change the input to the second branch by masking the words that are predicted as gaps by the first branch at each step of training. We employ cross-entropy loss on each branch and ignore the loss values for the tokens that are not masked in the second branch. The whole architecture is updated based on the sum of the two losses. Fine-tuning parameters are specified in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_35",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "258-ARR_v1_36",
            "content": "We compare our ELECTRA-based model to other systems, namely:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_37",
            "content": "Random baseline Generates a random set of gaps for each task based on the average probability distribution of gapped PoS in the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_38",
            "content": "Exercise Maker Generates gaps using rules and a pre-compiled list of commonly gapped words from a variety of Cambridge English main suite exams (Malafeev, 2014). Set to FCE mode for our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_39",
            "content": "BERT Predicts potentially good gaps using BERT (Devlin et al., 2019) for token classification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_40",
            "content": "We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture. Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_41",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "258-ARR_v1_42",
            "content": "We report precision (P), recall (R) and F 1 scores based on a strict matching between the gaps predicted by our models and those in the gold standard. While this evaluation strategy might seem strict, it has the advantage of being fully automatic, thus avoiding the subjectivity and time required by human evaluation, so we adopt it during development. In addition to letting the models decide the optimal number of gaps, we also evaluate system performance when we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard. The n-best predicted gaps are chosen based on their confidence scores. In this scenario, P, R and F 1 become the same.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_43",
            "content": "We also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task. The experts were asked to label each proposed gap in each task of our test set (a total of 360 gaps) as either good or bad and provide a reason and optional comments for their choice. The list of labels available to annotators is shown in Table 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_44",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "258-ARR_v1_45",
            "content": "Automatic evaluation",
            "ntype": "title",
            "meta": {
                "section": "7.1"
            }
        },
        {
            "ix": "258-ARR_v1_46",
            "content": "We carry out automatic evaluation by computing P, R and F 1 on our development set. Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT. This is our base model, which does not include any loss manipulation or post-processing. In this setting, the number of predicted gaps was decided by each model based on the confidence scores (> 0.5 for the positive class). Overall, we observe that performance increases with more sophisticated models. Exercise Maker relies on previously seen gaps and so outperforms the random baseline by a large margin. However, it can only create gaps for the 139 words in its predefined FCE word list, missing gaps that are not on that list. Neural transformer-based models are the best, with improvements over Exercise Maker of at least 25 F 1 on our development set. Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_47",
            "content": "Table 4 shows the performance of our multiobjective ELECTRA model as we increase the nbest list of gaps according to their confidence score. The first row indicates the results of the system when it is forced to predict the exact same number of gaps per task as in the gold standard. 7 This causes P and R to be the same. As we expect, the results show that the number of gaps in the gold data is actually the optimal number to achieve the best F 1 score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_48",
            "content": "Although our multi-objective model shows good performance based on automatic evaluation, a closer look at the output reveals that the structure of the cloze tests is far from ideal as they often contain repetitions and gaps that are too close to each other, aspects that are carefully controlled in the gold standard. Table 5 shows that system performance effectively improves as we add the extensions proposed in Section 4, indicating that global aspects of the task are not properly captured by our initial model and require further manipulation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_49",
            "content": "In order to make the structure of our output as similar as possible to our target tasks, we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard. Note that P and R are the same in this setting so we only report F 1 . The effect of this decision is shown is Table 6. We can see that adding loss manipulation to our model decreases the number of adjacent gaps from 40 to 23, but increases the number of repeated gapped words from 18 to 33. The decline in the restricted F 1 based on automatic evaluation is not favourable, but we make this sacrifice at the price of achieving a better-structured final test.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_50",
            "content": "After adding post-processing for repeated gaps, we observe that, although overall F 1 performance drops slightly, the number of repeated gapped words decreases favourably from 33 to 9 (Table 6). It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it). Post-processing also removes two cases in the development set where the gaps do not meet the minimum 4-word distance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_51",
            "content": "It is worth recalling that these extensions are highly effective when we do not restrict the number of predicted gaps. Table 5 shows that significantly improve R, which results in higher overall F 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_52",
            "content": "As a result of these experiments, we stick with our post-processing approach for the rest of our experiments and use it to produce the output submitted for human annotation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_53",
            "content": "Human evaluation",
            "ntype": "title",
            "meta": {
                "section": "7.2"
            }
        },
        {
            "ix": "258-ARR_v1_54",
            "content": "Following our intuition that test experts could find more value in our system than initially shown by our automatic evaluation, we asked a panel of three test experts to judge the quality of the gaps produced by our extended model on the test set. Inter-annotator agreement on gap classification (good/bad) was found to be moderate (percent agreement is 75.93%, Randolph's free-marginal kappa is 0.52 (Randolph, 2005)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_55",
            "content": "Unlike in automatic evaluation, we only report accuracy for our human experiment. System performance using automatic and human evaluation is compared in Table 7 (reported individually for each annotator). These results show that performance increases dramatically when the output is judged by human experts, confirming our suspicion that performance is underestimated by automatic evaluation and that there are many other words in the texts that could constitute equally useful gaps apart from those in the gold standard. With system accuracy ranging between 75% \u2212 82% for human judgements, we can conclude that at least 7 out of 10 gaps proposed by our system are considered good by our experts. We observed that differences between annotators' judgements and the gold-standard can occur for many reasons, e.g.:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_56",
            "content": "\u2022 non-gaps in the gold standard are not necessarily bad gaps, \u2022 gold standard gaps are derived from pilot testing while annotators' gaps are derived from their expertise, \u2022 previous judgements by the annotators can affect the judgement of new gaps (e.g. choosing the best of two close gaps), etc. Annotator accuracy against the gold standard ranges between 50% \u2212 60%.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_57",
            "content": "Following our classification in Table 2, we analysed the reasons why some gaps were not considered good by the annotators. Figure 4 shows the average frequency of the different reasons given by the annotators for rejecting a gap proposed by our system. Examples are included in Appendix C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_58",
            "content": "The most frequent reason is the violation of the minimum required distance between two gaps (42.43%). Although our loss-manipulation approach was successful in reducing these cases, we did not attempt to eradicate them completely since there are many factors at play when choosing more appropriate gaps than just distance. In many cases, gaps in close proximity test different words in the same phrase (e.g. take part in, in addition to, etc.) so we preferred to keep these cases and encourage annotators to comment on their preferences. Repetitions, on the contrary, are much better handled, accounting for only 0.87% of all bad gaps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_59",
            "content": "The second most frequent reason is 'unacceptable outlier' (32.47%), which normally accounts for cases where the difficulty of the gap is considered inappropriate for the target proficiency level (B2 in this case). This is an interesting phenomenon, since the fact that the text as a whole pertains to a given CEFR level does not guarantee that the gaps created will always be appropriate for the level. The remaining reasons are substantially less frequent than the first two and mostly related to aspects that were not explicitly controlled in our models, except for the third topmost reason ('Too many gaps of this type') which we did control by comparing PoS distributions. These results show that our system is able to capture many aspects of the task that were not explicitly modelled. Finally, we compared system accuracy per task computed from annotators' judgements vs. the gold standard. Average correlation across all annotators was found to be very weak (Pearson's r = 0.0558, Spearman's \u03c1 = 0.1474). This suggests that automatic scores are not a good proxy for human perception, with experts being much more positive about our model's output (as shown in Table 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_60",
            "content": "Predictions by Gapped Word Frequency",
            "ntype": "title",
            "meta": {
                "section": "7.3"
            }
        },
        {
            "ix": "258-ARR_v1_61",
            "content": "We found that our model does not overfit to words that are most frequently gapped in the training data, with correlation between gapped word frequency and F 1 scores in the test set being negligible (Pearson's r = 0.0108, Spearman's \u03c1 = 0.0915).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_62",
            "content": "Interestingly, while our model was unable to predict gaps not previously seen in the training data (turned, amount, pushed and started), it did predict a (previously unseen) gap for the word fewer, which did not match the gold standard but was unanimously deemed good by our annotators.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_63",
            "content": "Predictions by PoS",
            "ntype": "title",
            "meta": {
                "section": "7.4"
            }
        },
        {
            "ix": "258-ARR_v1_64",
            "content": "We also classified predictions based on their PoS tags 8 and report performance in Table 8. The most frequently gapped PoS tags in our datasets corre-Gardening It is early summer , the season of abundance , when my garden is at at its fullest . Flowers are in in bloom and the grass is growing so so fast that half an hour after cutting it , I seem to be back where where I started . This year for the first time I am attempting to grow my my own vegetables , an attempt that has so far far proved very successful . My vegetable plants have been yielding an abundance of produce , in fact much more more than I can possibly consume myself . I 'm convinced that you cannot plant even a single tomato without without feeling a connection to the earth and to the countless generations who have worked the land before you . To plant seeds and then to harvest what you have grown gives a a deep sense of satisfaction . I believe that many doctors and mental health organisations all around around the world now recognise the value of gardening to the well-being of those who who take part in this activity . The two worst performing classes are PART (the particles to and not) and AUX (auxiliary verbs) and, once again, we conjecture that these words are so common in the language and in non-gapped positions that the model is unable to get them right most of the time. The remaining PoS classes vary in performance but we found only very weak correlation between PoS gap frequency in the test set and F 1 scores (Pearson's r = 0.1932, Spearman's \u03c1 = 0.1350).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_65",
            "content": "When we look at human annotations on the test set, however, performance by PoS is consistently higher and more even across the board. If we require that gaps are rated 'good' by at least two annotators, accuracy values range between 75% and 100% for all PoS, with a mean of 85%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_66",
            "content": "Under these conditions, the best performing classes are NOUN (100%), INTJ (100%) and ADJ (95%), which agree with automatic evaluation. Out of these, only NOUN achieves perfect accuracy across all annotators. The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively). When we require agreement by all annotators, the worst overall class is CCONJ with 44%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_67",
            "content": "Qualitative Analysis",
            "ntype": "title",
            "meta": {
                "section": "7.5"
            }
        },
        {
            "ix": "258-ARR_v1_68",
            "content": "Figure 5 shows the output of our model for a sample text passage, where darker red indicates higher confidence in inserting a gap. The final model's predictions have a black frame (at, in, so, after, etc.) while the gold standard gaps are in yellow font (at, in, so, etc.). There are 8 matched gaps out of 11 in this example, yielding 72.73% accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_69",
            "content": "As can be seen in the figure, our model is able to identify appropriate gap candidates, even if they do not match the gold standard. In fact, annotators considered all the unmatched gaps in this example (after, for and take) to be good and the second matched gap (in) to be inappropriate. It is also interesting to see how the model prioritises function words and content words that are highly restricted in context (such as take or part), skilfully avoiding general gaps that could accept multiple answers and would be less effective for testing purposes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_70",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "258-ARR_v1_71",
            "content": "We described the first transformer-based approach to open cloze test generation. Our ELECTRAbased model is trained on two objectives: token classification (gap/non-gap) and language modelling (for predicting the expected answer). The model is further improved by manipulating the loss function and post-processing the results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_72",
            "content": "System accuracy using automatic evaluation is 53.89% while human evaluation ranges between 75% \u2212 82%, showing that at least 7 out of 10 gaps predicted are considered useful by experts. A detailed analysis of results reveals a few structural problems such as gaps in close proximity and inappropriate difficulty, which we plan to address in future work. Our test data and human annotations are released with this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "258-ARR_v1_73",
            "content": "UNKNOWN, None, 2017, Automatically generating questions to support the acquisition of particle verbs: Evaluating via crowdsourcing. In CALL in a climate of change: adapting to turbulent global conditions -short papers from EUROCALL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Automatically generating questions to support the acquisition of particle verbs: Evaluating via crowdsourcing. In CALL in a climate of change: adapting to turbulent global conditions -short papers from EUROCALL",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_74",
            "content": "Kevin Clark, Minh-Thang Luong, Quoc Le, Christopher Manning, ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Kevin Clark",
                    "Minh-Thang Luong",
                    "Quoc Le",
                    "Christopher Manning"
                ],
                "title": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_75",
            "content": "UNKNOWN, None, 2001, Common European Framework of Reference for Languages: learning, teaching, assessment, Cambridge University Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2001",
                "pub_title": "Common European Framework of Reference for Languages: learning, teaching, assessment",
                "pub": "Cambridge University Press"
            }
        },
        {
            "ix": "258-ARR_v1_76",
            "content": "Bidyut Das, Mukta Majumder, Factual open cloze question generation for assessment of learner's knowledge, 2017, International Journal of Educational Technology in Higher Education, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Bidyut Das",
                    "Mukta Majumder"
                ],
                "title": "Factual open cloze question generation for assessment of learner's knowledge",
                "pub_date": "2017",
                "pub_title": "International Journal of Educational Technology in Higher Education",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_77",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "258-ARR_v1_78",
            "content": "Mariano Felice, Paula Buttery, Entropy as a proxy for gap complexity in open cloze tests, 2019, Proceedings of the International Conference on Recent Advances in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Mariano Felice",
                    "Paula Buttery"
                ],
                "title": "Entropy as a proxy for gap complexity in open cloze tests",
                "pub_date": "2019",
                "pub_title": "Proceedings of the International Conference on Recent Advances in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_79",
            "content": "R\u00fcdiger Grotjahn, Christine Klein-Braley, Ulrich Raatz, C-tests: an overview, 2002, University language learning and the C-Test, AKS-Verlag.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "R\u00fcdiger Grotjahn",
                    "Christine Klein-Braley",
                    "Ulrich Raatz"
                ],
                "title": "C-tests: an overview",
                "pub_date": "2002",
                "pub_title": "University language learning and the C-Test",
                "pub": "AKS-Verlag"
            }
        },
        {
            "ix": "258-ARR_v1_80",
            "content": "UNKNOWN, None, 2020, A systematic review of automatic question generation for educational purposes, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "A systematic review of automatic question generation for educational purposes",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_81",
            "content": "UNKNOWN, None, , Artificial Intelligence in Education, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Artificial Intelligence in Education",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_82",
            "content": "Ji-Ung Lee, Erik Schwan, Christian Meyer, Manipulating the difficulty of C-tests, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ji-Ung Lee",
                    "Erik Schwan",
                    "Christian Meyer"
                ],
                "title": "Manipulating the difficulty of C-tests",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_83",
            "content": "Alexey Malafeev, Language exercise generation: Emulating cambridge open cloze, 2014, Int. J. Concept. Struct. Smart Appl, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Alexey Malafeev"
                ],
                "title": "Language exercise generation: Emulating cambridge open cloze",
                "pub_date": "2014",
                "pub_title": "Int. J. Concept. Struct. Smart Appl",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_84",
            "content": "Edison Marrese-Taylor, Ai Nakajima, Yutaka Matsuo, Ono Yuichi, Learning to automatically generate fill-in-the-blank quizzes, 2018, Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Edison Marrese-Taylor",
                    "Ai Nakajima",
                    "Yutaka Matsuo",
                    "Ono Yuichi"
                ],
                "title": "Learning to automatically generate fill-in-the-blank quizzes",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "258-ARR_v1_85",
            "content": "D Arya, Kevin Mccarthy, Geoff Yancey, Jesse Laflair, Manqian Egbert, Burr Liao,  Settles, Jump-starting item parameters for adaptive language tests, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "D Arya",
                    "Kevin Mccarthy",
                    "Geoff Yancey",
                    "Jesse Laflair",
                    "Manqian Egbert",
                    "Burr Liao",
                    " Settles"
                ],
                "title": "Jump-starting item parameters for adaptive language tests",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_86",
            "content": "Jack Mostow, Yi-Ting Huang, Hyeju Jang, Anders Weinstein, Joe Valeri, Donna Gates, Developing, evaluating, and refining an automatic generator of diagnostic multiple choice cloze questions to assess children's comprehension while reading, 2017, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jack Mostow",
                    "Yi-Ting Huang",
                    "Hyeju Jang",
                    "Anders Weinstein",
                    "Joe Valeri",
                    "Donna Gates"
                ],
                "title": "Developing, evaluating, and refining an automatic generator of diagnostic multiple choice cloze questions to assess children's comprehension while reading",
                "pub_date": "2017",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_87",
            "content": "UNKNOWN, None, 2005, Association of Language Testers in Europe (ALTE), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Association of Language Testers in Europe (ALTE)",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_88",
            "content": "UNKNOWN, None, , Association of Language Testers in Europe (ALTE). 2011. Manual for language test development and examining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Association of Language Testers in Europe (ALTE). 2011. Manual for language test development and examining",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_89",
            "content": "Juan Pino, Maxine Eskenazi, Measuring hint level in open cloze questions, 2009, Proceedings of the Twenty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS), AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Juan Pino",
                    "Maxine Eskenazi"
                ],
                "title": "Measuring hint level in open cloze questions",
                "pub_date": "2009",
                "pub_title": "Proceedings of the Twenty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS)",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "258-ARR_v1_90",
            "content": "UNKNOWN, None, 2008, A selection strategy to improve cloze question quality. Intelligent Tutoring Systems for Ill-Defined Domains: Assessment and Feedback in Ill-Defined Domains, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2008",
                "pub_title": "A selection strategy to improve cloze question quality. Intelligent Tutoring Systems for Ill-Defined Domains: Assessment and Feedback in Ill-Defined Domains",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_91",
            "content": "UNKNOWN, None, 2005, Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_92",
            "content": "Burr Settles, Geoffrey Laflair, Masato Hagiwara, Machine learning-driven language assessment, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Burr Settles",
                    "Geoffrey Laflair",
                    "Masato Hagiwara"
                ],
                "title": "Machine learning-driven language assessment",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_93",
            "content": "Tasanawan Soonklang, Sunee Pongpinigpinyo, Weenawadee Muangon, Sirak Kaewjamnong, Automatic question generation system for english exercise for secondary students, 2017, Proceedings of the 25th International Conference on Computers in Education (ICCE 2017), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Tasanawan Soonklang",
                    "Sunee Pongpinigpinyo",
                    "Weenawadee Muangon",
                    "Sirak Kaewjamnong"
                ],
                "title": "Automatic question generation system for english exercise for secondary students",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 25th International Conference on Computers in Education (ICCE 2017)",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_94",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems (NIPS), Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems (NIPS)",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "258-ARR_v1_95",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    " Drame"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_96",
            "content": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy, Large-scale cloze test dataset created by teachers, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Qizhe Xie",
                    "Guokun Lai",
                    "Zihang Dai",
                    "Eduard Hovy"
                ],
                "title": "Large-scale cloze test dataset created by teachers",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "258-ARR_v1_97",
            "content": "C Albert,  Yang, Y Irene, Brendan Chen, Hiroaki Flanagan,  Ogata, Automatic generation of cloze items for repeated testing to improve reading comprehension, 2021, Educational Technology & Society, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "C Albert",
                    " Yang",
                    "Y Irene",
                    "Brendan Chen",
                    "Hiroaki Flanagan",
                    " Ogata"
                ],
                "title": "Automatic generation of cloze items for repeated testing to improve reading comprehension",
                "pub_date": "2021",
                "pub_title": "Educational Technology & Society",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "258-ARR_v1_0@0",
            "content": "Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_0",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_2@0",
            "content": "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_2",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_2@1",
            "content": "Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_2",
            "start": 177,
            "end": 324,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_2@2",
            "content": "Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_2",
            "start": 326,
            "end": 492,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_2@3",
            "content": "We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_2",
            "start": 494,
            "end": 645,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_4@0",
            "content": "Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_4",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_4@1",
            "content": "Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_4",
            "start": 153,
            "end": 373,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_5@0",
            "content": "In this paper, we propose a strategy to construct open cloze exercises using transformer models (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_5",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_5@1",
            "content": "Our transformer-based architecture employs two objectives to predict the words that should be gapped in a text passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_5",
            "start": 120,
            "end": 238,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_5@2",
            "content": "One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_5",
            "start": 240,
            "end": 361,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_5@3",
            "content": "The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_5",
            "start": 363,
            "end": 519,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_5@4",
            "content": "Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_5",
            "start": 521,
            "end": 670,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_6@0",
            "content": "Our output aims to mimic the style of open cloze tests in the First Certificate in English (FCE) exam 1 , which is targeted at learners of English at the B2 proficiency level of the Common European Framework of Reference (CEFR) for languages (Council of Europe, 2001).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_6",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_6@1",
            "content": "Unlike other tests, the FCE open cloze task aims to simultaneously test many aspects of grammar and vocabulary that students are expected to know at this level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_6",
            "start": 269,
            "end": 428,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_6@2",
            "content": "Since the tests are created from a text passage, they must be skilfully designed in order to ensure an optimal distribution of gaps that adheres to guidelines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_6",
            "start": 430,
            "end": 588,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_6@3",
            "content": "A shortened example is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_6",
            "start": 590,
            "end": 630,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_6@4",
            "content": "Our system is evaluated under two settings: 1) automatic evaluation, where the generated gaps are compared to goldstandard gaps proposed by test experts, and 2) human evaluation, where the quality of the generated gaps is judged by test experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_6",
            "start": 632,
            "end": 876,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_7@0",
            "content": "The main contributions of our work are as follows: 1) we are the first to employ transformer models for open cloze test generation, 2) unlike previous studies, we work at the paragraph level, which is a much more challenging task, 3) we propose a multi-task learning approach with two objectives: one is to classify tokens into gaps/non-gaps and the other to minimise the error of re-generating the gapped word, 4) we report state-of-the-art results, outperforming previous work and a strong baseline, 5) we propose additional components to control the structure of the final cloze tests as human experts do, 6) we perform both automatic and human evaluation and 7) make our test data, system output and human annotations available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_7",
            "start": 0,
            "end": 731,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@0",
            "content": "While research into automatic cloze test generation is vast (Mostow et al., 2017;Kurdi et al., 2020;Yang et al., 2021), work on open cloze tests for language learning is scarce.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@1",
            "content": "Pino et al. (2008) generate open cloze questions using sample sen- I first saw some guys doing motorbike stunts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 178,
            "end": 289,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@2",
            "content": "I'd never seen anyone riding a motorbike using just the back wheel before and I was (3) impressed I went straight home and taught (4) to do the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 291,
            "end": 439,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@3",
            "content": "tences from a learners' dictionary based on four linguistic criteria: (grammatical) complexity, welldefined context (collocations), grammaticality and length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 441,
            "end": 598,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@4",
            "content": "A later version of their system adds hints for gapped words (Pino and Eskenazi, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 600,
            "end": 685,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@5",
            "content": "Exercise Maker (Malafeev, 2014) is a rule-based open source system that attempts to emulate exercises in Cambridge English examinations based on the most frequently tested words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 687,
            "end": 864,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@6",
            "content": "Most of the gaps it proposes were found to be useful and the automated exercises were hard to differentiate from authentic tests.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 866,
            "end": 994,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@7",
            "content": "Chinkina et al. (2017) generate open cloze exercises for phrasal verbs by extracting sentences from news articles and generating a pair of questions and answers where the identified particle verbs are gapped.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 996,
            "end": 1203,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@8",
            "content": "Similarly, Soonklang et al. (2017) gap words in sentences according to their part of speech in order to practise articles, prepositions, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 1205,
            "end": 1345,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@9",
            "content": "Finally, Marrese-Taylor et al. (2018) use LSTMs to build sequence labelling and classification models that decide where to insert a single gap in a single sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 1347,
            "end": 1510,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_9@10",
            "content": "Automatic evaluation against goldstandard gaps showed the method was effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_9",
            "start": 1512,
            "end": 1590,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_10@0",
            "content": "Other work has focused on creating automated cloze tests by controlling aspects of the proposed gaps so that they correlate with a target proficiency level. , for example, manipulate the difficulty of C-tests (open cloze tests with hints, Grotjahn et al. (2002)) by varying the position and word length of the gaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_10",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_10@1",
            "content": "A similar concept is presented by Settles et al. (2020) and McCarthy et al. (2021), although difficulty is predicted using a machine-learning model that correlates with CEFR levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_10",
            "start": 316,
            "end": 496,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_10@2",
            "content": "In these cases, tests are dynamically adapted to the examinee's proficiency level during the test session.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_10",
            "start": 498,
            "end": 603,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_10@3",
            "content": "From a different perspective, Felice and Buttery (2019) show that controlling gap entropy can be useful for designing open cloze tests at different CEFR levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_10",
            "start": 605,
            "end": 764,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_10@4",
            "content": "The work we present in this paper, however, aims to model the more complex task of predicting a full set of gaps at the paragraph level that comply with design and testing principles and is, to the best of our knowledge, the first to employ and adapt transformer-based models for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_10",
            "start": 766,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_11@0",
            "content": "System evaluation is also challenging, since there is usually more than one potential word in the text that could constitute a good gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_11",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_11@1",
            "content": "While previous work often made a choice between automatic (Marrese-Taylor et al., 2018) or human evaluation (Malafeev, 2014;Das and Majumder, 2017) for their experiments, we perform both: automatic evaluation to identify the best models during development and human evaluation to measure test quality in the final output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_11",
            "start": 137,
            "end": 457,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_12@0",
            "content": "Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_12",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_13@0",
            "content": "We define open cloze generation as the task of predicting a set of tokens that should be gapped in the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_13",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_13@1",
            "content": "Unlike previous approaches that work at the sentence level, our models work at the paragraph level (i.e. take the full text as input), since we believe the interactions between gaps can only be optimally captured when the text is processed as a whole rather than sentence by sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_13",
            "start": 109,
            "end": 392,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@0",
            "content": "Given a text passage, we aim to predict the words that should be gapped in order to create a cloze test that would reliably assess student ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@1",
            "content": "The task is modelled as a supervised sequence tagging problem where each token is classified as being a good potential gap or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 148,
            "end": 277,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@2",
            "content": "We employ ELECTRA (Clark et al., 2020), one of the state-of-the-art pre-trained transformer-based language representation models (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 279,
            "end": 427,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@3",
            "content": "ELECTRA is an extension of BERT (Devlin et al., 2019) with a different pretraining task which is a discriminator (rather than a generator) and aims to detect replaced tokens (rather than generating words for the masks).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 429,
            "end": 647,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@4",
            "content": "We fine-tune this model using two training objectives (see Figure 2): 1 A token classification objective which aims to minimise the error of classifying each token as a potential gap or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 649,
            "end": 838,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@5",
            "content": "2 A language modelling objective that aims to minimise the negative log-likelihood of regenerating the words that have been gapped.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 840,
            "end": 970,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@6",
            "content": "The first objective is typical of any standard token classification model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 972,
            "end": 1045,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@7",
            "content": "In particular, we use ELECTRA's discriminator head with softmax to tag each word in the input sequence as a 'good' Architecture of our multi-objective ELECTRA-based system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 1047,
            "end": 1218,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@8",
            "content": "The model is simultaneously trained on two objectives: 1) token classification and 2) LM prediction of gapped words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 1220,
            "end": 1335,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@9",
            "content": "gap or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 1337,
            "end": 1347,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_14@10",
            "content": "All the gaps in our training data are replaced with the first intended target answer and labelled positive, while the remaining tokens are labelled negative ( A ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_14",
            "start": 1349,
            "end": 1511,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_15@0",
            "content": "The second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_15",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_15@1",
            "content": "This is to avoid generating gaps that are too 'open' and therefore ineffective, such as a gap that accepts any noun or adjective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_15",
            "start": 175,
            "end": 303,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_15@2",
            "content": "Specifically, we mask the words in the positions that are predicted as gaps by the discriminator and use ELECTRA's generative head to generate the expected words in the blanks ( B ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_15",
            "start": 305,
            "end": 486,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_16@0",
            "content": "While the input layers are shared between the discriminator and the generator model, the two branches of the system leading to the two objectives are fine-tuned in parallel in a multi-task setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_16",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_17@0",
            "content": "Extensions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_17",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_18@0",
            "content": "Our neural transformer-based sequence tagging model can be very effective at proposing potentially good gaps, but the task becomes more challenging when we expect the output to meet additional requirements such as no repetitions, no gap interdependence, a minimum distance between gaps and a varied selection of lexico-grammatical items.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_18",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_18@1",
            "content": "We address these issues using two complementary strategies: a manipulation of the loss function and a post-processing module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_18",
            "start": 338,
            "end": 462,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_19@0",
            "content": "Loss manipulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_19",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_20@0",
            "content": "In order to spread gaps evenly throughout the text, we modify the token-level loss function of our tagging model by imposing a higher penalty on tokens that are in close proximity to a gap. Let g be the position of a gap in the sequence, then for each token in position i in the proximity of g, i.e. |g \u2212 i| < D, the loss function l i for the token in position i is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_20",
            "start": 0,
            "end": 376,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_21@0",
            "content": "l i = l i * W |g \u2212 i| (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_21",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_22@0",
            "content": "where W represents the penalty and D is the maximum distance scope for penalisation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_22",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_22@1",
            "content": "2 Equation 1 thus gives more weight to tokens closer to gaps, which results in higher penalisation of their cost functions whenever they are misclassified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_22",
            "start": 85,
            "end": 239,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_23@0",
            "content": "Post-processing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_23",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_24@0",
            "content": "We also employ a post-processing strategy where we replace the gaps that are repeated in the text with better options.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_24",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_24@1",
            "content": "We optimise the choice of these alternative gaps by considering the distance between them and the resulting distribution of gaps with different part-of-speech (PoS) tags.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_24",
            "start": 119,
            "end": 288,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@0",
            "content": "Our post-processing step can be seen as a reranking function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@1",
            "content": "The gap candidates that are originally ranked based on the model's confidence scores change their ranking to match other desirable requirements of a well-structured cloze test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 62,
            "end": 237,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@2",
            "content": "If the selected n-best gaps include repetitions, our post-processing algorithm randomly chooses one of them at a time and attempts to replace it with a better alternative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 239,
            "end": 409,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@3",
            "content": "An alternative gap is deemed better if 1) its answer is not a repetition of another gapped word, 2) its distance to other selected gaps meets the minimum required distance or is higher than the pairwise distances of the originally selected gaps, and 3) it improves the PoS distribution of the gapped words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 411,
            "end": 716,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@4",
            "content": "The PoS distribution of each new selection of gaps is compared to the average gapped PoS distribution of the cloze tests in the training data using Kullback-Leibler (KL) divergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 718,
            "end": 898,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_25@5",
            "content": "A combination of gaps that yields lower KL divergence is assumed to be a better selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_25",
            "start": 900,
            "end": 989,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_26@0",
            "content": "These extensions to the base model bring our final cloze tests closer to those created by human experts by automatically controlling variables that would otherwise need to be adjusted manually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_26",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_26@1",
            "content": "This makes our solution a fully-automated system that can produce ready-to-use cloze tests from an input text passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_26",
            "start": 194,
            "end": 311,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_27@0",
            "content": "Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_27",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_28@0",
            "content": "To the best of our knowledge, there are no public datasets of full-text open cloze tests that could be used for our task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_28",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_28@1",
            "content": "The CLOTH dataset (Xie et al., 2018), for example, contains gapped passages designed for language learners, but it is primarily focused on reasoning and reading comprehension and uses multiple choice questions where distractors play a major role, making it substantially different to the task we aim to model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_28",
            "start": 122,
            "end": 430,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_28@2",
            "content": "For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_28",
            "start": 432,
            "end": 589,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_28@3",
            "content": "Each task consists of a text passage of no more than 300 tokens, a variable number of gaps (between 8 and 16) and a list of valid answers for each gap (between 1 and 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_28",
            "start": 591,
            "end": 759,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_28@4",
            "content": "During the design process, the tasks undergo extensive quality control and pretesting, so their gaps are guaranteed to be very effective at assessing student ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_28",
            "start": 761,
            "end": 926,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_29@0",
            "content": "For training, we reconstruct the texts by replacing each gap with its first answer and we split the whole collection into train, dev and test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_29",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_29@1",
            "content": "Details of our dataset are shown in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_29",
            "start": 143,
            "end": 186,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_30@0",
            "content": "Given the lack of publicly available data, we make our test set available with this paper 3 so as to provide a common benchmark for the task and to encourage further research in this area.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_30",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_30@1",
            "content": "All the texts were tokenised and parsed using spaCy v2.3 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_30",
            "start": 189,
            "end": 248,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_31@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_31",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_32@0",
            "content": "Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_32",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_33@0",
            "content": "We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_33",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_33@1",
            "content": "Along with all the tokens in the sequences, we also input dependency parsing information to the system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_33",
            "start": 104,
            "end": 206,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_33@2",
            "content": "More specifically, we concatenate the ELECTRA representation of each token with the representation of its head in the dependency graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_33",
            "start": 208,
            "end": 342,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_33@3",
            "content": "6 On top of the encoding layers, we have two branches that are being learned simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_33",
            "start": 344,
            "end": 435,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@0",
            "content": "The first branch is a simple linear layer that aims to classify each token as a gap or non-gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@1",
            "content": "For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 96,
            "end": 234,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@2",
            "content": "We are only interested in predicting the answer words for the gaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 236,
            "end": 302,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@3",
            "content": "Therefore, we change the input to the second branch by masking the words that are predicted as gaps by the first branch at each step of training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 304,
            "end": 448,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@4",
            "content": "We employ cross-entropy loss on each branch and ignore the loss values for the tokens that are not masked in the second branch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 450,
            "end": 576,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@5",
            "content": "The whole architecture is updated based on the sum of the two losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 578,
            "end": 646,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_34@6",
            "content": "Fine-tuning parameters are specified in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_34",
            "start": 648,
            "end": 698,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_35@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_35",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_36@0",
            "content": "We compare our ELECTRA-based model to other systems, namely:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_36",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_37@0",
            "content": "Random baseline Generates a random set of gaps for each task based on the average probability distribution of gapped PoS in the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_37",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_38@0",
            "content": "Exercise Maker Generates gaps using rules and a pre-compiled list of commonly gapped words from a variety of Cambridge English main suite exams (Malafeev, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_38",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_38@1",
            "content": "Set to FCE mode for our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_38",
            "start": 162,
            "end": 197,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_39@0",
            "content": "BERT Predicts potentially good gaps using BERT (Devlin et al., 2019) for token classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_39",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_40@0",
            "content": "We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_40",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_40@1",
            "content": "Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_40",
            "start": 112,
            "end": 314,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_41@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_41",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_42@0",
            "content": "We report precision (P), recall (R) and F 1 scores based on a strict matching between the gaps predicted by our models and those in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_42",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_42@1",
            "content": "While this evaluation strategy might seem strict, it has the advantage of being fully automatic, thus avoiding the subjectivity and time required by human evaluation, so we adopt it during development.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_42",
            "start": 151,
            "end": 351,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_42@2",
            "content": "In addition to letting the models decide the optimal number of gaps, we also evaluate system performance when we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_42",
            "start": 353,
            "end": 565,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_42@3",
            "content": "The n-best predicted gaps are chosen based on their confidence scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_42",
            "start": 567,
            "end": 636,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_42@4",
            "content": "In this scenario, P, R and F 1 become the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_42",
            "start": 638,
            "end": 684,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_43@0",
            "content": "We also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_43",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_43@1",
            "content": "The experts were asked to label each proposed gap in each task of our test set (a total of 360 gaps) as either good or bad and provide a reason and optional comments for their choice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_43",
            "start": 128,
            "end": 310,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_43@2",
            "content": "The list of labels available to annotators is shown in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_43",
            "start": 312,
            "end": 374,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_44@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_44",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_45@0",
            "content": "Automatic evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_45",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@0",
            "content": "We carry out automatic evaluation by computing P, R and F 1 on our development set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@1",
            "content": "Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 84,
            "end": 243,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@2",
            "content": "This is our base model, which does not include any loss manipulation or post-processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 245,
            "end": 332,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@3",
            "content": "In this setting, the number of predicted gaps was decided by each model based on the confidence scores (> 0.5 for the positive class).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 334,
            "end": 467,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@4",
            "content": "Overall, we observe that performance increases with more sophisticated models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 469,
            "end": 546,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@5",
            "content": "Exercise Maker relies on previously seen gaps and so outperforms the random baseline by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 548,
            "end": 650,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@6",
            "content": "However, it can only create gaps for the 139 words in its predefined FCE word list, missing gaps that are not on that list.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 652,
            "end": 774,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@7",
            "content": "Neural transformer-based models are the best, with improvements over Exercise Maker of at least 25 F 1 on our development set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 776,
            "end": 901,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_46@8",
            "content": "Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_46",
            "start": 903,
            "end": 1252,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_47@0",
            "content": "Table 4 shows the performance of our multiobjective ELECTRA model as we increase the nbest list of gaps according to their confidence score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_47",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_47@1",
            "content": "The first row indicates the results of the system when it is forced to predict the exact same number of gaps per task as in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_47",
            "start": 141,
            "end": 282,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_47@2",
            "content": "7 This causes P and R to be the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_47",
            "start": 284,
            "end": 320,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_47@3",
            "content": "As we expect, the results show that the number of gaps in the gold data is actually the optimal number to achieve the best F 1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_47",
            "start": 322,
            "end": 454,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_48@0",
            "content": "Although our multi-objective model shows good performance based on automatic evaluation, a closer look at the output reveals that the structure of the cloze tests is far from ideal as they often contain repetitions and gaps that are too close to each other, aspects that are carefully controlled in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_48",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_48@1",
            "content": "Table 5 shows that system performance effectively improves as we add the extensions proposed in Section 4, indicating that global aspects of the task are not properly captured by our initial model and require further manipulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_48",
            "start": 318,
            "end": 547,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_49@0",
            "content": "In order to make the structure of our output as similar as possible to our target tasks, we fix the number of predicted gaps for each task to the number of gaps they have in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_49",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_49@1",
            "content": "Note that P and R are the same in this setting so we only report F 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_49",
            "start": 193,
            "end": 262,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_49@2",
            "content": "The effect of this decision is shown is Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_49",
            "start": 264,
            "end": 311,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_49@3",
            "content": "We can see that adding loss manipulation to our model decreases the number of adjacent gaps from 40 to 23, but increases the number of repeated gapped words from 18 to 33.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_49",
            "start": 313,
            "end": 483,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_49@4",
            "content": "The decline in the restricted F 1 based on automatic evaluation is not favourable, but we make this sacrifice at the price of achieving a better-structured final test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_49",
            "start": 485,
            "end": 651,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_50@0",
            "content": "After adding post-processing for repeated gaps, we observe that, although overall F 1 performance drops slightly, the number of repeated gapped words decreases favourably from 33 to 9 (Table 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_50",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_50@1",
            "content": "It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_50",
            "start": 195,
            "end": 408,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_50@2",
            "content": "Post-processing also removes two cases in the development set where the gaps do not meet the minimum 4-word distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_50",
            "start": 410,
            "end": 526,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_51@0",
            "content": "It is worth recalling that these extensions are highly effective when we do not restrict the number of predicted gaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_51",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_51@1",
            "content": "Table 5 shows that significantly improve R, which results in higher overall F 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_51",
            "start": 119,
            "end": 199,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_52@0",
            "content": "As a result of these experiments, we stick with our post-processing approach for the rest of our experiments and use it to produce the output submitted for human annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_52",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_53@0",
            "content": "Human evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_53",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_54@0",
            "content": "Following our intuition that test experts could find more value in our system than initially shown by our automatic evaluation, we asked a panel of three test experts to judge the quality of the gaps produced by our extended model on the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_54",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_54@1",
            "content": "Inter-annotator agreement on gap classification (good/bad) was found to be moderate (percent agreement is 75.93%, Randolph's free-marginal kappa is 0.52 (Randolph, 2005)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_54",
            "start": 248,
            "end": 418,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_55@0",
            "content": "Unlike in automatic evaluation, we only report accuracy for our human experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_55",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_55@1",
            "content": "System performance using automatic and human evaluation is compared in Table 7 (reported individually for each annotator).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_55",
            "start": 82,
            "end": 203,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_55@2",
            "content": "These results show that performance increases dramatically when the output is judged by human experts, confirming our suspicion that performance is underestimated by automatic evaluation and that there are many other words in the texts that could constitute equally useful gaps apart from those in the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_55",
            "start": 205,
            "end": 520,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_55@3",
            "content": "With system accuracy ranging between 75% \u2212 82% for human judgements, we can conclude that at least 7 out of 10 gaps proposed by our system are considered good by our experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_55",
            "start": 522,
            "end": 695,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_55@4",
            "content": "We observed that differences between annotators' judgements and the gold-standard can occur for many reasons, e.g.:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_55",
            "start": 697,
            "end": 811,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_56@0",
            "content": "\u2022 non-gaps in the gold standard are not necessarily bad gaps, \u2022 gold standard gaps are derived from pilot testing while annotators' gaps are derived from their expertise, \u2022 previous judgements by the annotators can affect the judgement of new gaps (e.g. choosing the best of two close gaps), etc. Annotator accuracy against the gold standard ranges between 50% \u2212 60%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_56",
            "start": 0,
            "end": 366,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_57@0",
            "content": "Following our classification in Table 2, we analysed the reasons why some gaps were not considered good by the annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_57",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_57@1",
            "content": "Figure 4 shows the average frequency of the different reasons given by the annotators for rejecting a gap proposed by our system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_57",
            "start": 123,
            "end": 251,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_57@2",
            "content": "Examples are included in Appendix C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_57",
            "start": 253,
            "end": 288,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_58@0",
            "content": "The most frequent reason is the violation of the minimum required distance between two gaps (42.43%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_58",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_58@1",
            "content": "Although our loss-manipulation approach was successful in reducing these cases, we did not attempt to eradicate them completely since there are many factors at play when choosing more appropriate gaps than just distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_58",
            "start": 102,
            "end": 321,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_58@2",
            "content": "In many cases, gaps in close proximity test different words in the same phrase (e.g. take part in, in addition to, etc.) so we preferred to keep these cases and encourage annotators to comment on their preferences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_58",
            "start": 323,
            "end": 536,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_58@3",
            "content": "Repetitions, on the contrary, are much better handled, accounting for only 0.87% of all bad gaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_58",
            "start": 538,
            "end": 634,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@0",
            "content": "The second most frequent reason is 'unacceptable outlier' (32.47%), which normally accounts for cases where the difficulty of the gap is considered inappropriate for the target proficiency level (B2 in this case).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@1",
            "content": "This is an interesting phenomenon, since the fact that the text as a whole pertains to a given CEFR level does not guarantee that the gaps created will always be appropriate for the level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 214,
            "end": 401,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@2",
            "content": "The remaining reasons are substantially less frequent than the first two and mostly related to aspects that were not explicitly controlled in our models, except for the third topmost reason ('Too many gaps of this type') which we did control by comparing PoS distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 403,
            "end": 675,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@3",
            "content": "These results show that our system is able to capture many aspects of the task that were not explicitly modelled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 677,
            "end": 789,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@4",
            "content": "Finally, we compared system accuracy per task computed from annotators' judgements vs. the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 791,
            "end": 895,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@5",
            "content": "Average correlation across all annotators was found to be very weak (Pearson's r = 0.0558, Spearman's \u03c1 = 0.1474).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 897,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_59@6",
            "content": "This suggests that automatic scores are not a good proxy for human perception, with experts being much more positive about our model's output (as shown in Table 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_59",
            "start": 1012,
            "end": 1175,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_60@0",
            "content": "Predictions by Gapped Word Frequency",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_60",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_61@0",
            "content": "We found that our model does not overfit to words that are most frequently gapped in the training data, with correlation between gapped word frequency and F 1 scores in the test set being negligible (Pearson's r = 0.0108, Spearman's \u03c1 = 0.0915).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_61",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_62@0",
            "content": "Interestingly, while our model was unable to predict gaps not previously seen in the training data (turned, amount, pushed and started), it did predict a (previously unseen) gap for the word fewer, which did not match the gold standard but was unanimously deemed good by our annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_62",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_63@0",
            "content": "Predictions by PoS",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_63",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@0",
            "content": "We also classified predictions based on their PoS tags 8 and report performance in Table 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@1",
            "content": "The most frequently gapped PoS tags in our datasets corre-Gardening It is early summer , the season of abundance , when my garden is at at its fullest . Flowers are in in bloom and the grass is growing so so fast that half an hour after cutting it , I seem to be back where where I started .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 92,
            "end": 382,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@2",
            "content": "This year for the first time I am attempting to grow my my own vegetables , an attempt that has so far far proved very successful .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 384,
            "end": 514,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@3",
            "content": "My vegetable plants have been yielding an abundance of produce , in fact much more more than I can possibly consume myself . I 'm convinced that you cannot plant even a single tomato without without feeling a connection to the earth and to the countless generations who have worked the land before you .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 516,
            "end": 818,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@4",
            "content": "To plant seeds and then to harvest what you have grown gives a a deep sense of satisfaction .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 820,
            "end": 912,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@5",
            "content": "I believe that many doctors and mental health organisations all around around the world now recognise the value of gardening to the well-being of those who who take part in this activity .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 914,
            "end": 1101,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@6",
            "content": "The two worst performing classes are PART (the particles to and not) and AUX (auxiliary verbs) and, once again, we conjecture that these words are so common in the language and in non-gapped positions that the model is unable to get them right most of the time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 1103,
            "end": 1363,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_64@7",
            "content": "The remaining PoS classes vary in performance but we found only very weak correlation between PoS gap frequency in the test set and F 1 scores (Pearson's r = 0.1932, Spearman's \u03c1 = 0.1350).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_64",
            "start": 1365,
            "end": 1553,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_65@0",
            "content": "When we look at human annotations on the test set, however, performance by PoS is consistently higher and more even across the board.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_65",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_65@1",
            "content": "If we require that gaps are rated 'good' by at least two annotators, accuracy values range between 75% and 100% for all PoS, with a mean of 85%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_65",
            "start": 134,
            "end": 277,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_66@0",
            "content": "Under these conditions, the best performing classes are NOUN (100%), INTJ (100%) and ADJ (95%), which agree with automatic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_66",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_66@1",
            "content": "Out of these, only NOUN achieves perfect accuracy across all annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_66",
            "start": 135,
            "end": 206,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_66@2",
            "content": "The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_66",
            "start": 208,
            "end": 361,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_66@3",
            "content": "When we require agreement by all annotators, the worst overall class is CCONJ with 44%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_66",
            "start": 363,
            "end": 449,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_67@0",
            "content": "Qualitative Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_67",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_68@0",
            "content": "Figure 5 shows the output of our model for a sample text passage, where darker red indicates higher confidence in inserting a gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_68",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_68@1",
            "content": "The final model's predictions have a black frame (at, in, so, after, etc.) while the gold standard gaps are in yellow font (at, in, so, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_68",
            "start": 131,
            "end": 272,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_68@2",
            "content": "There are 8 matched gaps out of 11 in this example, yielding 72.73% accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_68",
            "start": 274,
            "end": 350,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_69@0",
            "content": "As can be seen in the figure, our model is able to identify appropriate gap candidates, even if they do not match the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_69",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_69@1",
            "content": "In fact, annotators considered all the unmatched gaps in this example (after, for and take) to be good and the second matched gap (in) to be inappropriate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_69",
            "start": 133,
            "end": 287,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_69@2",
            "content": "It is also interesting to see how the model prioritises function words and content words that are highly restricted in context (such as take or part), skilfully avoiding general gaps that could accept multiple answers and would be less effective for testing purposes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_69",
            "start": 289,
            "end": 555,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_70@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_70",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_71@0",
            "content": "We described the first transformer-based approach to open cloze test generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_71",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_71@1",
            "content": "Our ELECTRAbased model is trained on two objectives: token classification (gap/non-gap) and language modelling (for predicting the expected answer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_71",
            "start": 81,
            "end": 228,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_71@2",
            "content": "The model is further improved by manipulating the loss function and post-processing the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_71",
            "start": 230,
            "end": 325,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_72@0",
            "content": "System accuracy using automatic evaluation is 53.89% while human evaluation ranges between 75% \u2212 82%, showing that at least 7 out of 10 gaps predicted are considered useful by experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_72",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_72@1",
            "content": "A detailed analysis of results reveals a few structural problems such as gaps in close proximity and inappropriate difficulty, which we plan to address in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_72",
            "start": 185,
            "end": 351,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_72@2",
            "content": "Our test data and human annotations are released with this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_72",
            "start": 353,
            "end": 417,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_73@0",
            "content": "UNKNOWN, None, 2017, Automatically generating questions to support the acquisition of particle verbs: Evaluating via crowdsourcing. In CALL in a climate of change: adapting to turbulent global conditions -short papers from EUROCALL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_73",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_74@0",
            "content": "Kevin Clark, Minh-Thang Luong, Quoc Le, Christopher Manning, ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_74",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2001, Common European Framework of Reference for Languages: learning, teaching, assessment, Cambridge University Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_75",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_76@0",
            "content": "Bidyut Das, Mukta Majumder, Factual open cloze question generation for assessment of learner's knowledge, 2017, International Journal of Educational Technology in Higher Education, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_76",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_77@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_77",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_78@0",
            "content": "Mariano Felice, Paula Buttery, Entropy as a proxy for gap complexity in open cloze tests, 2019, Proceedings of the International Conference on Recent Advances in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_78",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_79@0",
            "content": "R\u00fcdiger Grotjahn, Christine Klein-Braley, Ulrich Raatz, C-tests: an overview, 2002, University language learning and the C-Test, AKS-Verlag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_79",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2020, A systematic review of automatic question generation for educational purposes, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_80",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_81@0",
            "content": "UNKNOWN, None, , Artificial Intelligence in Education, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_81",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_82@0",
            "content": "Ji-Ung Lee, Erik Schwan, Christian Meyer, Manipulating the difficulty of C-tests, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_82",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_83@0",
            "content": "Alexey Malafeev, Language exercise generation: Emulating cambridge open cloze, 2014, Int. J. Concept. Struct. Smart Appl, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_83",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_84@0",
            "content": "Edison Marrese-Taylor, Ai Nakajima, Yutaka Matsuo, Ono Yuichi, Learning to automatically generate fill-in-the-blank quizzes, 2018, Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_84",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_85@0",
            "content": "D Arya, Kevin Mccarthy, Geoff Yancey, Jesse Laflair, Manqian Egbert, Burr Liao,  Settles, Jump-starting item parameters for adaptive language tests, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_85",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_86@0",
            "content": "Jack Mostow, Yi-Ting Huang, Hyeju Jang, Anders Weinstein, Joe Valeri, Donna Gates, Developing, evaluating, and refining an automatic generator of diagnostic multiple choice cloze questions to assess children's comprehension while reading, 2017, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_86",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2005, Association of Language Testers in Europe (ALTE), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_87",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_88@0",
            "content": "UNKNOWN, None, , Association of Language Testers in Europe (ALTE). 2011. Manual for language test development and examining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_88",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_89@0",
            "content": "Juan Pino, Maxine Eskenazi, Measuring hint level in open cloze questions, 2009, Proceedings of the Twenty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS), AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_89",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2008, A selection strategy to improve cloze question quality. Intelligent Tutoring Systems for Ill-Defined Domains: Assessment and Feedback in Ill-Defined Domains, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_90",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2005, Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_91",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_92@0",
            "content": "Burr Settles, Geoffrey Laflair, Masato Hagiwara, Machine learning-driven language assessment, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_92",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_93@0",
            "content": "Tasanawan Soonklang, Sunee Pongpinigpinyo, Weenawadee Muangon, Sirak Kaewjamnong, Automatic question generation system for english exercise for secondary students, 2017, Proceedings of the 25th International Conference on Computers in Education (ICCE 2017), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_93",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_94@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems (NIPS), Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_94",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_95@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger,  Drame, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_95",
            "start": 0,
            "end": 463,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_96@0",
            "content": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy, Large-scale cloze test dataset created by teachers, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_96",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "258-ARR_v1_97@0",
            "content": "C Albert,  Yang, Y Irene, Brendan Chen, Hiroaki Flanagan,  Ogata, Automatic generation of cloze items for repeated testing to improve reading comprehension, 2021, Educational Technology & Society, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "258-ARR_v1_97",
            "start": 0,
            "end": 197,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_1",
            "tgt_ix": "258-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_1",
            "tgt_ix": "258-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_2",
            "tgt_ix": "258-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_4",
            "tgt_ix": "258-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_7",
            "tgt_ix": "258-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_8",
            "tgt_ix": "258-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_8",
            "tgt_ix": "258-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_8",
            "tgt_ix": "258-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_8",
            "tgt_ix": "258-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_11",
            "tgt_ix": "258-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_13",
            "tgt_ix": "258-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_15",
            "tgt_ix": "258-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_16",
            "tgt_ix": "258-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_17",
            "tgt_ix": "258-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_17",
            "tgt_ix": "258-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_17",
            "tgt_ix": "258-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_18",
            "tgt_ix": "258-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_20",
            "tgt_ix": "258-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_21",
            "tgt_ix": "258-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_19",
            "tgt_ix": "258-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_19",
            "tgt_ix": "258-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_19",
            "tgt_ix": "258-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_19",
            "tgt_ix": "258-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_17",
            "tgt_ix": "258-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_22",
            "tgt_ix": "258-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_24",
            "tgt_ix": "258-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_23",
            "tgt_ix": "258-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_23",
            "tgt_ix": "258-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_23",
            "tgt_ix": "258-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_23",
            "tgt_ix": "258-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_26",
            "tgt_ix": "258-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_29",
            "tgt_ix": "258-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_27",
            "tgt_ix": "258-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_27",
            "tgt_ix": "258-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_27",
            "tgt_ix": "258-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_27",
            "tgt_ix": "258-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_30",
            "tgt_ix": "258-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_31",
            "tgt_ix": "258-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_31",
            "tgt_ix": "258-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_33",
            "tgt_ix": "258-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_32",
            "tgt_ix": "258-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_32",
            "tgt_ix": "258-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_32",
            "tgt_ix": "258-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_31",
            "tgt_ix": "258-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_36",
            "tgt_ix": "258-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_37",
            "tgt_ix": "258-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_38",
            "tgt_ix": "258-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_39",
            "tgt_ix": "258-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_31",
            "tgt_ix": "258-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_40",
            "tgt_ix": "258-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_41",
            "tgt_ix": "258-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_41",
            "tgt_ix": "258-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_41",
            "tgt_ix": "258-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_43",
            "tgt_ix": "258-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_47",
            "tgt_ix": "258-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_48",
            "tgt_ix": "258-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_50",
            "tgt_ix": "258-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_51",
            "tgt_ix": "258-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_52",
            "tgt_ix": "258-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_54",
            "tgt_ix": "258-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_57",
            "tgt_ix": "258-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_58",
            "tgt_ix": "258-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_61",
            "tgt_ix": "258-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_60",
            "tgt_ix": "258-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_60",
            "tgt_ix": "258-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_60",
            "tgt_ix": "258-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_62",
            "tgt_ix": "258-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_65",
            "tgt_ix": "258-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_63",
            "tgt_ix": "258-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_63",
            "tgt_ix": "258-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_63",
            "tgt_ix": "258-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_63",
            "tgt_ix": "258-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_66",
            "tgt_ix": "258-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_68",
            "tgt_ix": "258-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_67",
            "tgt_ix": "258-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_67",
            "tgt_ix": "258-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_67",
            "tgt_ix": "258-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_69",
            "tgt_ix": "258-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_71",
            "tgt_ix": "258-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_70",
            "tgt_ix": "258-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_70",
            "tgt_ix": "258-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_70",
            "tgt_ix": "258-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "258-ARR_v1_0",
            "tgt_ix": "258-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_1",
            "tgt_ix": "258-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_2",
            "tgt_ix": "258-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_2",
            "tgt_ix": "258-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_2",
            "tgt_ix": "258-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_2",
            "tgt_ix": "258-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_3",
            "tgt_ix": "258-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_4",
            "tgt_ix": "258-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_4",
            "tgt_ix": "258-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_5",
            "tgt_ix": "258-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_6",
            "tgt_ix": "258-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_7",
            "tgt_ix": "258-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_8",
            "tgt_ix": "258-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_9",
            "tgt_ix": "258-ARR_v1_9@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_10",
            "tgt_ix": "258-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_11",
            "tgt_ix": "258-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_11",
            "tgt_ix": "258-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_12",
            "tgt_ix": "258-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_13",
            "tgt_ix": "258-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_13",
            "tgt_ix": "258-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_14",
            "tgt_ix": "258-ARR_v1_14@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_15",
            "tgt_ix": "258-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_15",
            "tgt_ix": "258-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_15",
            "tgt_ix": "258-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_16",
            "tgt_ix": "258-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_17",
            "tgt_ix": "258-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_18",
            "tgt_ix": "258-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_18",
            "tgt_ix": "258-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_19",
            "tgt_ix": "258-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_20",
            "tgt_ix": "258-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_21",
            "tgt_ix": "258-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_22",
            "tgt_ix": "258-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_22",
            "tgt_ix": "258-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_23",
            "tgt_ix": "258-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_24",
            "tgt_ix": "258-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_24",
            "tgt_ix": "258-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_25",
            "tgt_ix": "258-ARR_v1_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_26",
            "tgt_ix": "258-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_26",
            "tgt_ix": "258-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_27",
            "tgt_ix": "258-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_28",
            "tgt_ix": "258-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_29",
            "tgt_ix": "258-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_29",
            "tgt_ix": "258-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_30",
            "tgt_ix": "258-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_30",
            "tgt_ix": "258-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_31",
            "tgt_ix": "258-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_32",
            "tgt_ix": "258-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_33",
            "tgt_ix": "258-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_33",
            "tgt_ix": "258-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_33",
            "tgt_ix": "258-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_33",
            "tgt_ix": "258-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_34",
            "tgt_ix": "258-ARR_v1_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_35",
            "tgt_ix": "258-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_36",
            "tgt_ix": "258-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_37",
            "tgt_ix": "258-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_38",
            "tgt_ix": "258-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_38",
            "tgt_ix": "258-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_39",
            "tgt_ix": "258-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_40",
            "tgt_ix": "258-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_40",
            "tgt_ix": "258-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_41",
            "tgt_ix": "258-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_42",
            "tgt_ix": "258-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_43",
            "tgt_ix": "258-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_43",
            "tgt_ix": "258-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_43",
            "tgt_ix": "258-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_44",
            "tgt_ix": "258-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_45",
            "tgt_ix": "258-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_46",
            "tgt_ix": "258-ARR_v1_46@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_47",
            "tgt_ix": "258-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_47",
            "tgt_ix": "258-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_47",
            "tgt_ix": "258-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_47",
            "tgt_ix": "258-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_48",
            "tgt_ix": "258-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_48",
            "tgt_ix": "258-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_49",
            "tgt_ix": "258-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_50",
            "tgt_ix": "258-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_50",
            "tgt_ix": "258-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_50",
            "tgt_ix": "258-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_51",
            "tgt_ix": "258-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_51",
            "tgt_ix": "258-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_52",
            "tgt_ix": "258-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_53",
            "tgt_ix": "258-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_54",
            "tgt_ix": "258-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_54",
            "tgt_ix": "258-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_55",
            "tgt_ix": "258-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_56",
            "tgt_ix": "258-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_57",
            "tgt_ix": "258-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_57",
            "tgt_ix": "258-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_57",
            "tgt_ix": "258-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_58",
            "tgt_ix": "258-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_58",
            "tgt_ix": "258-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_58",
            "tgt_ix": "258-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_58",
            "tgt_ix": "258-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_59",
            "tgt_ix": "258-ARR_v1_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_60",
            "tgt_ix": "258-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_61",
            "tgt_ix": "258-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_62",
            "tgt_ix": "258-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_63",
            "tgt_ix": "258-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_64",
            "tgt_ix": "258-ARR_v1_64@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_65",
            "tgt_ix": "258-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_65",
            "tgt_ix": "258-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_66",
            "tgt_ix": "258-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_66",
            "tgt_ix": "258-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_66",
            "tgt_ix": "258-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_66",
            "tgt_ix": "258-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_67",
            "tgt_ix": "258-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_68",
            "tgt_ix": "258-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_68",
            "tgt_ix": "258-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_68",
            "tgt_ix": "258-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_69",
            "tgt_ix": "258-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_69",
            "tgt_ix": "258-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_69",
            "tgt_ix": "258-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_70",
            "tgt_ix": "258-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_71",
            "tgt_ix": "258-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_71",
            "tgt_ix": "258-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_71",
            "tgt_ix": "258-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_72",
            "tgt_ix": "258-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_72",
            "tgt_ix": "258-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_72",
            "tgt_ix": "258-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_73",
            "tgt_ix": "258-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_74",
            "tgt_ix": "258-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_75",
            "tgt_ix": "258-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_76",
            "tgt_ix": "258-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_77",
            "tgt_ix": "258-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_78",
            "tgt_ix": "258-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_79",
            "tgt_ix": "258-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_80",
            "tgt_ix": "258-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_81",
            "tgt_ix": "258-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_82",
            "tgt_ix": "258-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_83",
            "tgt_ix": "258-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_84",
            "tgt_ix": "258-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_85",
            "tgt_ix": "258-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_86",
            "tgt_ix": "258-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_87",
            "tgt_ix": "258-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_88",
            "tgt_ix": "258-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_89",
            "tgt_ix": "258-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_90",
            "tgt_ix": "258-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_91",
            "tgt_ix": "258-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_92",
            "tgt_ix": "258-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_93",
            "tgt_ix": "258-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_94",
            "tgt_ix": "258-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_95",
            "tgt_ix": "258-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_96",
            "tgt_ix": "258-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "258-ARR_v1_97",
            "tgt_ix": "258-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1190,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "258-ARR",
        "version": 1
    }
}