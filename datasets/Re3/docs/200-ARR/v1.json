{
    "nodes": [
        {
            "ix": "200-ARR_v1_0",
            "content": "Reducing Disambiguation Biases in NMT by Leveraging Explicit Word Sense Information",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_2",
            "content": "Recent works have shed some light on a common pitfall of Neural Machine Translation (NMT) models, lying in their struggle to disambiguate polysemous words without lapsing into their most frequently occurring sense in the training corpus. In this paper, we first provide a novel approach for automatically creating highprecision sense-annotated parallel corpora, and then we put forward a specifically tailored finetuning strategy to exploit such sense annotations during training without introducing any additional requirement at inference time. The use of explicit senses proved to be beneficial to reduce the disambiguation bias of a baseline NMT model, while, at the same time, leading our system to attain higher BLEU scores than its vanilla counterpart in 3 language pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "200-ARR_v1_4",
            "content": "Translating a sentence requires the underlying meaning to be captured and then expressed in the target language. Nonetheless, only little attention has been devoted to studying the actual capabilities of Neural Machine Translation (NMT) approaches of modeling different senses of ambiguous words, with recent work showing that systems tend to be biased towards the most frequent meanings found within the training corpus (Emelin et al., 2020). This phenomenon is hard to measure through classical evaluation metrics, such as the BLEU score (Papineni et al., 2002), as they often rely on wordmatching heuristics that fail to capture the disambiguation capabilities of the evaluated systems. Therefore, several efforts have been recently invested to shed some light and create adversarial test beds (Gonzales et al., 2017;Raganato et al., 2019;Emelin et al., 2020) to challenge NMT models. Results showed that these models still struggle to deal with highly polysemous words, especially when used to express least frequent senses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_5",
            "content": "For example, given the sentence \"The energy comes from a distant plant.\", both Google Translate and DeepL disambiguate 1 plant to its sense of organism when translating to Italian, and produce the following incorrect sentence \"L'energia proviene da una pianta lontana.\", rather than \"L'energia proviene da un impianto lontano.\", where impianto is the translation for the factory meaning of plant. This suggests that, even when adequate context is provided (energy should be enough to correctly infer the right sense of plant), state-of-the-art models might still be biased towards the most frequent meanings found within training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_6",
            "content": "Some recent works have explored how to leverage explicit sense information within NMT models (Gonzales et al., 2017;Pu et al., 2018;Nguyen et al., 2018). Nevertheless, including such information is not trivial mainly for three reasons: i) sense-tagged parallel data is scarce; ii) Word Sense Disambiguation (WSD) systems were not accurate enough until very recently (Blevins and Zettlemoyer, 2020;; and iii) it is not straightforward how one should incorporate explicit senses within neural models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_7",
            "content": "In this paper, we first introduce a novel approach to make up for the paucity of sense annotations in parallel corpora, leveraging a multilingual WSD system to tag parallel sentences and refine its predictions by means of cross-lingual word alignments and information from a multilingual knowledge base. Then, we fine-tune our baseline models on our sense-tagged corpora via a specifically designed loss function, allowing the injection of wordlevel semantics into the architecture. We evaluate our approach on standard and challenge test sets, showing that it does indeed improve translation accuracy and mitigates the most frequent sense bias.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_8",
            "content": "To summarize, our contributions are manifold:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_9",
            "content": "1. We put forward a novel approach to produce high-precision sense annotations for parallel data, which we apply to three language pairs. 2. We propose a fine-tuning strategy that lets us inject word-level explicit semantics into Neural Machine Translation models, without introducing any additional requirement at inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_10",
            "content": "3. We show that employing explicit sense tags is beneficial both to mitigate the sense bias and to improve the translation quality in terms of BLEU score on standard benchmarks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_11",
            "content": "4. We present a case study on how a state-of-theart WSD system compares to a NMT model on disambiguating words within a challenging set for detecting sense bias in MT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_12",
            "content": "We make all the generated datasets, the code of the model and for the experiments available at ANONYMOUS_URL. 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_13",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "200-ARR_v1_14",
            "content": "Word Sense Disambiguation has been first formulated as a computational task by Weaver (1949) in the context of Machine Translation. The two fields then followed parallel paths, with more or less successful attempts over the years to try joining them back together (Carpuat and Wu, 2005;Vickrey et al., 2005;Carpuat and Wu, 2007). Indeed, while Carpuat and Wu (2005) reported negative results when trying to integrate the prediction of a supervised WSD approach into a Statistical Machine Translation (SMT) model, the same authors, two years later, successfully improved the performance of a phrase-based SMT approach by leveraging a new phrase-based WSD model (Carpuat and Wu, 2007). More recently, Pu et al. (2018) and Nguyen et al. (2018) proposed systems that successfully leverage sense information in NMT models, although they introduce a heavy requirement, i.e., that of disambiguating the ambiguous words in the sentence prior to generating a translation, which makes them unfeasible in many realworld settings. Furthermore, contextualized word embeddings have been employed to produce additional back-translated parallel training data via mining sense-specific target sentences, to improve handling of infrequent senses (Hangya et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_15",
            "content": "Nevertheless, the proper treatment of lexical ambiguity is still an open problem, with neural models struggling to translate least frequent senses and often relying on spurious correlations among words (Emelin et al., 2020;Raganato et al., 2019;Gonzales et al., 2017). Thus, the disambiguation bias topic received a renewed interest, and several benchmarks have been introduced in the most recent years with the goal of directly measuring the extent to which neural architectures are able to capture word semantics. One of the first of this kind has been ContraWSD (Gonzales et al., 2017). In this first attempt to evaluate WSD capabilities of NMT models, the authors built an adversarial test set where source sentences containing an ambiguous word are associated with a correct translation and several incorrect alternatives. These latter are built by replacing the reference translation for the ambiguous word with the translation of one of its other possible meanings. The task measures whether a model ranks the correct translation higher, i.e., it assigns a higher probability, than the adversarial ones. This work provides evaluation data for two language pairs only, i.e., German\u2192English and German\u2192French, and it is now outdated as modern NMT models can easily attain high performances (Emelin et al., 2019). Thus, MuCoW (Raganato et al., 2019) took a step further and leveraged BabelNet (Navigli and Ponzetto, 2012) -a large multilingual knowledge base -and sense embeddings (Camacho-Collados et al., 2016;Mancini et al., 2017) to automatically create adversarial translations for five language pairs while also increasing the difficulty of the task itself; unfortunately, the fully automatic nature of these challenge sets makes them noisy and prone to contain irrelevant challenge samples. Recently, Emelin et al. (2020) proposed two challenge sets for the English\u2192German pair, one measuring the model sensitivity to most frequent senses while the other estimating, through adversarial injections, the susceptibility to changing a correct sense to a wrong one. Contrarily to previous works, these challenge sets are based on correlations among words in the training set and rely on manually-refined sense clusters, making them a great test bed for measuring disambiguation bias.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_16",
            "content": "Despite the effort in putting forward challenging sets of data to test WSD capabilities of NMT models, to the best of our knowledge, only a few approaches (Gonzales et al., 2017;Liu et al., 2018) have been proposed to mitigate this issue, and none of them are effective with modern Transformerbased architectures. Furthermore, while parallel corpora have been exploited to produce sense annotations in the past (Bonansinga and Bond, 2016;Delli Bovi et al., 2017), they have been built by utilizing outdated disambiguation approaches that have been recently surpassed by more advanced neural architectures. Indeed, the Word Sense Disambiguation field has received much attention in the most recent years, with several supervised approaches (Conia and Navigli, 2021;Blevins and Zettlemoyer, 2020; and sense embedding models (Loureiro and Jorge, 2019;Scarlini et al., 2020a,b;Wang et al., 2020) performing close to the upper bound limit of the inter-annotator agreement, which finally makes them feasible for inclusion in other downstream tasks, e.g., Machine Translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_17",
            "content": "Thus, differently from previous works in the literature, we focus on closing the gap between these two fields, i.e., Neural Machine Translation and Word Sense Disambiguation, by putting the recent advances in WSD at the service of NMT models. We propose a novel approach to create high-quality sense-annotated parallel corpora, which leverages the most recent advances in WSD, and use this semantic information to regularize a NMT model, making it less biased and capable of producing higher-quality translations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_18",
            "content": "Reducing the Disambiguation Bias in NMT",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "200-ARR_v1_19",
            "content": "Neural Machine Translation models are typically trained end-to-end to produce a target translation given a source sentence and, thus, they can only rely on the input context to resolve the ambiguity of polysemous words therein. Being pattern recognition algorithms at heart, these models fall prey to the inherent bias carried by the frequency of co-occurrence of words within parallel sentences, and thus tend to disambiguate words to the sense they most frequently encountered during training, even when the sentence does provide enough context to identify the correct sense. At the same time, Word Sense Disambiguation models, i.e., models specialized in associating a word in context with one of the meanings within a given sense inventory, have recently displayed remarkable results across different benchmarks and languages: the time may hence be ripe for them to be successfully included into downstream applications such as Neural Machine Translation. However, data that would allow to bring these two worlds together, i.e., parallel corpora where words are associated with semantic labels, are produced automatically by leveraging outdated approaches to WSD (Delli Bovi et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_20",
            "content": "In what follows, we first provide some preliminary information about resources and tools that we employ in our method ( \u00a7 3.1); then, we introduce a new approach to automatically annotate tokens within parallel sentences with sense annotations, i.e., labels explicitly defining their meanings ( \u00a7 3.2); finally, we leverage such annotations within a new approach that we propose in order to mitigate the sense bias while also improving the translation quality overall ( \u00a7 3.3). The intuition behind our work is that fixed sense labels describing word senses would help NMT models better encode the underlying meaning of the input sentence, thus generating less biased and overall better translations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_21",
            "content": "Preliminaries",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "200-ARR_v1_22",
            "content": "We draw sense labels from BabelNet (Navigli and Ponzetto, 2012), a multilingual knowledge base created by merging several semantic resources in different languages such as WordNet (Miller et al., 1990), Wikipedia, Wikidata, etc. BabelNet is structured in synsets, i.e., sets of synonymous senses in different languages. For instance, the concept of plant organism contains the following lexicalizations: plant EN , pianta IT , Pflanze DE , among others. Additionally, BabelNet provides lemma-tosynsets mappings. For example, the English noun plant belongs to the following nominal synsets: organism, industrial plant, actor in the audience and something placed secretly. 3 Since BabelNet contains millions of synsets, which may make the computation too expensive, we restrict the vocabulary only to those containing at least one English sense from WordNet, as also done in several other works (Barba et al., 2020;Scarlini et al., 2020b;Bevilacqua and Navigli, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_23",
            "content": "Building a Sense-Annotated Parallel Corpus",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "200-ARR_v1_24",
            "content": "Let us assume that our running example sentence \"The energy comes from a distant plant.\" appears within a parallel corpus paired with the following Italian translation: \"L'energia viene da un impianto lontano.\". As we said, by considering the English sentence alone, the word plant could take several meanings, among which organism and power plant. However, among these, only one is shared with its translation impianto, i.e., the power plant meaning. Therefore, considering the cross-lingual alignment of words may drastically reduce the set of valid meanings, making the disambiguation task much easier. Based on this intuition, given a parallel corpus, we perform the following two steps:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_25",
            "content": "1. Sense Scoring, where we employ a WSD system to assign to each content word a distribution over its possible meanings;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_26",
            "content": "2. Annotation Refinement, where we compute cross-lingual word alignments to reduce lexical ambiguity and finally assign the most suitable sense to each content word.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_27",
            "content": "Sense Scoring In this step, our goal is to assign to every content word within a sentence a distribution over its possible senses in BabelNet. To this end, given as input a sentence s 4 from a parallel corpus C, we first lemmatize and POS-tag it, then pass it through our WSD system which returns a distribution over its possible meanings. Formally, let w i be a content word in a sentence s = [w 1 , . . . , w n ], and \u03c3(w i ) the set of synsets associated with w i in BabelNet. The WSD system assigns a score c(S|w i , s) to each synset S \u2208 \u03c3(w i ) and we denote the synset of w i with the highest confidence as S * w i . As a result, each content word in a source or target sentence is associated with a sense distribution. However, applying a WSD system alone may not be sufficient to ensure high-quality annotations as the application domain may be different from the one of its training set. Therefore, in the next step we take advantage of the translation each sentence is paired with to refine sense annotations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_28",
            "content": "We produce word-level cross-lingual alignments between the source and the target sentences of the parallel corpus: given a pair of parallel sentences (s, t), we compute a list of alignments A = {(w s i , w t j )|w s i \u2208 s, w t j \u2208 t}. Thus, given an aligned word pair P = (w s i , w t j ) \u2208 A, let \u03c3(P ) = \u03c3(w s i ) \u2229 \u03c3(w t j ), i.e., the intersection of synsets that the two words may denote according to BabelNet: we discard annotations for any word pair such that \u03c3(P ) = \u2205 \u2228 |\u03c3(w s i )| < 2. In 4 s can be either a source or a target sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_29",
            "content": "other words, we retain all the aligned pairs (w s i , w t j ) such that the source word is polysemous 5 and the intersection of their senses is non-empty, thus ensuring higher annotation precision by leveraging the parallelism of words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_30",
            "content": "Finally, we assign a synset S * to both words (w s i , w t j ) in P as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_31",
            "content": "S * = S * w s i = S * w t j = argmax S\u2208\u03c3(P ) c(S|w s i , s) Z s + c(S|w t j , t) Z t Z s = S\u2208\u03c3(P ) c(S|w s i , s) Z t = S\u2208\u03c3(P ) c(S|w t i , t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_32",
            "content": "that is, the synset with the highest combined confidence score after normalizing over \u03c3(P ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_33",
            "content": "Semantic Injection",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "200-ARR_v1_34",
            "content": "Now that we can generate high-quality sense annotations, we describe our fine-tuning method to inject word-level semantics into a Neural Machine Translation model. Ideally, we want the model to benefit from such annotations during training while not being dependent on them at inference time. To satisfy both these desiderata, we adapt the model's vocabulary to handle synsets as well as subwords, and propose a specific loss that exploits the injected senses to improve the base model's handling of ambiguous words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_35",
            "content": "Semantically Enhancing Sentences In order to work with concepts, we need a way to represent them. Let us consider once more the sentence \"The energy comes from a distant plant.\": we rewrite it in order to also include the exact meaning for plant, which we computed as described in \u00a7 3.2: \"The energy comes from a distant plant plant factory \". Formally, given a source sentence s and a word w i annotated with sense S * w i : we simply represent w i as its standard segmentation followed by S * w i . Additionally, to enforce the connection between the tagged word and its sense annotation, we set the position ids for the word and the sense embedding to the same value, as if they were a single token, and represent the sense with its sense embedding 6 passed through a linear projection layer (as shown in Figure 2). This encoding scheme gracefully extends to the whole sentence, yielding the senseenhanced input representation for a given sentence s.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_36",
            "content": "We hereby propose the Semantic Consistency Regularization (SCR) objective. Formally, let x \u2032 and x \u2032\u2032 be two encodings (plain and sense-enhanced) of the same input sentence x and let y be the target sentence, we define SCR as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_37",
            "content": "SCR(\u03b8) = \u2212 log P \u03b8 (y|x \u2032 ) \u2212 log P \u03b8 (y|x \u2032\u2032 ) + D KL (P \u03b8 (y|x \u2032 ) || P \u03b8 (y|x \u2032\u2032 ))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_38",
            "content": "where \u03b8 is the set of trainable weights, D KL is the unidirectional Kullback-Leibler divergence (Kullback and Leibler, 1951) and P \u03b8 (y|x) represents an output distribution (a visual representation of SCR is reported in Figure 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_39",
            "content": "With this formulation, SCR jointly uses the same sentence with and without sense annotations as two separate inputs: while we train the model to be able to translate both plain and sense-enhanced sentences, by minimizing the divergence between the output distributions we also force the model to transfer the sense information from the senseenhanced input to the plain input, much like in a self-distillation process. At the same time, we still maintain the model's capability of translating without sense annotations, thus dropping their requirement at inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_40",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "200-ARR_v1_41",
            "content": "Our Model",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "200-ARR_v1_42",
            "content": "We employ as underlying model the standard Transformer architecture (Vaswani et al., 2017), with 6 encoder and 6 decoder layers. 7 Note that, while 7 We use randomly-initialized MarianMT models available in HuggingFace's transformers library (Wolf et al., 2020) for easier comparability with their trained versions. SCR can be applied to any pre-trained model, we retrain one from scratch as most of the other models available online use part of our test data as their training data (see \u00a7 4.2). Additional details about training configuration and hyperparameters are provided in \u00a7 A.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_43",
            "content": "Fine-tuning with SCR Additionally, to jumpstart the model's capabilities, we encode synsets not as randomly initialized learnable vectors (e.g., by extending the vocabulary), but with frozen pre-trained sense embeddings projected into the model's input space by means of a linear layer, the only additional learnable component of the model (Projection in Figure 2), which is dropped after the fine-tuning stage. As pre-trained sense embeddings we use ARES, since they provide multilingual representations for each synset in our vocabulary (Scarlini et al., 2020b). We study the impact of this choice in \u00a7 5.4. To perform token-level alignments, we use MultiMirror (Procopio et al., 2021). 8",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_44",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "200-ARR_v1_45",
            "content": "We experiment on three distinct language pairs: EN\u2192DE, EN\u2192ES and EN\u2192FR. Following (Emelin et al., 2020), we gather the data from WMT14 for German and French and WMT13 for Spanish, considering only sentences coming from either CommonCrawl, News Commentary or Europarl, to maintain similar order of magnitudes among language pairs (and to contain pre-processing and training times). As validation sets, we employ newstest2014 for EN\u2192DE, newstest2013 for EN\u2192FR and newstest2012 for EN\u2192ES. All datasets employed in this work are freely available for research purposes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_46",
            "content": "We process each parallel sentence of the considered corpora with the procedure described in \u00a7 3.2, taking into account only content words whose Partof-Speech tag is noun, as the challenge sets we evaluate upon only target nominal words. 9 For POS-tagging and lemmatization we use Stanza (Qi et al., 2020). As disambiguation system, we use EWISER (Bevilacqua and Navigli, 2020), a neural WSD model based on BERT (Devlin et al., 2019), which attained state-of-the-art performances on English as well as other languages. EWISER has been trained on SemCor (Miller et al., 1993) -the standard training set for WSD -and the WordNet Gloss corpus (Langone et al., 2004) -a semi-automatically annotated dataset featuring sense definitions. Detailed statistics of the base and produced parallel corpora are provided in \u00a7 A.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_47",
            "content": "We evaluate standard translation quality through the newstest datasets available in the specific WMT year (i.e., WMTXX corresponds to new-stest20XX). The standard evaluation is carried out by means of SacreBLEU (Post, 2018), with signature BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.5.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_48",
            "content": "To measure the disambiguation bias of each model we employ the challenge sets introduced by Emelin et al. ( 2020), composed of sentences reserved from the WMT14 English\u2192German corpus. These challenge sets are based on sense clusters built by automatically merging together BabelNet synsets, and then are manually refined to ensure their correctness. Each sense cluster contains an English polysemous word and a set of German monosemous terms, which uniquely identify a certain meaning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_49",
            "content": "These clusters have been used to create the following two challenge sets: WSD Bias and Adversarial. The former quantifies the intrinsic bias the model learned during training, while the latter measures how sensitive the model is to the insertion of terms that are usually associated with another sense cluster during training. Both challenge sets evaluate in terms of accuracy of correct disambiguation. A more detailed description of these datasets and their evaluation process is provided in \u00a7 A.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_50",
            "content": "Comparison Systems",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "200-ARR_v1_51",
            "content": "We compare our sense-enhanced model with the following architectures: In what follows we refer to our model finetuned with SCR as Baseline+SCR.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_52",
            "content": "We note that, due to how the WSD Bias challenge sets were constructed (i.e., by using sentences reserved from WMT14, see \u00a7 4.2), any fair evaluation against OPUS and MBart-50 is to be considered impossible, as such models have seen the sentences in the challenge sets during training. We therefore evaluate these two models only on standard BLEU metrics, and point out that the resulting scores should only be regarded as references for our models' competence in the translation task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_53",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "200-ARR_v1_54",
            "content": "In what follows, first, we show that our model attains BLEU scores in the same ballpark as state-ofthe-art approaches such as OPUS and MBart-50, despite the large gap in terms of parameters or training data. Then, we focus our evaluation on the WSD Bias, and compare our full-fledged model (Baseline+SCR) against its baseline variant.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_55",
            "content": "General Translation Quality",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "200-ARR_v1_56",
            "content": "In Table 1 we observe that the trained baselines are more than competent in the translation task: indeed, when considering average BLEU scores, they place between OPUS, which is trained on much more data but has the same parameter count, and MBart-50 (Tang et al., 2020), which is 8 times larger but is capable of translating English to 50 languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_57",
            "content": "Contrarily to common debiasing techniques, which often observe a degradation in performance on standard benchmarks (Clark et al., 2019;He et al., 2019), we report consistent BLEU improvements on all language pairs, all of which are statistically significant at different p-values (Table 1), providing empirical proof that the proposed method does not hurt the model's general translation capability, while it helps models generate less biased translations (as discussed in the upcoming sections).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_58",
            "content": "Disambiguation Bias",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "200-ARR_v1_59",
            "content": "Results on the Disambiguation Bias Challenge Sets ( \u00a7 4.2) are reported in Table 2 (numbers represent error rates), for both of which we show improvements: on the WSD Bias challenge set, the bias is reduced, significantly, by more than 1%; similarly, on the Adversarial challenge set, we see a reduction of homographs mistakenly disambiguated due to the injection of adversarial adjectives of 0.27%. We attribute this lower impact to the artificial nature of the adversarial sentences, some of which, by manual inspection, display poor grammatical fluency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_60",
            "content": "WSD Performance",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "200-ARR_v1_61",
            "content": "We conducted an analysis of the performance of EWISER on the English sentences of the WSD Bias Challenge Set, to see how it would fare in comparison with our NMT models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_62",
            "content": "Unfortunately, as the sense clusters were not directly associated with BabelNet synsets, we reconstructed this association automatically and managed to retrieve only 1847 of the 3000 sentences in the challenge set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_63",
            "content": "Having retrieved BabelNet synsets for the target terms, we can apply EWISER and check 10 10k bootstrap samples of 50% the test set's size each.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_64",
            "content": "whether the disambiguated synset matches one of the synsets retrieved for the sense cluster of the challenge sentence. Let us consider our running example, \"The energy comes from a distant plant.\", one last time: if EWISER disambiguates the term plant to its sense of organism, we count it as a mistake, similarly as if our NMT model translated it as pianta instead of impianto (i.e., its sense of factory). With that in mind, we evaluated EWISER, Baseline and Baseline+SCR on the aforementioned subset of sentences; we report the results of this evaluation in Table 2 (bottom).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_65",
            "content": "The results indicate that, for this setting, both NMT models actually perform quite better than a pre-trained disambiguation system. One reason might be the different distributions the models are trained on: by design, the challenge sentences follow a distribution similar to the corpus used to train the NMT model, whereas EWISER was trained on sentences coming from news corpora from the 1960s and dictionary-like definitions. Moreover, in theory, if we were to apply the refinement process described in \u00a7 3.2 to disambiguate the challenge sentences, we would achieve a perfect score, as the target German lemmas are monosemous and thus the disambiguation is implicitly solved. The results of using EWISER's raw annotations are discussed in \u00a7 5.4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_66",
            "content": "Finally, we chose not to perform a similar comparison on the Adversarial challenge set, as its examples were designed to specifically target NMT models via adversarial injections; we leave studying their impact in WSD systems as future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_67",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "200-ARR_v1_68",
            "content": "Ablation on SCR To measure the importance of the KL term in the loss, we fine-tune the model without including it in the SCR objective ( \u00a7 3.3) and report the results in Table 1 and 2 (row Baseline+SCR \u2212KL ). We observe that, without KL, the model struggles to leverage the double inputs efficiently; indeed, its translation performance drops around 1 BLEU point on average, while the error rates increase by roughly 1% on both bias challenge sets. These results back our intuition that the KL divergence helps to distill sense information from the sense-enhanced inputs, and is indeed a crucial component to our formulation. beddings and report this result in Tables 1 and 2 (row Baseline+SCR \u2212ARES ). As expected, both translation quality and disambiguation bias drop consistently. Indeed, learning sense embeddings from scratch is much harder than learning a mapping between a fixed space and a trainable one.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_69",
            "content": "We evaluate our sense Annotation Refinement process ( \u00a7 3.2) by fine-tuning the model on the unconstrained sense annotations provided by EWISER (Baseline+SCR \u2212AR ). In the bias evaluation (Table 2), the performances on both challenge sets drop significantly (p < 0.001), which is in line with EWISER's performance on this challenge set ( \u00a7 5.3). Furthermore, the BLEU scores drop too, although not as significantly (Table 1), but still always underperforming with respect to Base-line+SCR.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_70",
            "content": "Ablation on Sense Annotations Finally, we test whether the sense annotations have an impact by replacing them with random senses for the specific word, drawn from the sense vocabulary with uniform probability, during the fine-tuning stage (Baseline+SCR RAND ). 11 As expected, we observe that randomly injecting senses is detrimental, with important performance drops in both the standard and the bias evaluation benchmarks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_71",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "200-ARR_v1_72",
            "content": "In this paper, we presented a fine-tuning strategy that, by leveraging the explicit sense annotations produced by a novel high-precision technique, effectively reduces the disambiguation bias of a baseline Neural Machine Translation model while also strengthening translation performances, without introducing any requirement at inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_73",
            "content": "Our analysis on a strong disambiguation system has shown that its ability to disambiguate polysemous nouns is worse than that of a baseline NMT model, at least in the studied out-of-domain setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_74",
            "content": "We believe that this work paves the way for better bias reduction techniques in MT, while also fostering interest in the issue represented by the disambiguation bias. As future work, we plan to further study the ability of NMT models to perform Word Sense Disambiguation and to strengthen research at the intersection of these two fields to build stronger and more reliable models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_75",
            "content": "A.1 Bias Evaluation Challenge Sets",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_76",
            "content": "We hereby report a more detailed description of the datasets introduced by (Emelin et al., 2020). From \u00a7 4.2, recall that these challenge sets are based on sense clusters built on BabelNet, where each sense cluster contains an English polysemous word and a set of German monosemous terms, which uniquely identify a certain meaning. We highlight that there is no direct link between the sense clusters and the data produced by our Annotation Refinement process, as the sense clusters are i) heavily manually refined 12 and ii) based off of the entire BabelNet4 inventory (16M concepts), while EWISER only covers the subgraph of Ba-belNet linked to WordNet (117k concepts), as is common in the multilingual WSD setting. As such, we do not consider the evaluation to be favorable in any way towards our system.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_77",
            "content": "WSD Bias contains sentences whose targeted English term is likely to be translated to a specific different sense due to co-occurrences of words in the sentence itself. For example, in the sentence \"a lot of money was spent to renovate the capital\" the word capital is likely to be translated to its sense of amount of money due to the presence of the words money and spent. A mistake is detected if the term is translated to any of the German words contained in the most likely sense cluster. The goal of this task is to measure the intrinsic bias the model learned during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_78",
            "content": "Adversarial contains two sets of sentences, the original sentence and its adversarial counterpart, built by injecting an adjective that is likely to flip the disambiguation performed by the NMT model towards a specific sense. For example, given the sentence \"they met in the spring of 2020\", the adversarial example would be \"they met in the hot spring of 2020\". The injection of hot leads the model to translate spring to its sense of water source as opposed to its correct sense of season. A mistake is detected every time the non-adversarial sentence is translated to the correct sense, whereas its adversarial counterpart is flipped to the sense cluster the adjective points to. The goal of this task is to measure how sensitive the model is to the insertion of terms that are usually associated with another sense cluster during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_79",
            "content": "Our work is based on the assumption that providing a neural model with sense annotations for ambiguous words helps disambiguating them. While this is rather intuitive, and has been shown in previous works (Nguyen et al., 2018;Pu et Model training required on average 4 days on a 3090, 7 days on a 2080 Ti. Fine-tuning epochs required around 10 hours each (on a 3090), with most finishing due to early stopping before the end of the second epoch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_80",
            "content": "MarianMT models available on Hugging-Face Transformers (Wolf et al., 2020) (e.g., for EN\u2192DE, the model name is Helsinki-NLP/opus-mt-en-de).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_81",
            "content": "For instance, EN\u2192DE has 74.4M parameters, EN\u2192ES has 77.9M, EN\u2192FR has 75.1M.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_82",
            "content": "For the fine-tuning stage we added ARES (frozen), thus adding a number of parameters equal to ARES's size (1536) times the number of unique synsets in the dataset (refer to Table 4 for approximate numbers). We also added a trainable projection layer of size 1536 * 512 (512 is the Transformer's hidden dimension), thus adding 786k trainable parameters (which we drop after the finetuning).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_83",
            "content": "Model training hyperparameters Similarly to (Emelin et al., 2020), we train it on the entire dataset for a max of 100,000 steps with approximately 24k tokens per batch, label smoothing at 0.1 and an inverse square root learning rate scheduler with 4000 warmup steps. As optimizer, we use Adam (Kingma and Ba, 2015) with betas (0.99, 0.98) and learning rate 7 \u2022 10 \u22124 , additionally employing an early stopping strategy with patience 5, monitoring the BLEU score on a validation set. We produce translations at inference time using a beam size of 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_84",
            "content": "Fine-tuning hyperparameters For the finetuning, we resume training using the weights of the baseline models, change the learning to 1 \u2022 10 \u22125 and reduce the warmup to 1000 steps; additionally, we evaluate the model every 10% of the fine-tuning steps rather than after each epoch, as we observed fast convergence during fine-tuning and multiple epochs were superfluous.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_85",
            "content": "Table 3 reports the same results displayed in the paper, but includes the percentage of Correct translations for both challenge sets as well as the percentage of errors made from sentences that, after the injection of the adversarial adjectives, were translated to a sense that wasn't either the correct one nor the one targeted by the adversarial injection (i.e., other).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_86",
            "content": "Our work focuses on reducing the disambiguation biases picked up by NMT models during training. We acknowledge some limitations in our work:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_87",
            "content": "1. Due to limited computational budget and the high number of resources required to train and fine-tune NMT models from scratch, we had to limit ourselves to one run per experiment, though the consistency across languages seems to point to the empirical correctness of the claims.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_88",
            "content": "2. We evaluate the bias reduction explicitly only on the English\u2192German language pair. The reason for this is twofold: first, the datasets introduced by Emelin et al. (2020) only cover said pair, and require the accompanying training data be used in order to fully exploit the co-occurrences (and hence the biases) that the model is evaluated upon; second, upon manual inspection, we found that MuCoW (Raganato et al., 2019) contains many irrelevant candidates in its translation suite, and is in general very much affected by the noisy nature of BabelNet.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_89",
            "content": "Table 5 shows some examples of disambiguations corrected by our model according to the WSD Bias challenge set. The baseline is translating the terms to their most frequent sense (column Wrong sense), instead of the correct one (column Target sense). Moreover, the third example shows that this is not only a word matching task, as the improved model is able choose the correct subword and can capture the nuances of meaning in more uncommon senses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "200-ARR_v1_90",
            "content": "Edoardo Barba, Tommaso Pasini, Roberto Navigli, Esc: Redesigning wsd with extractive sense comprehension, 2021, Proc. of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Edoardo Barba",
                    "Tommaso Pasini",
                    "Roberto Navigli"
                ],
                "title": "Esc: Redesigning wsd with extractive sense comprehension",
                "pub_date": "2021",
                "pub_title": "Proc. of NAACL",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_91",
            "content": "Edoardo Barba, Luigi Procopio, Niccol\u00f2 Campolungo, Tommaso Pasini, Roberto Navigli, Mu-LaN: Multilingual label propagatioN for word sense disambiguation, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Edoardo Barba",
                    "Luigi Procopio",
                    "Niccol\u00f2 Campolungo",
                    "Tommaso Pasini",
                    "Roberto Navigli"
                ],
                "title": "Mu-LaN: Multilingual label propagatioN for word sense disambiguation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_92",
            "content": "Michele Bevilacqua, Roberto Navigli, Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Michele Bevilacqua",
                    "Roberto Navigli"
                ],
                "title": "Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_93",
            "content": "Terra Blevins, Luke Zettlemoyer, Moving down the long tail of word sense disambiguation with gloss informed bi-encoders, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Terra Blevins",
                    "Luke Zettlemoyer"
                ],
                "title": "Moving down the long tail of word sense disambiguation with gloss informed bi-encoders",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_94",
            "content": "Giulia Bonansinga, Francis Bond, Multilingual sense intersection in a parallel corpus with diverse language families, 2016, Proceedings of the 8th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Giulia Bonansinga",
                    "Francis Bond"
                ],
                "title": "Multilingual sense intersection in a parallel corpus with diverse language families",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 8th",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_95",
            "content": "UNKNOWN, None, , Global WordNet Conference (GWC), Global Wordnet Association.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Global WordNet Conference (GWC)",
                "pub": "Global Wordnet Association"
            }
        },
        {
            "ix": "200-ARR_v1_96",
            "content": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar, Roberto Navigli, Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities, 2016, Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jos\u00e9 Camacho-Collados",
                    "Mohammad Taher Pilehvar",
                    "Roberto Navigli"
                ],
                "title": "Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities",
                "pub_date": "2016",
                "pub_title": "Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_97",
            "content": "Marine Carpuat, Dekai Wu, Word sense disambiguation vs. statistical machine translation, 2005, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Marine Carpuat",
                    "Dekai Wu"
                ],
                "title": "Word sense disambiguation vs. statistical machine translation",
                "pub_date": "2005",
                "pub_title": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_98",
            "content": "Marine Carpuat, Dekai Wu, Improving statistical machine translation using word sense disambiguation, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Marine Carpuat",
                    "Dekai Wu"
                ],
                "title": "Improving statistical machine translation using word sense disambiguation",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_99",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Christopher Clark",
                    "Mark Yatskar",
                    "Luke Zettlemoyer"
                ],
                "title": "Don't take the easy way out: Ensemble based methods for avoiding known dataset biases",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_100",
            "content": "Simone Conia, Roberto Navigli, Framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Simone Conia",
                    "Roberto Navigli"
                ],
                "title": "Framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_101",
            "content": "Claudio Bovi, Jose Camacho-Collados, Alessandro Raganato, Roberto Navigli, Eurosense: Automatic harvesting of multilingual sense annotations from parallel text, 2017, Proceedings of the 55th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Claudio Bovi",
                    "Jose Camacho-Collados",
                    "Alessandro Raganato",
                    "Roberto Navigli"
                ],
                "title": "Eurosense: Automatic harvesting of multilingual sense annotations from parallel text",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_102",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "200-ARR_v1_103",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_104",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Chris Dyer",
                    "Victor Chahuneau",
                    "Noah Smith"
                ],
                "title": "A simple, fast, and effective reparameterization of IBM model 2",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_105",
            "content": "Denis Emelin, Ivan Titov, Rico Sennrich, Widening the representation bottleneck in neural machine translation with lexical shortcuts, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Denis Emelin",
                    "Ivan Titov",
                    "Rico Sennrich"
                ],
                "title": "Widening the representation bottleneck in neural machine translation with lexical shortcuts",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_106",
            "content": "Denis Emelin, Ivan Titov, Rico Sennrich, Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Denis Emelin",
                    "Ivan Titov",
                    "Rico Sennrich"
                ],
                "title": "Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_107",
            "content": "Annette Rios Gonzales, Laura Mascarell, Rico Sennrich, Improving word sense disambiguation in neural machine translation with sense embeddings, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Annette Rios Gonzales",
                    "Laura Mascarell",
                    "Rico Sennrich"
                ],
                "title": "Improving word sense disambiguation in neural machine translation with sense embeddings",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_108",
            "content": "Viktor Hangya, Qianchu Liu, Dario Stojanovski, Alexander Fraser, Anna Korhonen, Improving machine translation of rare and unseen word senses, 2021, Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Viktor Hangya",
                    "Qianchu Liu",
                    "Dario Stojanovski",
                    "Alexander Fraser",
                    "Anna Korhonen"
                ],
                "title": "Improving machine translation of rare and unseen word senses",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_109",
            "content": "UNKNOWN, None, 2019, Unlearn dataset bias in natural language inference by fitting, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Unlearn dataset bias in natural language inference by fitting",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_110",
            "content": "Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci, Roberto Navigli, Embedding words and senses together via joint knowledgeenhanced training, 2017, Proceedings of the 21st Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Massimiliano Mancini",
                    "Jose Camacho-Collados",
                    "Ignacio Iacobacci",
                    "Roberto Navigli"
                ],
                "title": "Embedding words and senses together via joint knowledgeenhanced training",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 21st Conference on Computational Natural Language Learning",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_111",
            "content": "Quinn Mcnemar, Note on the sampling error of the difference between correlated proportions or percentages, 1947, Psychometrika, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Quinn Mcnemar"
                ],
                "title": "Note on the sampling error of the difference between correlated proportions or percentages",
                "pub_date": "1947",
                "pub_title": "Psychometrika",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_112",
            "content": "George Miller, R Beckwith, Christiane Fellbaum, D Gross, K Miller, WordNet: an online lexical database, 1990, Int. J. Lexicogr, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "George Miller",
                    "R Beckwith",
                    "Christiane Fellbaum",
                    "D Gross",
                    "K Miller"
                ],
                "title": "WordNet: an online lexical database",
                "pub_date": "1990",
                "pub_title": "Int. J. Lexicogr",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_113",
            "content": "George Miller, Claudia Leacock, Randee Tengi, Ross Bunker, A semantic concordance, 1993, Proc. of the Workshop on Human Language Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "George Miller",
                    "Claudia Leacock",
                    "Randee Tengi",
                    "Ross Bunker"
                ],
                "title": "A semantic concordance",
                "pub_date": "1993",
                "pub_title": "Proc. of the Workshop on Human Language Technology",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_114",
            "content": "Roberto Navigli, Simone Ponzetto, Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network, 2012, Artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Roberto Navigli",
                    "Simone Ponzetto"
                ],
                "title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
                "pub_date": "2012",
                "pub_title": "Artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_115",
            "content": "Quang-Phuoc Nguyen, Anh-Dung Vo, Joon-Choul Shin, Cheol-Young Ock, Effect of word sense disambiguation on neural machine translation: A case study in korean, 2018, IEEE Access, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Quang-Phuoc Nguyen",
                    "Anh-Dung Vo",
                    "Joon-Choul Shin",
                    "Cheol-Young Ock"
                ],
                "title": "Effect of word sense disambiguation on neural machine translation: A case study in korean",
                "pub_date": "2018",
                "pub_title": "IEEE Access",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_116",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_117",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_118",
            "content": "Luigi Procopio, Edoardo Barba, Federico Martelli, Roberto Navigli, Multimirror: Neural crosslingual word alignment for multilingual word sense disambiguation, 2021, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Luigi Procopio",
                    "Edoardo Barba",
                    "Federico Martelli",
                    "Roberto Navigli"
                ],
                "title": "Multimirror: Neural crosslingual word alignment for multilingual word sense disambiguation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_119",
            "content": "Xiao Pu, Nikolaos Pappas, James Henderson, Andrei Popescu-Belis, Integrating weakly supervised word sense disambiguation into neural machine translation, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Xiao Pu",
                    "Nikolaos Pappas",
                    "James Henderson",
                    "Andrei Popescu-Belis"
                ],
                "title": "Integrating weakly supervised word sense disambiguation into neural machine translation",
                "pub_date": "2018",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_120",
            "content": "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher Manning, Stanza: A python natural language processing toolkit for many human languages, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Peng Qi",
                    "Yuhao Zhang",
                    "Yuhui Zhang",
                    "Jason Bolton",
                    "Christopher Manning"
                ],
                "title": "Stanza: A python natural language processing toolkit for many human languages",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_121",
            "content": "Alessandro Raganato, Yves Scherrer, J\u00f6rg Tiedemann, The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Alessandro Raganato",
                    "Yves Scherrer",
                    "J\u00f6rg Tiedemann"
                ],
                "title": "The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_122",
            "content": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli, Sense-annotated corpora for word sense disambiguation in multiple languages and domains, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Bianca Scarlini",
                    "Tommaso Pasini",
                    "Roberto Navigli"
                ],
                "title": "Sense-annotated corpora for word sense disambiguation in multiple languages and domains",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": "European Language Resources Association"
            }
        },
        {
            "ix": "200-ARR_v1_123",
            "content": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli, With more contexts comes better performance: Contextualized sense embeddings for allround word sense disambiguation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Bianca Scarlini",
                    "Tommaso Pasini",
                    "Roberto Navigli"
                ],
                "title": "With more contexts comes better performance: Contextualized sense embeddings for allround word sense disambiguation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_124",
            "content": "UNKNOWN, None, 2020, Multilingual translation with extensible multilingual pretraining and finetuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Multilingual translation with extensible multilingual pretraining and finetuning",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_125",
            "content": "J\u00f6rg Tiedemann, Santhosh Thottingal, OPUS-MT -Building open translation services for the World, 2020, Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "J\u00f6rg Tiedemann",
                    "Santhosh Thottingal"
                ],
                "title": "OPUS-MT -Building open translation services for the World",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_126",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_127",
            "content": "David Vickrey, Luke Biewald, Marc Teyssier, Daphne Koller, Word-sense disambiguation for machine translation, 2005, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "David Vickrey",
                    "Luke Biewald",
                    "Marc Teyssier",
                    "Daphne Koller"
                ],
                "title": "Word-sense disambiguation for machine translation",
                "pub_date": "2005",
                "pub_title": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "200-ARR_v1_128",
            "content": "Zikang Wang, Linjing Li, Daniel Zeng, Knowledge-enhanced natural language inference based on knowledge graphs, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Zikang Wang",
                    "Linjing Li",
                    "Daniel Zeng"
                ],
                "title": "Knowledge-enhanced natural language inference based on knowledge graphs",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_129",
            "content": "UNKNOWN, None, 1949, Translation. Machine Translation of Languages: Fourteen Essays, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "1949",
                "pub_title": "Translation. Machine Translation of Languages: Fourteen Essays",
                "pub": null
            }
        },
        {
            "ix": "200-ARR_v1_130",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "200-ARR_v1_0@0",
            "content": "Reducing Disambiguation Biases in NMT by Leveraging Explicit Word Sense Information",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_0",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_2@0",
            "content": "Recent works have shed some light on a common pitfall of Neural Machine Translation (NMT) models, lying in their struggle to disambiguate polysemous words without lapsing into their most frequently occurring sense in the training corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_2",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_2@1",
            "content": "In this paper, we first provide a novel approach for automatically creating highprecision sense-annotated parallel corpora, and then we put forward a specifically tailored finetuning strategy to exploit such sense annotations during training without introducing any additional requirement at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_2",
            "start": 238,
            "end": 544,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_2@2",
            "content": "The use of explicit senses proved to be beneficial to reduce the disambiguation bias of a baseline NMT model, while, at the same time, leading our system to attain higher BLEU scores than its vanilla counterpart in 3 language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_2",
            "start": 546,
            "end": 777,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_4@0",
            "content": "Translating a sentence requires the underlying meaning to be captured and then expressed in the target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_4",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_4@1",
            "content": "Nonetheless, only little attention has been devoted to studying the actual capabilities of Neural Machine Translation (NMT) approaches of modeling different senses of ambiguous words, with recent work showing that systems tend to be biased towards the most frequent meanings found within the training corpus (Emelin et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_4",
            "start": 113,
            "end": 442,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_4@2",
            "content": "This phenomenon is hard to measure through classical evaluation metrics, such as the BLEU score (Papineni et al., 2002), as they often rely on wordmatching heuristics that fail to capture the disambiguation capabilities of the evaluated systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_4",
            "start": 444,
            "end": 688,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_4@3",
            "content": "Therefore, several efforts have been recently invested to shed some light and create adversarial test beds (Gonzales et al., 2017;Raganato et al., 2019;Emelin et al., 2020) to challenge NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_4",
            "start": 690,
            "end": 886,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_4@4",
            "content": "Results showed that these models still struggle to deal with highly polysemous words, especially when used to express least frequent senses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_4",
            "start": 888,
            "end": 1027,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_5@0",
            "content": "For example, given the sentence \"The energy comes from a distant plant.\", both Google Translate and DeepL disambiguate 1 plant to its sense of organism when translating to Italian, and produce the following incorrect sentence \"L'energia proviene da una pianta lontana.\", rather than \"L'energia proviene da un impianto lontano.\", where impianto is the translation for the factory meaning of plant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_5",
            "start": 0,
            "end": 395,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_5@1",
            "content": "This suggests that, even when adequate context is provided (energy should be enough to correctly infer the right sense of plant), state-of-the-art models might still be biased towards the most frequent meanings found within training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_5",
            "start": 397,
            "end": 634,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_6@0",
            "content": "Some recent works have explored how to leverage explicit sense information within NMT models (Gonzales et al., 2017;Pu et al., 2018;Nguyen et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_6",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_6@1",
            "content": "Nevertheless, including such information is not trivial mainly for three reasons: i) sense-tagged parallel data is scarce; ii) Word Sense Disambiguation (WSD) systems were not accurate enough until very recently (Blevins and Zettlemoyer, 2020;; and iii) it is not straightforward how one should incorporate explicit senses within neural models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_6",
            "start": 154,
            "end": 497,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_7@0",
            "content": "In this paper, we first introduce a novel approach to make up for the paucity of sense annotations in parallel corpora, leveraging a multilingual WSD system to tag parallel sentences and refine its predictions by means of cross-lingual word alignments and information from a multilingual knowledge base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_7",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_7@1",
            "content": "Then, we fine-tune our baseline models on our sense-tagged corpora via a specifically designed loss function, allowing the injection of wordlevel semantics into the architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_7",
            "start": 304,
            "end": 481,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_7@2",
            "content": "We evaluate our approach on standard and challenge test sets, showing that it does indeed improve translation accuracy and mitigates the most frequent sense bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_7",
            "start": 483,
            "end": 644,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_8@0",
            "content": "To summarize, our contributions are manifold:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_8",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_9@0",
            "content": "1. We put forward a novel approach to produce high-precision sense annotations for parallel data, which we apply to three language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_9",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_9@1",
            "content": "2. We propose a fine-tuning strategy that lets us inject word-level explicit semantics into Neural Machine Translation models, without introducing any additional requirement at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_9",
            "start": 138,
            "end": 329,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_10@0",
            "content": "3. We show that employing explicit sense tags is beneficial both to mitigate the sense bias and to improve the translation quality in terms of BLEU score on standard benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_10",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_11@0",
            "content": "4. We present a case study on how a state-of-theart WSD system compares to a NMT model on disambiguating words within a challenging set for detecting sense bias in MT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_11",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_12@0",
            "content": "We make all the generated datasets, the code of the model and for the experiments available at ANONYMOUS_URL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_12",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_12@1",
            "content": "2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_12",
            "start": 110,
            "end": 110,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_13@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_13",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_14@0",
            "content": "Word Sense Disambiguation has been first formulated as a computational task by Weaver (1949) in the context of Machine Translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_14",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_14@1",
            "content": "The two fields then followed parallel paths, with more or less successful attempts over the years to try joining them back together (Carpuat and Wu, 2005;Vickrey et al., 2005;Carpuat and Wu, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_14",
            "start": 132,
            "end": 328,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_14@2",
            "content": "Indeed, while Carpuat and Wu (2005) reported negative results when trying to integrate the prediction of a supervised WSD approach into a Statistical Machine Translation (SMT) model, the same authors, two years later, successfully improved the performance of a phrase-based SMT approach by leveraging a new phrase-based WSD model (Carpuat and Wu, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_14",
            "start": 330,
            "end": 682,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_14@3",
            "content": "More recently, Pu et al. (2018) and Nguyen et al. (2018) proposed systems that successfully leverage sense information in NMT models, although they introduce a heavy requirement, i.e., that of disambiguating the ambiguous words in the sentence prior to generating a translation, which makes them unfeasible in many realworld settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_14",
            "start": 684,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_14@4",
            "content": "Furthermore, contextualized word embeddings have been employed to produce additional back-translated parallel training data via mining sense-specific target sentences, to improve handling of infrequent senses (Hangya et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_14",
            "start": 1019,
            "end": 1249,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@0",
            "content": "Nevertheless, the proper treatment of lexical ambiguity is still an open problem, with neural models struggling to translate least frequent senses and often relying on spurious correlations among words (Emelin et al., 2020;Raganato et al., 2019;Gonzales et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@1",
            "content": "Thus, the disambiguation bias topic received a renewed interest, and several benchmarks have been introduced in the most recent years with the goal of directly measuring the extent to which neural architectures are able to capture word semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 269,
            "end": 514,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@2",
            "content": "One of the first of this kind has been ContraWSD (Gonzales et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 516,
            "end": 588,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@3",
            "content": "In this first attempt to evaluate WSD capabilities of NMT models, the authors built an adversarial test set where source sentences containing an ambiguous word are associated with a correct translation and several incorrect alternatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 590,
            "end": 826,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@4",
            "content": "These latter are built by replacing the reference translation for the ambiguous word with the translation of one of its other possible meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 828,
            "end": 971,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@5",
            "content": "The task measures whether a model ranks the correct translation higher, i.e., it assigns a higher probability, than the adversarial ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 973,
            "end": 1109,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@6",
            "content": "This work provides evaluation data for two language pairs only, i.e., German\u2192English and German\u2192French, and it is now outdated as modern NMT models can easily attain high performances (Emelin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 1111,
            "end": 1316,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@7",
            "content": "Thus, MuCoW (Raganato et al., 2019) took a step further and leveraged BabelNet (Navigli and Ponzetto, 2012) -a large multilingual knowledge base -and sense embeddings (Camacho-Collados et al., 2016;Mancini et al., 2017) to automatically create adversarial translations for five language pairs while also increasing the difficulty of the task itself; unfortunately, the fully automatic nature of these challenge sets makes them noisy and prone to contain irrelevant challenge samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 1318,
            "end": 1800,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@8",
            "content": "Recently, Emelin et al. (2020) proposed two challenge sets for the English\u2192German pair, one measuring the model sensitivity to most frequent senses while the other estimating, through adversarial injections, the susceptibility to changing a correct sense to a wrong one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 1802,
            "end": 2071,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_15@9",
            "content": "Contrarily to previous works, these challenge sets are based on correlations among words in the training set and rely on manually-refined sense clusters, making them a great test bed for measuring disambiguation bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_15",
            "start": 2073,
            "end": 2289,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_16@0",
            "content": "Despite the effort in putting forward challenging sets of data to test WSD capabilities of NMT models, to the best of our knowledge, only a few approaches (Gonzales et al., 2017;Liu et al., 2018) have been proposed to mitigate this issue, and none of them are effective with modern Transformerbased architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_16",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_16@1",
            "content": "Furthermore, while parallel corpora have been exploited to produce sense annotations in the past (Bonansinga and Bond, 2016;Delli Bovi et al., 2017), they have been built by utilizing outdated disambiguation approaches that have been recently surpassed by more advanced neural architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_16",
            "start": 314,
            "end": 604,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_16@2",
            "content": "Indeed, the Word Sense Disambiguation field has received much attention in the most recent years, with several supervised approaches (Conia and Navigli, 2021;Blevins and Zettlemoyer, 2020; and sense embedding models (Loureiro and Jorge, 2019;Scarlini et al., 2020a,b;Wang et al., 2020) performing close to the upper bound limit of the inter-annotator agreement, which finally makes them feasible for inclusion in other downstream tasks, e.g., Machine Translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_16",
            "start": 606,
            "end": 1068,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_17@0",
            "content": "Thus, differently from previous works in the literature, we focus on closing the gap between these two fields, i.e., Neural Machine Translation and Word Sense Disambiguation, by putting the recent advances in WSD at the service of NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_17",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_17@1",
            "content": "We propose a novel approach to create high-quality sense-annotated parallel corpora, which leverages the most recent advances in WSD, and use this semantic information to regularize a NMT model, making it less biased and capable of producing higher-quality translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_17",
            "start": 243,
            "end": 512,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_18@0",
            "content": "Reducing the Disambiguation Bias in NMT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_18",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_19@0",
            "content": "Neural Machine Translation models are typically trained end-to-end to produce a target translation given a source sentence and, thus, they can only rely on the input context to resolve the ambiguity of polysemous words therein.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_19",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_19@1",
            "content": "Being pattern recognition algorithms at heart, these models fall prey to the inherent bias carried by the frequency of co-occurrence of words within parallel sentences, and thus tend to disambiguate words to the sense they most frequently encountered during training, even when the sentence does provide enough context to identify the correct sense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_19",
            "start": 228,
            "end": 576,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_19@2",
            "content": "At the same time, Word Sense Disambiguation models, i.e., models specialized in associating a word in context with one of the meanings within a given sense inventory, have recently displayed remarkable results across different benchmarks and languages: the time may hence be ripe for them to be successfully included into downstream applications such as Neural Machine Translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_19",
            "start": 578,
            "end": 958,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_19@3",
            "content": "However, data that would allow to bring these two worlds together, i.e., parallel corpora where words are associated with semantic labels, are produced automatically by leveraging outdated approaches to WSD (Delli Bovi et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_19",
            "start": 960,
            "end": 1192,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_20@0",
            "content": "In what follows, we first provide some preliminary information about resources and tools that we employ in our method ( \u00a7 3.1); then, we introduce a new approach to automatically annotate tokens within parallel sentences with sense annotations, i.e., labels explicitly defining their meanings ( \u00a7 3.2); finally, we leverage such annotations within a new approach that we propose in order to mitigate the sense bias while also improving the translation quality overall ( \u00a7 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_20",
            "start": 0,
            "end": 476,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_20@1",
            "content": "The intuition behind our work is that fixed sense labels describing word senses would help NMT models better encode the underlying meaning of the input sentence, thus generating less biased and overall better translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_20",
            "start": 478,
            "end": 699,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_21@0",
            "content": "Preliminaries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_21",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@0",
            "content": "We draw sense labels from BabelNet (Navigli and Ponzetto, 2012), a multilingual knowledge base created by merging several semantic resources in different languages such as WordNet (Miller et al., 1990), Wikipedia, Wikidata, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@1",
            "content": "BabelNet is structured in synsets, i.e., sets of synonymous senses in different languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 229,
            "end": 318,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@2",
            "content": "For instance, the concept of plant organism contains the following lexicalizations: plant EN , pianta IT , Pflanze DE , among others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 320,
            "end": 452,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@3",
            "content": "Additionally, BabelNet provides lemma-tosynsets mappings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 454,
            "end": 510,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@4",
            "content": "For example, the English noun plant belongs to the following nominal synsets: organism, industrial plant, actor in the audience and something placed secretly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 512,
            "end": 669,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_22@5",
            "content": "3 Since BabelNet contains millions of synsets, which may make the computation too expensive, we restrict the vocabulary only to those containing at least one English sense from WordNet, as also done in several other works (Barba et al., 2020;Scarlini et al., 2020b;Bevilacqua and Navigli, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_22",
            "start": 671,
            "end": 965,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_23@0",
            "content": "Building a Sense-Annotated Parallel Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_23",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_24@0",
            "content": "Let us assume that our running example sentence \"The energy comes from a distant plant.\" appears within a parallel corpus paired with the following Italian translation: \"L'energia viene da un impianto lontano.\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_24",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_24@1",
            "content": "As we said, by considering the English sentence alone, the word plant could take several meanings, among which organism and power plant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_24",
            "start": 212,
            "end": 347,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_24@2",
            "content": "However, among these, only one is shared with its translation impianto, i.e., the power plant meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_24",
            "start": 349,
            "end": 450,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_24@3",
            "content": "Therefore, considering the cross-lingual alignment of words may drastically reduce the set of valid meanings, making the disambiguation task much easier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_24",
            "start": 452,
            "end": 604,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_24@4",
            "content": "Based on this intuition, given a parallel corpus, we perform the following two steps:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_24",
            "start": 606,
            "end": 690,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_25@0",
            "content": "1. Sense Scoring, where we employ a WSD system to assign to each content word a distribution over its possible meanings;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_25",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_26@0",
            "content": "2. Annotation Refinement, where we compute cross-lingual word alignments to reduce lexical ambiguity and finally assign the most suitable sense to each content word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_26",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@0",
            "content": "Sense Scoring In this step, our goal is to assign to every content word within a sentence a distribution over its possible senses in BabelNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@1",
            "content": "To this end, given as input a sentence s 4 from a parallel corpus C, we first lemmatize and POS-tag it, then pass it through our WSD system which returns a distribution over its possible meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 143,
            "end": 338,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@2",
            "content": "Formally, let w i be a content word in a sentence s = [w 1 , . . . , w n ], and \u03c3(w i ) the set of synsets associated with w i in BabelNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 340,
            "end": 478,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@3",
            "content": "The WSD system assigns a score c(S|w i , s) to each synset S \u2208 \u03c3(w i ) and we denote the synset of w i with the highest confidence as S * w i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 480,
            "end": 622,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@4",
            "content": "As a result, each content word in a source or target sentence is associated with a sense distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 624,
            "end": 725,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@5",
            "content": "However, applying a WSD system alone may not be sufficient to ensure high-quality annotations as the application domain may be different from the one of its training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 727,
            "end": 896,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_27@6",
            "content": "Therefore, in the next step we take advantage of the translation each sentence is paired with to refine sense annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_27",
            "start": 898,
            "end": 1019,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_28@0",
            "content": "We produce word-level cross-lingual alignments between the source and the target sentences of the parallel corpus: given a pair of parallel sentences (s, t), we compute a list of alignments A = {(w s i , w t j )|w s i \u2208 s, w t j \u2208 t}. Thus, given an aligned word pair P = (w s i , w t j ) \u2208 A, let \u03c3(P ) = \u03c3(w s i ) \u2229 \u03c3(w t j ), i.e., the intersection of synsets that the two words may denote according to BabelNet: we discard annotations for any word pair such that \u03c3(P ) = \u2205 \u2228 |\u03c3(w s i )| < 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_28",
            "start": 0,
            "end": 494,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_28@1",
            "content": "In 4 s can be either a source or a target sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_28",
            "start": 496,
            "end": 546,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_29@0",
            "content": "other words, we retain all the aligned pairs (w s i , w t j ) such that the source word is polysemous 5 and the intersection of their senses is non-empty, thus ensuring higher annotation precision by leveraging the parallelism of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_29",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_30@0",
            "content": "Finally, we assign a synset S * to both words (w s i , w t j ) in P as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_30",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_31@0",
            "content": "S * = S * w s i = S * w t j = argmax S\u2208\u03c3(P ) c(S|w s i , s) Z s + c(S|w t j , t) Z t Z s = S\u2208\u03c3(P ) c(S|w s i , s) Z t = S\u2208\u03c3(P ) c(S|w t i , t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_31",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_32@0",
            "content": "that is, the synset with the highest combined confidence score after normalizing over \u03c3(P ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_32",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_33@0",
            "content": "Semantic Injection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_33",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_34@0",
            "content": "Now that we can generate high-quality sense annotations, we describe our fine-tuning method to inject word-level semantics into a Neural Machine Translation model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_34",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_34@1",
            "content": "Ideally, we want the model to benefit from such annotations during training while not being dependent on them at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_34",
            "start": 164,
            "end": 291,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_34@2",
            "content": "To satisfy both these desiderata, we adapt the model's vocabulary to handle synsets as well as subwords, and propose a specific loss that exploits the injected senses to improve the base model's handling of ambiguous words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_34",
            "start": 293,
            "end": 515,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_35@0",
            "content": "Semantically Enhancing Sentences In order to work with concepts, we need a way to represent them. Let us consider once more the sentence \"The energy comes from a distant plant.\": we rewrite it in order to also include the exact meaning for plant, which we computed as described in \u00a7 3.2: \"The energy comes from a distant plant plant factory \".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_35",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_35@1",
            "content": "Formally, given a source sentence s and a word w i annotated with sense S * w i : we simply represent w i as its standard segmentation followed by S * w i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_35",
            "start": 344,
            "end": 499,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_35@2",
            "content": "Additionally, to enforce the connection between the tagged word and its sense annotation, we set the position ids for the word and the sense embedding to the same value, as if they were a single token, and represent the sense with its sense embedding 6 passed through a linear projection layer (as shown in Figure 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_35",
            "start": 501,
            "end": 817,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_35@3",
            "content": "This encoding scheme gracefully extends to the whole sentence, yielding the senseenhanced input representation for a given sentence s.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_35",
            "start": 819,
            "end": 952,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_36@0",
            "content": "We hereby propose the Semantic Consistency Regularization (SCR) objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_36",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_36@1",
            "content": "Formally, let x \u2032 and x \u2032\u2032 be two encodings (plain and sense-enhanced) of the same input sentence x and let y be the target sentence, we define SCR as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_36",
            "start": 75,
            "end": 225,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_37@0",
            "content": "SCR(\u03b8) = \u2212 log P \u03b8 (y|x \u2032 ) \u2212 log P \u03b8 (y|x \u2032\u2032 ) + D KL (P \u03b8 (y|x \u2032 ) || P \u03b8 (y|x \u2032\u2032 ))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_37",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_38@0",
            "content": "where \u03b8 is the set of trainable weights, D KL is the unidirectional Kullback-Leibler divergence (Kullback and Leibler, 1951) and P \u03b8 (y|x) represents an output distribution (a visual representation of SCR is reported in Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_38",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_39@0",
            "content": "With this formulation, SCR jointly uses the same sentence with and without sense annotations as two separate inputs: while we train the model to be able to translate both plain and sense-enhanced sentences, by minimizing the divergence between the output distributions we also force the model to transfer the sense information from the senseenhanced input to the plain input, much like in a self-distillation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_39",
            "start": 0,
            "end": 416,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_39@1",
            "content": "At the same time, we still maintain the model's capability of translating without sense annotations, thus dropping their requirement at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_39",
            "start": 418,
            "end": 568,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_40@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_40",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_41@0",
            "content": "Our Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_41",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_42@0",
            "content": "We employ as underlying model the standard Transformer architecture (Vaswani et al., 2017), with 6 encoder and 6 decoder layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_42",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_42@1",
            "content": "7 Note that, while 7 We use randomly-initialized MarianMT models available in HuggingFace's transformers library (Wolf et al., 2020) for easier comparability with their trained versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_42",
            "start": 129,
            "end": 314,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_42@2",
            "content": "SCR can be applied to any pre-trained model, we retrain one from scratch as most of the other models available online use part of our test data as their training data (see \u00a7 4.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_42",
            "start": 316,
            "end": 494,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_42@3",
            "content": "Additional details about training configuration and hyperparameters are provided in \u00a7 A.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_42",
            "start": 496,
            "end": 585,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_43@0",
            "content": "Fine-tuning with SCR Additionally, to jumpstart the model's capabilities, we encode synsets not as randomly initialized learnable vectors (e.g., by extending the vocabulary), but with frozen pre-trained sense embeddings projected into the model's input space by means of a linear layer, the only additional learnable component of the model (Projection in Figure 2), which is dropped after the fine-tuning stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_43",
            "start": 0,
            "end": 410,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_43@1",
            "content": "As pre-trained sense embeddings we use ARES, since they provide multilingual representations for each synset in our vocabulary (Scarlini et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_43",
            "start": 412,
            "end": 563,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_43@2",
            "content": "We study the impact of this choice in \u00a7 5.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_43",
            "start": 565,
            "end": 608,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_43@3",
            "content": "To perform token-level alignments, we use MultiMirror (Procopio et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_43",
            "start": 610,
            "end": 687,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_43@4",
            "content": "8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_43",
            "start": 689,
            "end": 689,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_44@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_44",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_45@0",
            "content": "We experiment on three distinct language pairs: EN\u2192DE, EN\u2192ES and EN\u2192FR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_45",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_45@1",
            "content": "Following (Emelin et al., 2020), we gather the data from WMT14 for German and French and WMT13 for Spanish, considering only sentences coming from either CommonCrawl, News Commentary or Europarl, to maintain similar order of magnitudes among language pairs (and to contain pre-processing and training times).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_45",
            "start": 72,
            "end": 379,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_45@2",
            "content": "As validation sets, we employ newstest2014 for EN\u2192DE, newstest2013 for EN\u2192FR and newstest2012 for EN\u2192ES.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_45",
            "start": 381,
            "end": 484,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_45@3",
            "content": "All datasets employed in this work are freely available for research purposes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_45",
            "start": 486,
            "end": 563,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_46@0",
            "content": "We process each parallel sentence of the considered corpora with the procedure described in \u00a7 3.2, taking into account only content words whose Partof-Speech tag is noun, as the challenge sets we evaluate upon only target nominal words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_46",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_46@1",
            "content": "9 For POS-tagging and lemmatization we use Stanza (Qi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_46",
            "start": 237,
            "end": 304,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_46@2",
            "content": "As disambiguation system, we use EWISER (Bevilacqua and Navigli, 2020), a neural WSD model based on BERT (Devlin et al., 2019), which attained state-of-the-art performances on English as well as other languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_46",
            "start": 306,
            "end": 516,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_46@3",
            "content": "EWISER has been trained on SemCor (Miller et al., 1993) -the standard training set for WSD -and the WordNet Gloss corpus (Langone et al., 2004) -a semi-automatically annotated dataset featuring sense definitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_46",
            "start": 518,
            "end": 729,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_46@4",
            "content": "Detailed statistics of the base and produced parallel corpora are provided in \u00a7 A.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_46",
            "start": 731,
            "end": 814,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_47@0",
            "content": "We evaluate standard translation quality through the newstest datasets available in the specific WMT year (i.e., WMTXX corresponds to new-stest20XX).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_47",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_47@1",
            "content": "The standard evaluation is carried out by means of SacreBLEU (Post, 2018), with signature BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.5.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_47",
            "start": 150,
            "end": 299,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_48@0",
            "content": "To measure the disambiguation bias of each model we employ the challenge sets introduced by Emelin et al. ( 2020), composed of sentences reserved from the WMT14 English\u2192German corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_48",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_48@1",
            "content": "These challenge sets are based on sense clusters built by automatically merging together BabelNet synsets, and then are manually refined to ensure their correctness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_48",
            "start": 184,
            "end": 348,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_48@2",
            "content": "Each sense cluster contains an English polysemous word and a set of German monosemous terms, which uniquely identify a certain meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_48",
            "start": 350,
            "end": 484,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_49@0",
            "content": "These clusters have been used to create the following two challenge sets: WSD Bias and Adversarial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_49",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_49@1",
            "content": "The former quantifies the intrinsic bias the model learned during training, while the latter measures how sensitive the model is to the insertion of terms that are usually associated with another sense cluster during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_49",
            "start": 100,
            "end": 325,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_49@2",
            "content": "Both challenge sets evaluate in terms of accuracy of correct disambiguation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_49",
            "start": 327,
            "end": 402,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_49@3",
            "content": "A more detailed description of these datasets and their evaluation process is provided in \u00a7 A.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_49",
            "start": 404,
            "end": 499,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_50@0",
            "content": "Comparison Systems",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_50",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_51@0",
            "content": "We compare our sense-enhanced model with the following architectures: In what follows we refer to our model finetuned with SCR as Baseline+SCR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_51",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_52@0",
            "content": "We note that, due to how the WSD Bias challenge sets were constructed (i.e., by using sentences reserved from WMT14, see \u00a7 4.2), any fair evaluation against OPUS and MBart-50 is to be considered impossible, as such models have seen the sentences in the challenge sets during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_52",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_52@1",
            "content": "We therefore evaluate these two models only on standard BLEU metrics, and point out that the resulting scores should only be regarded as references for our models' competence in the translation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_52",
            "start": 285,
            "end": 483,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_53@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_53",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_54@0",
            "content": "In what follows, first, we show that our model attains BLEU scores in the same ballpark as state-ofthe-art approaches such as OPUS and MBart-50, despite the large gap in terms of parameters or training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_54",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_54@1",
            "content": "Then, we focus our evaluation on the WSD Bias, and compare our full-fledged model (Baseline+SCR) against its baseline variant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_54",
            "start": 208,
            "end": 333,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_55@0",
            "content": "General Translation Quality",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_55",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_56@0",
            "content": "In Table 1 we observe that the trained baselines are more than competent in the translation task: indeed, when considering average BLEU scores, they place between OPUS, which is trained on much more data but has the same parameter count, and MBart-50 (Tang et al., 2020), which is 8 times larger but is capable of translating English to 50 languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_56",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_57@0",
            "content": "Contrarily to common debiasing techniques, which often observe a degradation in performance on standard benchmarks (Clark et al., 2019;He et al., 2019), we report consistent BLEU improvements on all language pairs, all of which are statistically significant at different p-values (Table 1), providing empirical proof that the proposed method does not hurt the model's general translation capability, while it helps models generate less biased translations (as discussed in the upcoming sections).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_57",
            "start": 0,
            "end": 495,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_58@0",
            "content": "Disambiguation Bias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_58",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_59@0",
            "content": "Results on the Disambiguation Bias Challenge Sets ( \u00a7 4.2) are reported in Table 2 (numbers represent error rates), for both of which we show improvements: on the WSD Bias challenge set, the bias is reduced, significantly, by more than 1%; similarly, on the Adversarial challenge set, we see a reduction of homographs mistakenly disambiguated due to the injection of adversarial adjectives of 0.27%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_59",
            "start": 0,
            "end": 398,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_59@1",
            "content": "We attribute this lower impact to the artificial nature of the adversarial sentences, some of which, by manual inspection, display poor grammatical fluency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_59",
            "start": 400,
            "end": 555,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_60@0",
            "content": "WSD Performance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_60",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_61@0",
            "content": "We conducted an analysis of the performance of EWISER on the English sentences of the WSD Bias Challenge Set, to see how it would fare in comparison with our NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_61",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_62@0",
            "content": "Unfortunately, as the sense clusters were not directly associated with BabelNet synsets, we reconstructed this association automatically and managed to retrieve only 1847 of the 3000 sentences in the challenge set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_62",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_63@0",
            "content": "Having retrieved BabelNet synsets for the target terms, we can apply EWISER and check 10 10k bootstrap samples of 50% the test set's size each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_63",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_64@0",
            "content": "whether the disambiguated synset matches one of the synsets retrieved for the sense cluster of the challenge sentence. Let us consider our running example, \"The energy comes from a distant plant.\", one last time: if EWISER disambiguates the term plant to its sense of organism, we count it as a mistake, similarly as if our NMT model translated it as pianta instead of impianto (i.e., its sense of factory).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_64",
            "start": 0,
            "end": 406,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_64@1",
            "content": "With that in mind, we evaluated EWISER, Baseline and Baseline+SCR on the aforementioned subset of sentences; we report the results of this evaluation in Table 2 (bottom).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_64",
            "start": 408,
            "end": 577,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_65@0",
            "content": "The results indicate that, for this setting, both NMT models actually perform quite better than a pre-trained disambiguation system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_65",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_65@1",
            "content": "One reason might be the different distributions the models are trained on: by design, the challenge sentences follow a distribution similar to the corpus used to train the NMT model, whereas EWISER was trained on sentences coming from news corpora from the 1960s and dictionary-like definitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_65",
            "start": 133,
            "end": 427,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_65@2",
            "content": "Moreover, in theory, if we were to apply the refinement process described in \u00a7 3.2 to disambiguate the challenge sentences, we would achieve a perfect score, as the target German lemmas are monosemous and thus the disambiguation is implicitly solved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_65",
            "start": 429,
            "end": 678,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_65@3",
            "content": "The results of using EWISER's raw annotations are discussed in \u00a7 5.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_65",
            "start": 680,
            "end": 748,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_66@0",
            "content": "Finally, we chose not to perform a similar comparison on the Adversarial challenge set, as its examples were designed to specifically target NMT models via adversarial injections; we leave studying their impact in WSD systems as future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_66",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_67@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_67",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@0",
            "content": "Ablation on SCR To measure the importance of the KL term in the loss, we fine-tune the model without including it in the SCR objective ( \u00a7 3.3) and report the results in Table 1 and 2 (row Baseline+SCR \u2212KL ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@1",
            "content": "We observe that, without KL, the model struggles to leverage the double inputs efficiently; indeed, its translation performance drops around 1 BLEU point on average, while the error rates increase by roughly 1% on both bias challenge sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 209,
            "end": 447,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@2",
            "content": "These results back our intuition that the KL divergence helps to distill sense information from the sense-enhanced inputs, and is indeed a crucial component to our formulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 449,
            "end": 624,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@3",
            "content": "beddings and report this result in Tables 1 and 2 (row Baseline+SCR \u2212ARES ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 626,
            "end": 701,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@4",
            "content": "As expected, both translation quality and disambiguation bias drop consistently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 703,
            "end": 782,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_68@5",
            "content": "Indeed, learning sense embeddings from scratch is much harder than learning a mapping between a fixed space and a trainable one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_68",
            "start": 784,
            "end": 911,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_69@0",
            "content": "We evaluate our sense Annotation Refinement process ( \u00a7 3.2) by fine-tuning the model on the unconstrained sense annotations provided by EWISER (Baseline+SCR \u2212AR ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_69",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_69@1",
            "content": "In the bias evaluation (Table 2), the performances on both challenge sets drop significantly (p < 0.001), which is in line with EWISER's performance on this challenge set ( \u00a7 5.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_69",
            "start": 165,
            "end": 344,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_69@2",
            "content": "Furthermore, the BLEU scores drop too, although not as significantly (Table 1), but still always underperforming with respect to Base-line+SCR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_69",
            "start": 346,
            "end": 488,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_70@0",
            "content": "Ablation on Sense Annotations Finally, we test whether the sense annotations have an impact by replacing them with random senses for the specific word, drawn from the sense vocabulary with uniform probability, during the fine-tuning stage (Baseline+SCR RAND ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_70",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_70@1",
            "content": "11 As expected, we observe that randomly injecting senses is detrimental, with important performance drops in both the standard and the bias evaluation benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_70",
            "start": 261,
            "end": 423,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_71@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_71",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_72@0",
            "content": "In this paper, we presented a fine-tuning strategy that, by leveraging the explicit sense annotations produced by a novel high-precision technique, effectively reduces the disambiguation bias of a baseline Neural Machine Translation model while also strengthening translation performances, without introducing any requirement at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_72",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_73@0",
            "content": "Our analysis on a strong disambiguation system has shown that its ability to disambiguate polysemous nouns is worse than that of a baseline NMT model, at least in the studied out-of-domain setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_73",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_74@0",
            "content": "We believe that this work paves the way for better bias reduction techniques in MT, while also fostering interest in the issue represented by the disambiguation bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_74",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_74@1",
            "content": "As future work, we plan to further study the ability of NMT models to perform Word Sense Disambiguation and to strengthen research at the intersection of these two fields to build stronger and more reliable models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_74",
            "start": 167,
            "end": 380,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_75@0",
            "content": "A.1 Bias Evaluation Challenge Sets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_75",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_76@0",
            "content": "We hereby report a more detailed description of the datasets introduced by (Emelin et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_76",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_76@1",
            "content": "From \u00a7 4.2, recall that these challenge sets are based on sense clusters built on BabelNet, where each sense cluster contains an English polysemous word and a set of German monosemous terms, which uniquely identify a certain meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_76",
            "start": 98,
            "end": 330,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_76@2",
            "content": "We highlight that there is no direct link between the sense clusters and the data produced by our Annotation Refinement process, as the sense clusters are i) heavily manually refined 12 and ii) based off of the entire BabelNet4 inventory (16M concepts), while EWISER only covers the subgraph of Ba-belNet linked to WordNet (117k concepts), as is common in the multilingual WSD setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_76",
            "start": 332,
            "end": 716,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_76@3",
            "content": "As such, we do not consider the evaluation to be favorable in any way towards our system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_76",
            "start": 718,
            "end": 806,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_77@0",
            "content": "WSD Bias contains sentences whose targeted English term is likely to be translated to a specific different sense due to co-occurrences of words in the sentence itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_77",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_77@1",
            "content": "For example, in the sentence \"a lot of money was spent to renovate the capital\" the word capital is likely to be translated to its sense of amount of money due to the presence of the words money and spent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_77",
            "start": 168,
            "end": 372,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_77@2",
            "content": "A mistake is detected if the term is translated to any of the German words contained in the most likely sense cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_77",
            "start": 374,
            "end": 491,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_77@3",
            "content": "The goal of this task is to measure the intrinsic bias the model learned during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_77",
            "start": 493,
            "end": 581,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_78@0",
            "content": "Adversarial contains two sets of sentences, the original sentence and its adversarial counterpart, built by injecting an adjective that is likely to flip the disambiguation performed by the NMT model towards a specific sense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_78",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_78@1",
            "content": "For example, given the sentence \"they met in the spring of 2020\", the adversarial example would be \"they met in the hot spring of 2020\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_78",
            "start": 226,
            "end": 361,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_78@2",
            "content": "The injection of hot leads the model to translate spring to its sense of water source as opposed to its correct sense of season.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_78",
            "start": 363,
            "end": 490,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_78@3",
            "content": "A mistake is detected every time the non-adversarial sentence is translated to the correct sense, whereas its adversarial counterpart is flipped to the sense cluster the adjective points to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_78",
            "start": 492,
            "end": 681,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_78@4",
            "content": "The goal of this task is to measure how sensitive the model is to the insertion of terms that are usually associated with another sense cluster during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_78",
            "start": 683,
            "end": 842,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_79@0",
            "content": "Our work is based on the assumption that providing a neural model with sense annotations for ambiguous words helps disambiguating them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_79",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_79@1",
            "content": "While this is rather intuitive, and has been shown in previous works (Nguyen et al., 2018;Pu et Model training required on average 4 days on a 3090, 7 days on a 2080 Ti.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_79",
            "start": 136,
            "end": 304,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_79@2",
            "content": "Fine-tuning epochs required around 10 hours each (on a 3090), with most finishing due to early stopping before the end of the second epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_79",
            "start": 306,
            "end": 444,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_80@0",
            "content": "MarianMT models available on Hugging-Face Transformers (Wolf et al., 2020) (e.g., for EN\u2192DE, the model name is Helsinki-NLP/opus-mt-en-de).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_80",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_81@0",
            "content": "For instance, EN\u2192DE has 74.4M parameters, EN\u2192ES has 77.9M, EN\u2192FR has 75.1M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_81",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_82@0",
            "content": "For the fine-tuning stage we added ARES (frozen), thus adding a number of parameters equal to ARES's size (1536) times the number of unique synsets in the dataset (refer to Table 4 for approximate numbers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_82",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_82@1",
            "content": "We also added a trainable projection layer of size 1536 * 512 (512 is the Transformer's hidden dimension), thus adding 786k trainable parameters (which we drop after the finetuning).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_82",
            "start": 207,
            "end": 388,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_83@0",
            "content": "Model training hyperparameters Similarly to (Emelin et al., 2020), we train it on the entire dataset for a max of 100,000 steps with approximately 24k tokens per batch, label smoothing at 0.1 and an inverse square root learning rate scheduler with 4000 warmup steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_83",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_83@1",
            "content": "As optimizer, we use Adam (Kingma and Ba, 2015) with betas (0.99, 0.98) and learning rate 7 \u2022 10 \u22124 , additionally employing an early stopping strategy with patience 5, monitoring the BLEU score on a validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_83",
            "start": 267,
            "end": 481,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_83@2",
            "content": "We produce translations at inference time using a beam size of 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_83",
            "start": 483,
            "end": 547,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_84@0",
            "content": "Fine-tuning hyperparameters For the finetuning, we resume training using the weights of the baseline models, change the learning to 1 \u2022 10 \u22125 and reduce the warmup to 1000 steps; additionally, we evaluate the model every 10% of the fine-tuning steps rather than after each epoch, as we observed fast convergence during fine-tuning and multiple epochs were superfluous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_84",
            "start": 0,
            "end": 367,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_85@0",
            "content": "Table 3 reports the same results displayed in the paper, but includes the percentage of Correct translations for both challenge sets as well as the percentage of errors made from sentences that, after the injection of the adversarial adjectives, were translated to a sense that wasn't either the correct one nor the one targeted by the adversarial injection (i.e., other).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_85",
            "start": 0,
            "end": 371,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_86@0",
            "content": "Our work focuses on reducing the disambiguation biases picked up by NMT models during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_86",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_86@1",
            "content": "We acknowledge some limitations in our work:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_86",
            "start": 96,
            "end": 139,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_87@0",
            "content": "1. Due to limited computational budget and the high number of resources required to train and fine-tune NMT models from scratch, we had to limit ourselves to one run per experiment, though the consistency across languages seems to point to the empirical correctness of the claims.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_87",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_88@0",
            "content": "2. We evaluate the bias reduction explicitly only on the English\u2192German language pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_88",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_88@1",
            "content": "The reason for this is twofold: first, the datasets introduced by Emelin et al. (2020) only cover said pair, and require the accompanying training data be used in order to fully exploit the co-occurrences (and hence the biases) that the model is evaluated upon; second, upon manual inspection, we found that MuCoW (Raganato et al., 2019) contains many irrelevant candidates in its translation suite, and is in general very much affected by the noisy nature of BabelNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_88",
            "start": 87,
            "end": 555,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_89@0",
            "content": "Table 5 shows some examples of disambiguations corrected by our model according to the WSD Bias challenge set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_89",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_89@1",
            "content": "The baseline is translating the terms to their most frequent sense (column Wrong sense), instead of the correct one (column Target sense).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_89",
            "start": 111,
            "end": 248,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_89@2",
            "content": "Moreover, the third example shows that this is not only a word matching task, as the improved model is able choose the correct subword and can capture the nuances of meaning in more uncommon senses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_89",
            "start": 250,
            "end": 447,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_90@0",
            "content": "Edoardo Barba, Tommaso Pasini, Roberto Navigli, Esc: Redesigning wsd with extractive sense comprehension, 2021, Proc. of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_90",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_91@0",
            "content": "Edoardo Barba, Luigi Procopio, Niccol\u00f2 Campolungo, Tommaso Pasini, Roberto Navigli, Mu-LaN: Multilingual label propagatioN for word sense disambiguation, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_91",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_92@0",
            "content": "Michele Bevilacqua, Roberto Navigli, Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_92",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_93@0",
            "content": "Terra Blevins, Luke Zettlemoyer, Moving down the long tail of word sense disambiguation with gloss informed bi-encoders, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_93",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_94@0",
            "content": "Giulia Bonansinga, Francis Bond, Multilingual sense intersection in a parallel corpus with diverse language families, 2016, Proceedings of the 8th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_94",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_95@0",
            "content": "UNKNOWN, None, , Global WordNet Conference (GWC), Global Wordnet Association.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_95",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_96@0",
            "content": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar, Roberto Navigli, Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities, 2016, Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_96",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_97@0",
            "content": "Marine Carpuat, Dekai Wu, Word sense disambiguation vs. statistical machine translation, 2005, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_97",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_98@0",
            "content": "Marine Carpuat, Dekai Wu, Improving statistical machine translation using word sense disambiguation, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_98",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_99@0",
            "content": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer, Don't take the easy way out: Ensemble based methods for avoiding known dataset biases, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_99",
            "start": 0,
            "end": 362,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_100@0",
            "content": "Simone Conia, Roberto Navigli, Framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_100",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_101@0",
            "content": "Claudio Bovi, Jose Camacho-Collados, Alessandro Raganato, Roberto Navigli, Eurosense: Automatic harvesting of multilingual sense annotations from parallel text, 2017, Proceedings of the 55th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_101",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_102@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_102",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_103@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_103",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_104@0",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_104",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_105@0",
            "content": "Denis Emelin, Ivan Titov, Rico Sennrich, Widening the representation bottleneck in neural machine translation with lexical shortcuts, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_105",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_106@0",
            "content": "Denis Emelin, Ivan Titov, Rico Sennrich, Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_106",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_107@0",
            "content": "Annette Rios Gonzales, Laura Mascarell, Rico Sennrich, Improving word sense disambiguation in neural machine translation with sense embeddings, 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_107",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_108@0",
            "content": "Viktor Hangya, Qianchu Liu, Dario Stojanovski, Alexander Fraser, Anna Korhonen, Improving machine translation of rare and unseen word senses, 2021, Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_108",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2019, Unlearn dataset bias in natural language inference by fitting, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_109",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_110@0",
            "content": "Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci, Roberto Navigli, Embedding words and senses together via joint knowledgeenhanced training, 2017, Proceedings of the 21st Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_110",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_111@0",
            "content": "Quinn Mcnemar, Note on the sampling error of the difference between correlated proportions or percentages, 1947, Psychometrika, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_111",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_112@0",
            "content": "George Miller, R Beckwith, Christiane Fellbaum, D Gross, K Miller, WordNet: an online lexical database, 1990, Int. J. Lexicogr, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_112",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_113@0",
            "content": "George Miller, Claudia Leacock, Randee Tengi, Ross Bunker, A semantic concordance, 1993, Proc. of the Workshop on Human Language Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_113",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_114@0",
            "content": "Roberto Navigli, Simone Ponzetto, Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network, 2012, Artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_114",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_115@0",
            "content": "Quang-Phuoc Nguyen, Anh-Dung Vo, Joon-Choul Shin, Cheol-Young Ock, Effect of word sense disambiguation on neural machine translation: A case study in korean, 2018, IEEE Access, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_115",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_116@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_116",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_117@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_117",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_118@0",
            "content": "Luigi Procopio, Edoardo Barba, Federico Martelli, Roberto Navigli, Multimirror: Neural crosslingual word alignment for multilingual word sense disambiguation, 2021, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_118",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_119@0",
            "content": "Xiao Pu, Nikolaos Pappas, James Henderson, Andrei Popescu-Belis, Integrating weakly supervised word sense disambiguation into neural machine translation, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_119",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_120@0",
            "content": "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher Manning, Stanza: A python natural language processing toolkit for many human languages, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_120",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_121@0",
            "content": "Alessandro Raganato, Yves Scherrer, J\u00f6rg Tiedemann, The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_121",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_122@0",
            "content": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli, Sense-annotated corpora for word sense disambiguation in multiple languages and domains, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_122",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_123@0",
            "content": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli, With more contexts comes better performance: Contextualized sense embeddings for allround word sense disambiguation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_123",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_124@0",
            "content": "UNKNOWN, None, 2020, Multilingual translation with extensible multilingual pretraining and finetuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_124",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_125@0",
            "content": "J\u00f6rg Tiedemann, Santhosh Thottingal, OPUS-MT -Building open translation services for the World, 2020, Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_125",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_126@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_126",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_127@0",
            "content": "David Vickrey, Luke Biewald, Marc Teyssier, Daphne Koller, Word-sense disambiguation for machine translation, 2005, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_127",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_128@0",
            "content": "Zikang Wang, Linjing Li, Daniel Zeng, Knowledge-enhanced natural language inference based on knowledge graphs, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_128",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_129@0",
            "content": "UNKNOWN, None, 1949, Translation. Machine Translation of Languages: Fourteen Essays, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_129",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "200-ARR_v1_130@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "200-ARR_v1_130",
            "start": 0,
            "end": 536,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_1",
            "tgt_ix": "200-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_1",
            "tgt_ix": "200-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_2",
            "tgt_ix": "200-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_5",
            "tgt_ix": "200-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_6",
            "tgt_ix": "200-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_7",
            "tgt_ix": "200-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_8",
            "tgt_ix": "200-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_9",
            "tgt_ix": "200-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_10",
            "tgt_ix": "200-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_11",
            "tgt_ix": "200-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_12",
            "tgt_ix": "200-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_16",
            "tgt_ix": "200-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_17",
            "tgt_ix": "200-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_19",
            "tgt_ix": "200-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_20",
            "tgt_ix": "200-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_21",
            "tgt_ix": "200-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_21",
            "tgt_ix": "200-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_25",
            "tgt_ix": "200-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_26",
            "tgt_ix": "200-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_28",
            "tgt_ix": "200-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_29",
            "tgt_ix": "200-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_30",
            "tgt_ix": "200-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_31",
            "tgt_ix": "200-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_32",
            "tgt_ix": "200-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_34",
            "tgt_ix": "200-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_36",
            "tgt_ix": "200-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_37",
            "tgt_ix": "200-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_38",
            "tgt_ix": "200-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_35",
            "tgt_ix": "200-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_39",
            "tgt_ix": "200-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_40",
            "tgt_ix": "200-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_40",
            "tgt_ix": "200-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_42",
            "tgt_ix": "200-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_41",
            "tgt_ix": "200-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_41",
            "tgt_ix": "200-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_41",
            "tgt_ix": "200-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_40",
            "tgt_ix": "200-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_45",
            "tgt_ix": "200-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_48",
            "tgt_ix": "200-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_47",
            "tgt_ix": "200-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_40",
            "tgt_ix": "200-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_49",
            "tgt_ix": "200-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_51",
            "tgt_ix": "200-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_50",
            "tgt_ix": "200-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_50",
            "tgt_ix": "200-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_50",
            "tgt_ix": "200-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_52",
            "tgt_ix": "200-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_54",
            "tgt_ix": "200-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_56",
            "tgt_ix": "200-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_55",
            "tgt_ix": "200-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_55",
            "tgt_ix": "200-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_55",
            "tgt_ix": "200-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_57",
            "tgt_ix": "200-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_58",
            "tgt_ix": "200-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_58",
            "tgt_ix": "200-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_59",
            "tgt_ix": "200-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_61",
            "tgt_ix": "200-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_62",
            "tgt_ix": "200-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_63",
            "tgt_ix": "200-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_64",
            "tgt_ix": "200-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_65",
            "tgt_ix": "200-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_66",
            "tgt_ix": "200-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_67",
            "tgt_ix": "200-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_67",
            "tgt_ix": "200-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_69",
            "tgt_ix": "200-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_67",
            "tgt_ix": "200-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_67",
            "tgt_ix": "200-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_70",
            "tgt_ix": "200-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_72",
            "tgt_ix": "200-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_73",
            "tgt_ix": "200-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_75",
            "tgt_ix": "200-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_76",
            "tgt_ix": "200-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_77",
            "tgt_ix": "200-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_74",
            "tgt_ix": "200-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_80",
            "tgt_ix": "200-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_81",
            "tgt_ix": "200-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_82",
            "tgt_ix": "200-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_83",
            "tgt_ix": "200-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_79",
            "tgt_ix": "200-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_84",
            "tgt_ix": "200-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_86",
            "tgt_ix": "200-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_87",
            "tgt_ix": "200-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_85",
            "tgt_ix": "200-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_88",
            "tgt_ix": "200-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "200-ARR_v1_0",
            "tgt_ix": "200-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_1",
            "tgt_ix": "200-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_2",
            "tgt_ix": "200-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_2",
            "tgt_ix": "200-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_2",
            "tgt_ix": "200-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_3",
            "tgt_ix": "200-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_4",
            "tgt_ix": "200-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_5",
            "tgt_ix": "200-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_5",
            "tgt_ix": "200-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_6",
            "tgt_ix": "200-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_6",
            "tgt_ix": "200-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_7",
            "tgt_ix": "200-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_7",
            "tgt_ix": "200-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_7",
            "tgt_ix": "200-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_8",
            "tgt_ix": "200-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_9",
            "tgt_ix": "200-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_9",
            "tgt_ix": "200-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_10",
            "tgt_ix": "200-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_11",
            "tgt_ix": "200-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_12",
            "tgt_ix": "200-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_12",
            "tgt_ix": "200-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_13",
            "tgt_ix": "200-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_14",
            "tgt_ix": "200-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_15",
            "tgt_ix": "200-ARR_v1_15@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_16",
            "tgt_ix": "200-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_16",
            "tgt_ix": "200-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_16",
            "tgt_ix": "200-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_17",
            "tgt_ix": "200-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_17",
            "tgt_ix": "200-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_18",
            "tgt_ix": "200-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_19",
            "tgt_ix": "200-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_19",
            "tgt_ix": "200-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_19",
            "tgt_ix": "200-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_19",
            "tgt_ix": "200-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_20",
            "tgt_ix": "200-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_20",
            "tgt_ix": "200-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_21",
            "tgt_ix": "200-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_22",
            "tgt_ix": "200-ARR_v1_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_23",
            "tgt_ix": "200-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_24",
            "tgt_ix": "200-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_25",
            "tgt_ix": "200-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_26",
            "tgt_ix": "200-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_27",
            "tgt_ix": "200-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_28",
            "tgt_ix": "200-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_28",
            "tgt_ix": "200-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_29",
            "tgt_ix": "200-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_30",
            "tgt_ix": "200-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_31",
            "tgt_ix": "200-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_32",
            "tgt_ix": "200-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_33",
            "tgt_ix": "200-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_34",
            "tgt_ix": "200-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_34",
            "tgt_ix": "200-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_34",
            "tgt_ix": "200-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_35",
            "tgt_ix": "200-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_35",
            "tgt_ix": "200-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_35",
            "tgt_ix": "200-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_35",
            "tgt_ix": "200-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_36",
            "tgt_ix": "200-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_36",
            "tgt_ix": "200-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_37",
            "tgt_ix": "200-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_38",
            "tgt_ix": "200-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_39",
            "tgt_ix": "200-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_39",
            "tgt_ix": "200-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_40",
            "tgt_ix": "200-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_41",
            "tgt_ix": "200-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_42",
            "tgt_ix": "200-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_42",
            "tgt_ix": "200-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_42",
            "tgt_ix": "200-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_42",
            "tgt_ix": "200-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_43",
            "tgt_ix": "200-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_44",
            "tgt_ix": "200-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_45",
            "tgt_ix": "200-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_45",
            "tgt_ix": "200-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_45",
            "tgt_ix": "200-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_45",
            "tgt_ix": "200-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_46",
            "tgt_ix": "200-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_47",
            "tgt_ix": "200-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_47",
            "tgt_ix": "200-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_48",
            "tgt_ix": "200-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_48",
            "tgt_ix": "200-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_48",
            "tgt_ix": "200-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_49",
            "tgt_ix": "200-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_49",
            "tgt_ix": "200-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_49",
            "tgt_ix": "200-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_49",
            "tgt_ix": "200-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_50",
            "tgt_ix": "200-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_51",
            "tgt_ix": "200-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_52",
            "tgt_ix": "200-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_52",
            "tgt_ix": "200-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_53",
            "tgt_ix": "200-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_54",
            "tgt_ix": "200-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_54",
            "tgt_ix": "200-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_55",
            "tgt_ix": "200-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_56",
            "tgt_ix": "200-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_57",
            "tgt_ix": "200-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_58",
            "tgt_ix": "200-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_59",
            "tgt_ix": "200-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_59",
            "tgt_ix": "200-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_60",
            "tgt_ix": "200-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_61",
            "tgt_ix": "200-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_62",
            "tgt_ix": "200-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_63",
            "tgt_ix": "200-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_64",
            "tgt_ix": "200-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_64",
            "tgt_ix": "200-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_65",
            "tgt_ix": "200-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_65",
            "tgt_ix": "200-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_65",
            "tgt_ix": "200-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_65",
            "tgt_ix": "200-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_66",
            "tgt_ix": "200-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_67",
            "tgt_ix": "200-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_68",
            "tgt_ix": "200-ARR_v1_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_69",
            "tgt_ix": "200-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_69",
            "tgt_ix": "200-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_69",
            "tgt_ix": "200-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_70",
            "tgt_ix": "200-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_70",
            "tgt_ix": "200-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_71",
            "tgt_ix": "200-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_72",
            "tgt_ix": "200-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_73",
            "tgt_ix": "200-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_74",
            "tgt_ix": "200-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_74",
            "tgt_ix": "200-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_75",
            "tgt_ix": "200-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_76",
            "tgt_ix": "200-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_76",
            "tgt_ix": "200-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_76",
            "tgt_ix": "200-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_76",
            "tgt_ix": "200-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_77",
            "tgt_ix": "200-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_77",
            "tgt_ix": "200-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_77",
            "tgt_ix": "200-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_77",
            "tgt_ix": "200-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_78",
            "tgt_ix": "200-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_79",
            "tgt_ix": "200-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_79",
            "tgt_ix": "200-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_79",
            "tgt_ix": "200-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_80",
            "tgt_ix": "200-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_81",
            "tgt_ix": "200-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_82",
            "tgt_ix": "200-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_82",
            "tgt_ix": "200-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_83",
            "tgt_ix": "200-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_83",
            "tgt_ix": "200-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_83",
            "tgt_ix": "200-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_84",
            "tgt_ix": "200-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_85",
            "tgt_ix": "200-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_86",
            "tgt_ix": "200-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_86",
            "tgt_ix": "200-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_87",
            "tgt_ix": "200-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_88",
            "tgt_ix": "200-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_88",
            "tgt_ix": "200-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_89",
            "tgt_ix": "200-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_89",
            "tgt_ix": "200-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_89",
            "tgt_ix": "200-ARR_v1_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_90",
            "tgt_ix": "200-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_91",
            "tgt_ix": "200-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_92",
            "tgt_ix": "200-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_93",
            "tgt_ix": "200-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_94",
            "tgt_ix": "200-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_95",
            "tgt_ix": "200-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_96",
            "tgt_ix": "200-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_97",
            "tgt_ix": "200-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_98",
            "tgt_ix": "200-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_99",
            "tgt_ix": "200-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_100",
            "tgt_ix": "200-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_101",
            "tgt_ix": "200-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_102",
            "tgt_ix": "200-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_103",
            "tgt_ix": "200-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_104",
            "tgt_ix": "200-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_105",
            "tgt_ix": "200-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_106",
            "tgt_ix": "200-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_107",
            "tgt_ix": "200-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_108",
            "tgt_ix": "200-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_109",
            "tgt_ix": "200-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_110",
            "tgt_ix": "200-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_111",
            "tgt_ix": "200-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_112",
            "tgt_ix": "200-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_113",
            "tgt_ix": "200-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_114",
            "tgt_ix": "200-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_115",
            "tgt_ix": "200-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_116",
            "tgt_ix": "200-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_117",
            "tgt_ix": "200-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_118",
            "tgt_ix": "200-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_119",
            "tgt_ix": "200-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_120",
            "tgt_ix": "200-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_121",
            "tgt_ix": "200-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_122",
            "tgt_ix": "200-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_123",
            "tgt_ix": "200-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_124",
            "tgt_ix": "200-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_125",
            "tgt_ix": "200-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_126",
            "tgt_ix": "200-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_127",
            "tgt_ix": "200-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_128",
            "tgt_ix": "200-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_129",
            "tgt_ix": "200-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "200-ARR_v1_130",
            "tgt_ix": "200-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1263,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "200-ARR",
        "version": 1
    }
}