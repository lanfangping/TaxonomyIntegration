{
    "nodes": [
        {
            "ix": "169-ARR_v2_0",
            "content": "Simulating Bandit Learning from User Feedback for Extractive Question Answering",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_2",
            "content": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on a small number of examples can dramatically improve given feedback from users on modelpredicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation, but instead improving the system on-the-fly via user feedback.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "169-ARR_v2_4",
            "content": "Explicit feedback from users of NLP systems can be used to continually improve system performance. For example, a user posing a question to a question-answering (QA) system can mark if a predicted phrase is a valid answer given the context from which it was extracted. However, the dominant paradigm in NLP separates model training from deployment, leaving models static following learning and throughout interaction with users. This approach misses opportunities for learning during system usage, which beside several exceptions we discuss in Section 8 is understudied in NLP. In this paper, we study the potential of learning from explicit user feedback for extractive QA through simulation studies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_5",
            "content": "Extractive QA is a popular testbed for language reasoning, with rich prior work on datasets (e.g., Rajpurkar et al., 2016), task design (Yang et al., 2018;Choi et al., 2018), and model architecture development (Seo et al., 2017;Yu et al., 2018). Learning from interaction with users remains relatively understudied, even though QA is well positioned to elicit user feedback. An extracted answer can be clearly visualized within its supporting context, and a language-proficient user can then easily validate Figure 1: Illustration of an interaction setup for learning from user feedback for QA, and its potential. Given a user question, the system outputs an answer and highlights it in its context. The user validates the answer given the context with binary feedback. We show performance progression from one of our online learning experiments on SQUAD with hand-crafted illustrative examples at two time steps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_6",
            "content": "if the answer is supported or not. 1 This allows for simple binary feedback, and creates a contextual bandit learning scenario (Auer et al., 2002;Langford and Zhang, 2007). Figure 1 illustrates this learning signal and its potential.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_7",
            "content": "We simulate user feedback using several widely used QA datasets, and use it as a bandit signal for learning. We study the empirical characteristics of the learning process, including its performance, sensitivity to initial system performance, and tradeoffs between online and offline learning. We also simulate zero-annotation domain adaptation, where we deploy a QA system trained from supervised data in one domain and adapt it solely from user feedback in a new domain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_8",
            "content": "This learning scenario can mitigate fundamental problems in extractive QA. It reduces data collection costs, by delegating much of the learning to interaction with users. It can avoid data collection artifacts because the data comes from the actual system deployment, unlike data from an annotation effort that often involves design decisions immaterial to the system's use case. For example, sharing question-and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018). Finally, it gives systems the potential to evolve over time as the world changes (Lazaridou et al., 2021;Zhang and Choi, 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_9",
            "content": "Our simulation experiments show that user feedback is an effective signal to continually improve QA systems across multiple benchmarks. For example, an initial system trained with a small amount of SQUAD (Rajpurkar et al., 2016) annotations (64 examples) improves from 18 to 81.6 F1 score, and adapting a SearchQA (Dunn et al., 2017) system to SQUAD through user feedback improves it from 45 to 84 F1 score. Our study shows the impact of initial system performance, trade-offs between online and offline learning, and the impact of source domain on adaptation. These results create the base for future work that goes beyond simulation to use feedback from human users to improve extractive QA systems. Our code is publicly available at https://github.com/ lil-lab/bandit-qa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_10",
            "content": "Learning and Interaction Scenario",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "169-ARR_v2_11",
            "content": "We study a scenario where a QA model learns from explicit user feedback. We formulate learning as a contextual bandit problem. The input to the learner is a question-context pair, where the context paragraph contains the answer to the question. The output is a single span in the context paragraph that is the answer to the question.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_12",
            "content": "Given a question-context pair, the model predicts an answer span. The user then provides feedback about the model's predicted answer, which is used to update the model parameters. We intentionally experiment with simple binary feedback and basic learning algorithms, to provide a baseline for what more advanced methods could achieve with as few assumptions as possible.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_13",
            "content": "Background: Contextual Bandit Learning In a stochastic (i.i.d.) contextual bandit learning problem, at each time step t, the learner independently observes a context 2 x (t) \u223c D sampled from the data distribution D, chooses an action y (t) according to a policy \u03c0, and observes a reward r (t) \u2208 R. The learner only observes the reward r (t) corresponding to the chosen action y (t) . The learner aims to minimize the cumulative regret. Intuitively, regret is the deficit suffered by the learner relative to the optimal policy up to a specific time step. Formally, the cumulative regret at time T is computed with respect to the optimal policy \u03c0 * \u2208 arg max \u03c0\u2208\u03a0 E (x,y,r)\u223c(D,\u03c0) [r]:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_14",
            "content": "R T := T t=1 r * (t) \u2212 T t=1 r (t) , (1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_15",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_16",
            "content": "where \u03a0 is the set of all policies, r (t) is the reward observed at time t and r * (t) is the reward that the optimal policy \u03c0 * would observe. Minimising the cumulative regret is equivalent to maximising the total reward. 3 A key challenge in contextual bandit learning is to balance exploration and exploitation to minimize overall regret.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_17",
            "content": "Scenario Formulation Let a question q be a sequence of m tokens q 1 , . . . , q m and a context paragraph c be a sequence of n tokens c 1 , . . . , c n . An extractive QA model 4 \u03c0 predicts a span \u0177 = c i , . . . , c j where i, j \u2208 [1, n] and i \u2264 j in the context c as an answer. When relevant, we denote \u03c0 \u03b8 as a QA model parameterized by \u03b8.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_18",
            "content": "We formalize learning as a contextual bandit process: at each time step t, the model is given a question-context pair (q (t) , c(t) ), predicts an answer span \u0177, and receives a reward r (t) \u2208 IR. The learner's goal is to maximize the total reward T t=1 r (t) . This formulation reflects a setup where, given a question-context pair, the QA system interacts with a user, who validates the model-predicted answer in context, and provides feedback which is mapped to numerical reward.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_19",
            "content": "Algorithm 1 Online learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_20",
            "content": "1: for t = 1 \u2022 \u2022 \u2022 do 2:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_21",
            "content": "Receive a question q(t) and context c(k) 3:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_22",
            "content": "Predict an answer \u0177(t) \u2190 arg maxy \u03c0 \u03b8 (y | q(t) , c(t) ) 4:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_23",
            "content": "Observe a reward r (t) 5:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_24",
            "content": "Update the model parameters \u03b8 using the gradient r (t) \u2207 \u03b8 log \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 6: end for Learning Algorithm We learn using policy gradient. Our learner is similar to REINFORCE (Sutton and Barto, 1998;Williams, 2004), but we use arg max to predict answers instead of Monte Carlo sampling from the model's output distribution. 5 We study online and offline learning, also referred to as on-and off-policy. In online learning (Algorithm 1), the model identity is maintained between prediction and update; the parameter values that are updated are the same that were used to generate the output receiving reward. This entails that a reward is only used once, to update the model after observing it. In offline learning (Algorithm 2), this relation between update and prediction does not hold. The learner observes reward, often across many examples, and may use it to update the model many times, even after the parameters drifted arbitrarily far from these that generated the prediction. In practice, we observe reward for the entire length of the simulation (T steps) and then update for E epochs. The reward is re-weighted to provide an unbiased estimation using inverse propensity score (IPS;Horvitz and Thompson, 1952). We clip the debiasing coefficient to avoid amplifying examples with large coefficients (line 10, Algorithm 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_25",
            "content": "In general, offline learning is easier to implement because updating the model is not integrated with its deployment. Offline learning also uses a training loop that is similar to optimization practices in supervised learning. This allows to iterate over the data multiple times, albeit with the same feedback signal on each example. However, online learning often has lower regret as the model is updated after each interaction. It may also lead to higher overall performance, because as the model improves early on, it may observe more positive feedback overall, which is generally more informative. We empiri- 5 Early experiments showed that sampling is not as beneficial as arg max, potentially because of the relatively large output space of extractive QA. Yao et al. (2020) Algorithm 2 Offline learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_26",
            "content": "1: for t = 1 \u2022 \u2022 \u2022 T do 2:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_27",
            "content": "Receive a question q(t) and context c(t) 3:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_28",
            "content": "Predict an answer \u0177(t) \u2190 arg maxy \u03c0 \u03b8 (y | q(t) , c(t) ) 4:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_29",
            "content": "p (t) \u2190 \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 5:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_30",
            "content": "Observe a reward r (t) 6: end for 7: for E epochs do 8:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_31",
            "content": "for t = 1 \u2022 \u2022 \u2022 T do 9:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_32",
            "content": "Compute clipped importance-weighted reward according to the current model parameters:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_33",
            "content": "10: r \u2190 clip( \u03c0 \u03b8 (\u0177 (t) |q (t) ,c (t) ) p (t)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_34",
            "content": ", 0, 1)r (t) 11:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_35",
            "content": "Update the model parameters \u03b8 using the gradient",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_36",
            "content": "r \u2207 \u03b8 log \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 12:",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_37",
            "content": "end for 13: end for cally study these trade-offs in Section 5 and 6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_38",
            "content": "Evaluating Performance We evaluate model performance using token-level F1 on a held-out test set, as commonly done in the QA literature (Rajpurkar et al., 2016). We also estimate the learner regret (Equation 1). Computing regret requires access to the an oracle \u03c0 * . We use human annotation as an estimate (Section 3). 6 Comparison to Supervised Learning In supervised learning, the data distribution is not dependent on the model, but on a fixed training set {(q (t) , c(t) , y (t) )} T t=1 . In contrast, bandit learners are provided with reward data that depends on the model itself: {(q (t) , c(t) , \u0177(t) , r (t) )} T t=1 where r is the reward for the model prediction \u0177(t) = arg max y \u03c0 \u03b8 (y | q(t) , c(t) ) at time step t. Such feedback can be freely gathered from users interacting with the model, while building supervised datasets requires costly annotation. This learning signal can also reflect changing task properties (e.g., world changes) to allow systems to adapt, and its origin in the deployed system use makes it more robust to biases introduced during annotation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_39",
            "content": "Simulation Setup",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "169-ARR_v2_40",
            "content": "We initialize our model with supervised data, and then simulate bandit feedback using supervised data annotations. Initialization is critical so the model does not return random answers, which are likely to be all bad because of the large output space. We use relatively little supervised data from the same domain for in-domain experiments (Section 5 and 6) to focus on the data annotation reduction potential of user feedback. For domain adaptation, we assume access to a large amount of training data in the source domain, and no annotated data in the target domain (Section 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_41",
            "content": "Reward We use supervised data annotations to simulate the reward. If the predicted answer span is an exact match index-wise to the annotated span, the learner observes a positive reward of 1.0, and a negative reward of -0.1 otherwise. 7 This reward signal is stricter than QA evaluation metrics (tokenlevel F1 or exact match after normalization). 8",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_42",
            "content": "Noise Simulation We study robustness by simulating noisy feedback via reward perturbation: randomly flipping the binary reward with a fixed probability of 8% or 20% as the noise ratio. 9",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_43",
            "content": "4 Experimental Setup Data We use six English QA datasets that provide substantial amount of annotated training data taken from the MRQA training portion (Fisch et al., 2019): SQUAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), Hot-potQA (Yang et al., 2018), and NaturalQuestions (NQ; Kwiatkowski et al., 2019). The MRQA benchmark simplifies all datasets so that each example has a single span answer with a limited evidence document length (truncated at 800 tokens). Table 7 in Appendix B provides dataset details. We compute performance measures and learning curves on development sets following prior work (Rajpurkar et al., 2016;Ram et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_44",
            "content": "We conduct experiments with a pretrained SpanBERT model (Joshi et al., 2020). We finetune the pre-trained SpanBERT-base model during initial learning and our simulations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_45",
            "content": "Implementation Details We use Hugging Face Transformers (Wolf et al., 2020). When training initial models with little in-domain supervised data (Section 5; Section 6), we use a learning rate of 3e-5 with a linear schedule, batch size 10, and 10 epochs. We obtain the sets of 64, 256, or 1,024 examples from prior work (Ram et al., 2021). 10 For models initially trained on complete datasets (Section 7), we use a learning rate 2e-5 with a linear schedule, batch size 40, and 4 epochs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_46",
            "content": "In simulation experiments, we use batch size 40. We turn off dropout to simulate interaction with users in deployment. For single-pass online learning experiments (Section 5; Section 7), we use a constant learning rate of 1e-5. For offline learning experiments (Section 6), we train the model for 3 epochs on the collected feedback with a linear schedule learning rate of 3e-5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_47",
            "content": "Online experiments with SQUAD, HotpotQA, NQ, and NewsQA take 2-4h each on one NVIDIA GeForce RTX 2080 Ti; 2.5-6h for offline. For TriviaQA and SearchQA, each online simulation experiment on one NVIDIA TITAN RTX takes 4-9.5h; 9-20h for offline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_48",
            "content": "Online Learning",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "169-ARR_v2_49",
            "content": "We simulate a scenario where only a limited amount of supervised data is available, and the model mainly learns from explicit user feedback on predicted answers. We use 64, 256, or 1,024 in-domain annotated examples to train an initial model. This section focuses on online learning, where the learner updates the model parameters after each feedback is observed (Algorithm 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_50",
            "content": "Figure 2 presents the performance of in-domain simulation with online learning. The performance pattern varies across different datasets. Bandit learning consistently improves performance on SQUAD, HotpotQA, and NQ across different amounts of supervised data used to train the initial model. The performance gain is larger with weaker initial models (i.e., trained on 64 supervised examples): 63.6 on SQUAD, 42.7 on HotpotQA, and 40.0 on NQ. Bandit learning is not always effective on NewsQA, TriviaQA, and SearchQA, especially with weaker initial models. This may be attributed to the quality of training set annotations, which determines the accuracy of reward in our setup. SearchQA and TriviaQA use distant supervision to match questions and relevant contexts from the web, likely decreasing reward quality in our setup. While NewsQA is crowdsourced, Trischler et al. (2017)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_51",
            "content": "Offline Learning",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "169-ARR_v2_52",
            "content": "We simulate offline bandit learning (Algorithm 2), where feedback is collected all at once with the initial model. The learning scenario follows the previous section: only a limited amount of supervised data is available (64, 256, or 1,024 in-domain examples) to train initial models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_53",
            "content": "Table 1 shows the performance of offline simulation experiments compared to online simulations. We observe mixed results. On SQUAD, HotpotQA, NQ, and NewsQA, offline learning outperforms online learning when using stronger initial models (i.e., models trained on 256 and 1,024 examples). This illustrates the benefit of the more standard training loop, especially with our Transformerbased model that is better optimized with a linear learning rate schedule and multiple epochs, both incompatible with the online setup. On TriviaQA and SearchQA, offline simulation is ineffective regardless of the performance of initial models. This result echoes the learning challenges in the online counterparts on these two datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_54",
            "content": "Online vs. Offline Regret Table 2 compares online and offline regret. Regret numbers are averaged over the number of feedback observations. 11 Online learning generally displays lower regret for similar initial models on SQUAD, HotpotQA, and NQ. This is expected because later interactions in the simulation can benefit from early feedback in online learning. In contrast, in our offline scenario, we only update after seeing all examples, so regret numbers depend on the initial model only. Regret results on NewsQA, TriviaQA, and SearchQA are counterintuitive, generally showing that online learning has similar or higher regret. The cases showing significantly higher online regret (64+sim on NewsQA and SearchQA) can be explained by the learning failing, which impacts online regret, but not our offline regret. The others are more complex, and we hypothesize that they may be because of combination of (a) inherent noise in the data; and (b) in cases where online learning is effective, the gap between the strictly-defined reward that is used to compute regret and the relaxed F1 evaluation metric. Further analysis is required for a more conclusive conclusion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_55",
            "content": "Domain Adaptation",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "169-ARR_v2_56",
            "content": "Learning from user feedback creates a compelling avenue to deploy systems that target new domains not addressed by existing datasets. The scenario we simulate in this section starts with training a QA model on a complete existing annotated dataset, and deploying it to interact with users and learn from their feedback in a new domain. We do not assume access to any annotated training data in Figure 4 shows online domain adaptation performance. On 22/30 configurations, online adaptation introduces significant performance gains (>2 F1 score). For example, adapting from TriviaQA and SearchQA to the other four domains improves performance by 27-72.8 F1. On HotpotQA, the model initially trained on TriviaQA shows an impressive adaptation, improving from 0.2 F1 to 73 F1. 12 Our simulations show reduced effectiveness when the target domain is either TriviaQA or SearchQA, likely because the simulated feedback is based on noisy distantly supervised data. For SearchQA, the low performance of initial models from other domains may also contribute to the adaptation failure. As expected, this indicates the effectiveness of the process depends on the relation between the source and target domains. SearchQA seems farthest from the other domains, mirroring observations from prior work (Su et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_57",
            "content": "Figure 5 shows learning curves for our simulation experiments. Generally, we observe the choice of source and target domains influences adaptation rates. Models quickly adapt to SQUAD, Hot-potQA, and NQ, reaching near final performance with a quarter of the total feedback provided. On NewsQA, models initially trained on TriviaQA and SearchQA adapt slower than those initially trained on other three datasets. On TriviaQA, we observe little change in performance throughout simulation. On SearchQA, only the model initially trained on TriviaQA shows a performance gain. Both SearchQA and TriviaQA include context paragraphs from the web, potentially making domain adaptation from one to the other easier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_58",
            "content": "Lastly, we compare bandit learning with initial models trained on a small amount of in-domain data (Section 5) and initial models trained on a large amount of out-of-domain data. Table 3 compares online learning with initial models trained on 1,024 in-domain supervised examples and online domain adaptation with a SQUAD-initialized model. SQUAD initialization provides a robust starting point for all datasets except SearchQA. On four out of five datasets, the final performance is better with SQUAD-initialized model. This is potentially because the model is exposed to different signals from two datasets and overall sees more data, either as supervised examples or through feedback. However, on SearchQA, learning with SQUAD-initialized model performs much worse than learning with the initial model trained on 1,024 in-domain examples, potentially because of the gap in initial model performance (23.5 vs. 65 F1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_59",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "169-ARR_v2_60",
            "content": "Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021). Explicit human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Artzi and Zettlemoyer, 2011;Lawrence and Riezler, 2018), and summarization (Stiennon et al., 2020). Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations. Our work shows that simulated bandit feedback is an effective learning signal for extractive question answering tasks. Our work differs in focus on reducing annotation costs by relying on few annotated examples only to train the initial model, or by eliminating the need for in-domain annotation completely by relying on data in other domains to train initial models. Implicit human feedback, where feedback is derived from human behavior rather than explicitly requested, has also been studied, including for dialogue (Jaques et al., 2020) and instruction generation (Kojima et al., 2021). We focus on explicit feedback, but implicit signals also hold promise to improve QA systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_61",
            "content": "Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020;Khashabi et al., 2020a). Kratzwald et al. (2020) resembles our setting in seeking binary feedback to replace span annotation, but their goal is to create supervised data more economically. Campos et al. (2020) proposes feedback-weighted learning to improves conversational QA using simulated binary feedback. Their approach relies on multiple samples (i.e., feedback signals) per example, training for multiple epochs online by re-visiting the same questions repeatedly, and tuning two additional hyperparameters. In contrast, we study improving QA systems via feedback as a bandit learning problem. In both online and offline setups, we assume only one feedback sample per example. We also provide extensive sensitivity studies to the amount of annotations available, different model initialization, and noisy feedback across various datasets. Domain adaptation for QA has been widely studied (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_62",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "9"
            }
        },
        {
            "ix": "169-ARR_v2_63",
            "content": "We present a simulation study of learning from user feedback for extractive QA. We formulate the problem as contextual bandit learning. We conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation. Our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_64",
            "content": "We intentionally adopt a basic setup, including a simple binary reward and vanilla learning algorithms, to illustrate what can be achieved with a relatively simple variant of the contextual bandit learning scenario. Our results already indicate the strong potential of learning from feedback, which more advanced methods are likely to further improve. For example, the balance between online and offline learning can be further explored using proximal policy optimization (PPO; Schulman et al., 2017) or replay memory (Mnih et al., 2015). With welldesigned interface, human users may be able to provide more sophisticated feedback (Lamm et al., 2021), which will provide a stronger signal compared to our binary reward.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_65",
            "content": "Our aim in this study is to lay the foundation for future work, by formalizing the setup and showing its potential. This is a critical step in enabling future research, especially going beyond simulation to study using real human feedback for QA systems. Another important direction for future work is studying user feedback for QA systems that do both context retrieval and answer generation (Lewis et al., 2020), where assigning the feedback to the appropriate stage in the process poses a challenge. Beyond extractive QA, we hope our work will inspire research of user feedback as a signal to improve other types of NLP systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_66",
            "content": "Our work's limitations are discussed in Section 1 and Section 9. All six datasets we use are from prior work, are publicly available, and are commonly used for the study of extractive QA. Section 4 reports our computational budget and experimental setup in detail. Our codebase is available at https://github.com/ lil-lab/bandit-qa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_67",
            "content": "Reward Function Intuitively, partial credit reward may improve learning over binary rewards. We experiment with using F1 score of the predicted answer span as a more refined feedback. 13 In practice, this does not introduce a stronger learning signal, potentially because the distribution over F1 scores is bimodal and focused on extreme values: around 85 % F1 scores are either 0 or 1 for predicted spans from a SQUAD-trained model on 8% NQ training data. We observe similar trends on all six datasets across all setups. Experiments with BLEU score (Papineni et al., 2002) as feedback show similar conclusion and distribution to F1 score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_68",
            "content": "Perturbation In practice, noise in feedback is likely to be more systematic than the statistical simplification which defines noise as the random percentage of wrong feedback. For example, prior work (Nguyen et al., 2017) on bandit neural machine translation (NMT) proposes that noisy human feedback is granular, high-variance, and skewed, which can be approximated by mathematical functions and shows to significantly impact the bandit NMT learning. We experiment with the three perturbation functions from Nguyen et al. (2017) on F1 reward. Our experiments show that the effect of adding these perturbation functions is negligible. We hypothesize that the reward distribution for NMT is likely to be closer to a normal distribution, rather than a bimodal one like QA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_69",
            "content": "While arg max can bias towards exploitation, sampling can encourage more exploration. We experiment with prediction via arg max and sampling from the output distribution over spans. Table 4 shows that arg max performs better than random sampling on three datasets. This set of experiments is conducted with batch size 80.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_70",
            "content": "Table 5 shows the sensitivity analysis results for online in-domain simulation on HotpotQA and TriviaQA. We experiment with five initial models trained on different sets of 64 or 1,024 supervised examples, each used to initiate a separate simulation experiment. For weaker initial models trained on 64 supervised examples, four out of five experiments on HotpotQA show performance gains similar to our main results, except one experiment that starts with a very low initialization performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_71",
            "content": "Nearly all experiments on TriviaQA collapse (mean F1 of 7.3). Our sensitivity analysis with stronger initial models trained on 1,024 examples shows that the final performance is stable across runs on both HotpotQA and TriviaQA (standard deviations are 0.5 and 2.6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_72",
            "content": "We perform domain adaptation with offline learning, and compare its performance with online adaptation. Table 6 shows the performance gain of offline adaptation simulation compared to the online setup. In most settings, online learning proves to be more effective, possibly because it observes feedback from partially adapted model predictions. In a few settings (4/30), we observe better adaptation with offline settings (+1.1 to +4.6). Overall, we observe that online learning is more effective on domain adaptation, while offline adaption performs slightly better when both domains are related (e.g., same source domain).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v2_73",
            "content": "Yoav Artzi, Luke Zettlemoyer, Bootstrapping semantic parsers from conversations, 2011, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Yoav Artzi",
                    "Luke Zettlemoyer"
                ],
                "title": "Bootstrapping semantic parsers from conversations",
                "pub_date": "2011",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_74",
            "content": "Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, Robert Schapire, The nonstochastic multiarmed bandit problem, 2002, SIAM Journal on Computing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Peter Auer",
                    "Nicol\u00f2 Cesa-Bianchi",
                    "Yoav Freund",
                    "Robert Schapire"
                ],
                "title": "The nonstochastic multiarmed bandit problem",
                "pub_date": "2002",
                "pub_title": "SIAM Journal on Computing",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_75",
            "content": "UNKNOWN, None, 2018, A contextual bandit bake-off, JMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "A contextual bandit bake-off",
                "pub": "JMLR"
            }
        },
        {
            "ix": "169-ARR_v2_76",
            "content": "UNKNOWN, None, 2020, Improving conversational question answering systems after deployment using feedbackweighted learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Improving conversational question answering systems after deployment using feedbackweighted learning",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_77",
            "content": "UNKNOWN, None, 2018, Quac: Question answering in context. EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Quac: Question answering in context. EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_78",
            "content": "UNKNOWN, None, 2020, Benefits of intermediate annotations in reading comprehension, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Benefits of intermediate annotations in reading comprehension",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_79",
            "content": "UNKNOWN, None, 2017, Searchqa: A new q&a dataset augmented with context from a search engine, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Searchqa: A new q&a dataset augmented with context from a search engine",
                "pub": "CoRR"
            }
        },
        {
            "ix": "169-ARR_v2_80",
            "content": "Tobias Falke, Patrick Lehnen, Feedback attribution for counterfactual bandit learning in multidomain spoken language understanding, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tobias Falke",
                    "Patrick Lehnen"
                ],
                "title": "Feedback attribution for counterfactual bandit learning in multidomain spoken language understanding",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_81",
            "content": "Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, Danqi Chen, MRQA 2019 shared task: Evaluating generalization in reading comprehension, 2019, MRQA@EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Adam Fisch",
                    "Alon Talmor",
                    "Robin Jia",
                    "Minjoon Seo",
                    "Eunsol Choi",
                    "Danqi Chen"
                ],
                "title": "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
                "pub_date": "2019",
                "pub_title": "MRQA@EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_82",
            "content": "Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov, Sachindra Joshi, Using question answering rewards to improve abstractive summarization, 2021, Findings@EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Chulaka Gunasekara",
                    "Guy Feigenblat",
                    "Benjamin Sznajder",
                    "Ranit Aharonov",
                    "Sachindra Joshi"
                ],
                "title": "Using question answering rewards to improve abstractive summarization",
                "pub_date": "2021",
                "pub_title": "Findings@EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_83",
            "content": "G Daniel, D Horvitz,  Thompson, A generalization of sampling without replacement from a finite universe, 1952, Journal of the American Statistical Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "G Daniel",
                    "D Horvitz",
                    " Thompson"
                ],
                "title": "A generalization of sampling without replacement from a finite universe",
                "pub_date": "1952",
                "pub_title": "Journal of the American Statistical Association",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_84",
            "content": "Natasha Jaques, Judy Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard, Human-centric dialog training via offline reinforcement learning, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Natasha Jaques",
                    "Judy Shen",
                    "Asma Ghandeharioun",
                    "Craig Ferguson",
                    "Agata Lapedriza",
                    "Noah Jones",
                    "Shixiang Gu",
                    "Rosalind Picard"
                ],
                "title": "Human-centric dialog training via offline reinforcement learning",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_85",
            "content": "UNKNOWN, None, 2020, Spanbert: Improving pre-training by representing and predicting spans, TACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Spanbert: Improving pre-training by representing and predicting spans",
                "pub": "TACL"
            }
        },
        {
            "ix": "169-ARR_v2_86",
            "content": "UNKNOWN, None, 2017, Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_87",
            "content": "UNKNOWN, None, 2020, More bang for your buck: Natural perturbation for robust question answering, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "More bang for your buck: Natural perturbation for robust question answering",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_88",
            "content": "UNKNOWN, None, 2020, Unifiedqa: Crossing format boundaries with a single qa system. Findings@EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Unifiedqa: Crossing format boundaries with a single qa system. Findings@EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_89",
            "content": "UNKNOWN, None, 2021, Continual learning for grounded instruction generation by observing human following behavior, TACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Continual learning for grounded instruction generation by observing human following behavior",
                "pub": "TACL"
            }
        },
        {
            "ix": "169-ARR_v2_90",
            "content": "Bernhard Kratzwald, Stefan Feuerriegel, Huan Sun, Learning a cost-effective annotation policy for question answering, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Bernhard Kratzwald",
                    "Stefan Feuerriegel",
                    "Huan Sun"
                ],
                "title": "Learning a cost-effective annotation policy for question answering",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_91",
            "content": "UNKNOWN, None, 2018, Can neural machine translation be improved with user feedback? NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Can neural machine translation be improved with user feedback? NAACL",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_92",
            "content": "UNKNOWN, None, 2018, Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_93",
            "content": "Devang Kulshreshtha, Robert Belfer, Iulian Serban, Siva Reddy, Back-training excels selftraining at unsupervised domain adaptation of question generation and passage retrieval, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Devang Kulshreshtha",
                    "Robert Belfer",
                    "Iulian Serban",
                    "Siva Reddy"
                ],
                "title": "Back-training excels selftraining at unsupervised domain adaptation of question generation and passage retrieval",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_94",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. TACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Natural questions: A benchmark for question answering research. TACL",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_95",
            "content": "UNKNOWN, None, 2021, Qed: A framework and dataset for explanations in question answering, TACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Qed: A framework and dataset for explanations in question answering",
                "pub": "TACL"
            }
        },
        {
            "ix": "169-ARR_v2_96",
            "content": "John Langford, Tong Zhang, The epochgreedy algorithm for contextual multi-armed bandits, 2007, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "John Langford",
                    "Tong Zhang"
                ],
                "title": "The epochgreedy algorithm for contextual multi-armed bandits",
                "pub_date": "2007",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_97",
            "content": "Carolin Lawrence, Stefan Riezler, Improving a neural semantic parser by counterfactual learning from human bandit feedback, 2018, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Carolin Lawrence",
                    "Stefan Riezler"
                ],
                "title": "Improving a neural semantic parser by counterfactual learning from human bandit feedback",
                "pub_date": "2018",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_98",
            "content": "UNKNOWN, None, 2017, Counterfactual learning from bandit feedback under deterministic logging : A case study in statistical machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Counterfactual learning from bandit feedback under deterministic logging : A case study in statistical machine translation",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_99",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_100",
            "content": "UNKNOWN, None, 2019, Domain-agnostic question-answering with adversarial training, MRQA@EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Domain-agnostic question-answering with adversarial training",
                "pub": "MRQA@EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_101",
            "content": "UNKNOWN, None, 2020, Retrieval-augmented generation for knowledge-intensive nlp tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_102",
            "content": "UNKNOWN, None, 2021, Online learning meets machine translation evaluation: Finding the best systems with the least human effort, ACL/IJCNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Online learning meets machine translation evaluation: Finding the best systems with the least human effort",
                "pub": "ACL/IJCNLP"
            }
        },
        {
            "ix": "169-ARR_v2_103",
            "content": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas Riedmiller, Georg Fidjeland,  Ostrovski, Human-level control through deep reinforcement learning, 2015, Nature, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Volodymyr Mnih",
                    "Koray Kavukcuoglu",
                    "David Silver",
                    "Andrei Rusu",
                    "Joel Veness",
                    "G Marc",
                    "Alex Bellemare",
                    "Martin Graves",
                    "Andreas Riedmiller",
                    "Georg Fidjeland",
                    " Ostrovski"
                ],
                "title": "Human-level control through deep reinforcement learning",
                "pub_date": "2015",
                "pub_title": "Nature",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_104",
            "content": "UNKNOWN, None, 2017, Reinforcement learning for bandit neural machine translation with simulated human feedback, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Reinforcement learning for bandit neural machine translation with simulated human feedback",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_105",
            "content": "UNKNOWN, None, 2002, Bleu: a method for automatic evaluation of machine translation, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "Bleu: a method for automatic evaluation of machine translation",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_106",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100,000+ questions for machine comprehension of text, 2016, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "Squad: 100,000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_107",
            "content": "UNKNOWN, None, 2021, Few-shot question answering by pretraining span selection, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Few-shot question answering by pretraining span selection",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_108",
            "content": "UNKNOWN, None, 2017, Proximal policy optimization algorithms. arXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Proximal policy optimization algorithms. arXiv",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_109",
            "content": "UNKNOWN, None, 2017, Bidirectional attention flow for machine comprehension, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Bidirectional attention flow for machine comprehension",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_110",
            "content": "UNKNOWN, None, 2016, Learning structured predictors from bandit feedback for interactive nlp, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Learning structured predictors from bandit feedback for interactive nlp",
                "pub": "ACL"
            }
        },
        {
            "ix": "169-ARR_v2_111",
            "content": "UNKNOWN, None, 2017, A shared task on bandit learning for machine translation, WMT.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A shared task on bandit learning for machine translation",
                "pub": "WMT"
            }
        },
        {
            "ix": "169-ARR_v2_112",
            "content": "UNKNOWN, None, 2020, Learning to summarize from human feedback, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Learning to summarize from human feedback",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_113",
            "content": "UNKNOWN, None, 2019, Generalizing question answering system with pre-trained language model fine-tuning, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Generalizing question answering system with pre-trained language model fine-tuning",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_114",
            "content": "UNKNOWN, None, 1998, Reinforcement learning: An introduction, MIT press.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "1998",
                "pub_title": "Reinforcement learning: An introduction",
                "pub": "MIT press"
            }
        },
        {
            "ix": "169-ARR_v2_115",
            "content": "UNKNOWN, None, 2017, Newsqa: A machine comprehension dataset, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Newsqa: A machine comprehension dataset",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_116",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 2004, Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Ronald Williams"
                ],
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "pub_date": "2004",
                "pub_title": "Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_117",
            "content": "UNKNOWN, None, , Morgan Funtowicz, and Jamie Brew. 2020. Huggingface's transformers: State-of-the-art natural language processing. EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Morgan Funtowicz, and Jamie Brew. 2020. Huggingface's transformers: State-of-the-art natural language processing. EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_118",
            "content": "UNKNOWN, None, 2018, Hotpotqa: A dataset for diverse, explainable multi-hop question answering, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_119",
            "content": "UNKNOWN, None, 2020, An imitation game for learning semantic parsers from user interaction, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "An imitation game for learning semantic parsers from user interaction",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "169-ARR_v2_120",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_121",
            "content": "UNKNOWN, None, 2018, Qanet: Combining local convolution with global self-attention for reading comprehension, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Qanet: Combining local convolution with global self-attention for reading comprehension",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_122",
            "content": "Zhenrui Yue, Bernhard Kratzwald, Stefan Feuerriegel, Contrastive domain adaptation for question answering using limited text corpora, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Zhenrui Yue",
                    "Bernhard Kratzwald",
                    "Stefan Feuerriegel"
                ],
                "title": "Contrastive domain adaptation for question answering using limited text corpora",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_123",
            "content": "J Michael, Eunsol Zhang,  Choi, Situat-edQA: Incorporating extra-linguistic contexts into QA, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "J Michael",
                    "Eunsol Zhang",
                    " Choi"
                ],
                "title": "Situat-edQA: Incorporating extra-linguistic contexts into QA",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_124",
            "content": "UNKNOWN, None, 2021, Revisiting fewsample bert fine-tuning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Revisiting fewsample bert fine-tuning",
                "pub": null
            }
        },
        {
            "ix": "169-ARR_v2_125",
            "content": "Haichao Zhu, Zekun Wang, Heng Zhang, Ming Liu, Sendong Zhao, Bing Qin, Less is more: Domain adaptation with lottery ticket for reading comprehension, 2021, Findings@EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Haichao Zhu",
                    "Zekun Wang",
                    "Heng Zhang",
                    "Ming Liu",
                    "Sendong Zhao",
                    "Bing Qin"
                ],
                "title": "Less is more: Domain adaptation with lottery ticket for reading comprehension",
                "pub_date": "2021",
                "pub_title": "Findings@EMNLP",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "169-ARR_v2_0@0",
            "content": "Simulating Bandit Learning from User Feedback for Extractive Question Answering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_0",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_2@0",
            "content": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_2",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_2@1",
            "content": "We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_2",
            "start": 117,
            "end": 268,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_2@2",
            "content": "We show that systems initially trained on a small number of examples can dramatically improve given feedback from users on modelpredicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation, but instead improving the system on-the-fly via user feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_2",
            "start": 270,
            "end": 574,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_4@0",
            "content": "Explicit feedback from users of NLP systems can be used to continually improve system performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_4",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_4@1",
            "content": "For example, a user posing a question to a question-answering (QA) system can mark if a predicted phrase is a valid answer given the context from which it was extracted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_4",
            "start": 99,
            "end": 267,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_4@2",
            "content": "However, the dominant paradigm in NLP separates model training from deployment, leaving models static following learning and throughout interaction with users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_4",
            "start": 269,
            "end": 427,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_4@3",
            "content": "This approach misses opportunities for learning during system usage, which beside several exceptions we discuss in Section 8 is understudied in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_4",
            "start": 429,
            "end": 576,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_4@4",
            "content": "In this paper, we study the potential of learning from explicit user feedback for extractive QA through simulation studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_4",
            "start": 578,
            "end": 700,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@0",
            "content": "Extractive QA is a popular testbed for language reasoning, with rich prior work on datasets (e.g., Rajpurkar et al., 2016), task design (Yang et al., 2018;Choi et al., 2018), and model architecture development (Seo et al., 2017;Yu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@1",
            "content": "Learning from interaction with users remains relatively understudied, even though QA is well positioned to elicit user feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 246,
            "end": 373,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@2",
            "content": "An extracted answer can be clearly visualized within its supporting context, and a language-proficient user can then easily validate Figure 1: Illustration of an interaction setup for learning from user feedback for QA, and its potential.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 375,
            "end": 612,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@3",
            "content": "Given a user question, the system outputs an answer and highlights it in its context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 614,
            "end": 698,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@4",
            "content": "The user validates the answer given the context with binary feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 700,
            "end": 768,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_5@5",
            "content": "We show performance progression from one of our online learning experiments on SQUAD with hand-crafted illustrative examples at two time steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_5",
            "start": 770,
            "end": 912,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_6@0",
            "content": "if the answer is supported or not. 1 This allows for simple binary feedback, and creates a contextual bandit learning scenario (Auer et al., 2002;Langford and Zhang, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_6",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_6@1",
            "content": "Figure 1 illustrates this learning signal and its potential.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_6",
            "start": 173,
            "end": 232,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_7@0",
            "content": "We simulate user feedback using several widely used QA datasets, and use it as a bandit signal for learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_7",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_7@1",
            "content": "We study the empirical characteristics of the learning process, including its performance, sensitivity to initial system performance, and tradeoffs between online and offline learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_7",
            "start": 109,
            "end": 292,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_7@2",
            "content": "We also simulate zero-annotation domain adaptation, where we deploy a QA system trained from supervised data in one domain and adapt it solely from user feedback in a new domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_7",
            "start": 294,
            "end": 471,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_8@0",
            "content": "This learning scenario can mitigate fundamental problems in extractive QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_8",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_8@1",
            "content": "It reduces data collection costs, by delegating much of the learning to interaction with users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_8",
            "start": 75,
            "end": 169,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_8@2",
            "content": "It can avoid data collection artifacts because the data comes from the actual system deployment, unlike data from an annotation effort that often involves design decisions immaterial to the system's use case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_8",
            "start": 171,
            "end": 378,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_8@3",
            "content": "For example, sharing question-and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_8",
            "start": 380,
            "end": 543,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_8@4",
            "content": "Finally, it gives systems the potential to evolve over time as the world changes (Lazaridou et al., 2021;Zhang and Choi, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_8",
            "start": 545,
            "end": 671,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_9@0",
            "content": "Our simulation experiments show that user feedback is an effective signal to continually improve QA systems across multiple benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_9",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_9@1",
            "content": "For example, an initial system trained with a small amount of SQUAD (Rajpurkar et al., 2016) annotations (64 examples) improves from 18 to 81.6 F1 score, and adapting a SearchQA (Dunn et al., 2017) system to SQUAD through user feedback improves it from 45 to 84 F1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_9",
            "start": 136,
            "end": 406,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_9@2",
            "content": "Our study shows the impact of initial system performance, trade-offs between online and offline learning, and the impact of source domain on adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_9",
            "start": 408,
            "end": 559,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_9@3",
            "content": "These results create the base for future work that goes beyond simulation to use feedback from human users to improve extractive QA systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_9",
            "start": 561,
            "end": 700,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_9@4",
            "content": "Our code is publicly available at https://github.com/ lil-lab/bandit-qa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_9",
            "start": 702,
            "end": 773,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_10@0",
            "content": "Learning and Interaction Scenario",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_10",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_11@0",
            "content": "We study a scenario where a QA model learns from explicit user feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_11",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_11@1",
            "content": "We formulate learning as a contextual bandit problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_11",
            "start": 73,
            "end": 125,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_11@2",
            "content": "The input to the learner is a question-context pair, where the context paragraph contains the answer to the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_11",
            "start": 127,
            "end": 243,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_11@3",
            "content": "The output is a single span in the context paragraph that is the answer to the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_11",
            "start": 245,
            "end": 332,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_12@0",
            "content": "Given a question-context pair, the model predicts an answer span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_12",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_12@1",
            "content": "The user then provides feedback about the model's predicted answer, which is used to update the model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_12",
            "start": 66,
            "end": 178,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_12@2",
            "content": "We intentionally experiment with simple binary feedback and basic learning algorithms, to provide a baseline for what more advanced methods could achieve with as few assumptions as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_12",
            "start": 180,
            "end": 369,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_13@0",
            "content": "Background: Contextual Bandit Learning In a stochastic (i.i.d.) contextual bandit learning problem, at each time step t, the learner independently observes a context 2 x (t) \u223c D sampled from the data distribution D, chooses an action y (t) according to a policy \u03c0, and observes a reward r (t) \u2208 R. The learner only observes the reward r (t) corresponding to the chosen action y (t) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_13",
            "start": 0,
            "end": 382,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_13@1",
            "content": "The learner aims to minimize the cumulative regret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_13",
            "start": 384,
            "end": 434,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_13@2",
            "content": "Intuitively, regret is the deficit suffered by the learner relative to the optimal policy up to a specific time step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_13",
            "start": 436,
            "end": 552,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_13@3",
            "content": "Formally, the cumulative regret at time T is computed with respect to the optimal policy \u03c0 * \u2208 arg max \u03c0\u2208\u03a0 E (x,y,r)\u223c(D,\u03c0) [r]:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_13",
            "start": 554,
            "end": 680,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_14@0",
            "content": "R T := T t=1 r * (t) \u2212 T t=1 r (t) , (1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_14",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_15@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_15",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_16@0",
            "content": "where \u03a0 is the set of all policies, r (t) is the reward observed at time t and r * (t) is the reward that the optimal policy \u03c0 * would observe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_16",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_16@1",
            "content": "Minimising the cumulative regret is equivalent to maximising the total reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_16",
            "start": 144,
            "end": 221,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_16@2",
            "content": "3 A key challenge in contextual bandit learning is to balance exploration and exploitation to minimize overall regret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_16",
            "start": 223,
            "end": 340,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_17@0",
            "content": "Scenario Formulation Let a question q be a sequence of m tokens q 1 , . . . , q m and a context paragraph c be a sequence of n tokens c 1 , . . . , c n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_17",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_17@1",
            "content": "An extractive QA model 4 \u03c0 predicts a span \u0177 = c i , . . . , c j where i, j \u2208 [1, n] and i \u2264 j in the context c as an answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_17",
            "start": 154,
            "end": 278,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_17@2",
            "content": "When relevant, we denote \u03c0 \u03b8 as a QA model parameterized by \u03b8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_17",
            "start": 280,
            "end": 341,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_18@0",
            "content": "We formalize learning as a contextual bandit process: at each time step t, the model is given a question-context pair (q (t) , c(t) ), predicts an answer span \u0177, and receives a reward r (t) \u2208 IR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_18",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_18@1",
            "content": "The learner's goal is to maximize the total reward T t=1 r (t) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_18",
            "start": 196,
            "end": 259,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_18@2",
            "content": "This formulation reflects a setup where, given a question-context pair, the QA system interacts with a user, who validates the model-predicted answer in context, and provides feedback which is mapped to numerical reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_18",
            "start": 261,
            "end": 480,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_19@0",
            "content": "Algorithm 1 Online learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_19",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_20@0",
            "content": "1: for t = 1 \u2022 \u2022 \u2022 do 2:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_20",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_21@0",
            "content": "Receive a question q(t) and context c(k) 3:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_21",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_22@0",
            "content": "Predict an answer \u0177(t) \u2190 arg maxy \u03c0 \u03b8 (y | q(t) , c(t) ) 4:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_22",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_23@0",
            "content": "Observe a reward r (t) 5:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_23",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@0",
            "content": "Update the model parameters \u03b8 using the gradient r (t) \u2207 \u03b8 log \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 6: end for Learning Algorithm We learn using policy gradient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@1",
            "content": "Our learner is similar to REINFORCE (Sutton and Barto, 1998;Williams, 2004), but we use arg max to predict answers instead of Monte Carlo sampling from the model's output distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 152,
            "end": 335,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@2",
            "content": "5 We study online and offline learning, also referred to as on-and off-policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 337,
            "end": 414,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@3",
            "content": "In online learning (Algorithm 1), the model identity is maintained between prediction and update; the parameter values that are updated are the same that were used to generate the output receiving reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 416,
            "end": 619,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@4",
            "content": "This entails that a reward is only used once, to update the model after observing it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 621,
            "end": 705,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@5",
            "content": "In offline learning (Algorithm 2), this relation between update and prediction does not hold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 707,
            "end": 799,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@6",
            "content": "The learner observes reward, often across many examples, and may use it to update the model many times, even after the parameters drifted arbitrarily far from these that generated the prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 801,
            "end": 995,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@7",
            "content": "In practice, we observe reward for the entire length of the simulation (T steps) and then update for E epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 997,
            "end": 1106,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@8",
            "content": "The reward is re-weighted to provide an unbiased estimation using inverse propensity score (IPS;Horvitz and Thompson, 1952).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 1108,
            "end": 1231,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_24@9",
            "content": "We clip the debiasing coefficient to avoid amplifying examples with large coefficients (line 10, Algorithm 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_24",
            "start": 1233,
            "end": 1342,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@0",
            "content": "In general, offline learning is easier to implement because updating the model is not integrated with its deployment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@1",
            "content": "Offline learning also uses a training loop that is similar to optimization practices in supervised learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 118,
            "end": 225,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@2",
            "content": "This allows to iterate over the data multiple times, albeit with the same feedback signal on each example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 227,
            "end": 332,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@3",
            "content": "However, online learning often has lower regret as the model is updated after each interaction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 334,
            "end": 428,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@4",
            "content": "It may also lead to higher overall performance, because as the model improves early on, it may observe more positive feedback overall, which is generally more informative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 430,
            "end": 600,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@5",
            "content": "We empiri- 5 Early experiments showed that sampling is not as beneficial as arg max, potentially because of the relatively large output space of extractive QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 602,
            "end": 760,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_25@6",
            "content": "Yao et al. (2020) Algorithm 2 Offline learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_25",
            "start": 762,
            "end": 808,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_26@0",
            "content": "1: for t = 1 \u2022 \u2022 \u2022 T do 2:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_26",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_27@0",
            "content": "Receive a question q(t) and context c(t) 3:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_27",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_28@0",
            "content": "Predict an answer \u0177(t) \u2190 arg maxy \u03c0 \u03b8 (y | q(t) , c(t) ) 4:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_28",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_29@0",
            "content": "p (t) \u2190 \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 5:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_29",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_30@0",
            "content": "Observe a reward r (t) 6: end for 7: for E epochs do 8:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_30",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_31@0",
            "content": "for t = 1 \u2022 \u2022 \u2022 T do 9:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_31",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_32@0",
            "content": "Compute clipped importance-weighted reward according to the current model parameters:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_32",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_33@0",
            "content": "10: r \u2190 clip( \u03c0 \u03b8 (\u0177 (t) |q (t) ,c (t) ) p (t)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_33",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_34@0",
            "content": ", 0, 1)r (t) 11:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_34",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_35@0",
            "content": "Update the model parameters \u03b8 using the gradient",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_35",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_36@0",
            "content": "r \u2207 \u03b8 log \u03c0 \u03b8 (\u0177 (t) | q(t) , c(t) ) 12:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_36",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_37@0",
            "content": "end for 13: end for cally study these trade-offs in Section 5 and 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_37",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@0",
            "content": "Evaluating Performance We evaluate model performance using token-level F1 on a held-out test set, as commonly done in the QA literature (Rajpurkar et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@1",
            "content": "We also estimate the learner regret (Equation 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 162,
            "end": 210,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@2",
            "content": "Computing regret requires access to the an oracle \u03c0 * .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 212,
            "end": 266,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@3",
            "content": "We use human annotation as an estimate (Section 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 268,
            "end": 318,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@4",
            "content": "6 Comparison to Supervised Learning In supervised learning, the data distribution is not dependent on the model, but on a fixed training set {(q (t) , c(t) , y (t) )} T t=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 320,
            "end": 493,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@5",
            "content": "In contrast, bandit learners are provided with reward data that depends on the model itself: {(q (t) , c(t) , \u0177(t) , r (t) )} T t=1 where r is the reward for the model prediction \u0177(t) = arg max y \u03c0 \u03b8 (y | q(t) , c(t) ) at time step t. Such feedback can be freely gathered from users interacting with the model, while building supervised datasets requires costly annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 495,
            "end": 867,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_38@6",
            "content": "This learning signal can also reflect changing task properties (e.g., world changes) to allow systems to adapt, and its origin in the deployed system use makes it more robust to biases introduced during annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_38",
            "start": 869,
            "end": 1082,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_39@0",
            "content": "Simulation Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_39",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_40@0",
            "content": "We initialize our model with supervised data, and then simulate bandit feedback using supervised data annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_40",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_40@1",
            "content": "Initialization is critical so the model does not return random answers, which are likely to be all bad because of the large output space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_40",
            "start": 115,
            "end": 251,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_40@2",
            "content": "We use relatively little supervised data from the same domain for in-domain experiments (Section 5 and 6) to focus on the data annotation reduction potential of user feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_40",
            "start": 253,
            "end": 427,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_40@3",
            "content": "For domain adaptation, we assume access to a large amount of training data in the source domain, and no annotated data in the target domain (Section 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_40",
            "start": 429,
            "end": 580,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_41@0",
            "content": "Reward We use supervised data annotations to simulate the reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_41",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_41@1",
            "content": "If the predicted answer span is an exact match index-wise to the annotated span, the learner observes a positive reward of 1.0, and a negative reward of -0.1 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_41",
            "start": 66,
            "end": 233,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_41@2",
            "content": "7 This reward signal is stricter than QA evaluation metrics (tokenlevel F1 or exact match after normalization).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_41",
            "start": 235,
            "end": 345,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_41@3",
            "content": "8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_41",
            "start": 347,
            "end": 347,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_42@0",
            "content": "Noise Simulation We study robustness by simulating noisy feedback via reward perturbation: randomly flipping the binary reward with a fixed probability of 8% or 20% as the noise ratio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_42",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_42@1",
            "content": "9",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_42",
            "start": 185,
            "end": 185,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_43@0",
            "content": "4 Experimental Setup Data We use six English QA datasets that provide substantial amount of annotated training data taken from the MRQA training portion (Fisch et al., 2019): SQUAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), Hot-potQA (Yang et al., 2018), and NaturalQuestions (NQ; Kwiatkowski et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_43",
            "start": 0,
            "end": 383,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_43@1",
            "content": "The MRQA benchmark simplifies all datasets so that each example has a single span answer with a limited evidence document length (truncated at 800 tokens).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_43",
            "start": 385,
            "end": 539,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_43@2",
            "content": "Table 7 in Appendix B provides dataset details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_43",
            "start": 541,
            "end": 587,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_43@3",
            "content": "We compute performance measures and learning curves on development sets following prior work (Rajpurkar et al., 2016;Ram et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_43",
            "start": 589,
            "end": 723,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_44@0",
            "content": "We conduct experiments with a pretrained SpanBERT model (Joshi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_44",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_44@1",
            "content": "We finetune the pre-trained SpanBERT-base model during initial learning and our simulations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_44",
            "start": 78,
            "end": 169,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_45@0",
            "content": "Implementation Details We use Hugging Face Transformers (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_45",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_45@1",
            "content": "When training initial models with little in-domain supervised data (Section 5; Section 6), we use a learning rate of 3e-5 with a linear schedule, batch size 10, and 10 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_45",
            "start": 77,
            "end": 251,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_45@2",
            "content": "We obtain the sets of 64, 256, or 1,024 examples from prior work (Ram et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_45",
            "start": 253,
            "end": 336,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_45@3",
            "content": "10 For models initially trained on complete datasets (Section 7), we use a learning rate 2e-5 with a linear schedule, batch size 40, and 4 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_45",
            "start": 338,
            "end": 483,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_46@0",
            "content": "In simulation experiments, we use batch size 40.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_46",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_46@1",
            "content": "We turn off dropout to simulate interaction with users in deployment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_46",
            "start": 49,
            "end": 117,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_46@2",
            "content": "For single-pass online learning experiments (Section 5; Section 7), we use a constant learning rate of 1e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_46",
            "start": 119,
            "end": 226,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_46@3",
            "content": "For offline learning experiments (Section 6), we train the model for 3 epochs on the collected feedback with a linear schedule learning rate of 3e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_46",
            "start": 228,
            "end": 376,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_47@0",
            "content": "Online experiments with SQUAD, HotpotQA, NQ, and NewsQA take 2-4h each on one NVIDIA GeForce RTX 2080 Ti; 2.5-6h for offline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_47",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_47@1",
            "content": "For TriviaQA and SearchQA, each online simulation experiment on one NVIDIA TITAN RTX takes 4-9.5h; 9-20h for offline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_47",
            "start": 126,
            "end": 242,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_48@0",
            "content": "Online Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_48",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_49@0",
            "content": "We simulate a scenario where only a limited amount of supervised data is available, and the model mainly learns from explicit user feedback on predicted answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_49",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_49@1",
            "content": "We use 64, 256, or 1,024 in-domain annotated examples to train an initial model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_49",
            "start": 162,
            "end": 241,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_49@2",
            "content": "This section focuses on online learning, where the learner updates the model parameters after each feedback is observed (Algorithm 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_49",
            "start": 243,
            "end": 376,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@0",
            "content": "Figure 2 presents the performance of in-domain simulation with online learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@1",
            "content": "The performance pattern varies across different datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 80,
            "end": 136,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@2",
            "content": "Bandit learning consistently improves performance on SQUAD, HotpotQA, and NQ across different amounts of supervised data used to train the initial model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 138,
            "end": 290,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@3",
            "content": "The performance gain is larger with weaker initial models (i.e., trained on 64 supervised examples): 63.6 on SQUAD, 42.7 on HotpotQA, and 40.0 on NQ.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 292,
            "end": 440,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@4",
            "content": "Bandit learning is not always effective on NewsQA, TriviaQA, and SearchQA, especially with weaker initial models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 442,
            "end": 554,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@5",
            "content": "This may be attributed to the quality of training set annotations, which determines the accuracy of reward in our setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 556,
            "end": 675,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@6",
            "content": "SearchQA and TriviaQA use distant supervision to match questions and relevant contexts from the web, likely decreasing reward quality in our setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 677,
            "end": 823,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_50@7",
            "content": "While NewsQA is crowdsourced, Trischler et al. (2017)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_50",
            "start": 825,
            "end": 877,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_51@0",
            "content": "Offline Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_51",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_52@0",
            "content": "We simulate offline bandit learning (Algorithm 2), where feedback is collected all at once with the initial model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_52",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_52@1",
            "content": "The learning scenario follows the previous section: only a limited amount of supervised data is available (64, 256, or 1,024 in-domain examples) to train initial models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_52",
            "start": 115,
            "end": 283,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@0",
            "content": "Table 1 shows the performance of offline simulation experiments compared to online simulations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@1",
            "content": "We observe mixed results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 96,
            "end": 120,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@2",
            "content": "On SQUAD, HotpotQA, NQ, and NewsQA, offline learning outperforms online learning when using stronger initial models (i.e., models trained on 256 and 1,024 examples).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 122,
            "end": 286,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@3",
            "content": "This illustrates the benefit of the more standard training loop, especially with our Transformerbased model that is better optimized with a linear learning rate schedule and multiple epochs, both incompatible with the online setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 288,
            "end": 518,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@4",
            "content": "On TriviaQA and SearchQA, offline simulation is ineffective regardless of the performance of initial models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 520,
            "end": 627,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_53@5",
            "content": "This result echoes the learning challenges in the online counterparts on these two datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_53",
            "start": 629,
            "end": 720,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@0",
            "content": "Online vs. Offline Regret Table 2 compares online and offline regret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@1",
            "content": "Regret numbers are averaged over the number of feedback observations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 70,
            "end": 138,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@2",
            "content": "11 Online learning generally displays lower regret for similar initial models on SQUAD, HotpotQA, and NQ.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 140,
            "end": 244,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@3",
            "content": "This is expected because later interactions in the simulation can benefit from early feedback in online learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 246,
            "end": 358,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@4",
            "content": "In contrast, in our offline scenario, we only update after seeing all examples, so regret numbers depend on the initial model only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 360,
            "end": 490,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@5",
            "content": "Regret results on NewsQA, TriviaQA, and SearchQA are counterintuitive, generally showing that online learning has similar or higher regret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 492,
            "end": 630,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@6",
            "content": "The cases showing significantly higher online regret (64+sim on NewsQA and SearchQA) can be explained by the learning failing, which impacts online regret, but not our offline regret.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 632,
            "end": 814,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@7",
            "content": "The others are more complex, and we hypothesize that they may be because of combination of (a) inherent noise in the data; and (b) in cases where online learning is effective, the gap between the strictly-defined reward that is used to compute regret and the relaxed F1 evaluation metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 816,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_54@8",
            "content": "Further analysis is required for a more conclusive conclusion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_54",
            "start": 1105,
            "end": 1166,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_55@0",
            "content": "Domain Adaptation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_55",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@0",
            "content": "Learning from user feedback creates a compelling avenue to deploy systems that target new domains not addressed by existing datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@1",
            "content": "The scenario we simulate in this section starts with training a QA model on a complete existing annotated dataset, and deploying it to interact with users and learn from their feedback in a new domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 134,
            "end": 334,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@2",
            "content": "We do not assume access to any annotated training data in Figure 4 shows online domain adaptation performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 336,
            "end": 445,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@3",
            "content": "On 22/30 configurations, online adaptation introduces significant performance gains (>2 F1 score).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 447,
            "end": 544,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@4",
            "content": "For example, adapting from TriviaQA and SearchQA to the other four domains improves performance by 27-72.8 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 546,
            "end": 655,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@5",
            "content": "On HotpotQA, the model initially trained on TriviaQA shows an impressive adaptation, improving from 0.2 F1 to 73 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 657,
            "end": 772,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@6",
            "content": "12 Our simulations show reduced effectiveness when the target domain is either TriviaQA or SearchQA, likely because the simulated feedback is based on noisy distantly supervised data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 774,
            "end": 956,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@7",
            "content": "For SearchQA, the low performance of initial models from other domains may also contribute to the adaptation failure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 958,
            "end": 1074,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@8",
            "content": "As expected, this indicates the effectiveness of the process depends on the relation between the source and target domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 1076,
            "end": 1198,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_56@9",
            "content": "SearchQA seems farthest from the other domains, mirroring observations from prior work (Su et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_56",
            "start": 1200,
            "end": 1304,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@0",
            "content": "Figure 5 shows learning curves for our simulation experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@1",
            "content": "Generally, we observe the choice of source and target domains influences adaptation rates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 63,
            "end": 152,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@2",
            "content": "Models quickly adapt to SQUAD, Hot-potQA, and NQ, reaching near final performance with a quarter of the total feedback provided.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 154,
            "end": 281,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@3",
            "content": "On NewsQA, models initially trained on TriviaQA and SearchQA adapt slower than those initially trained on other three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 283,
            "end": 409,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@4",
            "content": "On TriviaQA, we observe little change in performance throughout simulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 411,
            "end": 485,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@5",
            "content": "On SearchQA, only the model initially trained on TriviaQA shows a performance gain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 487,
            "end": 569,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_57@6",
            "content": "Both SearchQA and TriviaQA include context paragraphs from the web, potentially making domain adaptation from one to the other easier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_57",
            "start": 571,
            "end": 704,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@0",
            "content": "Lastly, we compare bandit learning with initial models trained on a small amount of in-domain data (Section 5) and initial models trained on a large amount of out-of-domain data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@1",
            "content": "Table 3 compares online learning with initial models trained on 1,024 in-domain supervised examples and online domain adaptation with a SQUAD-initialized model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 179,
            "end": 338,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@2",
            "content": "SQUAD initialization provides a robust starting point for all datasets except SearchQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 340,
            "end": 426,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@3",
            "content": "On four out of five datasets, the final performance is better with SQUAD-initialized model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 428,
            "end": 518,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@4",
            "content": "This is potentially because the model is exposed to different signals from two datasets and overall sees more data, either as supervised examples or through feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 520,
            "end": 685,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_58@5",
            "content": "However, on SearchQA, learning with SQUAD-initialized model performs much worse than learning with the initial model trained on 1,024 in-domain examples, potentially because of the gap in initial model performance (23.5 vs. 65 F1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_58",
            "start": 687,
            "end": 917,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_59@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_59",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@0",
            "content": "Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@1",
            "content": "Explicit human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Artzi and Zettlemoyer, 2011;Lawrence and Riezler, 2018), and summarization (Stiennon et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 338,
            "end": 580,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@2",
            "content": "Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 582,
            "end": 753,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@3",
            "content": "Our work shows that simulated bandit feedback is an effective learning signal for extractive question answering tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 755,
            "end": 872,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@4",
            "content": "Our work differs in focus on reducing annotation costs by relying on few annotated examples only to train the initial model, or by eliminating the need for in-domain annotation completely by relying on data in other domains to train initial models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 874,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@5",
            "content": "Implicit human feedback, where feedback is derived from human behavior rather than explicitly requested, has also been studied, including for dialogue (Jaques et al., 2020) and instruction generation (Kojima et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 1123,
            "end": 1344,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_60@6",
            "content": "We focus on explicit feedback, but implicit signals also hold promise to improve QA systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_60",
            "start": 1346,
            "end": 1437,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@0",
            "content": "Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020;Khashabi et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@1",
            "content": "Kratzwald et al. (2020) resembles our setting in seeking binary feedback to replace span annotation, but their goal is to create supervised data more economically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 171,
            "end": 333,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@2",
            "content": "Campos et al. (2020) proposes feedback-weighted learning to improves conversational QA using simulated binary feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 335,
            "end": 453,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@3",
            "content": "Their approach relies on multiple samples (i.e., feedback signals) per example, training for multiple epochs online by re-visiting the same questions repeatedly, and tuning two additional hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 455,
            "end": 658,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@4",
            "content": "In contrast, we study improving QA systems via feedback as a bandit learning problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 660,
            "end": 744,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@5",
            "content": "In both online and offline setups, we assume only one feedback sample per example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 746,
            "end": 827,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@6",
            "content": "We also provide extensive sensitivity studies to the amount of annotations available, different model initialization, and noisy feedback across various datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 829,
            "end": 989,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_61@7",
            "content": "Domain adaptation for QA has been widely studied (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_61",
            "start": 991,
            "end": 1302,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_62@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_62",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_63@0",
            "content": "We present a simulation study of learning from user feedback for extractive QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_63",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_63@1",
            "content": "We formulate the problem as contextual bandit learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_63",
            "start": 80,
            "end": 134,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_63@2",
            "content": "We conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_63",
            "start": 136,
            "end": 369,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_63@3",
            "content": "Our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_63",
            "start": 371,
            "end": 519,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_64@0",
            "content": "We intentionally adopt a basic setup, including a simple binary reward and vanilla learning algorithms, to illustrate what can be achieved with a relatively simple variant of the contextual bandit learning scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_64",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_64@1",
            "content": "Our results already indicate the strong potential of learning from feedback, which more advanced methods are likely to further improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_64",
            "start": 216,
            "end": 350,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_64@2",
            "content": "For example, the balance between online and offline learning can be further explored using proximal policy optimization (PPO; Schulman et al., 2017) or replay memory (Mnih et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_64",
            "start": 352,
            "end": 537,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_64@3",
            "content": "With welldesigned interface, human users may be able to provide more sophisticated feedback (Lamm et al., 2021), which will provide a stronger signal compared to our binary reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_64",
            "start": 539,
            "end": 718,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_65@0",
            "content": "Our aim in this study is to lay the foundation for future work, by formalizing the setup and showing its potential.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_65",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_65@1",
            "content": "This is a critical step in enabling future research, especially going beyond simulation to study using real human feedback for QA systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_65",
            "start": 116,
            "end": 253,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_65@2",
            "content": "Another important direction for future work is studying user feedback for QA systems that do both context retrieval and answer generation (Lewis et al., 2020), where assigning the feedback to the appropriate stage in the process poses a challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_65",
            "start": 255,
            "end": 501,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_65@3",
            "content": "Beyond extractive QA, we hope our work will inspire research of user feedback as a signal to improve other types of NLP systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_65",
            "start": 503,
            "end": 630,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_66@0",
            "content": "Our work's limitations are discussed in Section 1 and Section 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_66",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_66@1",
            "content": "All six datasets we use are from prior work, are publicly available, and are commonly used for the study of extractive QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_66",
            "start": 65,
            "end": 186,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_66@2",
            "content": "Section 4 reports our computational budget and experimental setup in detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_66",
            "start": 188,
            "end": 263,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_66@3",
            "content": "Our codebase is available at https://github.com/ lil-lab/bandit-qa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_66",
            "start": 265,
            "end": 331,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_67@0",
            "content": "Reward Function Intuitively, partial credit reward may improve learning over binary rewards.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_67",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_67@1",
            "content": "We experiment with using F1 score of the predicted answer span as a more refined feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_67",
            "start": 93,
            "end": 182,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_67@2",
            "content": "13 In practice, this does not introduce a stronger learning signal, potentially because the distribution over F1 scores is bimodal and focused on extreme values: around 85 % F1 scores are either 0 or 1 for predicted spans from a SQUAD-trained model on 8% NQ training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_67",
            "start": 184,
            "end": 455,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_67@3",
            "content": "We observe similar trends on all six datasets across all setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_67",
            "start": 457,
            "end": 520,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_67@4",
            "content": "Experiments with BLEU score (Papineni et al., 2002) as feedback show similar conclusion and distribution to F1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_67",
            "start": 522,
            "end": 638,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_68@0",
            "content": "Perturbation In practice, noise in feedback is likely to be more systematic than the statistical simplification which defines noise as the random percentage of wrong feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_68",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_68@1",
            "content": "For example, prior work (Nguyen et al., 2017) on bandit neural machine translation (NMT) proposes that noisy human feedback is granular, high-variance, and skewed, which can be approximated by mathematical functions and shows to significantly impact the bandit NMT learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_68",
            "start": 176,
            "end": 449,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_68@2",
            "content": "We experiment with the three perturbation functions from Nguyen et al. (2017) on F1 reward.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_68",
            "start": 451,
            "end": 541,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_68@3",
            "content": "Our experiments show that the effect of adding these perturbation functions is negligible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_68",
            "start": 543,
            "end": 632,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_68@4",
            "content": "We hypothesize that the reward distribution for NMT is likely to be closer to a normal distribution, rather than a bimodal one like QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_68",
            "start": 634,
            "end": 768,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_69@0",
            "content": "While arg max can bias towards exploitation, sampling can encourage more exploration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_69",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_69@1",
            "content": "We experiment with prediction via arg max and sampling from the output distribution over spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_69",
            "start": 86,
            "end": 180,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_69@2",
            "content": "Table 4 shows that arg max performs better than random sampling on three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_69",
            "start": 182,
            "end": 263,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_69@3",
            "content": "This set of experiments is conducted with batch size 80.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_69",
            "start": 265,
            "end": 320,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_70@0",
            "content": "Table 5 shows the sensitivity analysis results for online in-domain simulation on HotpotQA and TriviaQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_70",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_70@1",
            "content": "We experiment with five initial models trained on different sets of 64 or 1,024 supervised examples, each used to initiate a separate simulation experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_70",
            "start": 105,
            "end": 260,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_70@2",
            "content": "For weaker initial models trained on 64 supervised examples, four out of five experiments on HotpotQA show performance gains similar to our main results, except one experiment that starts with a very low initialization performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_70",
            "start": 262,
            "end": 492,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_71@0",
            "content": "Nearly all experiments on TriviaQA collapse (mean F1 of 7.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_71",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_71@1",
            "content": "Our sensitivity analysis with stronger initial models trained on 1,024 examples shows that the final performance is stable across runs on both HotpotQA and TriviaQA (standard deviations are 0.5 and 2.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_71",
            "start": 62,
            "end": 264,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_72@0",
            "content": "We perform domain adaptation with offline learning, and compare its performance with online adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_72",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_72@1",
            "content": "Table 6 shows the performance gain of offline adaptation simulation compared to the online setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_72",
            "start": 104,
            "end": 200,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_72@2",
            "content": "In most settings, online learning proves to be more effective, possibly because it observes feedback from partially adapted model predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_72",
            "start": 202,
            "end": 343,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_72@3",
            "content": "In a few settings (4/30), we observe better adaptation with offline settings (+1.1 to +4.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_72",
            "start": 345,
            "end": 436,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_72@4",
            "content": "Overall, we observe that online learning is more effective on domain adaptation, while offline adaption performs slightly better when both domains are related (e.g., same source domain).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_72",
            "start": 438,
            "end": 623,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_73@0",
            "content": "Yoav Artzi, Luke Zettlemoyer, Bootstrapping semantic parsers from conversations, 2011, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_73",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_74@0",
            "content": "Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, Robert Schapire, The nonstochastic multiarmed bandit problem, 2002, SIAM Journal on Computing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_74",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_75@0",
            "content": "UNKNOWN, None, 2018, A contextual bandit bake-off, JMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_75",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_76@0",
            "content": "UNKNOWN, None, 2020, Improving conversational question answering systems after deployment using feedbackweighted learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_76",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_77@0",
            "content": "UNKNOWN, None, 2018, Quac: Question answering in context. EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_77",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_78@0",
            "content": "UNKNOWN, None, 2020, Benefits of intermediate annotations in reading comprehension, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_78",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_79@0",
            "content": "UNKNOWN, None, 2017, Searchqa: A new q&a dataset augmented with context from a search engine, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_79",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_80@0",
            "content": "Tobias Falke, Patrick Lehnen, Feedback attribution for counterfactual bandit learning in multidomain spoken language understanding, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_80",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_81@0",
            "content": "Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, Danqi Chen, MRQA 2019 shared task: Evaluating generalization in reading comprehension, 2019, MRQA@EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_81",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_82@0",
            "content": "Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov, Sachindra Joshi, Using question answering rewards to improve abstractive summarization, 2021, Findings@EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_82",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_83@0",
            "content": "G Daniel, D Horvitz,  Thompson, A generalization of sampling without replacement from a finite universe, 1952, Journal of the American Statistical Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_83",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_84@0",
            "content": "Natasha Jaques, Judy Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard, Human-centric dialog training via offline reinforcement learning, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_84",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2020, Spanbert: Improving pre-training by representing and predicting spans, TACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_85",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2017, Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_86",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_87@0",
            "content": "UNKNOWN, None, 2020, More bang for your buck: Natural perturbation for robust question answering, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_87",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_88@0",
            "content": "UNKNOWN, None, 2020, Unifiedqa: Crossing format boundaries with a single qa system. Findings@EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_88",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_89@0",
            "content": "UNKNOWN, None, 2021, Continual learning for grounded instruction generation by observing human following behavior, TACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_89",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_90@0",
            "content": "Bernhard Kratzwald, Stefan Feuerriegel, Huan Sun, Learning a cost-effective annotation policy for question answering, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_90",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_91@0",
            "content": "UNKNOWN, None, 2018, Can neural machine translation be improved with user feedback? NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_91",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_92@0",
            "content": "UNKNOWN, None, 2018, Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_92",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_93@0",
            "content": "Devang Kulshreshtha, Robert Belfer, Iulian Serban, Siva Reddy, Back-training excels selftraining at unsupervised domain adaptation of question generation and passage retrieval, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_93",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. TACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_94",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_95@0",
            "content": "UNKNOWN, None, 2021, Qed: A framework and dataset for explanations in question answering, TACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_95",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_96@0",
            "content": "John Langford, Tong Zhang, The epochgreedy algorithm for contextual multi-armed bandits, 2007, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_96",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_97@0",
            "content": "Carolin Lawrence, Stefan Riezler, Improving a neural semantic parser by counterfactual learning from human bandit feedback, 2018, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_97",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2017, Counterfactual learning from bandit feedback under deterministic logging : A case study in statistical machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_98",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_99@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_99",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_100@0",
            "content": "UNKNOWN, None, 2019, Domain-agnostic question-answering with adversarial training, MRQA@EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_100",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_101@0",
            "content": "UNKNOWN, None, 2020, Retrieval-augmented generation for knowledge-intensive nlp tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_101",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_102@0",
            "content": "UNKNOWN, None, 2021, Online learning meets machine translation evaluation: Finding the best systems with the least human effort, ACL/IJCNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_102",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_103@0",
            "content": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas Riedmiller, Georg Fidjeland,  Ostrovski, Human-level control through deep reinforcement learning, 2015, Nature, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_103",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2017, Reinforcement learning for bandit neural machine translation with simulated human feedback, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_104",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2002, Bleu: a method for automatic evaluation of machine translation, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_105",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_106@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100,000+ questions for machine comprehension of text, 2016, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_106",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2021, Few-shot question answering by pretraining span selection, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_107",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_108@0",
            "content": "UNKNOWN, None, 2017, Proximal policy optimization algorithms. arXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_108",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_109@0",
            "content": "UNKNOWN, None, 2017, Bidirectional attention flow for machine comprehension, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_109",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_110@0",
            "content": "UNKNOWN, None, 2016, Learning structured predictors from bandit feedback for interactive nlp, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_110",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_111@0",
            "content": "UNKNOWN, None, 2017, A shared task on bandit learning for machine translation, WMT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_111",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2020, Learning to summarize from human feedback, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_112",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_113@0",
            "content": "UNKNOWN, None, 2019, Generalizing question answering system with pre-trained language model fine-tuning, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_113",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_114@0",
            "content": "UNKNOWN, None, 1998, Reinforcement learning: An introduction, MIT press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_114",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_115@0",
            "content": "UNKNOWN, None, 2017, Newsqa: A machine comprehension dataset, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_115",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_116@0",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 2004, Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_116",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_117@0",
            "content": "UNKNOWN, None, , Morgan Funtowicz, and Jamie Brew. 2020. Huggingface's transformers: State-of-the-art natural language processing. EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_117",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_118@0",
            "content": "UNKNOWN, None, 2018, Hotpotqa: A dataset for diverse, explainable multi-hop question answering, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_118",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2020, An imitation game for learning semantic parsers from user interaction, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_119",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_120@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_120",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2018, Qanet: Combining local convolution with global self-attention for reading comprehension, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_121",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_122@0",
            "content": "Zhenrui Yue, Bernhard Kratzwald, Stefan Feuerriegel, Contrastive domain adaptation for question answering using limited text corpora, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_122",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_123@0",
            "content": "J Michael, Eunsol Zhang,  Choi, Situat-edQA: Incorporating extra-linguistic contexts into QA, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_123",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2021, Revisiting fewsample bert fine-tuning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_124",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "169-ARR_v2_125@0",
            "content": "Haichao Zhu, Zekun Wang, Heng Zhang, Ming Liu, Sendong Zhao, Bing Qin, Less is more: Domain adaptation with lottery ticket for reading comprehension, 2021, Findings@EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v2_125",
            "start": 0,
            "end": 172,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_1",
            "tgt_ix": "169-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_1",
            "tgt_ix": "169-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_2",
            "tgt_ix": "169-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_6",
            "tgt_ix": "169-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_7",
            "tgt_ix": "169-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_11",
            "tgt_ix": "169-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_12",
            "tgt_ix": "169-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_13",
            "tgt_ix": "169-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_14",
            "tgt_ix": "169-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_15",
            "tgt_ix": "169-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_16",
            "tgt_ix": "169-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_17",
            "tgt_ix": "169-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_18",
            "tgt_ix": "169-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_19",
            "tgt_ix": "169-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_20",
            "tgt_ix": "169-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_21",
            "tgt_ix": "169-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_22",
            "tgt_ix": "169-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_23",
            "tgt_ix": "169-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_26",
            "tgt_ix": "169-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_27",
            "tgt_ix": "169-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_28",
            "tgt_ix": "169-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_29",
            "tgt_ix": "169-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_30",
            "tgt_ix": "169-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_31",
            "tgt_ix": "169-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_32",
            "tgt_ix": "169-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_33",
            "tgt_ix": "169-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_34",
            "tgt_ix": "169-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_35",
            "tgt_ix": "169-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_36",
            "tgt_ix": "169-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_37",
            "tgt_ix": "169-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_40",
            "tgt_ix": "169-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_41",
            "tgt_ix": "169-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_42",
            "tgt_ix": "169-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_44",
            "tgt_ix": "169-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_45",
            "tgt_ix": "169-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_46",
            "tgt_ix": "169-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_43",
            "tgt_ix": "169-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_47",
            "tgt_ix": "169-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_49",
            "tgt_ix": "169-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_48",
            "tgt_ix": "169-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_48",
            "tgt_ix": "169-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_48",
            "tgt_ix": "169-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_52",
            "tgt_ix": "169-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_51",
            "tgt_ix": "169-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_51",
            "tgt_ix": "169-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_51",
            "tgt_ix": "169-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_51",
            "tgt_ix": "169-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_55",
            "tgt_ix": "169-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_55",
            "tgt_ix": "169-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_55",
            "tgt_ix": "169-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_55",
            "tgt_ix": "169-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_59",
            "tgt_ix": "169-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_59",
            "tgt_ix": "169-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_59",
            "tgt_ix": "169-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_63",
            "tgt_ix": "169-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_64",
            "tgt_ix": "169-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_65",
            "tgt_ix": "169-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_66",
            "tgt_ix": "169-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_70",
            "tgt_ix": "169-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_69",
            "tgt_ix": "169-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_71",
            "tgt_ix": "169-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v2_0",
            "tgt_ix": "169-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_1",
            "tgt_ix": "169-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_2",
            "tgt_ix": "169-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_2",
            "tgt_ix": "169-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_2",
            "tgt_ix": "169-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_3",
            "tgt_ix": "169-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_4",
            "tgt_ix": "169-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_5",
            "tgt_ix": "169-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_6",
            "tgt_ix": "169-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_6",
            "tgt_ix": "169-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_7",
            "tgt_ix": "169-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_7",
            "tgt_ix": "169-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_7",
            "tgt_ix": "169-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_8",
            "tgt_ix": "169-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_9",
            "tgt_ix": "169-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_10",
            "tgt_ix": "169-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_11",
            "tgt_ix": "169-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_11",
            "tgt_ix": "169-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_11",
            "tgt_ix": "169-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_11",
            "tgt_ix": "169-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_12",
            "tgt_ix": "169-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_12",
            "tgt_ix": "169-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_12",
            "tgt_ix": "169-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_13",
            "tgt_ix": "169-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_13",
            "tgt_ix": "169-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_13",
            "tgt_ix": "169-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_13",
            "tgt_ix": "169-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_14",
            "tgt_ix": "169-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_15",
            "tgt_ix": "169-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_16",
            "tgt_ix": "169-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_16",
            "tgt_ix": "169-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_16",
            "tgt_ix": "169-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_17",
            "tgt_ix": "169-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_17",
            "tgt_ix": "169-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_17",
            "tgt_ix": "169-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_18",
            "tgt_ix": "169-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_18",
            "tgt_ix": "169-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_18",
            "tgt_ix": "169-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_19",
            "tgt_ix": "169-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_20",
            "tgt_ix": "169-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_21",
            "tgt_ix": "169-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_22",
            "tgt_ix": "169-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_23",
            "tgt_ix": "169-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_24",
            "tgt_ix": "169-ARR_v2_24@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_25",
            "tgt_ix": "169-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_26",
            "tgt_ix": "169-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_27",
            "tgt_ix": "169-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_28",
            "tgt_ix": "169-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_29",
            "tgt_ix": "169-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_30",
            "tgt_ix": "169-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_31",
            "tgt_ix": "169-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_32",
            "tgt_ix": "169-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_33",
            "tgt_ix": "169-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_34",
            "tgt_ix": "169-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_35",
            "tgt_ix": "169-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_36",
            "tgt_ix": "169-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_37",
            "tgt_ix": "169-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_38",
            "tgt_ix": "169-ARR_v2_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_39",
            "tgt_ix": "169-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_40",
            "tgt_ix": "169-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_40",
            "tgt_ix": "169-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_40",
            "tgt_ix": "169-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_40",
            "tgt_ix": "169-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_41",
            "tgt_ix": "169-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_41",
            "tgt_ix": "169-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_41",
            "tgt_ix": "169-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_41",
            "tgt_ix": "169-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_42",
            "tgt_ix": "169-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_42",
            "tgt_ix": "169-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_43",
            "tgt_ix": "169-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_43",
            "tgt_ix": "169-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_43",
            "tgt_ix": "169-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_43",
            "tgt_ix": "169-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_44",
            "tgt_ix": "169-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_44",
            "tgt_ix": "169-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_45",
            "tgt_ix": "169-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_45",
            "tgt_ix": "169-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_45",
            "tgt_ix": "169-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_45",
            "tgt_ix": "169-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_46",
            "tgt_ix": "169-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_46",
            "tgt_ix": "169-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_46",
            "tgt_ix": "169-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_46",
            "tgt_ix": "169-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_47",
            "tgt_ix": "169-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_47",
            "tgt_ix": "169-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_48",
            "tgt_ix": "169-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_49",
            "tgt_ix": "169-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_49",
            "tgt_ix": "169-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_49",
            "tgt_ix": "169-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_50",
            "tgt_ix": "169-ARR_v2_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_51",
            "tgt_ix": "169-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_52",
            "tgt_ix": "169-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_52",
            "tgt_ix": "169-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_53",
            "tgt_ix": "169-ARR_v2_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_54",
            "tgt_ix": "169-ARR_v2_54@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_55",
            "tgt_ix": "169-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_56",
            "tgt_ix": "169-ARR_v2_56@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_57",
            "tgt_ix": "169-ARR_v2_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_58",
            "tgt_ix": "169-ARR_v2_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_59",
            "tgt_ix": "169-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_60",
            "tgt_ix": "169-ARR_v2_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_61",
            "tgt_ix": "169-ARR_v2_61@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_62",
            "tgt_ix": "169-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_63",
            "tgt_ix": "169-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_63",
            "tgt_ix": "169-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_63",
            "tgt_ix": "169-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_63",
            "tgt_ix": "169-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_64",
            "tgt_ix": "169-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_64",
            "tgt_ix": "169-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_64",
            "tgt_ix": "169-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_64",
            "tgt_ix": "169-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_65",
            "tgt_ix": "169-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_65",
            "tgt_ix": "169-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_65",
            "tgt_ix": "169-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_65",
            "tgt_ix": "169-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_66",
            "tgt_ix": "169-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_66",
            "tgt_ix": "169-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_66",
            "tgt_ix": "169-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_66",
            "tgt_ix": "169-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_67",
            "tgt_ix": "169-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_68",
            "tgt_ix": "169-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_69",
            "tgt_ix": "169-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_69",
            "tgt_ix": "169-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_69",
            "tgt_ix": "169-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_69",
            "tgt_ix": "169-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_70",
            "tgt_ix": "169-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_70",
            "tgt_ix": "169-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_70",
            "tgt_ix": "169-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_71",
            "tgt_ix": "169-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_71",
            "tgt_ix": "169-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_72",
            "tgt_ix": "169-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_72",
            "tgt_ix": "169-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_72",
            "tgt_ix": "169-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_72",
            "tgt_ix": "169-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_72",
            "tgt_ix": "169-ARR_v2_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_73",
            "tgt_ix": "169-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_74",
            "tgt_ix": "169-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_75",
            "tgt_ix": "169-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_76",
            "tgt_ix": "169-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_77",
            "tgt_ix": "169-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_78",
            "tgt_ix": "169-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_79",
            "tgt_ix": "169-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_80",
            "tgt_ix": "169-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_81",
            "tgt_ix": "169-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_82",
            "tgt_ix": "169-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_83",
            "tgt_ix": "169-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_84",
            "tgt_ix": "169-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_85",
            "tgt_ix": "169-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_86",
            "tgt_ix": "169-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_87",
            "tgt_ix": "169-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_88",
            "tgt_ix": "169-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_89",
            "tgt_ix": "169-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_90",
            "tgt_ix": "169-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_91",
            "tgt_ix": "169-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_92",
            "tgt_ix": "169-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_93",
            "tgt_ix": "169-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_94",
            "tgt_ix": "169-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_95",
            "tgt_ix": "169-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_96",
            "tgt_ix": "169-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_97",
            "tgt_ix": "169-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_98",
            "tgt_ix": "169-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_99",
            "tgt_ix": "169-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_100",
            "tgt_ix": "169-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_101",
            "tgt_ix": "169-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_102",
            "tgt_ix": "169-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_103",
            "tgt_ix": "169-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_104",
            "tgt_ix": "169-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_105",
            "tgt_ix": "169-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_106",
            "tgt_ix": "169-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_107",
            "tgt_ix": "169-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_108",
            "tgt_ix": "169-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_109",
            "tgt_ix": "169-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_110",
            "tgt_ix": "169-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_111",
            "tgt_ix": "169-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_112",
            "tgt_ix": "169-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_113",
            "tgt_ix": "169-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_114",
            "tgt_ix": "169-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_115",
            "tgt_ix": "169-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_116",
            "tgt_ix": "169-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_117",
            "tgt_ix": "169-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_118",
            "tgt_ix": "169-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_119",
            "tgt_ix": "169-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_120",
            "tgt_ix": "169-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_121",
            "tgt_ix": "169-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_122",
            "tgt_ix": "169-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_123",
            "tgt_ix": "169-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_124",
            "tgt_ix": "169-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v2_125",
            "tgt_ix": "169-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 853,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "169-ARR",
        "version": 2
    }
}