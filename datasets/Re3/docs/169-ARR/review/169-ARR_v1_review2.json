{
    "nodes": [
        {
            "ix": "169-ARR_v1_review2_0",
            "content": "169-ARR_v1_review2",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "169-ARR_v1_review2_1",
            "content": "paper_summary. This paper focuses on using bandit learning to learn from user feedback for Extractive QA (EQA), the binary supervisory signals from user feedback serve as rewards pushing QA systems to evolve. The learning algorithm aims to maximise the rewards of all QA examples, which consists of online learning and offline learning, the online learning receives user feedback and updates model parameters after seeing one QA example, whereas offline learning updates model parameters after seeing all QA examples.  The experimental results on QA datasets from MRQA support the effectiveness of the proposed bandit learning approach, proving that the proposed approach can consistently improve model\u2019s performance on SQuAD, HotpotQA and NQ in in-domain experiments under online learning especially when there are extremely little QA examples available for SQuAD. Besides, a set of experiments are conducted to investigate the difference between online learning and offline learning, and the importance of model initialisation in the proposed bandit learning approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v1_review2_2",
            "content": "summary_of_strengths. 1. The proposed bandit learning approach that learns from user feedback for EQA is novel, which simulates real deployment environment and provides insights for further exploration in bridging the gap between QA model training and deployment. \n2. Empirical results show the effectiveness of the proposed approach, especially the in-domain experimental results for online learning. \n3. Conducting extensive experiments studying the effect of domain transfer and model initialisation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v1_review2_3",
            "content": "summary_of_weaknesses. 1. The binary reward from user feedback is weak due to the large search space for EQA, resulting in the incapability of providing precise supervisory signals. Need to design a more sophisticated reward. \n2. The proposed approach heavily relies on how accurate the initial model is, which means it is highly sensitive to model initialisation, limiting its usefullness. \n3. In in-domain experiments of online and offline learning, bandit learning approach hurts model\u2019s performance under some scenarios especially for TriviaQA and SearchQA. \n4. Some other papers of learning from feedback for QA should be compared, such as Learning by Asking Questions, Misra et al. CVPR 2017.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "169-ARR_v1_review2_4",
            "content": "comments,_suggestions_and_typos. Questions:      1. Why only use single-pass in online learning?",
            "ntype": "p",
            "meta": null
        }
    ],
    "span_nodes": [
        {
            "ix": "169-ARR_v1_review2_0@0",
            "content": "169-ARR_v1_review2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_0",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_1@0",
            "content": "paper_summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_1",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_1@1",
            "content": "This paper focuses on using bandit learning to learn from user feedback for Extractive QA (EQA), the binary supervisory signals from user feedback serve as rewards pushing QA systems to evolve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_1",
            "start": 15,
            "end": 207,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_1@2",
            "content": "The learning algorithm aims to maximise the rewards of all QA examples, which consists of online learning and offline learning, the online learning receives user feedback and updates model parameters after seeing one QA example, whereas offline learning updates model parameters after seeing all QA examples.  ",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_1",
            "start": 209,
            "end": 518,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_1@3",
            "content": "The experimental results on QA datasets from MRQA support the effectiveness of the proposed bandit learning approach, proving that the proposed approach can consistently improve model\u2019s performance on SQuAD, HotpotQA and NQ in in-domain experiments under online learning especially when there are extremely little QA examples available for SQuAD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_1",
            "start": 519,
            "end": 864,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_1@4",
            "content": "Besides, a set of experiments are conducted to investigate the difference between online learning and offline learning, and the importance of model initialisation in the proposed bandit learning approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_1",
            "start": 866,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_2@0",
            "content": "summary_of_strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_2",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_2@1",
            "content": "1. The proposed bandit learning approach that learns from user feedback for EQA is novel, which simulates real deployment environment and provides insights for further exploration in bridging the gap between QA model training and deployment. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_2",
            "start": 22,
            "end": 264,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_2@2",
            "content": "2. Empirical results show the effectiveness of the proposed approach, especially the in-domain experimental results for online learning. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_2",
            "start": 265,
            "end": 402,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_2@3",
            "content": "3. Conducting extensive experiments studying the effect of domain transfer and model initialisation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_2",
            "start": 403,
            "end": 502,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@0",
            "content": "summary_of_weaknesses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@1",
            "content": "1. The binary reward from user feedback is weak due to the large search space for EQA, resulting in the incapability of providing precise supervisory signals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 23,
            "end": 180,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@2",
            "content": "Need to design a more sophisticated reward. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 182,
            "end": 226,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@3",
            "content": "2. The proposed approach heavily relies on how accurate the initial model is, which means it is highly sensitive to model initialisation, limiting its usefullness. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 227,
            "end": 391,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@4",
            "content": "3. In in-domain experiments of online and offline learning, bandit learning approach hurts model\u2019s performance under some scenarios especially for TriviaQA and SearchQA. \n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 392,
            "end": 562,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_3@5",
            "content": "4. Some other papers of learning from feedback for QA should be compared, such as Learning by Asking Questions, Misra et al. CVPR 2017.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_3",
            "start": 563,
            "end": 697,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_4@0",
            "content": "comments,_suggestions_and_typos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_4",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_4@1",
            "content": "Questions:      1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_4",
            "start": 33,
            "end": 50,
            "label": {}
        },
        {
            "ix": "169-ARR_v1_review2_4@2",
            "content": "Why only use single-pass in online learning?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "169-ARR_v1_review2_4",
            "start": 52,
            "end": 95,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_2",
            "tgt_ix": "169-ARR_v1_review2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "169-ARR_v1_review2_0",
            "tgt_ix": "169-ARR_v1_review2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_1@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_1@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_1@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_1",
            "tgt_ix": "169-ARR_v1_review2_1@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_2",
            "tgt_ix": "169-ARR_v1_review2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_2",
            "tgt_ix": "169-ARR_v1_review2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_2",
            "tgt_ix": "169-ARR_v1_review2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_2",
            "tgt_ix": "169-ARR_v1_review2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_3",
            "tgt_ix": "169-ARR_v1_review2_3@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_4",
            "tgt_ix": "169-ARR_v1_review2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_4",
            "tgt_ix": "169-ARR_v1_review2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "169-ARR_v1_review2_4",
            "tgt_ix": "169-ARR_v1_review2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "169-ARR_v1_review2",
    "meta": {
        "ix_counter": 23,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy"
    }
}