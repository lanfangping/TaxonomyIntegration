{
    "nodes": [
        {
            "ix": "31-ARR_v1_0",
            "content": "ParaDetox: Detoxification with Parallel Data",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_2",
            "content": "We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distil a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_3",
            "content": "We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluation. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems substantially.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "31-ARR_v1_5",
            "content": "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.g. microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al., 2020), is a popular topic of research in NLP. However, detection of harmful messages does not offer any proactive ways of fighting them (besides deletion). We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_6",
            "content": "The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018;Tran et al., 2020). It is considered a variant of style transfer task, the task of rewriting a text saving the content and changing the style (style is defined as a characteristic of text such as sentiment, level of formality or politeness, author profile (gender, political preferences), etc.). As a sequenceto-sequence task, style transfer can be performed with an encoder-decoder model trained on parallel data. However, there exist only a few parallel style transfer corpora (Carlson et al., 2018;Pryzant et al., 2020). Since they usually do not exist \"naturally\", they need to be written from scratch. This is an expensive and laborious process. Thus, such parallel datasets are extremely rare.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_7",
            "content": "Jigsaw so why would anyone believe this moron?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_8",
            "content": "Paraphrase so why would anyone believe this person? so why would anyone believe somebody like him?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_9",
            "content": "Reddit dude ham sandwich is the good sh*t .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_10",
            "content": "dude ham sandwich is the good thing The ham sandwich, buddy, is the bomb. Dude ham sandwich is good.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_11",
            "content": "Twitter now i feel like an a*s Paraphrase now i feel like worthless now i feel very bad now i feel bad",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_12",
            "content": "Table 1: Examples of detoxified sentences from the collected parallel corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_13",
            "content": "We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases. We suggest a new crowdsourcing pipeline for collecting parallel style transfer data. It does not employ experts, which makes the data collection faster and cheaper. In addition to generating the detoxified versions of texts, we consider a way to distil existing datasets of paraphrases for style-specific data. In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup. The pipelines are described in detail to make them easy to replicate. Thus, we suggest that by reusing these pipelines the new parallel style transfer datasets can be collected in a fast and cheap way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_14",
            "content": "Finally, we validate the usefulness of our datasets by training detoxification models on them and comparing their performance with state-of-theart methods. Models trained on parallel data significantly outperform other models in terms of automatic metrics and human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_15",
            "content": "The contributions of our work are as three-fold:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_16",
            "content": "\u2022 We suggest a novel pipeline for collection of parallel data for the detoxification task, \u2022 We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, \u2022 Using collected data we train supervised detoxification models that yield SOTA results.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_17",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "31-ARR_v1_18",
            "content": "Style Transfer Datasets When collecting nonparallel style transfer corpora, style labels often already exist in the data (e.g. positive and negative reviews (Li et al., 2018a) 1 ) or its source serves as a label (e.g. Twitter, academic texts, legal documents, etc.). Thus, data collection is reduced to fetching the texts from their sources, and the corpus size depends only on the available amount of text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_19",
            "content": "Conversely, parallel corpora are usually more difficult to get. There exist parallel style transfer datasets fetched from \"naturally\" available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_20",
            "content": "Besides these special cases, there exists a large style transfer dataset that was created from scratch. This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts. Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference -we replace expert validation of crowdsourced sentences with crowd validation and additionally optimise the cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_21",
            "content": "The vast majority of style transfer models (including detoxification models) are trained on non-parallel data. They can perform pointwise corrections of stylemarked words (Li et al., 2018b;Wu et al., 2019;Malmi et al., 2020). Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017;Fu et al., 2018). As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_22",
            "content": "Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labelled for toxicity and considers toxic and neutral sentences as two subcorpora. Laugier et al. (2021) use the Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 for training, Nogueira dos Santos et al. ( 2018) create their own toxicity-labelled datasets of sentences from Reddit and Twitter. Following them, we also fetch sentences for rewriting from these datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_23",
            "content": "Works on detoxification often rely on style transfer models tested on other domains. Nogueira dos Santos et al. (2018) follow Shen et al. (2017) and Fu et al. (2018) and train an autoencoder with additional style classification and cycle-consistency losses. Laugier et al. (2021) perform a similar finetuning of T5 as a denoising autoencoder. Tran et al. (2020) apply pointwise corrections approach similar to that of Wu et al. (2019) and then improve the fluency of a text with a seq2seq model. Likewise, Dale et al. (2021) use a masked language model to perform pointwise edits of toxic sentences. They also suggest an alternative model which enhances a style-agnostic seq2seq model with style-informed language models which reweigh the seq2seq hypotheses with respect to the desired style.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_24",
            "content": "When the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models to perform style transfer. We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_25",
            "content": "Data Collection Pipeline",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "31-ARR_v1_26",
            "content": "Our goal is to yield pairs of sentences that have the same meanings and are contrasted in terms of Rewrite this text so that it does not sound offensive and its meaning stays the same You realize that's stupid, don't you? offensiveness -one of the sentences is toxic and the other is neutral. We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases. Unlike a similar work of Rao and Tetreault (2018), we hire crowd workers not only for the generation of paraphrases but also for their validation, which reduces both time and cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_27",
            "content": "Crowdsourcing Tasks",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "31-ARR_v1_28",
            "content": "We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity. Each task is implemented as a separate crowdsourcing project. We use the crowdsourcing platform Yandex.Toloka. 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_29",
            "content": "Task 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1). However, it is not always possible. Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless. On the other hand, in some cases toxicity cannot be removed. Consider the examples:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_30",
            "content": "\u2022 Are you that dumb you can't figure it out? \u2022 I've finally understood that wiki is nothing but a bunch of American racists.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_31",
            "content": "Not only the form but also the content of the messages are offensive, so trying to detoxify them would inevitably lead to a substantial change of sense. We prefer not to include such cases in the parallel dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_32",
            "content": "If workers have to detoxify all inputs without a possibility to skip them, a large proportion of the generated paraphrases will be of low quality. Thus, we add the control \"I can't rewrite the text\" and optional controls to indicate the reasons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_33",
            "content": "Task 2: Content Preservation Check We show users the generated paraphrases along with their original variants and ask to indicate if they have close meanings. Besides ensuring content preservation, this task implicitly filters out senseless outputs, because they obviously do not keep the original content. The task interface is shown in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_34",
            "content": "Task 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity. We ask users to indicate if the paraphrases contain any offence or swear words (see Figure 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_35",
            "content": "In addition to filtering out unsuitable paraphrases, we use Tasks 2 and 3 for paying for Task 1. We accept or reject the generated paraphrases based on the labels they get in Tasks 2 and 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_36",
            "content": "Pipelines",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "31-ARR_v1_37",
            "content": "Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting. We fetch them from corpora labelled for toxicity. We also additionally filter them with a toxicity classifier (described in Section 3.3). The overall data collection pipeline (shown in Figure 4) is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_38",
            "content": "\u2022 Select toxic sentences for rewriting, \u2022 Pay for \"I can't rewrite\" answers in Task 1 if two or more workers agreed on them.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_39",
            "content": "The generation pipeline can be used for cases when no parallel data is available. However, we suggest that a sufficiently large parallel corpus of paraphrases can contain pairs of sentences belonging to different styles, and it is possible to distil such corpus into a style transfer dataset. We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018). We partially reuse the previously described setup. We do not need Task 1 since both toxic and neutral sentences are already available. However, we run Task 3 twice, because we need to check both parts of the pair for toxicity. Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral. The parallel data retrieval pipeline is shown in Figure 5. It is simpler because Tasks 2 and 3 do not serve for paying for the generated paraphrases and are only used for data filtering. The pipeline is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_40",
            "content": "\u2022 Select a pair of sentences (toxic and non-toxic) from the parallel data, \u2022 Feed the toxic sentence candidate to Task 3 to make sure it is toxic, \u2022 Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, \u2022 Feed both sentences to Task 2 to check if their content matches.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_41",
            "content": "Crowdsourcing Settings",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "31-ARR_v1_42",
            "content": "Preprocessing To pre-select toxic sentences, we need a toxicity classifier. We fine-tune a RoBERTa model (Liu et al., 2019) 3 on half of the three merged Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 (1 million sentences) and get a classifier which yields the F 1 -score of 0.76 on the Jigsaw test 3 https://huggingface.co/roberta-large set (Jigsaw, 2018). We consider a sentence toxic if the classifier confidence is above 0.8. To make the sentences easier for reading and rewriting, we choose the ones consisting of 5 to 20 tokens. For the retrieval pipeline, we also select parallel sentences with the cosine similarity of embeddings between 0.65 and 0.8. Sentences with lower similarity are often not exact paraphrases, and too similar sentences are either both toxic or both non-toxic. In Task 1 we perform different quality control. We ban users who submit paraphrases which are: (i) a copy of the input, (ii) too short (< 3 tokens) or too long (more than doubled original length), (iii) contain too many rare words or non-words. The latter condition is checked as follows. We compute the ratio of the number of whitespaceseparated tokens and the number of tokens identified by the BPE tokenizer (Sennrich et al., 2016). The rationale behind this check is that the BPE tokenizer tends to divide rare words into multiple subtokens. If the number of BPE tokens in a sentence is two times more than the number of regular tokens, it might indicate the presence of non-words. We filter out these answers and also ban users who produce them too often.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_43",
            "content": "In addition to that, we ban malicious workers using built-in Yandex.Toloka tools: (i) captcha, (ii) number of skipped questions -we ban users who skip 10 task pages in a row, and (iii) task completion time -we ban those who accomplish tasks too fast (this usually means that they choose a random answer without reading).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_44",
            "content": "Payment In Yandex.Toloka, a worker is paid for a page that can have multiple tasks (the number is set by customer). In Task 1, a page contains 5 tasks and costs $0.02. In Tasks 2 and 3, we pay $0.02 and $0.01, respectively for 12 tasks. In addition to that, in these tasks, we use skill-based payment. If a worker has the labelling skill of above 90%, the payment is increased to $0.03 (Task 2) and $0.02 (Task 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_45",
            "content": "Tasks 2 and 3 are paid instantly, whereas in Task 1 we check the paraphrases before paying. If a worker indicated that a sentence cannot be paraphrased, we pay for this answer only if at least one other worker agreed with that. If a worker typed in a paraphrase, we send it to Tasks 2 and 3 and pay only for the ones approved by both tasks. The payment procedure is shown in Figure 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_46",
            "content": "Postprocessing To ensure the correctness of labelling, we ask several workers to label each example. In Task 1, this gives us multiple paraphrases and also verifies the \"I can't rewrite\" answers. For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often. The number of people to label an example ranges from 3 to 5 depending on the workers' agreement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_47",
            "content": "Dawid-Skene aggregation returns the final label and its confidence. To improve the quality of data, we accept only labels with the confidence of over 90% and do not include the rest in the final data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_48",
            "content": "Data Analysis",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "31-ARR_v1_49",
            "content": "We collected ParaDetox -a parallel detoxification dataset with 1-3 paraphrases for almost 12,000 toxic sentences. We also manually filtered ParaNMT dataset and get 1,400 toxic-neutral pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_50",
            "content": "ParaDetox: Generated Paraphrases",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "31-ARR_v1_51",
            "content": "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by (Nogueira dos Santos et al., 2018). We selected 6,500 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers. We get paraphrases for 11,939 toxic sentences (on average 1.66 paraphrases per sentence), 19,766 paraphrases total. Running 1,000 input sentences through the pipeline costs $41.2, and the cost of one output sample is $0.07. The overall cost of the dataset is $811.55. We give the examples of sentences in Appendix A. The statistics of the paraphrases written by crowd workers are presented in Table 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_52",
            "content": "The distribution of sentences from different datasets in the final data is not equal. Jigsaw turned out to be the most difficult to paraphrase. Fewer sentences from it are successfully paraphrased, making it the most expensive part of the collected corpus ($0.08 per sample). Figure 7 shows that the number of untransferable sentences in the Jigsaw dataset is larger than that of other corpora. Out of all crowdsourced paraphrases, only a small part was of high quality. We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8. It also corroborates the difficulty of the Jigsaw dataset. While the overall number of generated paraphrases was slightly higher for it, much more of them were discarded.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_53",
            "content": "Analysis of Edits",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "31-ARR_v1_54",
            "content": "Although we did not give any special instructions to workers about editing, they often followed the minimal editing principle, making 1.36 changes per sentence on average. A change is deletion, insertion, or rewriting of a word or multiple adjacent words. Many of the changes are supposedly deletions because the average sentence length drops from 12.1 to 10.4 words after editing. The nature of editing differs for the three datasets. We compute the percentage of edits which consisted in removing the most common swear words (f*ck, sh*t, a*s and their variants) or replacing them with more neutral words. Table 3 shows that the deletion or replacements of the most common swearing constituted a large part of all edits for Reddit and Twitter datasets (22% and 30%), while for Jigsaw it was only 3%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_55",
            "content": "Another surprisingly common type of editing is the normalisation of sentences. The users often fixed casing, punctuation, typos (e.g. dont \u2192 don't, there's \u2192 there is). They also tended to replace colloquial phrases with more formal and standard language. Finally, some users overcorrected the sentences. For example, they replaced neutral words such as dead, murder, penis with euphemisms. This tendency indicates that workers consider any sensitive topic to be inappropriate content and try to avoid it as much as possible.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_56",
            "content": "ParaNMT: Existing Paraphrases",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "31-ARR_v1_57",
            "content": "Our automatic filtering of ParaNMT for content and yields 500,000 potentially detoxifying sentence pairs, which is 1% of the corpus. We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation. This leaves with 1,393 sentences, meaning that around 23% of the pre-selected sentence pairs were approved (for ParaDetox we get paraphrases for 61% input sentences). Thus, although the cost per 1,000 inputs is much lower than that of generating the paraphrases, the cost per output sample is the same as that of generated paraphrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_58",
            "content": "ParaNMT dataset is different from ParaDetox. First, each sentence has only one paraphrase. These paraphrases were not gained via manual editing but via a chain of translation models. Thus, neutral sentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_59",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "31-ARR_v1_60",
            "content": "To evaluate the collected corpora, we use them to train several supervised detoxification models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_61",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "31-ARR_v1_62",
            "content": "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020) 4 on our data. We test BART trained on the following datasets:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_63",
            "content": "\u2022 ParaDetox -our full crowdsourced dataset. \u2022 ParaDetox-unique -a subset of ParaDetox where each toxic sentence has only one paraphrase (selected randomly). \u2022 ParaNMT -filtered ParaNMT corpus, auto stands for automatically filtered 500,000 samples, manual are 1,393 manually selected sentence pairs.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_64",
            "content": "We also compare our models to other style transfer approaches:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_65",
            "content": "Metrics",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "31-ARR_v1_66",
            "content": "We evaluate the paraphrasers on 671 parallel sentences generated by crowd workers and additionally validated by experts. We compute the BLEU score on this test set. In addition to that, we perform automatic reference-free evaluation which is used in many style transfer works. Namely, we evaluate:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_67",
            "content": "\u2022 Style accuracy (STA) -percentage of nontoxic outputs identified by a style classifier. We use a classifier from Section 3.3 trained on a different half of Jigsaw data. \u2022 Content preservation (SIM) -cosine similarity between the embeddings of the original text and the output computed with the model of Wieting et al. (2019). \u2022 Fluency (FL) -percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2018).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_68",
            "content": "We compute the final joint metric (J) as the multiplication of the three parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_69",
            "content": "Since the automatic evaluation can be unreliable, we evaluate some models manually. We randomly select 200 sentences from the test set and ask assessors to evaluate them along the same three parameters: style accuracy (STA m ), content preservation (SIM m ), and fluency (FL m ). All parameters can take values of 1 (good) and 0 (bad). We also report the joint metric J m which is the percentage of sentences whose STA m , SIM m , and FL m are 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_70",
            "content": "The evaluation was conducted by 6 NLP researchers with a good command of English. Each sample was evaluated by 3 assessors. The interannotator agreement (Krippendorff's \u03b1) reaches 0.64 (STA m ), 0.67 (SIM m ), and 0.68 (FL m ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_71",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "31-ARR_v1_72",
            "content": "Table 5 shows the automatic scores of all tested models. Our BART models trained on ParaDetox outperform other systems in terms of BLEU and J. The much lower scores of BART-zero-shot confirm that this success is due to fine-tuning and not the innate ability of BART. The majority of unsupervised SOTA approaches are not only worse than BART but also perform below the \"change nothing\" baseline. The closest competitor of our models is the Delete model. This can be explained by the fact that crowd workers often only removed or replaced swear words which what the Delete model does.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_73",
            "content": "When comparing models trained on supervised data, we can see that BART does not benefit from multiple detoxifications per sentence, its performance is the same when trained on ParaDetox and ParaDetox-unique. On the other hand, manual filtering of ParaNMT is beneficial, it increases the quality of BART trained on it, although the number of training sentences drops from 500,000 to 1,400.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_74",
            "content": "Table 4 shows examples of different models output. Delete performs deterministic operations which can return disfluent text. CondBERT has to insert something instead of a toxic word, which is not always a good strategy. ParaGeDi generates sentences from scratch, which sometimes results in a distorted sense. BART trained on parallel data is usually free of these drawbacks. More examples of outputs are available in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_75",
            "content": "Manual evaluation (Table 6) confirms the usefulness of parallel data. BARTs trained on parallel data outperform other competitors, even if the size of this data is small. However, manual and automatic evaluations do not always match. Here, wellperforming Delete model gets the lowest score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_76",
            "content": "Overall, assessors agree with automatic metrics only in terms of fluency, their Spearman correlation r is 0.89. The manual style accuracy and content preservation are only moderately correlated with their automatic counterparts, and J and J m do not correlate. Besides that, BLEU correlates only with content preservation score and is moderately inversely correlated with the style accuracy. Thus, BLEU measures only the degree of content preservation and cannot replace other metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_77",
            "content": "Conclusions and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "31-ARR_v1_78",
            "content": "We presented ParaDetox -an English parallel corpus for the detoxification task. It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers. To the best of our knowledge, this is the first parallel detoxification dataset. We present a novel data collection pipeline and show that parallel data can be generated using only crowdsourcing. We also adapt this pipeline to the style-based distillation of paraphrase corpus. We confirm the usefulness of our datasets by training sequence-to-sequence models on them. The experiments show that the use of parallel data yields models which significantly outperform style transfer models trained on non-parallel data. Besides that, we confirm that filtering the noisy parallel data can lead to considerable improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_79",
            "content": "Finally, we investigate the relationship between metrics and find that automatic evaluation does not always match the manual judgements and reference-based BLEU cannot replace human evaluation, because it measures content preservation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_80",
            "content": "Luke Breitfeller, Emily Ahn, David Jurgens, Yulia Tsvetkov, Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Luke Breitfeller",
                    "Emily Ahn",
                    "David Jurgens",
                    "Yulia Tsvetkov"
                ],
                "title": "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_81",
            "content": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault, Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Eleftheria Briakou",
                    "Di Lu",
                    "Ke Zhang",
                    "Joel Tetreault"
                ],
                "title": "Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_82",
            "content": "Keith Carlson, Allen Riddell, Daniel Rockmore, Evaluating prose style transfer with the bible, 2018, Royal Society Open Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Keith Carlson",
                    "Allen Riddell",
                    "Daniel Rockmore"
                ],
                "title": "Evaluating prose style transfer with the bible",
                "pub_date": "2018",
                "pub_title": "Royal Society Open Science",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_83",
            "content": "David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko, Text detoxification using large pre-trained neural models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "David Dale",
                    "Anton Voronov",
                    "Daryna Dementieva",
                    "Varvara Logacheva",
                    "Olga Kozlova",
                    "Nikita Semenov",
                    "Alexander Panchenko"
                ],
                "title": "Text detoxification using large pre-trained neural models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_84",
            "content": "A Dawid, A Skene, Maximum likelihood estimation of observer error-rates using the em algorithm, 1979, Journal of The Royal Statistical Society Series C-applied Statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "A Dawid",
                    "A Skene"
                ],
                "title": "Maximum likelihood estimation of observer error-rates using the em algorithm",
                "pub_date": "1979",
                "pub_title": "Journal of The Royal Statistical Society Series C-applied Statistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_85",
            "content": "Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Style transfer in text: Exploration and evaluation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Zhenxin Fu",
                    "Xiaoye Tan",
                    "Nanyun Peng",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Style transfer in text: Exploration and evaluation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_86",
            "content": "Junxian He, Xinyi Wang, Graham Neubig, Taylor Berg-Kirkpatrick, A probabilistic formulation of unsupervised text style transfer, 2020, Proceedings of ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Junxian He",
                    "Xinyi Wang",
                    "Graham Neubig",
                    "Taylor Berg-Kirkpatrick"
                ],
                "title": "A probabilistic formulation of unsupervised text style transfer",
                "pub_date": "2020",
                "pub_title": "Proceedings of ICLR",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_87",
            "content": "UNKNOWN, None, 2018, Toxic comment classification challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Toxic comment classification challenge",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_88",
            "content": "UNKNOWN, None, 2019, Jigsaw unintended bias in toxicity classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Jigsaw unintended bias in toxicity classification",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_89",
            "content": "UNKNOWN, None, 2020, Jigsaw multilingual toxic comment classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Jigsaw multilingual toxic comment classification",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_90",
            "content": "UNKNOWN, None, 2021, Civil rephrases of toxic texts with self-supervised transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Civil rephrases of toxic texts with self-supervised transformers",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_91",
            "content": "UNKNOWN, None, 2020, Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_92",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_93",
            "content": "Juncen Li, Robin Jia, He He, Percy Liang, Delete, retrieve, generate: a simple approach to sentiment and style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Juncen Li",
                    "Robin Jia",
                    "He He",
                    "Percy Liang"
                ],
                "title": "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v1_94",
            "content": "UNKNOWN, None, 2018, Delete, retrieve, generate: A simple approach to sentiment and style transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_95",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_96",
            "content": "Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui, A dual reinforcement learning framework for unsupervised text style transfer, 2019-08-10, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Fuli Luo",
                    "Peng Li",
                    "Jie Zhou",
                    "Pengcheng Yang",
                    "Baobao Chang",
                    "Xu Sun",
                    "Zhifang Sui"
                ],
                "title": "A dual reinforcement learning framework for unsupervised text style transfer",
                "pub_date": "2019-08-10",
                "pub_title": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_97",
            "content": "Eric Malmi, Aliaksei Severyn, Sascha Rothe, Unsupervised text style transfer with padded masked language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Eric Malmi",
                    "Aliaksei Severyn",
                    "Sascha Rothe"
                ],
                "title": "Unsupervised text style transfer with padded masked language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_98",
            "content": "Cicero Nogueira Dos Santos, Igor Melnyk, Inkit Padhi, Fighting offensive language on social media with unsupervised text style transfer, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Cicero Nogueira Dos Santos",
                    "Igor Melnyk",
                    "Inkit Padhi"
                ],
                "title": "Fighting offensive language on social media with unsupervised text style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "31-ARR_v1_99",
            "content": "Carla Perez Almendros, Luis Espinosa Anke, Steven Schockaert, Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Carla Perez Almendros",
                    "Luis Espinosa Anke",
                    "Steven Schockaert"
                ],
                "title": "Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_100",
            "content": "Reid Pryzant, Richard Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, Diyi Yang, Automatically neutralizing subjective bias in text, 2020, Proceedings of the aaai conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Reid Pryzant",
                    "Richard Martinez",
                    "Nathan Dass",
                    "Sadao Kurohashi",
                    "Dan Jurafsky",
                    "Diyi Yang"
                ],
                "title": "Automatically neutralizing subjective bias in text",
                "pub_date": "2020",
                "pub_title": "Proceedings of the aaai conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_101",
            "content": "Sudha Rao, Joel Tetreault, Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Sudha Rao",
                    "Joel Tetreault"
                ],
                "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v1_102",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v1_103",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Tianxiao Shen",
                    "Tao Lei",
                    "Regina Barzilay",
                    "Tommi Jaakkola"
                ],
                "title": "Style transfer from non-parallel text by cross-alignment",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "31-ARR_v1_104",
            "content": "Minh Tran, Yipeng Zhang, Mohammad Soleymani, Towards a friendly online community: An unsupervised style transfer framework for profanity redaction, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Minh Tran",
                    "Yipeng Zhang",
                    "Mohammad Soleymani"
                ],
                "title": "Towards a friendly online community: An unsupervised style transfer framework for profanity redaction",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_105",
            "content": "UNKNOWN, None, 2018, Neural network acceptability judgments, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Neural network acceptability judgments",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_106",
            "content": "UNKNOWN, None, 2019, Beyond bleu: Training neural machine translation with semantic similarity, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Beyond bleu: Training neural machine translation with semantic similarity",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_107",
            "content": "John Wieting, Kevin Gimpel, ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "John Wieting",
                    "Kevin Gimpel"
                ],
                "title": "ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v1_108",
            "content": "UNKNOWN, None, 2019, Applying masked language model to sentiment transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Applying masked language model to sentiment transfer",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_109",
            "content": "Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval), 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Marcos Zampieri",
                    "Shervin Malmasi",
                    "Preslav Nakov",
                    "Sara Rosenthal",
                    "Noura Farra",
                    "Ritesh Kumar"
                ],
                "title": "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 13th International Workshop on Semantic Evaluation",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v1_110",
            "content": "Yi Zhang, Tao Ge, Xu Sun, Parallel data augmentation for formality style transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Yi Zhang",
                    "Tao Ge",
                    "Xu Sun"
                ],
                "title": "Parallel data augmentation for formality style transfer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "31-ARR_v1_0@0",
            "content": "ParaDetox: Detoxification with Parallel Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_0",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@0",
            "content": "We present a novel pipeline for the collection of parallel data for the detoxification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@1",
            "content": "We collect non-toxic paraphrases for over 10,000 English toxic sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 93,
            "end": 165,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@2",
            "content": "We also show that this pipeline can be used to distil a large existing corpus of paraphrases to get toxic-neutral sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 167,
            "end": 295,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@3",
            "content": "We release two parallel corpora which can be used for the training of detoxification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 297,
            "end": 388,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@4",
            "content": "To the best of our knowledge, these are the first parallel datasets for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 390,
            "end": 471,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_2@5",
            "content": "We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_2",
            "start": 473,
            "end": 640,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_3@0",
            "content": "We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_3",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_3@1",
            "content": "We conduct both automatic and manual evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_3",
            "start": 147,
            "end": 194,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_3@2",
            "content": "All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_3",
            "start": 196,
            "end": 301,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_3@3",
            "content": "This suggests that our novel datasets can boost the performance of detoxification systems substantially.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_3",
            "start": 303,
            "end": 406,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_5@0",
            "content": "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.g. microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al., 2020), is a popular topic of research in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_5",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_5@1",
            "content": "However, detection of harmful messages does not offer any proactive ways of fighting them (besides deletion).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_5",
            "start": 219,
            "end": 327,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_5@2",
            "content": "We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_5",
            "start": 329,
            "end": 448,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@0",
            "content": "The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018;Tran et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@1",
            "content": "It is considered a variant of style transfer task, the task of rewriting a text saving the content and changing the style (style is defined as a characteristic of text such as sentiment, level of formality or politeness, author profile (gender, political preferences), etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 152,
            "end": 426,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@2",
            "content": "As a sequenceto-sequence task, style transfer can be performed with an encoder-decoder model trained on parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 428,
            "end": 545,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@3",
            "content": "However, there exist only a few parallel style transfer corpora (Carlson et al., 2018;Pryzant et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 547,
            "end": 654,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@4",
            "content": "Since they usually do not exist \"naturally\", they need to be written from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 656,
            "end": 737,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@5",
            "content": "This is an expensive and laborious process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 739,
            "end": 781,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_6@6",
            "content": "Thus, such parallel datasets are extremely rare.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_6",
            "start": 783,
            "end": 830,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_7@0",
            "content": "Jigsaw so why would anyone believe this moron?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_7",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_8@0",
            "content": "Paraphrase so why would anyone believe this person? so why would anyone believe somebody like him?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_8",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_9@0",
            "content": "Reddit dude ham sandwich is the good sh*t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_9",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_10@0",
            "content": "dude ham sandwich is the good thing The ham sandwich, buddy, is the bomb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_10",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_10@1",
            "content": "Dude ham sandwich is good.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_10",
            "start": 74,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_11@0",
            "content": "Twitter now i feel like an a*s Paraphrase now i feel like worthless now i feel very bad now i feel bad",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_11",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_12@0",
            "content": "Table 1: Examples of detoxified sentences from the collected parallel corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_12",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@0",
            "content": "We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@1",
            "content": "We suggest a new crowdsourcing pipeline for collecting parallel style transfer data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 143,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@2",
            "content": "It does not employ experts, which makes the data collection faster and cheaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 228,
            "end": 306,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@3",
            "content": "In addition to generating the detoxified versions of texts, we consider a way to distil existing datasets of paraphrases for style-specific data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 308,
            "end": 452,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@4",
            "content": "In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 454,
            "end": 638,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@5",
            "content": "The pipelines are described in detail to make them easy to replicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 640,
            "end": 708,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_13@6",
            "content": "Thus, we suggest that by reusing these pipelines the new parallel style transfer datasets can be collected in a fast and cheap way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_13",
            "start": 710,
            "end": 840,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_14@0",
            "content": "Finally, we validate the usefulness of our datasets by training detoxification models on them and comparing their performance with state-of-theart methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_14",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_14@1",
            "content": "Models trained on parallel data significantly outperform other models in terms of automatic metrics and human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_14",
            "start": 156,
            "end": 276,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_15@0",
            "content": "The contributions of our work are as three-fold:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_15",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_16@0",
            "content": "\u2022 We suggest a novel pipeline for collection of parallel data for the detoxification task, \u2022 We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, \u2022 Using collected data we train supervised detoxification models that yield SOTA results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_16",
            "start": 0,
            "end": 347,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_17@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_17",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_18@0",
            "content": "Style Transfer Datasets When collecting nonparallel style transfer corpora, style labels often already exist in the data (e.g. positive and negative reviews (Li et al., 2018a) 1 ) or its source serves as a label (e.g. Twitter, academic texts, legal documents, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_18",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_18@1",
            "content": "Thus, data collection is reduced to fetching the texts from their sources, and the corpus size depends only on the available amount of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_18",
            "start": 267,
            "end": 406,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_19@0",
            "content": "Conversely, parallel corpora are usually more difficult to get.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_19",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_19@1",
            "content": "There exist parallel style transfer datasets fetched from \"naturally\" available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_19",
            "start": 64,
            "end": 365,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_20@0",
            "content": "Besides these special cases, there exists a large style transfer dataset that was created from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_20",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_20@1",
            "content": "This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_20",
            "start": 104,
            "end": 252,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_20@2",
            "content": "Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference -we replace expert validation of crowdsourced sentences with crowd validation and additionally optimise the cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_20",
            "start": 254,
            "end": 486,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_21@0",
            "content": "The vast majority of style transfer models (including detoxification models) are trained on non-parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_21",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_21@1",
            "content": "They can perform pointwise corrections of stylemarked words (Li et al., 2018b;Wu et al., 2019;Malmi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_21",
            "start": 111,
            "end": 224,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_21@2",
            "content": "Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017;Fu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_21",
            "start": 226,
            "end": 409,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_21@3",
            "content": "As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_21",
            "start": 411,
            "end": 710,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_22@0",
            "content": "Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labelled for toxicity and considers toxic and neutral sentences as two subcorpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_22",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_22@1",
            "content": "Laugier et al. (2021) use the Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 for training, Nogueira dos Santos et al. ( 2018) create their own toxicity-labelled datasets of sentences from Reddit and Twitter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_22",
            "start": 212,
            "end": 431,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_22@2",
            "content": "Following them, we also fetch sentences for rewriting from these datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_22",
            "start": 433,
            "end": 506,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@0",
            "content": "Works on detoxification often rely on style transfer models tested on other domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@1",
            "content": "Nogueira dos Santos et al. (2018) follow Shen et al. (2017) and Fu et al. (2018) and train an autoencoder with additional style classification and cycle-consistency losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 85,
            "end": 256,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@2",
            "content": "Laugier et al. (2021) perform a similar finetuning of T5 as a denoising autoencoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 258,
            "end": 341,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@3",
            "content": "Tran et al. (2020) apply pointwise corrections approach similar to that of Wu et al. (2019) and then improve the fluency of a text with a seq2seq model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 343,
            "end": 494,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@4",
            "content": "Likewise, Dale et al. (2021) use a masked language model to perform pointwise edits of toxic sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 496,
            "end": 598,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_23@5",
            "content": "They also suggest an alternative model which enhances a style-agnostic seq2seq model with style-informed language models which reweigh the seq2seq hypotheses with respect to the desired style.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_23",
            "start": 600,
            "end": 791,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_24@0",
            "content": "When the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models to perform style transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_24",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_24@1",
            "content": "We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_24",
            "start": 177,
            "end": 259,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_25@0",
            "content": "Data Collection Pipeline",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_25",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_26@0",
            "content": "Our goal is to yield pairs of sentences that have the same meanings and are contrasted in terms of Rewrite this text so that it does not sound offensive and its meaning stays the same You realize that's stupid, don't you? offensiveness -one of the sentences is toxic and the other is neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_26",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_26@1",
            "content": "We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_26",
            "start": 293,
            "end": 444,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_26@2",
            "content": "Unlike a similar work of Rao and Tetreault (2018), we hire crowd workers not only for the generation of paraphrases but also for their validation, which reduces both time and cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_26",
            "start": 446,
            "end": 625,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_27@0",
            "content": "Crowdsourcing Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_27",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_28@0",
            "content": "We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_28",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_28@1",
            "content": "Each task is implemented as a separate crowdsourcing project.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_28",
            "start": 107,
            "end": 167,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_28@2",
            "content": "We use the crowdsourcing platform Yandex.Toloka.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_28",
            "start": 169,
            "end": 216,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_28@3",
            "content": "2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_28",
            "start": 218,
            "end": 218,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_29@0",
            "content": "Task 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_29",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_29@1",
            "content": "However, it is not always possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_29",
            "start": 180,
            "end": 214,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_29@2",
            "content": "Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_29",
            "start": 216,
            "end": 321,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_29@3",
            "content": "On the other hand, in some cases toxicity cannot be removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_29",
            "start": 323,
            "end": 382,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_29@4",
            "content": "Consider the examples:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_29",
            "start": 384,
            "end": 405,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_30@0",
            "content": "\u2022 Are you that dumb you can't figure it out? \u2022 I've finally understood that wiki is nothing but a bunch of American racists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_30",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_31@0",
            "content": "Not only the form but also the content of the messages are offensive, so trying to detoxify them would inevitably lead to a substantial change of sense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_31",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_31@1",
            "content": "We prefer not to include such cases in the parallel dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_31",
            "start": 153,
            "end": 212,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_32@0",
            "content": "If workers have to detoxify all inputs without a possibility to skip them, a large proportion of the generated paraphrases will be of low quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_32",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_32@1",
            "content": "Thus, we add the control \"I can't rewrite the text\" and optional controls to indicate the reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_32",
            "start": 147,
            "end": 244,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_33@0",
            "content": "Task 2: Content Preservation Check We show users the generated paraphrases along with their original variants and ask to indicate if they have close meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_33",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_33@1",
            "content": "Besides ensuring content preservation, this task implicitly filters out senseless outputs, because they obviously do not keep the original content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_33",
            "start": 159,
            "end": 305,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_33@2",
            "content": "The task interface is shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_33",
            "start": 307,
            "end": 346,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_34@0",
            "content": "Task 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_34",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_34@1",
            "content": "We ask users to indicate if the paraphrases contain any offence or swear words (see Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_34",
            "start": 88,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_35@0",
            "content": "In addition to filtering out unsuitable paraphrases, we use Tasks 2 and 3 for paying for Task 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_35",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_35@1",
            "content": "We accept or reject the generated paraphrases based on the labels they get in Tasks 2 and 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_35",
            "start": 97,
            "end": 188,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_36@0",
            "content": "Pipelines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_36",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_37@0",
            "content": "Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_37",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_37@1",
            "content": "We fetch them from corpora labelled for toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_37",
            "start": 101,
            "end": 149,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_37@2",
            "content": "We also additionally filter them with a toxicity classifier (described in Section 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_37",
            "start": 151,
            "end": 237,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_37@3",
            "content": "The overall data collection pipeline (shown in Figure 4) is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_37",
            "start": 239,
            "end": 309,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_38@0",
            "content": "\u2022 Select toxic sentences for rewriting, \u2022 Pay for \"I can't rewrite\" answers in Task 1 if two or more workers agreed on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_38",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@0",
            "content": "The generation pipeline can be used for cases when no parallel data is available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@1",
            "content": "However, we suggest that a sufficiently large parallel corpus of paraphrases can contain pairs of sentences belonging to different styles, and it is possible to distil such corpus into a style transfer dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 82,
            "end": 291,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@2",
            "content": "We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 293,
            "end": 400,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@3",
            "content": "We partially reuse the previously described setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 402,
            "end": 451,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@4",
            "content": "We do not need Task 1 since both toxic and neutral sentences are already available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 453,
            "end": 535,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@5",
            "content": "However, we run Task 3 twice, because we need to check both parts of the pair for toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 537,
            "end": 627,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@6",
            "content": "Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 629,
            "end": 787,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@7",
            "content": "The parallel data retrieval pipeline is shown in Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 789,
            "end": 846,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@8",
            "content": "It is simpler because Tasks 2 and 3 do not serve for paying for the generated paraphrases and are only used for data filtering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 848,
            "end": 974,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_39@9",
            "content": "The pipeline is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_39",
            "start": 976,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_40@0",
            "content": "\u2022 Select a pair of sentences (toxic and non-toxic) from the parallel data, \u2022 Feed the toxic sentence candidate to Task 3 to make sure it is toxic, \u2022 Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, \u2022 Feed both sentences to Task 2 to check if their content matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_40",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_41@0",
            "content": "Crowdsourcing Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_41",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@0",
            "content": "Preprocessing To pre-select toxic sentences, we need a toxicity classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@1",
            "content": "We fine-tune a RoBERTa model (Liu et al., 2019) 3 on half of the three merged Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 (1 million sentences) and get a classifier which yields the F 1 -score of 0.76 on the Jigsaw test 3 https://huggingface.co/roberta-large set (Jigsaw, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 76,
            "end": 369,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@2",
            "content": "We consider a sentence toxic if the classifier confidence is above 0.8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 371,
            "end": 441,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@3",
            "content": "To make the sentences easier for reading and rewriting, we choose the ones consisting of 5 to 20 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 443,
            "end": 546,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@4",
            "content": "For the retrieval pipeline, we also select parallel sentences with the cosine similarity of embeddings between 0.65 and 0.8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 548,
            "end": 671,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@5",
            "content": "Sentences with lower similarity are often not exact paraphrases, and too similar sentences are either both toxic or both non-toxic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 673,
            "end": 803,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@6",
            "content": "In Task 1 we perform different quality control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 805,
            "end": 851,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@7",
            "content": "We ban users who submit paraphrases which are: (i) a copy of the input, (ii) too short (< 3 tokens) or too long (more than doubled original length), (iii) contain too many rare words or non-words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 853,
            "end": 1048,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@8",
            "content": "The latter condition is checked as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 1050,
            "end": 1092,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@9",
            "content": "We compute the ratio of the number of whitespaceseparated tokens and the number of tokens identified by the BPE tokenizer (Sennrich et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 1094,
            "end": 1239,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@10",
            "content": "The rationale behind this check is that the BPE tokenizer tends to divide rare words into multiple subtokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 1241,
            "end": 1349,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@11",
            "content": "If the number of BPE tokens in a sentence is two times more than the number of regular tokens, it might indicate the presence of non-words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 1351,
            "end": 1489,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_42@12",
            "content": "We filter out these answers and also ban users who produce them too often.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_42",
            "start": 1491,
            "end": 1564,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_43@0",
            "content": "In addition to that, we ban malicious workers using built-in Yandex.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_43",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_43@1",
            "content": "Toloka tools: (i) captcha, (ii) number of skipped questions -we ban users who skip 10 task pages in a row, and (iii) task completion time -we ban those who accomplish tasks too fast (this usually means that they choose a random answer without reading).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_43",
            "start": 68,
            "end": 319,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_44@0",
            "content": "Payment In Yandex.Toloka, a worker is paid for a page that can have multiple tasks (the number is set by customer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_44",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_44@1",
            "content": "In Task 1, a page contains 5 tasks and costs $0.02.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_44",
            "start": 116,
            "end": 166,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_44@2",
            "content": "In Tasks 2 and 3, we pay $0.02 and $0.01, respectively for 12 tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_44",
            "start": 168,
            "end": 235,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_44@3",
            "content": "In addition to that, in these tasks, we use skill-based payment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_44",
            "start": 237,
            "end": 300,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_44@4",
            "content": "If a worker has the labelling skill of above 90%, the payment is increased to $0.03 (Task 2) and $0.02 (Task 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_44",
            "start": 302,
            "end": 413,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_45@0",
            "content": "Tasks 2 and 3 are paid instantly, whereas in Task 1 we check the paraphrases before paying.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_45",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_45@1",
            "content": "If a worker indicated that a sentence cannot be paraphrased, we pay for this answer only if at least one other worker agreed with that.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_45",
            "start": 92,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_45@2",
            "content": "If a worker typed in a paraphrase, we send it to Tasks 2 and 3 and pay only for the ones approved by both tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_45",
            "start": 228,
            "end": 339,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_45@3",
            "content": "The payment procedure is shown in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_45",
            "start": 341,
            "end": 383,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_46@0",
            "content": "Postprocessing To ensure the correctness of labelling, we ask several workers to label each example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_46",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_46@1",
            "content": "In Task 1, this gives us multiple paraphrases and also verifies the \"I can't rewrite\" answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_46",
            "start": 101,
            "end": 194,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_46@2",
            "content": "For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_46",
            "start": 196,
            "end": 432,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_46@3",
            "content": "The number of people to label an example ranges from 3 to 5 depending on the workers' agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_46",
            "start": 434,
            "end": 529,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_47@0",
            "content": "Dawid-Skene aggregation returns the final label and its confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_47",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_47@1",
            "content": "To improve the quality of data, we accept only labels with the confidence of over 90% and do not include the rest in the final data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_47",
            "start": 68,
            "end": 199,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_48@0",
            "content": "Data Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_48",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_49@0",
            "content": "We collected ParaDetox -a parallel detoxification dataset with 1-3 paraphrases for almost 12,000 toxic sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_49",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_49@1",
            "content": "We also manually filtered ParaNMT dataset and get 1,400 toxic-neutral pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_49",
            "start": 114,
            "end": 189,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_50@0",
            "content": "ParaDetox: Generated Paraphrases",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_50",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@0",
            "content": "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by (Nogueira dos Santos et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@1",
            "content": "We selected 6,500 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 169,
            "end": 280,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@2",
            "content": "We get paraphrases for 11,939 toxic sentences (on average 1.66 paraphrases per sentence), 19,766 paraphrases total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 282,
            "end": 396,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@3",
            "content": "Running 1,000 input sentences through the pipeline costs $41.2, and the cost of one output sample is $0.07.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 398,
            "end": 504,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@4",
            "content": "The overall cost of the dataset is $811.55.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 506,
            "end": 548,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_51@5",
            "content": "We give the examples of sentences in Appendix A. The statistics of the paraphrases written by crowd workers are presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_51",
            "start": 550,
            "end": 682,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@0",
            "content": "The distribution of sentences from different datasets in the final data is not equal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@1",
            "content": "Jigsaw turned out to be the most difficult to paraphrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 86,
            "end": 142,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@2",
            "content": "Fewer sentences from it are successfully paraphrased, making it the most expensive part of the collected corpus ($0.08 per sample).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 144,
            "end": 274,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@3",
            "content": "Figure 7 shows that the number of untransferable sentences in the Jigsaw dataset is larger than that of other corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 276,
            "end": 393,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@4",
            "content": "Out of all crowdsourced paraphrases, only a small part was of high quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 395,
            "end": 469,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@5",
            "content": "We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 471,
            "end": 575,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@6",
            "content": "It also corroborates the difficulty of the Jigsaw dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 577,
            "end": 634,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_52@7",
            "content": "While the overall number of generated paraphrases was slightly higher for it, much more of them were discarded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_52",
            "start": 636,
            "end": 746,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_53@0",
            "content": "Analysis of Edits",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_53",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@0",
            "content": "Although we did not give any special instructions to workers about editing, they often followed the minimal editing principle, making 1.36 changes per sentence on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@1",
            "content": "A change is deletion, insertion, or rewriting of a word or multiple adjacent words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 172,
            "end": 254,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@2",
            "content": "Many of the changes are supposedly deletions because the average sentence length drops from 12.1 to 10.4 words after editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 256,
            "end": 380,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@3",
            "content": "The nature of editing differs for the three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 382,
            "end": 434,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@4",
            "content": "We compute the percentage of edits which consisted in removing the most common swear words (f*ck, sh*t, a*s and their variants) or replacing them with more neutral words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 436,
            "end": 605,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_54@5",
            "content": "Table 3 shows that the deletion or replacements of the most common swearing constituted a large part of all edits for Reddit and Twitter datasets (22% and 30%), while for Jigsaw it was only 3%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_54",
            "start": 607,
            "end": 799,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@0",
            "content": "Another surprisingly common type of editing is the normalisation of sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@1",
            "content": "The users often fixed casing, punctuation, typos (e.g. dont \u2192 don't, there's \u2192 there is).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 79,
            "end": 167,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@2",
            "content": "They also tended to replace colloquial phrases with more formal and standard language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 169,
            "end": 254,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@3",
            "content": "Finally, some users overcorrected the sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 256,
            "end": 303,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@4",
            "content": "For example, they replaced neutral words such as dead, murder, penis with euphemisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 305,
            "end": 389,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_55@5",
            "content": "This tendency indicates that workers consider any sensitive topic to be inappropriate content and try to avoid it as much as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_55",
            "start": 391,
            "end": 524,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_56@0",
            "content": "ParaNMT: Existing Paraphrases",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_56",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_57@0",
            "content": "Our automatic filtering of ParaNMT for content and yields 500,000 potentially detoxifying sentence pairs, which is 1% of the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_57",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_57@1",
            "content": "We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_57",
            "start": 133,
            "end": 252,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_57@2",
            "content": "This leaves with 1,393 sentences, meaning that around 23% of the pre-selected sentence pairs were approved (for ParaDetox we get paraphrases for 61% input sentences).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_57",
            "start": 254,
            "end": 419,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_57@3",
            "content": "Thus, although the cost per 1,000 inputs is much lower than that of generating the paraphrases, the cost per output sample is the same as that of generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_57",
            "start": 421,
            "end": 588,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_58@0",
            "content": "ParaNMT dataset is different from ParaDetox.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_58",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_58@1",
            "content": "First, each sentence has only one paraphrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_58",
            "start": 45,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_58@2",
            "content": "These paraphrases were not gained via manual editing but via a chain of translation models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_58",
            "start": 91,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_58@3",
            "content": "Thus, neutral sentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_58",
            "start": 183,
            "end": 339,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_59@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_59",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_60@0",
            "content": "To evaluate the collected corpora, we use them to train several supervised detoxification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_60",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_61@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_61",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_62@0",
            "content": "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020) 4 on our data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_62",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_62@1",
            "content": "We test BART trained on the following datasets:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_62",
            "start": 91,
            "end": 137,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_63@0",
            "content": "\u2022 ParaDetox -our full crowdsourced dataset. \u2022 ParaDetox-unique -a subset of ParaDetox where each toxic sentence has only one paraphrase (selected randomly). \u2022 ParaNMT -filtered ParaNMT corpus, auto stands for automatically filtered 500,000 samples, manual are 1,393 manually selected sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_63",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_64@0",
            "content": "We also compare our models to other style transfer approaches:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_64",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_65@0",
            "content": "Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_65",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_66@0",
            "content": "We evaluate the paraphrasers on 671 parallel sentences generated by crowd workers and additionally validated by experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_66",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_66@1",
            "content": "We compute the BLEU score on this test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_66",
            "start": 121,
            "end": 163,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_66@2",
            "content": "In addition to that, we perform automatic reference-free evaluation which is used in many style transfer works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_66",
            "start": 165,
            "end": 275,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_66@3",
            "content": "Namely, we evaluate:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_66",
            "start": 277,
            "end": 296,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_67@0",
            "content": "\u2022 Style accuracy (STA) -percentage of nontoxic outputs identified by a style classifier. We use a classifier from Section 3.3 trained on a different half of Jigsaw data. \u2022 Content preservation (SIM) -cosine similarity between the embeddings of the original text and the output computed with the model of Wieting et al. (2019). \u2022 Fluency (FL) -percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_67",
            "start": 0,
            "end": 494,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_68@0",
            "content": "We compute the final joint metric (J) as the multiplication of the three parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_68",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_69@0",
            "content": "Since the automatic evaluation can be unreliable, we evaluate some models manually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_69",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_69@1",
            "content": "We randomly select 200 sentences from the test set and ask assessors to evaluate them along the same three parameters: style accuracy (STA m ), content preservation (SIM m ), and fluency (FL m ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_69",
            "start": 84,
            "end": 278,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_69@2",
            "content": "All parameters can take values of 1 (good) and 0 (bad).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_69",
            "start": 280,
            "end": 334,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_69@3",
            "content": "We also report the joint metric J m which is the percentage of sentences whose STA m , SIM m , and FL m are 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_69",
            "start": 336,
            "end": 445,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_70@0",
            "content": "The evaluation was conducted by 6 NLP researchers with a good command of English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_70",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_70@1",
            "content": "Each sample was evaluated by 3 assessors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_70",
            "start": 82,
            "end": 122,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_70@2",
            "content": "The interannotator agreement (Krippendorff's \u03b1) reaches 0.64 (STA m ), 0.67 (SIM m ), and 0.68 (FL m ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_70",
            "start": 124,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_71@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_71",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_72@0",
            "content": "Table 5 shows the automatic scores of all tested models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_72",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_72@1",
            "content": "Our BART models trained on ParaDetox outperform other systems in terms of BLEU and J. The much lower scores of BART-zero-shot confirm that this success is due to fine-tuning and not the innate ability of BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_72",
            "start": 57,
            "end": 265,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_72@2",
            "content": "The majority of unsupervised SOTA approaches are not only worse than BART but also perform below the \"change nothing\" baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_72",
            "start": 267,
            "end": 393,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_72@3",
            "content": "The closest competitor of our models is the Delete model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_72",
            "start": 395,
            "end": 451,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_72@4",
            "content": "This can be explained by the fact that crowd workers often only removed or replaced swear words which what the Delete model does.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_72",
            "start": 453,
            "end": 581,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_73@0",
            "content": "When comparing models trained on supervised data, we can see that BART does not benefit from multiple detoxifications per sentence, its performance is the same when trained on ParaDetox and ParaDetox-unique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_73",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_73@1",
            "content": "On the other hand, manual filtering of ParaNMT is beneficial, it increases the quality of BART trained on it, although the number of training sentences drops from 500,000 to 1,400.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_73",
            "start": 208,
            "end": 387,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@0",
            "content": "Table 4 shows examples of different models output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@1",
            "content": "Delete performs deterministic operations which can return disfluent text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 51,
            "end": 123,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@2",
            "content": "CondBERT has to insert something instead of a toxic word, which is not always a good strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 125,
            "end": 218,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@3",
            "content": "ParaGeDi generates sentences from scratch, which sometimes results in a distorted sense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 220,
            "end": 307,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@4",
            "content": "BART trained on parallel data is usually free of these drawbacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 309,
            "end": 373,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_74@5",
            "content": "More examples of outputs are available in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_74",
            "start": 375,
            "end": 427,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_75@0",
            "content": "Manual evaluation (Table 6) confirms the usefulness of parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_75",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_75@1",
            "content": "BARTs trained on parallel data outperform other competitors, even if the size of this data is small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_75",
            "start": 70,
            "end": 169,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_75@2",
            "content": "However, manual and automatic evaluations do not always match.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_75",
            "start": 171,
            "end": 232,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_75@3",
            "content": "Here, wellperforming Delete model gets the lowest score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_75",
            "start": 234,
            "end": 289,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_76@0",
            "content": "Overall, assessors agree with automatic metrics only in terms of fluency, their Spearman correlation r is 0.89.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_76",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_76@1",
            "content": "The manual style accuracy and content preservation are only moderately correlated with their automatic counterparts, and J and J m do not correlate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_76",
            "start": 112,
            "end": 259,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_76@2",
            "content": "Besides that, BLEU correlates only with content preservation score and is moderately inversely correlated with the style accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_76",
            "start": 261,
            "end": 390,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_76@3",
            "content": "Thus, BLEU measures only the degree of content preservation and cannot replace other metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_76",
            "start": 392,
            "end": 484,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_77@0",
            "content": "Conclusions and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_77",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@0",
            "content": "We presented ParaDetox -an English parallel corpus for the detoxification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@1",
            "content": "It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 80,
            "end": 172,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@2",
            "content": "To the best of our knowledge, this is the first parallel detoxification dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 174,
            "end": 253,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@3",
            "content": "We present a novel data collection pipeline and show that parallel data can be generated using only crowdsourcing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 255,
            "end": 368,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@4",
            "content": "We also adapt this pipeline to the style-based distillation of paraphrase corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 370,
            "end": 450,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@5",
            "content": "We confirm the usefulness of our datasets by training sequence-to-sequence models on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 452,
            "end": 541,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@6",
            "content": "The experiments show that the use of parallel data yields models which significantly outperform style transfer models trained on non-parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 543,
            "end": 689,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_78@7",
            "content": "Besides that, we confirm that filtering the noisy parallel data can lead to considerable improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_78",
            "start": 691,
            "end": 791,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_79@0",
            "content": "Finally, we investigate the relationship between metrics and find that automatic evaluation does not always match the manual judgements and reference-based BLEU cannot replace human evaluation, because it measures content preservation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_79",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_80@0",
            "content": "Luke Breitfeller, Emily Ahn, David Jurgens, Yulia Tsvetkov, Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_80",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_81@0",
            "content": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault, Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_81",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_82@0",
            "content": "Keith Carlson, Allen Riddell, Daniel Rockmore, Evaluating prose style transfer with the bible, 2018, Royal Society Open Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_82",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_83@0",
            "content": "David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko, Text detoxification using large pre-trained neural models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_83",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_84@0",
            "content": "A Dawid, A Skene, Maximum likelihood estimation of observer error-rates using the em algorithm, 1979, Journal of The Royal Statistical Society Series C-applied Statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_84",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_85@0",
            "content": "Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Style transfer in text: Exploration and evaluation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_85",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_86@0",
            "content": "Junxian He, Xinyi Wang, Graham Neubig, Taylor Berg-Kirkpatrick, A probabilistic formulation of unsupervised text style transfer, 2020, Proceedings of ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_86",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2018, Toxic comment classification challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_87",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2019, Jigsaw unintended bias in toxicity classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_88",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2020, Jigsaw multilingual toxic comment classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_89",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2021, Civil rephrases of toxic texts with self-supervised transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_90",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2020, Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_91",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_92@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_92",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_93@0",
            "content": "Juncen Li, Robin Jia, He He, Percy Liang, Delete, retrieve, generate: a simple approach to sentiment and style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_93",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2018, Delete, retrieve, generate: A simple approach to sentiment and style transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_94",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_95@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_95",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_96@0",
            "content": "Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui, A dual reinforcement learning framework for unsupervised text style transfer, 2019-08-10, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_96",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_97@0",
            "content": "Eric Malmi, Aliaksei Severyn, Sascha Rothe, Unsupervised text style transfer with padded masked language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_97",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_98@0",
            "content": "Cicero Nogueira Dos Santos, Igor Melnyk, Inkit Padhi, Fighting offensive language on social media with unsupervised text style transfer, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_98",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_99@0",
            "content": "Carla Perez Almendros, Luis Espinosa Anke, Steven Schockaert, Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_99",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_100@0",
            "content": "Reid Pryzant, Richard Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, Diyi Yang, Automatically neutralizing subjective bias in text, 2020, Proceedings of the aaai conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_100",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_101@0",
            "content": "Sudha Rao, Joel Tetreault, Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_101",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_102@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_102",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_103@0",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_103",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_104@0",
            "content": "Minh Tran, Yipeng Zhang, Mohammad Soleymani, Towards a friendly online community: An unsupervised style transfer framework for profanity redaction, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_104",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2018, Neural network acceptability judgments, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_105",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2019, Beyond bleu: Training neural machine translation with semantic similarity, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_106",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_107@0",
            "content": "John Wieting, Kevin Gimpel, ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_107",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_108@0",
            "content": "UNKNOWN, None, 2019, Applying masked language model to sentiment transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_108",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_109@0",
            "content": "Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval), 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_109",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_110@0",
            "content": "Yi Zhang, Tao Ge, Xu Sun, Parallel data augmentation for formality style transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_110",
            "start": 0,
            "end": 178,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_1",
            "tgt_ix": "31-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_1",
            "tgt_ix": "31-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_1",
            "tgt_ix": "31-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_3",
            "tgt_ix": "31-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_5",
            "tgt_ix": "31-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_7",
            "tgt_ix": "31-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_8",
            "tgt_ix": "31-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_10",
            "tgt_ix": "31-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_11",
            "tgt_ix": "31-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_12",
            "tgt_ix": "31-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_14",
            "tgt_ix": "31-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_15",
            "tgt_ix": "31-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_9",
            "tgt_ix": "31-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_18",
            "tgt_ix": "31-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_19",
            "tgt_ix": "31-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_21",
            "tgt_ix": "31-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_22",
            "tgt_ix": "31-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_20",
            "tgt_ix": "31-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_24",
            "tgt_ix": "31-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_26",
            "tgt_ix": "31-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_28",
            "tgt_ix": "31-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_31",
            "tgt_ix": "31-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_32",
            "tgt_ix": "31-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_33",
            "tgt_ix": "31-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_34",
            "tgt_ix": "31-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_35",
            "tgt_ix": "31-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_37",
            "tgt_ix": "31-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_43",
            "tgt_ix": "31-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_45",
            "tgt_ix": "31-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_46",
            "tgt_ix": "31-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_47",
            "tgt_ix": "31-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_49",
            "tgt_ix": "31-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_50",
            "tgt_ix": "31-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_50",
            "tgt_ix": "31-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_50",
            "tgt_ix": "31-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_53",
            "tgt_ix": "31-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_53",
            "tgt_ix": "31-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_53",
            "tgt_ix": "31-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_57",
            "tgt_ix": "31-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_56",
            "tgt_ix": "31-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_56",
            "tgt_ix": "31-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_56",
            "tgt_ix": "31-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_58",
            "tgt_ix": "31-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_60",
            "tgt_ix": "31-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_62",
            "tgt_ix": "31-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_61",
            "tgt_ix": "31-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_61",
            "tgt_ix": "31-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_61",
            "tgt_ix": "31-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_61",
            "tgt_ix": "31-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_64",
            "tgt_ix": "31-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_66",
            "tgt_ix": "31-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_68",
            "tgt_ix": "31-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_69",
            "tgt_ix": "31-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_70",
            "tgt_ix": "31-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_73",
            "tgt_ix": "31-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_75",
            "tgt_ix": "31-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_76",
            "tgt_ix": "31-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_77",
            "tgt_ix": "31-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_77",
            "tgt_ix": "31-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_77",
            "tgt_ix": "31-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_0",
            "tgt_ix": "31-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_1",
            "tgt_ix": "31-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_2",
            "tgt_ix": "31-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_3",
            "tgt_ix": "31-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_3",
            "tgt_ix": "31-ARR_v1_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_3",
            "tgt_ix": "31-ARR_v1_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_3",
            "tgt_ix": "31-ARR_v1_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_4",
            "tgt_ix": "31-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_5",
            "tgt_ix": "31-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_5",
            "tgt_ix": "31-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_5",
            "tgt_ix": "31-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_6",
            "tgt_ix": "31-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_7",
            "tgt_ix": "31-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_8",
            "tgt_ix": "31-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_9",
            "tgt_ix": "31-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_10",
            "tgt_ix": "31-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_10",
            "tgt_ix": "31-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_11",
            "tgt_ix": "31-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_12",
            "tgt_ix": "31-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_13",
            "tgt_ix": "31-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_14",
            "tgt_ix": "31-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_14",
            "tgt_ix": "31-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_15",
            "tgt_ix": "31-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_16",
            "tgt_ix": "31-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_17",
            "tgt_ix": "31-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_18",
            "tgt_ix": "31-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_18",
            "tgt_ix": "31-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_19",
            "tgt_ix": "31-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_19",
            "tgt_ix": "31-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_20",
            "tgt_ix": "31-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_20",
            "tgt_ix": "31-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_20",
            "tgt_ix": "31-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_21",
            "tgt_ix": "31-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_21",
            "tgt_ix": "31-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_21",
            "tgt_ix": "31-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_21",
            "tgt_ix": "31-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_22",
            "tgt_ix": "31-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_22",
            "tgt_ix": "31-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_22",
            "tgt_ix": "31-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_23",
            "tgt_ix": "31-ARR_v1_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_24",
            "tgt_ix": "31-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_24",
            "tgt_ix": "31-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_25",
            "tgt_ix": "31-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_26",
            "tgt_ix": "31-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_26",
            "tgt_ix": "31-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_26",
            "tgt_ix": "31-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_27",
            "tgt_ix": "31-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_28",
            "tgt_ix": "31-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_28",
            "tgt_ix": "31-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_28",
            "tgt_ix": "31-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_28",
            "tgt_ix": "31-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_29",
            "tgt_ix": "31-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_30",
            "tgt_ix": "31-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_31",
            "tgt_ix": "31-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_31",
            "tgt_ix": "31-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_32",
            "tgt_ix": "31-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_32",
            "tgt_ix": "31-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_33",
            "tgt_ix": "31-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_33",
            "tgt_ix": "31-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_33",
            "tgt_ix": "31-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_34",
            "tgt_ix": "31-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_34",
            "tgt_ix": "31-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_35",
            "tgt_ix": "31-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_35",
            "tgt_ix": "31-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_36",
            "tgt_ix": "31-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_37",
            "tgt_ix": "31-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_37",
            "tgt_ix": "31-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_37",
            "tgt_ix": "31-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_37",
            "tgt_ix": "31-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_38",
            "tgt_ix": "31-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_39",
            "tgt_ix": "31-ARR_v1_39@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_40",
            "tgt_ix": "31-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_41",
            "tgt_ix": "31-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_42",
            "tgt_ix": "31-ARR_v1_42@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_43",
            "tgt_ix": "31-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_43",
            "tgt_ix": "31-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_44",
            "tgt_ix": "31-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_45",
            "tgt_ix": "31-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_45",
            "tgt_ix": "31-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_45",
            "tgt_ix": "31-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_45",
            "tgt_ix": "31-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_46",
            "tgt_ix": "31-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_46",
            "tgt_ix": "31-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_46",
            "tgt_ix": "31-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_46",
            "tgt_ix": "31-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_47",
            "tgt_ix": "31-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_47",
            "tgt_ix": "31-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_48",
            "tgt_ix": "31-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_49",
            "tgt_ix": "31-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_49",
            "tgt_ix": "31-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_50",
            "tgt_ix": "31-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_51",
            "tgt_ix": "31-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_52",
            "tgt_ix": "31-ARR_v1_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_53",
            "tgt_ix": "31-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_54",
            "tgt_ix": "31-ARR_v1_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_55",
            "tgt_ix": "31-ARR_v1_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_56",
            "tgt_ix": "31-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_57",
            "tgt_ix": "31-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_57",
            "tgt_ix": "31-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_57",
            "tgt_ix": "31-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_57",
            "tgt_ix": "31-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_58",
            "tgt_ix": "31-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_58",
            "tgt_ix": "31-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_58",
            "tgt_ix": "31-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_58",
            "tgt_ix": "31-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_59",
            "tgt_ix": "31-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_60",
            "tgt_ix": "31-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_61",
            "tgt_ix": "31-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_62",
            "tgt_ix": "31-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_62",
            "tgt_ix": "31-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_63",
            "tgt_ix": "31-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_64",
            "tgt_ix": "31-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_65",
            "tgt_ix": "31-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_66",
            "tgt_ix": "31-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_66",
            "tgt_ix": "31-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_66",
            "tgt_ix": "31-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_66",
            "tgt_ix": "31-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_67",
            "tgt_ix": "31-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_68",
            "tgt_ix": "31-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_69",
            "tgt_ix": "31-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_69",
            "tgt_ix": "31-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_69",
            "tgt_ix": "31-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_69",
            "tgt_ix": "31-ARR_v1_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_70",
            "tgt_ix": "31-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_70",
            "tgt_ix": "31-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_70",
            "tgt_ix": "31-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_71",
            "tgt_ix": "31-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_72",
            "tgt_ix": "31-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_73",
            "tgt_ix": "31-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_73",
            "tgt_ix": "31-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_74",
            "tgt_ix": "31-ARR_v1_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_75",
            "tgt_ix": "31-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_75",
            "tgt_ix": "31-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_75",
            "tgt_ix": "31-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_75",
            "tgt_ix": "31-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_76",
            "tgt_ix": "31-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_76",
            "tgt_ix": "31-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_76",
            "tgt_ix": "31-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_76",
            "tgt_ix": "31-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_77",
            "tgt_ix": "31-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_78",
            "tgt_ix": "31-ARR_v1_78@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_79",
            "tgt_ix": "31-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_80",
            "tgt_ix": "31-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_81",
            "tgt_ix": "31-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_82",
            "tgt_ix": "31-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_83",
            "tgt_ix": "31-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_84",
            "tgt_ix": "31-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_85",
            "tgt_ix": "31-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_86",
            "tgt_ix": "31-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_87",
            "tgt_ix": "31-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_88",
            "tgt_ix": "31-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_89",
            "tgt_ix": "31-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_90",
            "tgt_ix": "31-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_91",
            "tgt_ix": "31-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_92",
            "tgt_ix": "31-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_93",
            "tgt_ix": "31-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_94",
            "tgt_ix": "31-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_95",
            "tgt_ix": "31-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_96",
            "tgt_ix": "31-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_97",
            "tgt_ix": "31-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_98",
            "tgt_ix": "31-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_99",
            "tgt_ix": "31-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_100",
            "tgt_ix": "31-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_101",
            "tgt_ix": "31-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_102",
            "tgt_ix": "31-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_103",
            "tgt_ix": "31-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_104",
            "tgt_ix": "31-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_105",
            "tgt_ix": "31-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_106",
            "tgt_ix": "31-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_107",
            "tgt_ix": "31-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_108",
            "tgt_ix": "31-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_109",
            "tgt_ix": "31-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_110",
            "tgt_ix": "31-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1249,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "31-ARR",
        "version": 1
    }
}