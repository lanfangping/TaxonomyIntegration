{
    "nodes": [
        {
            "ix": "31-ARR_v1_review2_0",
            "content": "31-ARR_v1_review2",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_review2_1",
            "content": "paper_summary. The authors present a new crowdsourcing pipeline to collect parallel data for the task of detoxification (i.e., a style-transfer task to convert toxic text into its non-toxic variant). The authors used their proposed pipeline to collect the first parallel English detoxification dataset (ParaDetox). ParaDetox has almost 12,000 toxic sentences where each sentence is paired with 1-3 non-toxic paraphrases. Moreover, the authors also show that their pipeline could be used to retrieve toxic and non-toxic sentence pairs from existing paraphrasing parallel datasets (e.g., ParaNMT). Furthermore, the authors demonstrated the effectiveness of their created parallel data by training various detoxification seq2seq BART-based models and showing that the trained models outperform current STOA methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_review2_2",
            "content": "summary_of_strengths. - A novel dataset that will encourage research and development on the task of detoxification.\n- A useful pipeline that could be used to create new parallel datasets for detoxification. The pipeline also provides a faster and cheaper way to take advantage of existing parallel paraphrasing datasets to retrieve toxic/non-toxic sentence pairs.\n- Clear description of the crowdsourcing pipeline.   * Nice examples of what the data looks like. I found it very useful to understand the task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_review2_3",
            "content": "summary_of_weaknesses. - There are some technical aspects of the paper that weren't clear to me:   * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?\n  * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? \n     * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?\n * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.\n * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v1_review2_4",
            "content": "comments,_suggestions_and_typos. I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?)",
            "ntype": "p",
            "meta": null
        }
    ],
    "span_nodes": [
        {
            "ix": "31-ARR_v1_review2_0@0",
            "content": "31-ARR_v1_review2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_0",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@0",
            "content": "paper_summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@1",
            "content": "The authors present a new crowdsourcing pipeline to collect parallel data for the task of detoxification (i.e., a style-transfer task to convert toxic text into its non-toxic variant).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 15,
            "end": 198,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@2",
            "content": "The authors used their proposed pipeline to collect the first parallel English detoxification dataset (ParaDetox).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 200,
            "end": 313,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@3",
            "content": "ParaDetox has almost 12,000 toxic sentences where each sentence is paired with 1-3 non-toxic paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 315,
            "end": 419,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@4",
            "content": "Moreover, the authors also show that their pipeline could be used to retrieve toxic and non-toxic sentence pairs from existing paraphrasing parallel datasets (e.g., ParaNMT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 421,
            "end": 594,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_1@5",
            "content": "Furthermore, the authors demonstrated the effectiveness of their created parallel data by training various detoxification seq2seq BART-based models and showing that the trained models outperform current STOA methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_1",
            "start": 596,
            "end": 811,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@0",
            "content": "summary_of_strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@1",
            "content": "- A novel dataset that will encourage research and development on the task of detoxification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 22,
            "end": 114,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@2",
            "content": "\n- A useful pipeline that could be used to create new parallel datasets for detoxification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 115,
            "end": 205,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@3",
            "content": "The pipeline also provides a faster and cheaper way to take advantage of existing parallel paraphrasing datasets to retrieve toxic/non-toxic sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 207,
            "end": 362,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@4",
            "content": "\n- Clear description of the crowdsourcing pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 363,
            "end": 413,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_2@5",
            "content": "  * Nice examples of what the data looks like. I found it very useful to understand the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_2",
            "start": 415,
            "end": 507,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_3@0",
            "content": "summary_of_weaknesses. - There are some technical aspects of the paper that weren't clear to me:   * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_3",
            "start": 0,
            "end": 341,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_3@1",
            "content": "\n  * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? \n     * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_3",
            "start": 342,
            "end": 574,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_3@2",
            "content": "If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?\n * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_3",
            "start": 576,
            "end": 838,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_3@3",
            "content": "\n * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_3",
            "start": 839,
            "end": 955,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_4@0",
            "content": "comments,_suggestions_and_typos. I think the paper would benefit from another round of revisions to fix some typos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_4",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "31-ARR_v1_review2_4@1",
            "content": "It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v1_review2_4",
            "start": 116,
            "end": 322,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_3",
            "tgt_ix": "31-ARR_v1_review2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v1_review2_0",
            "tgt_ix": "31-ARR_v1_review2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_1",
            "tgt_ix": "31-ARR_v1_review2_1@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_2",
            "tgt_ix": "31-ARR_v1_review2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_3",
            "tgt_ix": "31-ARR_v1_review2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_3",
            "tgt_ix": "31-ARR_v1_review2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_3",
            "tgt_ix": "31-ARR_v1_review2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_3",
            "tgt_ix": "31-ARR_v1_review2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_4",
            "tgt_ix": "31-ARR_v1_review2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v1_review2_4",
            "tgt_ix": "31-ARR_v1_review2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "31-ARR_v1_review2",
    "meta": {
        "ix_counter": 23,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy"
    }
}