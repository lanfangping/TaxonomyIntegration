{
    "nodes": [
        {
            "ix": "31-ARR_v2_0",
            "content": "ParaDetox: Detoxification with Parallel Data",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_2",
            "content": "We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxicneutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_3",
            "content": "We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "31-ARR_v2_5",
            "content": "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.g. microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al., 2020), is a popular topic of research in NLP. However, detection of harmful messages does not offer any proactive ways of fighting them (besides deletion). We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_6",
            "content": "The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018;Tran et al., 2020). It is considered a variant of style transfer task, the task of rewriting a text saving * Equal contribution the content and changing the style (style is defined as a characteristic of text such as sentiment, level of formality, or politeness, author profile (gender, political preferences), etc.). As a sequence-tosequence task, style transfer can be performed with an encoder-decoder model trained on parallel data. However, there exist only a few parallel style transfer corpora (Carlson et al., 2018;Pryzant et al., 2020). Since they usually do not exist \"naturally\", they need to be written from scratch. This is an expensive and laborious process. Thus, such parallel datasets are extremely rare.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_7",
            "content": "Jigsaw so why would anyone believe this moron?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_8",
            "content": "Paraphrase so why would anyone believe this person? so why would anyone believe somebody like him?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_9",
            "content": "Reddit dude ham sandwich is the good sh*t .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_10",
            "content": "dude ham sandwich is the good thing The ham sandwich, buddy, is the bomb. Dude ham sandwich is good.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_11",
            "content": "Twitter now i feel like an a*s Paraphrase now i feel like worthless now i feel very bad now i feel bad",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_12",
            "content": "Table 1: Examples of detoxified sentences from the collected parallel corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_13",
            "content": "We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases. We suggest a new crowdsourcing pipeline for collecting parallel style transfer data. It does not employ experts, which makes the data collection faster and cheaper. In addition to generating the detoxified versions of texts, we consider a way to distill existing datasets of paraphrases for style-specific data. In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup. The pipelines are described in detail to make them easy to replicate. Thus, we suggest that by reusing these pipelines the new parallel style transfer datasets can be collected in a fast and affordable way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_14",
            "content": "Finally, we validate the usefulness of our datasets by training detoxification models on them and comparing their performance with state-of-theart methods. Models trained on parallel data significantly outperform other models in terms of automatic metrics and human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_15",
            "content": "The contributions of our work are three-fold:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_16",
            "content": "\u2022 We suggest a novel pipeline for collection of parallel data for the detoxification task, \u2022 We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, 1 \u2022 Using collected data we train supervised detoxification models that yield SOTA results.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_17",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "31-ARR_v2_18",
            "content": "Style Transfer Datasets When collecting nonparallel style transfer corpora, style labels often already exist in the data (e.g. positive and negative reviews (Li et al., 2018)) or its source serves as a label (e.g. Twitter, academic texts, legal documents, etc.). Thus, data collection is reduced to fetching the texts from their sources, and the corpus size depends only on the available amount of text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_19",
            "content": "Conversely, parallel corpora are usually more difficult to get. There exist parallel style transfer datasets fetched from \"naturally\" available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_20",
            "content": "Besides these special cases, there exists a large style transfer dataset that was created from scratch. This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts. Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference -we replace expert validation of crowdsourced sentences with crowd validation and additionally optimize the cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_21",
            "content": "The vast majority of style transfer models (including detoxification models) are trained on non-parallel data. They can perform pointwise corrections of stylemarked words (Li et al., 2018;Wu et al., 2019;Malmi et al., 2020). Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017;Fu et al., 2018). As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_22",
            "content": "Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora. Laugier et al. (2021) use the Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 for training, Nogueira dos Santos et al. ( 2018) create their own toxicity-labelled datasets of sentences from Reddit and Twitter. Following them, we also fetch sentences for rewriting from these datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_23",
            "content": "Works on detoxification often rely on style transfer models tested on other domains. Nogueira dos Santos et al. (2018) follow Shen et al. (2017) and Fu et al. (2018) and train an autoencoder with additional style classification and cycle-consistency losses. Laugier et al. (2021) perform a similar finetuning of T5 as a denoising autoencoder. Tran et al. (2020) apply pointwise corrections approach similar to that of Wu et al. (2019) and then improve the fluency of a text with a seq2seq model. Likewise, Dale et al. (2021) use a masked language model to perform pointwise edits of toxic sentences. They also suggest an alternative model which enhances a style-agnostic seq2seq model with style-informed language models which reweigh the seq2seq hypotheses with respect to the desired style.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_24",
            "content": "When the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models to perform style transfer. We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_25",
            "content": "Data Collection Pipeline",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "31-ARR_v2_26",
            "content": "Our goal is to yield pairs of sentences that have the same meanings and are contrasted in terms of Rewrite this text so that it does not sound offensive and its meaning stays the same You realize that's stupid, don't you? offensiveness -one of the sentences is toxic and the other is neutral. We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases. Unlike a similar work of Rao and Tetreault (2018), we hire crowd workers not only for the generation of paraphrases but also for their validation, which reduces both time and cost.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_27",
            "content": "Crowdsourcing Tasks",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "31-ARR_v2_28",
            "content": "We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity. Each task is implemented as a separate crowdsourcing project. We use the crowdsourcing platform Yandex.Toloka. 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_29",
            "content": "Task 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1). However, it is not always possible. Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless. Moreover, in some cases toxicity cannot be removed. Consider the examples:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_30",
            "content": "\u2022 Are you that dumb you can't figure it out? \u2022 I've finally understood that wiki is nothing but a bunch of American racists.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_31",
            "content": "Not only the form but also the content of the messages are offensive, so trying to detoxify them would inevitably lead to a substantial change of sense. We prefer not to include such cases in the parallel dataset. If workers have to detoxify all inputs without a possibility to skip them, a large proportion of the generated paraphrases will be of low quality. Thus, we add the control \"I can't rewrite the text\" and optional controls to indicate the reasons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_32",
            "content": "Task 2: Content Preservation Check We show users the generated paraphrases along with their original variants and ask them to indicate if they have close meanings. Besides ensuring content preservation, this task implicitly filters out senseless outputs, because they do not keep the original content. The task interface is shown in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_33",
            "content": "Task 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity. We ask users to indicate if the paraphrases contain any offense or swear words (see Figure 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_34",
            "content": "In addition to filtering out unsuitable paraphrases, we use Tasks 2 and 3 for paying for Task 1. We accept or reject the generated paraphrases based on the labels they get in Tasks 2 and 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_35",
            "content": "Pipelines",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "31-ARR_v2_36",
            "content": "Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting. We fetch them from corpora labeled for toxicity and additionally filter them with a toxicity classifier (described in Section 3.3). The overall data collection pipeline (see Figure 4) is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_37",
            "content": "\u2022 Select toxic sentences for rewriting, \u2022 Feed the sentences to Task 1, \u2022 Feed the paraphrases generated in Task 1 to Task 2, \u2022 Feed the paraphrases which passed Task 2 to Task 3, \u2022 Pay for paraphrases from Task 1, if they passed checks in Task 2 and Task 3, \u2022 Pay for \"I can't rewrite\" answers in Task 1 if two or more workers agreed on them. Retrieval Pipeline The generation pipeline can be used for cases when no parallel data is available. However, we suggest that a sufficiently large parallel corpus of paraphrases can contain pairs of sentences belonging to different styles, and it is possible to distill such corpus into a style transfer dataset. We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_38",
            "content": "We partially reuse the previously described setup. We do not need Task 1 since both toxic and neutral sentences are already available. However, we run Task 3 twice, because we need to check both parts of the pair for toxicity. Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral. The parallel data retrieval pipeline is shown in Figure 5. It is simpler because Tasks 2 and 3 do not serve for paying for the generated paraphrases and are only used for data filtering. The pipeline is as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_39",
            "content": "\u2022 Select a pair of sentences (toxic and non-toxic) from the parallel data, \u2022 Feed the toxic sentence candidate to Task 3 to make sure it is toxic, \u2022 Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, \u2022 Feed both sentences to Task 2 to check if their content matches.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_40",
            "content": "Crowdsourcing Settings",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "31-ARR_v2_41",
            "content": "Preprocessing To pre-select toxic sentences, we need a toxicity classifier. We fine-tune a RoBERTa model (Liu et al., 2019) 3 on half of the three merged Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 (1 million sentences) and get a classifier which yields the F 1 -score of 0.76 on the Jigsaw test set (Jigsaw, 2018). We consider a sentence toxic if the classifier confidence is above 0.8. To make the sentences easier for reading and rewriting, we choose the ones consisting of 5 to 20 tokens. For the retrieval pipeline, we also select parallel sentences with the cosine similarity of embeddings between 0.65 and 0.8. The similarity scores were provided as a part of ParaNMT dataset, the embeddings come from the PARAGRAM-PHRASE model (Wieting et al., 2016). Based on a manual validation, sentences with lower similarity are often not exact paraphrases, and too-similar sentences are either both toxic or both non-toxic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_42",
            "content": "Quality Control To perform paid tasks, users need to pass training and exam sets of tasks. Each of them has a corresponding skill -the percentage of correct answers. It is assigned to a user upon completing training or exam and serves for filtering out low-performing users. Besides that, users are occasionally given control questions during labeling. They serve for computing the labeling skill which can be used for banning low-performing and rewarding well-performing workers. The overall training and control pipeline is shown in Figure 6. It is used in Tasks 2 and 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_43",
            "content": "In Task 1 we perform different quality control. We ban users who submit answers which are: (i) a copy of the input, (ii) too short (< 3 tokens) or too long (more than doubled original length), (iii) contain too many rare words or non-words. The latter condition is checked as follows. We compute the ratio of the number of whitespace-separated tokens and the number of tokens identified by the BPE tokeniser (Sennrich et al., 2016). 4 The rationale behind this check is that the BPE tokenizer tends to divide rare words into multiple tokens. If the number of BPE tokens in a sentence is two times more than the number of regular tokens, it might indicate the presence of non-words. We filter out these answers and ban users who produce them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_44",
            "content": "In addition to that, we ban malicious workers using built-in Yandex.Toloka tools: (i) captcha, (ii) number of skipped questions -we ban users who skip 10 task pages in a row, and (iii) task completion time -we ban those who accomplish tasks too fast (this usually means that they choose a random answer without reading).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_45",
            "content": "Payment In Yandex.Toloka, a worker is paid for a page that can have multiple tasks (the number is set by customer). In Task 1, a page contains 5 tasks and costs $0.02. In Tasks 2 and 3, we pay $0.02 and $0.01, respectively, for 12 tasks. In addition to that, in these tasks, we use skill-based payment. If a worker has the labeling skill of above 90%, the payment is increased to $0.03 (Task 2) and $0.02 (Task 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_46",
            "content": "Tasks 2 and 3 are paid instantly, whereas in Task 1 we check the paraphrases before paying. If a worker indicated that a sentence cannot be paraphrased, we pay for this answer only if at least one other worker agreed with that. If a worker typed in a paraphrase, we send it to Tasks 2 and 3 and pay only for the ones approved by both tasks. The payment procedure is shown in Figure 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_47",
            "content": "Postprocessing To ensure the correctness of labeling, we ask several workers to label each example. In Task 1, this gives us multiple paraphrases and also verifies the \"I can't rewrite\" answers. For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often. The number of people to label an example ranges from 3 to 5 depending on the workers' agreement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_48",
            "content": "Dawid-Skene aggregation returns the final label and its confidence. To improve the quality of the data, we accept only labels with the confidence of over 90% and do not include the rest in the final data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_49",
            "content": "The Pipeline Scalability",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "31-ARR_v2_50",
            "content": "The Yandex.Toloka platform has an interface in English and workers from a large number of countries. Workers can be filtered by their location and asked to pass built-in language tests (available for many languages) to ensure the knowledge of a particular language. This enables the use of Toloka for the creation of NLP resources in many languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_51",
            "content": "In our work, crowd workers manually rephrase sentences from non-parallel datasets. The pipeline does not require any specific data format and can be applied to any text. The only prerequisites are to define the source and target styles and to formulate the task of transferring between them. Thus, we believe that the pipeline is suitable for creating parallel datasets for any other style transfer tasks, at least those which have non-parallel datasets and clear definitions of style (positive \u2194 negative, complex \u2194 simple, impolite \u2194 polite, etc.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_52",
            "content": "We should admit that our pipeline suggests the availability of (non-parallel) datasets in the chosen styles or at least publicly available sources of such data (e.g. social networks, question answering platforms). However, this is also a prerequisite for any style transfer model trained on non-parallel data. Therefore, any work on style transfer suggests that there exists enough data in the chosen style pair and language. This should not be considered a specific limitation of the pipeline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_53",
            "content": "ParaDetox: Generated Paraphrases",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "31-ARR_v2_54",
            "content": "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by Nogueira dos Santos et al. (2018). We selected 7,000 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers. We get paraphrases for 12,610 toxic sentences (on average 1.66 paraphrases per sentence), 20,437 paraphrases total. Running 1,000 input sentences through the pipeline costs $41.2, and the cost of one output sample is $0.07. The overall cost of the dataset is $811.55. We give them examples of sentences in Appendix A. In addition to that, we provide some samples which could not be detoxified in Appendix C. The statistics of the paraphrases written by crowd workers are presented in Table 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_55",
            "content": "The distribution of sentences from different datasets in the final data is not equal. Jigsaw turned out to be the most difficult to paraphrase. Fewer sentences from it are successfully paraphrased, making it the most expensive part of the collected corpus ($0.08 per sample). Figure 7 shows that the number of untransferable sentences in the Jigsaw dataset is larger than that of other corpora. Out of all crowdsourced paraphrases, only a small part was of high quality. We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8. It also corroborates the difficulty of the Jigsaw dataset. While the overall number of generated paraphrases was slightly higher for it, much more of them were discarded.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_56",
            "content": "Analysis of Edits",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "31-ARR_v2_57",
            "content": "Although we did not give any special instructions to workers about editing, they often followed the minimal editing principle, making 1.36 changes per sentence on average. A change is deletion, insertion, or rewriting of a word or multiple adjacent words. Many of the changes are supposedly deletions because the average sentence length drops from 12.1 to 10.4 words after editing. The nature of editing differs for the three datasets. We compute the percentage of edits which consisted of removing the most common swear words or replacing them with neutral words. We first define the differences between the original and transformed string with the difflib Python library and then compute the percentage of differences that consist in editing swear words and other (non-offensive) words. We use a small manually compiled list of swear words which includes words f*ck, sh*t, a*s, b*tch, d*mn and their variants. Table 3 shows that the deletion or replacements of the most common swearing constituted a large part of all edits for Reddit and Twitter datasets (22% and 30%), while for Jigsaw it was only 3%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_58",
            "content": "Another surprisingly common type of editing is the normalization of sentences. The users often fixed casing, punctuation, typos (e.g. dont \u2192 don't, there's \u2192 there is). They also tended to replace colloquial phrases with more formal and standard language. Finally, some users overcorrected the sentences. For example, they replaced neutral words such as dead, murder, penis with euphemisms. This tendency indicates that workers consider any sensitive topic to be inappropriate content and try to avoid it as much as possible.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_59",
            "content": "ParaNMT: Existing Paraphrases",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "31-ARR_v2_60",
            "content": "Our automatic filtering of ParaNMT for content yields 500,000 potentially detoxifying sentence pairs, which is 1% of the corpus. We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation. This leaves with 1,393 sentences, meaning that around 23% of the pre-selected sentence pairs were approved (for ParaDetox we get paraphrases for 61% input sentences). Thus, although the cost per 1,000 inputs is much lower than that of generating the paraphrases, the cost per output sample is the same as that of generated paraphrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_61",
            "content": "ParaNMT dataset is different from ParaDetox. First, each sentence has only one paraphrase. These paraphrases were not gained via manual editing but via a chain of translation models. Thus, neutral sentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_62",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "31-ARR_v2_63",
            "content": "To evaluate the collected corpora, we use them to train several supervised detoxification models. We separate the ParaDetox dataset into training and test parts (11,939 and 671 sentence pairs, respectively). The test sentences have one reference per sentence. We manually validate the test set to exclude the appearance of non-detoxifiable sentences or sentences which stayed toxic after rewriting (we need to verify that since the corpus was generated via crowdsourcing only). We do not use the test set neither for training nor for parameter selection of the models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_64",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "31-ARR_v2_65",
            "content": "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020) 5 on our data. We test BART trained on the following datasets:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_66",
            "content": "5 We use model https://huggingface.co/facebook/bart-base",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_67",
            "content": "\u2022 ParaDetox -our full crowdsourced dataset. \u2022 ParaDetox-unique -a subset of ParaDetox where each toxic sentence has only one paraphrase (selected randomly). \u2022 ParaDetox-1000 -1,000 samples from the crowdsourced dataset (distributed evenly across data sources, each toxic sample has multiple non-toxic variants). \u2022 ParaNMT -filtered ParaNMT corpus, auto stands for automatically filtered 500,000 samples, manual are 1,393 manually selected sentence pairs.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_68",
            "content": "We train BART for 10,000 epochs with the learning rate of 3e-5 and the number of gradient accumulation steps set to 1. The other parameters are set to their default values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_69",
            "content": "We also compare our models to other style transfer approaches:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_70",
            "content": "\u2022",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_71",
            "content": "Metrics",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "31-ARR_v2_72",
            "content": "We compute the BLEU score on the test set. In addition to that, we perform automatic referencefree evaluation which is used in many style transfer works. Namely, we evaluate:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_73",
            "content": "\u2022 Style accuracy (STA) -percentage of nontoxic outputs identified by a style classifier.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_74",
            "content": "We use a classifier from Section 3.3 trained on a different half of Jigsaw data. \u2022 Content preservation (SIM) -cosine similarity between the embeddings of the original text and the output computed with the model of Wieting et al. (2019). This model is trained on paraphrase pairs extracted from ParaNMT corpus. The model's training objective is to yield embeddings such that the similarity of embeddings of paraphrases is higher than the similarity between sentences that are not paraphrases. \u2022 Fluency (FL) -percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2019). We compute the final joint metric (J) as the multiplication of the three individual metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_75",
            "content": "Since the automatic evaluation can be unreliable, we evaluate some models manually. We randomly select 200 sentences from the test set and ask assessors to evaluate them along the same three parameters: style accuracy (STA m ), content preservation (SIM m ), and fluency (FL m ). All parameters can take values of 1 (good) and 0 (bad). We also report the joint metric J m which is the percentage of sentences whose STA m , SIM m , and FL m are 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_76",
            "content": "The evaluation was conducted by 6 NLP researchers with a good command of English. Each sample was evaluated by 3 assessors. The interannotator agreement (Krippendorff's \u03b1) reaches 0.64 (STA m ), 0.67 (SIM m ), and 0.68 (FL m ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_77",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "31-ARR_v2_78",
            "content": "Automatic Evaluation Table 4 shows the automatic scores of all tested models. Our BART models trained on ParaDetox outperform other systems in terms of BLEU and J. The much lower scores of BART-zero-shot confirm that this success is due to fine-tuning and not the innate ability of BART. The majority of unsupervised SOTA approaches are not only worse than BART but also perform below the \"change nothing\" baseline. The closest competitor of our models is the Delete model. This can be explained by the fact that crowd workers often only remove or replaced swear words which is what the Delete model does.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_79",
            "content": "When comparing models trained on supervised data, we can see that BART does not benefit from multiple detoxifications per sentence, its performance is the same when trained on ParaDetox and ParaDetox-unique. On the other hand, manual filtering of ParaNMT is beneficial, it increases the quality of BART trained on it, although the number of training sentences drops from 500,000 to 1,400.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_80",
            "content": "We also check which amount of data is sufficient for a high detoxification quality. We train the BART model on subsets of ParaDetox of different sizes. Figure 9 and the performance of ParaDetox-1000 model (Table 4) show that 1,000 training samples is enough to get a good detoxification. While SIM and FL are already high for vanilla BART (see BART-zero-shot model), STA can be improved with only a few parallel examples. This suggests that style transfer does not need large parallel corpora, making our pipeline more useful for other style transfer tasks. However, this is the result of the automatic evaluation, which as we show below is not always reliable. It needs extra investigation. Manual Evaluation Manual evaluation (Table 6) confirms the usefulness of parallel data. BARTs trained on parallel data outperform other competitors, even if the size of this data is small. However, manual and automatic evaluations do not always match. Here, the well-performing Delete model gets the lowest score. Overall, assessors agree with automatic metrics only in terms of fluency, their Spearman correlation r is 0.89. The manual style accuracy and content preservation are only moderately correlated with their automatic counterparts leaving space for further improvements. J and J m almost do not correlate. Besides that, BLEU correlates only with content preservation score and is moderately inversely correlated with the style accuracy. Thus, BLEU measures only the degree of content preservation and cannot replace other metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_81",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "31-ARR_v2_82",
            "content": "We present ParaDetox -an English parallel corpus for the detoxification task. It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers. To the best of our knowledge, this is the first parallel detoxification dataset. We present a novel data collection pipeline and show that parallel data can be generated using only crowdsourcing. We also adopt this pipeline to the style-based distillation of paraphrase corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_83",
            "content": "We confirm the usefulness of our datasets by training sequence-to-sequence models on them. The experiments show that the use of parallel data yields models which significantly outperform style transfer models trained on non-parallel data. Besides that, we confirm that filtering the noisy parallel data can lead to considerable improvement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_84",
            "content": "We see that it is enough to get 1,000 parallel sentences to perform detoxification with high quality. This suggests that our pipeline can be successfully applied to create useful parallel resources for style transfer even in cases of limited finance or lack of crowd workers because the cost of generating 1,000 examples is very low.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_85",
            "content": "Finally, we investigate the relationship between metrics and find that automatic evaluation does not always match the manual judgments and referencebased BLEU cannot replace human evaluation, because it measures content preservation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_86",
            "content": "The research on toxicity raises some ethical issues. In terms of our work, the parallel corpus we created can indeed be used in the reverse direction, i.e. to \"toxify\" sentences. However, although we did not thoroughly evaluate the quality of such toxification, our intuition is that it would not be high enough to make the corrupted sentences look natural. The reason is that the toxic part of our corpus consists of real toxic sentences fetched on the Internet, whereas their non-toxic counterparts are \"translations\" performed by crowd workers. We suggest that they obey the common regularities observed for translationese (texts manually translated from their original language into a different one): they differ from regular texts in terms of vocabulary (Koppel and Ordan, 2011) and syntax (Lembersky et al., 2011). The manually detoxified texts are different from the original non-toxic texts written by Internet users from scratch. While they are still recognized by human assessors as plausible sentences, we suggest that a sequence-to-sequence model trained to get translationese as input would not be as successful in transforming real texts (as it was shown for machine translation models (Freitag et al., 2019)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_87",
            "content": "Thus, although our corpus can be used in the reverse direction, it is not symmetric, which makes it less efficient as training datasets for \"toxifiers\". However, we should emphasize that these statements are our hypotheses and should be further investigated. Finally, we argue that the risk of using our corpus for toxification is perhaps not game-changing, as simpler approaches based on patterns (e.g. including a set of predefined obscene fragments into neutral texts) can serve the same purpose relatively well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "31-ARR_v2_88",
            "content": "Luke Breitfeller, Emily Ahn, David Jurgens, Yulia Tsvetkov, Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Luke Breitfeller",
                    "Emily Ahn",
                    "David Jurgens",
                    "Yulia Tsvetkov"
                ],
                "title": "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_89",
            "content": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault, Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Eleftheria Briakou",
                    "Di Lu",
                    "Ke Zhang",
                    "Joel Tetreault"
                ],
                "title": "Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_90",
            "content": "Keith Carlson, Allen Riddell, Daniel Rockmore, Evaluating prose style transfer with the bible, 2018, Royal Society Open Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Keith Carlson",
                    "Allen Riddell",
                    "Daniel Rockmore"
                ],
                "title": "Evaluating prose style transfer with the bible",
                "pub_date": "2018",
                "pub_title": "Royal Society Open Science",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_91",
            "content": "David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko, Text detoxification using large pre-trained neural models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "David Dale",
                    "Anton Voronov",
                    "Daryna Dementieva",
                    "Varvara Logacheva",
                    "Olga Kozlova",
                    "Nikita Semenov",
                    "Alexander Panchenko"
                ],
                "title": "Text detoxification using large pre-trained neural models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_92",
            "content": "Alexander Dawid, Allan Skene, Maximum likelihood estimation of observer error-rates using the em algorithm, 1979, Journal of The Royal Statistical Society Series C-applied Statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Alexander Dawid",
                    "Allan Skene"
                ],
                "title": "Maximum likelihood estimation of observer error-rates using the em algorithm",
                "pub_date": "1979",
                "pub_title": "Journal of The Royal Statistical Society Series C-applied Statistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_93",
            "content": "Markus Freitag, Isaac Caswell, Scott Roy, APE at scale and its implications on MT evaluation biases, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Markus Freitag",
                    "Isaac Caswell",
                    "Scott Roy"
                ],
                "title": "APE at scale and its implications on MT evaluation biases",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_94",
            "content": "Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Style transfer in text: Exploration and evaluation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Zhenxin Fu",
                    "Xiaoye Tan",
                    "Nanyun Peng",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Style transfer in text: Exploration and evaluation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_95",
            "content": "Junxian He, Xinyi Wang, Graham Neubig, Taylor Berg-Kirkpatrick, A probabilistic formulation of unsupervised text style transfer, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Junxian He",
                    "Xinyi Wang",
                    "Graham Neubig",
                    "Taylor Berg-Kirkpatrick"
                ],
                "title": "A probabilistic formulation of unsupervised text style transfer",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_96",
            "content": "UNKNOWN, None, 2018, Toxic comment classification challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Toxic comment classification challenge",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_97",
            "content": "UNKNOWN, None, 2019, Jigsaw unintended bias in toxicity classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Jigsaw unintended bias in toxicity classification",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_98",
            "content": "UNKNOWN, None, 2020, Jigsaw multilingual toxic comment classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Jigsaw multilingual toxic comment classification",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_99",
            "content": "Moshe Koppel, Noam Ordan, Translationese and its dialects, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Moshe Koppel",
                    "Noam Ordan"
                ],
                "title": "Translationese and its dialects",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v2_100",
            "content": "L\u00e9o Laugier, John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Civil rephrases of toxic texts with self-supervised transformers, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "L\u00e9o Laugier",
                    "John Pavlopoulos",
                    "Jeffrey Sorensen",
                    "Lucas Dixon"
                ],
                "title": "Civil rephrases of toxic texts with self-supervised transformers",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v2_101",
            "content": "Joosung Lee, Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer, 2020, Proceedings of the 13th International Conference on Natural Language Generation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Joosung Lee"
                ],
                "title": "Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 13th International Conference on Natural Language Generation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v2_102",
            "content": "Gennadi Lembersky, Noam Ordan, Shuly Wintner, Language models for machine translation: Original vs, 2011, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Gennadi Lembersky",
                    "Noam Ordan",
                    "Shuly Wintner"
                ],
                "title": "Language models for machine translation: Original vs",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v2_103",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_104",
            "content": "Juncen Li, Robin Jia, He He, Percy Liang, Delete, retrieve, generate: a simple approach to sentiment and style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Juncen Li",
                    "Robin Jia",
                    "He He",
                    "Percy Liang"
                ],
                "title": "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v2_105",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_106",
            "content": "Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui, A dual reinforcement learning framework for unsupervised text style transfer, 2019-08-10, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Fuli Luo",
                    "Peng Li",
                    "Jie Zhou",
                    "Pengcheng Yang",
                    "Baobao Chang",
                    "Xu Sun",
                    "Zhifang Sui"
                ],
                "title": "A dual reinforcement learning framework for unsupervised text style transfer",
                "pub_date": "2019-08-10",
                "pub_title": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_107",
            "content": "Eric Malmi, Aliaksei Severyn, Sascha Rothe, Unsupervised text style transfer with padded masked language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Eric Malmi",
                    "Aliaksei Severyn",
                    "Sascha Rothe"
                ],
                "title": "Unsupervised text style transfer with padded masked language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_108",
            "content": "Cicero Nogueira Dos Santos, Igor Melnyk, Inkit Padhi, Fighting offensive language on social media with unsupervised text style transfer, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Cicero Nogueira Dos Santos",
                    "Igor Melnyk",
                    "Inkit Padhi"
                ],
                "title": "Fighting offensive language on social media with unsupervised text style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "31-ARR_v2_109",
            "content": "Carla Perez Almendros, Luis Espinosa Anke, Steven Schockaert, Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Carla Perez Almendros",
                    "Luis Espinosa Anke",
                    "Steven Schockaert"
                ],
                "title": "Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_110",
            "content": "Reid Pryzant, Richard Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, Diyi Yang, Automatically neutralizing subjective bias in text, 2020, Proceedings of the aaai conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Reid Pryzant",
                    "Richard Martinez",
                    "Nathan Dass",
                    "Sadao Kurohashi",
                    "Dan Jurafsky",
                    "Diyi Yang"
                ],
                "title": "Automatically neutralizing subjective bias in text",
                "pub_date": "2020",
                "pub_title": "Proceedings of the aaai conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_111",
            "content": "Sudha Rao, Joel Tetreault, Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Sudha Rao",
                    "Joel Tetreault"
                ],
                "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v2_112",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "31-ARR_v2_113",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Tianxiao Shen",
                    "Tao Lei",
                    "Regina Barzilay",
                    "Tommi Jaakkola"
                ],
                "title": "Style transfer from non-parallel text by cross-alignment",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "31-ARR_v2_114",
            "content": "Minh Tran, Yipeng Zhang, Mohammad Soleymani, Towards a friendly online community: An unsupervised style transfer framework for profanity redaction, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Minh Tran",
                    "Yipeng Zhang",
                    "Mohammad Soleymani"
                ],
                "title": "Towards a friendly online community: An unsupervised style transfer framework for profanity redaction",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_115",
            "content": "Alex Warstadt, Amanpreet Singh, Samuel , Neural network acceptability judgments, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Alex Warstadt",
                    "Amanpreet Singh",
                    "Samuel "
                ],
                "title": "Neural network acceptability judgments",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_116",
            "content": "John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Towards universal paraphrastic sentence embeddings, 2016-05-02, 4th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "John Wieting",
                    "Mohit Bansal",
                    "Kevin Gimpel",
                    "Karen Livescu"
                ],
                "title": "Towards universal paraphrastic sentence embeddings",
                "pub_date": "2016-05-02",
                "pub_title": "4th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_117",
            "content": "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, Graham Neubig, Beyond BLEU:training neural machine translation with semantic similarity, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "John Wieting",
                    "Taylor Berg-Kirkpatrick",
                    "Kevin Gimpel",
                    "Graham Neubig"
                ],
                "title": "Beyond BLEU:training neural machine translation with semantic similarity",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_118",
            "content": "John Wieting, Kevin Gimpel, ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "John Wieting",
                    "Kevin Gimpel"
                ],
                "title": "ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "31-ARR_v2_119",
            "content": "Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, Songlin Hu, Mask and infill: Applying masked language model for sentiment transfer, 2019, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Xing Wu",
                    "Tao Zhang",
                    "Liangjun Zang",
                    "Jizhong Han",
                    "Songlin Hu"
                ],
                "title": "Mask and infill: Applying masked language model for sentiment transfer",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_120",
            "content": "Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval), 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Marcos Zampieri",
                    "Shervin Malmasi",
                    "Preslav Nakov",
                    "Sara Rosenthal",
                    "Noura Farra",
                    "Ritesh Kumar"
                ],
                "title": "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 13th International Workshop on Semantic Evaluation",
                "pub": null
            }
        },
        {
            "ix": "31-ARR_v2_121",
            "content": "Yi Zhang, Tao Ge, Xu Sun, Parallel data augmentation for formality style transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Yi Zhang",
                    "Tao Ge",
                    "Xu Sun"
                ],
                "title": "Parallel data augmentation for formality style transfer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "31-ARR_v2_0@0",
            "content": "ParaDetox: Detoxification with Parallel Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_0",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@0",
            "content": "We present a novel pipeline for the collection of parallel data for the detoxification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@1",
            "content": "We collect non-toxic paraphrases for over 10,000 English toxic sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 93,
            "end": 165,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@2",
            "content": "We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxicneutral sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 167,
            "end": 295,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@3",
            "content": "We release two parallel corpora which can be used for the training of detoxification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 297,
            "end": 388,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@4",
            "content": "To the best of our knowledge, these are the first parallel datasets for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 390,
            "end": 471,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_2@5",
            "content": "We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_2",
            "start": 473,
            "end": 640,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_3@0",
            "content": "We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_3",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_3@1",
            "content": "We conduct both automatic and manual evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_3",
            "start": 147,
            "end": 195,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_3@2",
            "content": "All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_3",
            "start": 197,
            "end": 302,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_3@3",
            "content": "This suggests that our novel datasets can boost the performance of detoxification systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_3",
            "start": 304,
            "end": 393,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_5@0",
            "content": "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.g. microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al., 2020), is a popular topic of research in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_5",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_5@1",
            "content": "However, detection of harmful messages does not offer any proactive ways of fighting them (besides deletion).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_5",
            "start": 219,
            "end": 327,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_5@2",
            "content": "We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_5",
            "start": 329,
            "end": 448,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@0",
            "content": "The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018;Tran et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@1",
            "content": "It is considered a variant of style transfer task, the task of rewriting a text saving * Equal contribution the content and changing the style (style is defined as a characteristic of text such as sentiment, level of formality, or politeness, author profile (gender, political preferences), etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 152,
            "end": 448,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@2",
            "content": "As a sequence-tosequence task, style transfer can be performed with an encoder-decoder model trained on parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 450,
            "end": 567,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@3",
            "content": "However, there exist only a few parallel style transfer corpora (Carlson et al., 2018;Pryzant et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 569,
            "end": 676,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@4",
            "content": "Since they usually do not exist \"naturally\", they need to be written from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 678,
            "end": 759,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@5",
            "content": "This is an expensive and laborious process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 761,
            "end": 803,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_6@6",
            "content": "Thus, such parallel datasets are extremely rare.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_6",
            "start": 805,
            "end": 852,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_7@0",
            "content": "Jigsaw so why would anyone believe this moron?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_7",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_8@0",
            "content": "Paraphrase so why would anyone believe this person? so why would anyone believe somebody like him?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_8",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_9@0",
            "content": "Reddit dude ham sandwich is the good sh*t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_9",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_10@0",
            "content": "dude ham sandwich is the good thing The ham sandwich, buddy, is the bomb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_10",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_10@1",
            "content": "Dude ham sandwich is good.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_10",
            "start": 74,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_11@0",
            "content": "Twitter now i feel like an a*s Paraphrase now i feel like worthless now i feel very bad now i feel bad",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_11",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_12@0",
            "content": "Table 1: Examples of detoxified sentences from the collected parallel corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_12",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@0",
            "content": "We aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@1",
            "content": "We suggest a new crowdsourcing pipeline for collecting parallel style transfer data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 143,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@2",
            "content": "It does not employ experts, which makes the data collection faster and cheaper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 228,
            "end": 306,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@3",
            "content": "In addition to generating the detoxified versions of texts, we consider a way to distill existing datasets of paraphrases for style-specific data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 308,
            "end": 453,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@4",
            "content": "In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases and filter them using our crowdsourcing setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 455,
            "end": 639,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@5",
            "content": "The pipelines are described in detail to make them easy to replicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 641,
            "end": 709,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_13@6",
            "content": "Thus, we suggest that by reusing these pipelines the new parallel style transfer datasets can be collected in a fast and affordable way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_13",
            "start": 711,
            "end": 846,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_14@0",
            "content": "Finally, we validate the usefulness of our datasets by training detoxification models on them and comparing their performance with state-of-theart methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_14",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_14@1",
            "content": "Models trained on parallel data significantly outperform other models in terms of automatic metrics and human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_14",
            "start": 156,
            "end": 276,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_15@0",
            "content": "The contributions of our work are three-fold:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_15",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_16@0",
            "content": "\u2022 We suggest a novel pipeline for collection of parallel data for the detoxification task, \u2022 We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, 1 \u2022 Using collected data we train supervised detoxification models that yield SOTA results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_16",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_17@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_17",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_18@0",
            "content": "Style Transfer Datasets When collecting nonparallel style transfer corpora, style labels often already exist in the data (e.g. positive and negative reviews (Li et al., 2018)) or its source serves as a label (e.g. Twitter, academic texts, legal documents, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_18",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_18@1",
            "content": "Thus, data collection is reduced to fetching the texts from their sources, and the corpus size depends only on the available amount of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_18",
            "start": 263,
            "end": 402,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_19@0",
            "content": "Conversely, parallel corpora are usually more difficult to get.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_19",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_19@1",
            "content": "There exist parallel style transfer datasets fetched from \"naturally\" available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_19",
            "start": 64,
            "end": 365,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_20@0",
            "content": "Besides these special cases, there exists a large style transfer dataset that was created from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_20",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_20@1",
            "content": "This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_20",
            "start": 104,
            "end": 252,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_20@2",
            "content": "Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference -we replace expert validation of crowdsourced sentences with crowd validation and additionally optimize the cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_20",
            "start": 254,
            "end": 486,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_21@0",
            "content": "The vast majority of style transfer models (including detoxification models) are trained on non-parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_21",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_21@1",
            "content": "They can perform pointwise corrections of stylemarked words (Li et al., 2018;Wu et al., 2019;Malmi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_21",
            "start": 111,
            "end": 223,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_21@2",
            "content": "Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017;Fu et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_21",
            "start": 225,
            "end": 408,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_21@3",
            "content": "As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_21",
            "start": 410,
            "end": 709,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_22@0",
            "content": "Detoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labeled for toxicity and considers toxic and neutral sentences as two subcorpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_22",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_22@1",
            "content": "Laugier et al. (2021) use the Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 for training, Nogueira dos Santos et al. ( 2018) create their own toxicity-labelled datasets of sentences from Reddit and Twitter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_22",
            "start": 211,
            "end": 430,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_22@2",
            "content": "Following them, we also fetch sentences for rewriting from these datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_22",
            "start": 432,
            "end": 505,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@0",
            "content": "Works on detoxification often rely on style transfer models tested on other domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@1",
            "content": "Nogueira dos Santos et al. (2018) follow Shen et al. (2017) and Fu et al. (2018) and train an autoencoder with additional style classification and cycle-consistency losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 85,
            "end": 256,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@2",
            "content": "Laugier et al. (2021) perform a similar finetuning of T5 as a denoising autoencoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 258,
            "end": 341,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@3",
            "content": "Tran et al. (2020) apply pointwise corrections approach similar to that of Wu et al. (2019) and then improve the fluency of a text with a seq2seq model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 343,
            "end": 494,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@4",
            "content": "Likewise, Dale et al. (2021) use a masked language model to perform pointwise edits of toxic sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 496,
            "end": 598,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_23@5",
            "content": "They also suggest an alternative model which enhances a style-agnostic seq2seq model with style-informed language models which reweigh the seq2seq hypotheses with respect to the desired style.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_23",
            "start": 600,
            "end": 791,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_24@0",
            "content": "When the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models to perform style transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_24",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_24@1",
            "content": "We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_24",
            "start": 177,
            "end": 259,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_25@0",
            "content": "Data Collection Pipeline",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_25",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_26@0",
            "content": "Our goal is to yield pairs of sentences that have the same meanings and are contrasted in terms of Rewrite this text so that it does not sound offensive and its meaning stays the same You realize that's stupid, don't you? offensiveness -one of the sentences is toxic and the other is neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_26",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_26@1",
            "content": "We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_26",
            "start": 293,
            "end": 444,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_26@2",
            "content": "Unlike a similar work of Rao and Tetreault (2018), we hire crowd workers not only for the generation of paraphrases but also for their validation, which reduces both time and cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_26",
            "start": 446,
            "end": 625,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_27@0",
            "content": "Crowdsourcing Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_27",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_28@0",
            "content": "We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_28",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_28@1",
            "content": "Each task is implemented as a separate crowdsourcing project.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_28",
            "start": 107,
            "end": 167,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_28@2",
            "content": "We use the crowdsourcing platform Yandex.Toloka.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_28",
            "start": 169,
            "end": 216,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_28@3",
            "content": "2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_28",
            "start": 218,
            "end": 218,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_29@0",
            "content": "Task 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_29",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_29@1",
            "content": "However, it is not always possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_29",
            "start": 180,
            "end": 214,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_29@2",
            "content": "Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_29",
            "start": 216,
            "end": 321,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_29@3",
            "content": "Moreover, in some cases toxicity cannot be removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_29",
            "start": 323,
            "end": 373,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_29@4",
            "content": "Consider the examples:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_29",
            "start": 375,
            "end": 396,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_30@0",
            "content": "\u2022 Are you that dumb you can't figure it out? \u2022 I've finally understood that wiki is nothing but a bunch of American racists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_30",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_31@0",
            "content": "Not only the form but also the content of the messages are offensive, so trying to detoxify them would inevitably lead to a substantial change of sense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_31",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_31@1",
            "content": "We prefer not to include such cases in the parallel dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_31",
            "start": 153,
            "end": 212,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_31@2",
            "content": "If workers have to detoxify all inputs without a possibility to skip them, a large proportion of the generated paraphrases will be of low quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_31",
            "start": 214,
            "end": 359,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_31@3",
            "content": "Thus, we add the control \"I can't rewrite the text\" and optional controls to indicate the reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_31",
            "start": 361,
            "end": 458,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_32@0",
            "content": "Task 2: Content Preservation Check We show users the generated paraphrases along with their original variants and ask them to indicate if they have close meanings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_32",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_32@1",
            "content": "Besides ensuring content preservation, this task implicitly filters out senseless outputs, because they do not keep the original content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_32",
            "start": 164,
            "end": 300,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_32@2",
            "content": "The task interface is shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_32",
            "start": 302,
            "end": 341,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_33@0",
            "content": "Task 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_33",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_33@1",
            "content": "We ask users to indicate if the paraphrases contain any offense or swear words (see Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_33",
            "start": 88,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_34@0",
            "content": "In addition to filtering out unsuitable paraphrases, we use Tasks 2 and 3 for paying for Task 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_34",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_34@1",
            "content": "We accept or reject the generated paraphrases based on the labels they get in Tasks 2 and 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_34",
            "start": 97,
            "end": 188,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_35@0",
            "content": "Pipelines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_35",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_36@0",
            "content": "Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_36",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_36@1",
            "content": "We fetch them from corpora labeled for toxicity and additionally filter them with a toxicity classifier (described in Section 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_36",
            "start": 101,
            "end": 231,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_36@2",
            "content": "The overall data collection pipeline (see Figure 4) is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_36",
            "start": 233,
            "end": 298,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_37@0",
            "content": "\u2022 Select toxic sentences for rewriting, \u2022 Feed the sentences to Task 1, \u2022 Feed the paraphrases generated in Task 1 to Task 2, \u2022 Feed the paraphrases which passed Task 2 to Task 3, \u2022 Pay for paraphrases from Task 1, if they passed checks in Task 2 and Task 3, \u2022 Pay for \"I can't rewrite\" answers in Task 1 if two or more workers agreed on them. Retrieval Pipeline The generation pipeline can be used for cases when no parallel data is available. However, we suggest that a sufficiently large parallel corpus of paraphrases can contain pairs of sentences belonging to different styles, and it is possible to distill such corpus into a style transfer dataset. We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_37",
            "start": 0,
            "end": 764,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@0",
            "content": "We partially reuse the previously described setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@1",
            "content": "We do not need Task 1 since both toxic and neutral sentences are already available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 51,
            "end": 133,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@2",
            "content": "However, we run Task 3 twice, because we need to check both parts of the pair for toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 135,
            "end": 225,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@3",
            "content": "Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 227,
            "end": 385,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@4",
            "content": "The parallel data retrieval pipeline is shown in Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 387,
            "end": 444,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@5",
            "content": "It is simpler because Tasks 2 and 3 do not serve for paying for the generated paraphrases and are only used for data filtering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 446,
            "end": 572,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_38@6",
            "content": "The pipeline is as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_38",
            "start": 574,
            "end": 600,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_39@0",
            "content": "\u2022 Select a pair of sentences (toxic and non-toxic) from the parallel data, \u2022 Feed the toxic sentence candidate to Task 3 to make sure it is toxic, \u2022 Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, \u2022 Feed both sentences to Task 2 to check if their content matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_39",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_40@0",
            "content": "Crowdsourcing Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_40",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@0",
            "content": "Preprocessing To pre-select toxic sentences, we need a toxicity classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@1",
            "content": "We fine-tune a RoBERTa model (Liu et al., 2019) 3 on half of the three merged Jigsaw datasets (Jigsaw, 2018(Jigsaw, , 2019(Jigsaw, , 2020 (1 million sentences) and get a classifier which yields the F 1 -score of 0.76 on the Jigsaw test set (Jigsaw, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 76,
            "end": 330,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@2",
            "content": "We consider a sentence toxic if the classifier confidence is above 0.8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 332,
            "end": 402,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@3",
            "content": "To make the sentences easier for reading and rewriting, we choose the ones consisting of 5 to 20 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 404,
            "end": 507,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@4",
            "content": "For the retrieval pipeline, we also select parallel sentences with the cosine similarity of embeddings between 0.65 and 0.8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 509,
            "end": 632,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@5",
            "content": "The similarity scores were provided as a part of ParaNMT dataset, the embeddings come from the PARAGRAM-PHRASE model (Wieting et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 634,
            "end": 773,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_41@6",
            "content": "Based on a manual validation, sentences with lower similarity are often not exact paraphrases, and too-similar sentences are either both toxic or both non-toxic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_41",
            "start": 775,
            "end": 935,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@0",
            "content": "Quality Control To perform paid tasks, users need to pass training and exam sets of tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@1",
            "content": "Each of them has a corresponding skill -the percentage of correct answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 91,
            "end": 164,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@2",
            "content": "It is assigned to a user upon completing training or exam and serves for filtering out low-performing users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 166,
            "end": 273,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@3",
            "content": "Besides that, users are occasionally given control questions during labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 275,
            "end": 351,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@4",
            "content": "They serve for computing the labeling skill which can be used for banning low-performing and rewarding well-performing workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 353,
            "end": 479,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@5",
            "content": "The overall training and control pipeline is shown in Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 481,
            "end": 543,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_42@6",
            "content": "It is used in Tasks 2 and 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_42",
            "start": 545,
            "end": 572,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@0",
            "content": "In Task 1 we perform different quality control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@1",
            "content": "We ban users who submit answers which are: (i) a copy of the input, (ii) too short (< 3 tokens) or too long (more than doubled original length), (iii) contain too many rare words or non-words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 48,
            "end": 239,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@2",
            "content": "The latter condition is checked as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 241,
            "end": 283,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@3",
            "content": "We compute the ratio of the number of whitespace-separated tokens and the number of tokens identified by the BPE tokeniser (Sennrich et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 285,
            "end": 431,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@4",
            "content": "4 The rationale behind this check is that the BPE tokenizer tends to divide rare words into multiple tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 433,
            "end": 540,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@5",
            "content": "If the number of BPE tokens in a sentence is two times more than the number of regular tokens, it might indicate the presence of non-words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 542,
            "end": 680,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_43@6",
            "content": "We filter out these answers and ban users who produce them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_43",
            "start": 682,
            "end": 740,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_44@0",
            "content": "In addition to that, we ban malicious workers using built-in Yandex.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_44",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_44@1",
            "content": "Toloka tools: (i) captcha, (ii) number of skipped questions -we ban users who skip 10 task pages in a row, and (iii) task completion time -we ban those who accomplish tasks too fast (this usually means that they choose a random answer without reading).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_44",
            "start": 68,
            "end": 319,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_45@0",
            "content": "Payment In Yandex.Toloka, a worker is paid for a page that can have multiple tasks (the number is set by customer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_45",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_45@1",
            "content": "In Task 1, a page contains 5 tasks and costs $0.02.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_45",
            "start": 116,
            "end": 166,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_45@2",
            "content": "In Tasks 2 and 3, we pay $0.02 and $0.01, respectively, for 12 tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_45",
            "start": 168,
            "end": 236,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_45@3",
            "content": "In addition to that, in these tasks, we use skill-based payment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_45",
            "start": 238,
            "end": 301,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_45@4",
            "content": "If a worker has the labeling skill of above 90%, the payment is increased to $0.03 (Task 2) and $0.02 (Task 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_45",
            "start": 303,
            "end": 413,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_46@0",
            "content": "Tasks 2 and 3 are paid instantly, whereas in Task 1 we check the paraphrases before paying.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_46",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_46@1",
            "content": "If a worker indicated that a sentence cannot be paraphrased, we pay for this answer only if at least one other worker agreed with that.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_46",
            "start": 92,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_46@2",
            "content": "If a worker typed in a paraphrase, we send it to Tasks 2 and 3 and pay only for the ones approved by both tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_46",
            "start": 228,
            "end": 339,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_46@3",
            "content": "The payment procedure is shown in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_46",
            "start": 341,
            "end": 383,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_47@0",
            "content": "Postprocessing To ensure the correctness of labeling, we ask several workers to label each example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_47",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_47@1",
            "content": "In Task 1, this gives us multiple paraphrases and also verifies the \"I can't rewrite\" answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_47",
            "start": 100,
            "end": 193,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_47@2",
            "content": "For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_47",
            "start": 195,
            "end": 431,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_47@3",
            "content": "The number of people to label an example ranges from 3 to 5 depending on the workers' agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_47",
            "start": 433,
            "end": 528,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_48@0",
            "content": "Dawid-Skene aggregation returns the final label and its confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_48",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_48@1",
            "content": "To improve the quality of the data, we accept only labels with the confidence of over 90% and do not include the rest in the final data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_48",
            "start": 68,
            "end": 203,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_49@0",
            "content": "The Pipeline Scalability",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_49",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_50@0",
            "content": "The Yandex.Toloka platform has an interface in English and workers from a large number of countries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_50",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_50@1",
            "content": "Workers can be filtered by their location and asked to pass built-in language tests (available for many languages) to ensure the knowledge of a particular language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_50",
            "start": 101,
            "end": 264,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_50@2",
            "content": "This enables the use of Toloka for the creation of NLP resources in many languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_50",
            "start": 266,
            "end": 348,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_51@0",
            "content": "In our work, crowd workers manually rephrase sentences from non-parallel datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_51",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_51@1",
            "content": "The pipeline does not require any specific data format and can be applied to any text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_51",
            "start": 83,
            "end": 168,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_51@2",
            "content": "The only prerequisites are to define the source and target styles and to formulate the task of transferring between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_51",
            "start": 170,
            "end": 290,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_51@3",
            "content": "Thus, we believe that the pipeline is suitable for creating parallel datasets for any other style transfer tasks, at least those which have non-parallel datasets and clear definitions of style (positive \u2194 negative, complex \u2194 simple, impolite \u2194 polite, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_51",
            "start": 292,
            "end": 549,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_52@0",
            "content": "We should admit that our pipeline suggests the availability of (non-parallel) datasets in the chosen styles or at least publicly available sources of such data (e.g. social networks, question answering platforms).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_52",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_52@1",
            "content": "However, this is also a prerequisite for any style transfer model trained on non-parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_52",
            "start": 214,
            "end": 308,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_52@2",
            "content": "Therefore, any work on style transfer suggests that there exists enough data in the chosen style pair and language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_52",
            "start": 310,
            "end": 424,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_52@3",
            "content": "This should not be considered a specific limitation of the pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_52",
            "start": 426,
            "end": 493,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_53@0",
            "content": "ParaDetox: Generated Paraphrases",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_53",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@0",
            "content": "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by Nogueira dos Santos et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@1",
            "content": "We selected 7,000 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 168,
            "end": 279,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@2",
            "content": "We get paraphrases for 12,610 toxic sentences (on average 1.66 paraphrases per sentence), 20,437 paraphrases total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 281,
            "end": 395,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@3",
            "content": "Running 1,000 input sentences through the pipeline costs $41.2, and the cost of one output sample is $0.07.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 397,
            "end": 503,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@4",
            "content": "The overall cost of the dataset is $811.55.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 505,
            "end": 547,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_54@5",
            "content": "We give them examples of sentences in Appendix A. In addition to that, we provide some samples which could not be detoxified in Appendix C. The statistics of the paraphrases written by crowd workers are presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_54",
            "start": 549,
            "end": 772,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@0",
            "content": "The distribution of sentences from different datasets in the final data is not equal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@1",
            "content": "Jigsaw turned out to be the most difficult to paraphrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 86,
            "end": 142,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@2",
            "content": "Fewer sentences from it are successfully paraphrased, making it the most expensive part of the collected corpus ($0.08 per sample).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 144,
            "end": 274,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@3",
            "content": "Figure 7 shows that the number of untransferable sentences in the Jigsaw dataset is larger than that of other corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 276,
            "end": 393,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@4",
            "content": "Out of all crowdsourced paraphrases, only a small part was of high quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 395,
            "end": 469,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@5",
            "content": "We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 471,
            "end": 575,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@6",
            "content": "It also corroborates the difficulty of the Jigsaw dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 577,
            "end": 634,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_55@7",
            "content": "While the overall number of generated paraphrases was slightly higher for it, much more of them were discarded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_55",
            "start": 636,
            "end": 746,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_56@0",
            "content": "Analysis of Edits",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_56",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@0",
            "content": "Although we did not give any special instructions to workers about editing, they often followed the minimal editing principle, making 1.36 changes per sentence on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@1",
            "content": "A change is deletion, insertion, or rewriting of a word or multiple adjacent words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 172,
            "end": 254,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@2",
            "content": "Many of the changes are supposedly deletions because the average sentence length drops from 12.1 to 10.4 words after editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 256,
            "end": 380,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@3",
            "content": "The nature of editing differs for the three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 382,
            "end": 434,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@4",
            "content": "We compute the percentage of edits which consisted of removing the most common swear words or replacing them with neutral words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 436,
            "end": 563,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@5",
            "content": "We first define the differences between the original and transformed string with the difflib Python library and then compute the percentage of differences that consist in editing swear words and other (non-offensive) words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 565,
            "end": 787,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@6",
            "content": "We use a small manually compiled list of swear words which includes words f*ck, sh*t, a*s, b*tch, d*mn and their variants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 789,
            "end": 910,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_57@7",
            "content": "Table 3 shows that the deletion or replacements of the most common swearing constituted a large part of all edits for Reddit and Twitter datasets (22% and 30%), while for Jigsaw it was only 3%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_57",
            "start": 912,
            "end": 1104,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@0",
            "content": "Another surprisingly common type of editing is the normalization of sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@1",
            "content": "The users often fixed casing, punctuation, typos (e.g. dont \u2192 don't, there's \u2192 there is).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 79,
            "end": 167,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@2",
            "content": "They also tended to replace colloquial phrases with more formal and standard language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 169,
            "end": 254,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@3",
            "content": "Finally, some users overcorrected the sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 256,
            "end": 303,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@4",
            "content": "For example, they replaced neutral words such as dead, murder, penis with euphemisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 305,
            "end": 389,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_58@5",
            "content": "This tendency indicates that workers consider any sensitive topic to be inappropriate content and try to avoid it as much as possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_58",
            "start": 391,
            "end": 524,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_59@0",
            "content": "ParaNMT: Existing Paraphrases",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_59",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_60@0",
            "content": "Our automatic filtering of ParaNMT for content yields 500,000 potentially detoxifying sentence pairs, which is 1% of the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_60",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_60@1",
            "content": "We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_60",
            "start": 129,
            "end": 248,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_60@2",
            "content": "This leaves with 1,393 sentences, meaning that around 23% of the pre-selected sentence pairs were approved (for ParaDetox we get paraphrases for 61% input sentences).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_60",
            "start": 250,
            "end": 415,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_60@3",
            "content": "Thus, although the cost per 1,000 inputs is much lower than that of generating the paraphrases, the cost per output sample is the same as that of generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_60",
            "start": 417,
            "end": 584,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_61@0",
            "content": "ParaNMT dataset is different from ParaDetox.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_61",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_61@1",
            "content": "First, each sentence has only one paraphrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_61",
            "start": 45,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_61@2",
            "content": "These paraphrases were not gained via manual editing but via a chain of translation models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_61",
            "start": 91,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_61@3",
            "content": "Thus, neutral sentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_61",
            "start": 183,
            "end": 339,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_62@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_62",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_63@0",
            "content": "To evaluate the collected corpora, we use them to train several supervised detoxification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_63",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_63@1",
            "content": "We separate the ParaDetox dataset into training and test parts (11,939 and 671 sentence pairs, respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_63",
            "start": 98,
            "end": 206,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_63@2",
            "content": "The test sentences have one reference per sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_63",
            "start": 208,
            "end": 258,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_63@3",
            "content": "We manually validate the test set to exclude the appearance of non-detoxifiable sentences or sentences which stayed toxic after rewriting (we need to verify that since the corpus was generated via crowdsourcing only).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_63",
            "start": 260,
            "end": 476,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_63@4",
            "content": "We do not use the test set neither for training nor for parameter selection of the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_63",
            "start": 478,
            "end": 567,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_64@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_64",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_65@0",
            "content": "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020) 5 on our data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_65",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_65@1",
            "content": "We test BART trained on the following datasets:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_65",
            "start": 91,
            "end": 137,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_66@0",
            "content": "5 We use model https://huggingface.co/facebook/bart-base",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_66",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_67@0",
            "content": "\u2022 ParaDetox -our full crowdsourced dataset. \u2022 ParaDetox-unique -a subset of ParaDetox where each toxic sentence has only one paraphrase (selected randomly). \u2022 ParaDetox-1000 -1,000 samples from the crowdsourced dataset (distributed evenly across data sources, each toxic sample has multiple non-toxic variants). \u2022 ParaNMT -filtered ParaNMT corpus, auto stands for automatically filtered 500,000 samples, manual are 1,393 manually selected sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_67",
            "start": 0,
            "end": 453,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_68@0",
            "content": "We train BART for 10,000 epochs with the learning rate of 3e-5 and the number of gradient accumulation steps set to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_68",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_68@1",
            "content": "The other parameters are set to their default values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_68",
            "start": 119,
            "end": 171,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_69@0",
            "content": "We also compare our models to other style transfer approaches:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_69",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_70@0",
            "content": "\u2022",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_70",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_71@0",
            "content": "Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_71",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_72@0",
            "content": "We compute the BLEU score on the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_72",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_72@1",
            "content": "In addition to that, we perform automatic referencefree evaluation which is used in many style transfer works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_72",
            "start": 43,
            "end": 152,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_72@2",
            "content": "Namely, we evaluate:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_72",
            "start": 154,
            "end": 173,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_73@0",
            "content": "\u2022 Style accuracy (STA) -percentage of nontoxic outputs identified by a style classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_73",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@0",
            "content": "We use a classifier from Section 3.3 trained on a different half of Jigsaw data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@1",
            "content": "\u2022 Content preservation (SIM) -cosine similarity between the embeddings of the original text and the output computed with the model of Wieting et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 81,
            "end": 236,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@2",
            "content": "This model is trained on paraphrase pairs extracted from ParaNMT corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 238,
            "end": 309,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@3",
            "content": "The model's training objective is to yield embeddings such that the similarity of embeddings of paraphrases is higher than the similarity between sentences that are not paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 311,
            "end": 491,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@4",
            "content": "\u2022 Fluency (FL) -percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 493,
            "end": 660,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_74@5",
            "content": "We compute the final joint metric (J) as the multiplication of the three individual metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_74",
            "start": 662,
            "end": 753,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_75@0",
            "content": "Since the automatic evaluation can be unreliable, we evaluate some models manually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_75",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_75@1",
            "content": "We randomly select 200 sentences from the test set and ask assessors to evaluate them along the same three parameters: style accuracy (STA m ), content preservation (SIM m ), and fluency (FL m ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_75",
            "start": 84,
            "end": 278,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_75@2",
            "content": "All parameters can take values of 1 (good) and 0 (bad).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_75",
            "start": 280,
            "end": 334,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_75@3",
            "content": "We also report the joint metric J m which is the percentage of sentences whose STA m , SIM m , and FL m are 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_75",
            "start": 336,
            "end": 445,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_76@0",
            "content": "The evaluation was conducted by 6 NLP researchers with a good command of English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_76",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_76@1",
            "content": "Each sample was evaluated by 3 assessors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_76",
            "start": 82,
            "end": 122,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_76@2",
            "content": "The interannotator agreement (Krippendorff's \u03b1) reaches 0.64 (STA m ), 0.67 (SIM m ), and 0.68 (FL m ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_76",
            "start": 124,
            "end": 226,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_77@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_77",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_78@0",
            "content": "Automatic Evaluation Table 4 shows the automatic scores of all tested models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_78",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_78@1",
            "content": "Our BART models trained on ParaDetox outperform other systems in terms of BLEU and J. The much lower scores of BART-zero-shot confirm that this success is due to fine-tuning and not the innate ability of BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_78",
            "start": 78,
            "end": 286,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_78@2",
            "content": "The majority of unsupervised SOTA approaches are not only worse than BART but also perform below the \"change nothing\" baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_78",
            "start": 288,
            "end": 414,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_78@3",
            "content": "The closest competitor of our models is the Delete model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_78",
            "start": 416,
            "end": 472,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_78@4",
            "content": "This can be explained by the fact that crowd workers often only remove or replaced swear words which is what the Delete model does.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_78",
            "start": 474,
            "end": 604,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_79@0",
            "content": "When comparing models trained on supervised data, we can see that BART does not benefit from multiple detoxifications per sentence, its performance is the same when trained on ParaDetox and ParaDetox-unique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_79",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_79@1",
            "content": "On the other hand, manual filtering of ParaNMT is beneficial, it increases the quality of BART trained on it, although the number of training sentences drops from 500,000 to 1,400.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_79",
            "start": 208,
            "end": 387,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@0",
            "content": "We also check which amount of data is sufficient for a high detoxification quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@1",
            "content": "We train the BART model on subsets of ParaDetox of different sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 84,
            "end": 150,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@2",
            "content": "Figure 9 and the performance of ParaDetox-1000 model (Table 4) show that 1,000 training samples is enough to get a good detoxification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 152,
            "end": 286,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@3",
            "content": "While SIM and FL are already high for vanilla BART (see BART-zero-shot model), STA can be improved with only a few parallel examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 288,
            "end": 420,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@4",
            "content": "This suggests that style transfer does not need large parallel corpora, making our pipeline more useful for other style transfer tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 422,
            "end": 556,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@5",
            "content": "However, this is the result of the automatic evaluation, which as we show below is not always reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 558,
            "end": 660,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@6",
            "content": "It needs extra investigation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 662,
            "end": 690,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@7",
            "content": "Manual Evaluation Manual evaluation (Table 6) confirms the usefulness of parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 692,
            "end": 778,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@8",
            "content": "BARTs trained on parallel data outperform other competitors, even if the size of this data is small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 780,
            "end": 879,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@9",
            "content": "However, manual and automatic evaluations do not always match.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 881,
            "end": 942,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@10",
            "content": "Here, the well-performing Delete model gets the lowest score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 944,
            "end": 1004,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@11",
            "content": "Overall, assessors agree with automatic metrics only in terms of fluency, their Spearman correlation r is 0.89.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 1006,
            "end": 1116,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@12",
            "content": "The manual style accuracy and content preservation are only moderately correlated with their automatic counterparts leaving space for further improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 1118,
            "end": 1272,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@13",
            "content": "J and J m almost do not correlate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 1274,
            "end": 1307,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@14",
            "content": "Besides that, BLEU correlates only with content preservation score and is moderately inversely correlated with the style accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 1309,
            "end": 1438,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_80@15",
            "content": "Thus, BLEU measures only the degree of content preservation and cannot replace other metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_80",
            "start": 1440,
            "end": 1532,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_81@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_81",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_82@0",
            "content": "We present ParaDetox -an English parallel corpus for the detoxification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_82",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_82@1",
            "content": "It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_82",
            "start": 78,
            "end": 170,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_82@2",
            "content": "To the best of our knowledge, this is the first parallel detoxification dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_82",
            "start": 172,
            "end": 251,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_82@3",
            "content": "We present a novel data collection pipeline and show that parallel data can be generated using only crowdsourcing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_82",
            "start": 253,
            "end": 366,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_82@4",
            "content": "We also adopt this pipeline to the style-based distillation of paraphrase corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_82",
            "start": 368,
            "end": 448,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_83@0",
            "content": "We confirm the usefulness of our datasets by training sequence-to-sequence models on them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_83",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_83@1",
            "content": "The experiments show that the use of parallel data yields models which significantly outperform style transfer models trained on non-parallel data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_83",
            "start": 91,
            "end": 237,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_83@2",
            "content": "Besides that, we confirm that filtering the noisy parallel data can lead to considerable improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_83",
            "start": 239,
            "end": 339,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_84@0",
            "content": "We see that it is enough to get 1,000 parallel sentences to perform detoxification with high quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_84",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_84@1",
            "content": "This suggests that our pipeline can be successfully applied to create useful parallel resources for style transfer even in cases of limited finance or lack of crowd workers because the cost of generating 1,000 examples is very low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_84",
            "start": 102,
            "end": 332,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_85@0",
            "content": "Finally, we investigate the relationship between metrics and find that automatic evaluation does not always match the manual judgments and referencebased BLEU cannot replace human evaluation, because it measures content preservation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_85",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@0",
            "content": "The research on toxicity raises some ethical issues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@1",
            "content": "In terms of our work, the parallel corpus we created can indeed be used in the reverse direction, i.e. to \"toxify\" sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 53,
            "end": 177,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@2",
            "content": "However, although we did not thoroughly evaluate the quality of such toxification, our intuition is that it would not be high enough to make the corrupted sentences look natural.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 179,
            "end": 356,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@3",
            "content": "The reason is that the toxic part of our corpus consists of real toxic sentences fetched on the Internet, whereas their non-toxic counterparts are \"translations\" performed by crowd workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 358,
            "end": 546,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@4",
            "content": "We suggest that they obey the common regularities observed for translationese (texts manually translated from their original language into a different one): they differ from regular texts in terms of vocabulary (Koppel and Ordan, 2011) and syntax (Lembersky et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 548,
            "end": 819,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@5",
            "content": "The manually detoxified texts are different from the original non-toxic texts written by Internet users from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 821,
            "end": 937,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_86@6",
            "content": "While they are still recognized by human assessors as plausible sentences, we suggest that a sequence-to-sequence model trained to get translationese as input would not be as successful in transforming real texts (as it was shown for machine translation models (Freitag et al., 2019)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_86",
            "start": 939,
            "end": 1223,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_87@0",
            "content": "Thus, although our corpus can be used in the reverse direction, it is not symmetric, which makes it less efficient as training datasets for \"toxifiers\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_87",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_87@1",
            "content": "However, we should emphasize that these statements are our hypotheses and should be further investigated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_87",
            "start": 153,
            "end": 257,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_87@2",
            "content": "Finally, we argue that the risk of using our corpus for toxification is perhaps not game-changing, as simpler approaches based on patterns (e.g. including a set of predefined obscene fragments into neutral texts) can serve the same purpose relatively well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_87",
            "start": 259,
            "end": 514,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_88@0",
            "content": "Luke Breitfeller, Emily Ahn, David Jurgens, Yulia Tsvetkov, Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_88",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_89@0",
            "content": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault, Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_89",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_90@0",
            "content": "Keith Carlson, Allen Riddell, Daniel Rockmore, Evaluating prose style transfer with the bible, 2018, Royal Society Open Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_90",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_91@0",
            "content": "David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko, Text detoxification using large pre-trained neural models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_91",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_92@0",
            "content": "Alexander Dawid, Allan Skene, Maximum likelihood estimation of observer error-rates using the em algorithm, 1979, Journal of The Royal Statistical Society Series C-applied Statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_92",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_93@0",
            "content": "Markus Freitag, Isaac Caswell, Scott Roy, APE at scale and its implications on MT evaluation biases, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_93",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_94@0",
            "content": "Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Style transfer in text: Exploration and evaluation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_94",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_95@0",
            "content": "Junxian He, Xinyi Wang, Graham Neubig, Taylor Berg-Kirkpatrick, A probabilistic formulation of unsupervised text style transfer, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_95",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_96@0",
            "content": "UNKNOWN, None, 2018, Toxic comment classification challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_96",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_97@0",
            "content": "UNKNOWN, None, 2019, Jigsaw unintended bias in toxicity classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_97",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2020, Jigsaw multilingual toxic comment classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_98",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_99@0",
            "content": "Moshe Koppel, Noam Ordan, Translationese and its dialects, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_99",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_100@0",
            "content": "L\u00e9o Laugier, John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Civil rephrases of toxic texts with self-supervised transformers, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_100",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_101@0",
            "content": "Joosung Lee, Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer, 2020, Proceedings of the 13th International Conference on Natural Language Generation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_101",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_102@0",
            "content": "Gennadi Lembersky, Noam Ordan, Shuly Wintner, Language models for machine translation: Original vs, 2011, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_102",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_103@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_103",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_104@0",
            "content": "Juncen Li, Robin Jia, He He, Percy Liang, Delete, retrieve, generate: a simple approach to sentiment and style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_104",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_105@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_105",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_106@0",
            "content": "Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui, A dual reinforcement learning framework for unsupervised text style transfer, 2019-08-10, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_106",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_107@0",
            "content": "Eric Malmi, Aliaksei Severyn, Sascha Rothe, Unsupervised text style transfer with padded masked language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_107",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_108@0",
            "content": "Cicero Nogueira Dos Santos, Igor Melnyk, Inkit Padhi, Fighting offensive language on social media with unsupervised text style transfer, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_108",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_109@0",
            "content": "Carla Perez Almendros, Luis Espinosa Anke, Steven Schockaert, Don't patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_109",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_110@0",
            "content": "Reid Pryzant, Richard Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, Diyi Yang, Automatically neutralizing subjective bias in text, 2020, Proceedings of the aaai conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_110",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_111@0",
            "content": "Sudha Rao, Joel Tetreault, Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_111",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_112@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_112",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_113@0",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_113",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_114@0",
            "content": "Minh Tran, Yipeng Zhang, Mohammad Soleymani, Towards a friendly online community: An unsupervised style transfer framework for profanity redaction, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_114",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_115@0",
            "content": "Alex Warstadt, Amanpreet Singh, Samuel , Neural network acceptability judgments, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_115",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_116@0",
            "content": "John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Towards universal paraphrastic sentence embeddings, 2016-05-02, 4th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_116",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_117@0",
            "content": "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, Graham Neubig, Beyond BLEU:training neural machine translation with semantic similarity, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_117",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_118@0",
            "content": "John Wieting, Kevin Gimpel, ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_118",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_119@0",
            "content": "Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, Songlin Hu, Mask and infill: Applying masked language model for sentiment transfer, 2019, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_119",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_120@0",
            "content": "Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval), 2019, Proceedings of the 13th International Workshop on Semantic Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_120",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "31-ARR_v2_121@0",
            "content": "Yi Zhang, Tao Ge, Xu Sun, Parallel data augmentation for formality style transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "31-ARR_v2_121",
            "start": 0,
            "end": 178,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_1",
            "tgt_ix": "31-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_1",
            "tgt_ix": "31-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_1",
            "tgt_ix": "31-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_3",
            "tgt_ix": "31-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_5",
            "tgt_ix": "31-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_7",
            "tgt_ix": "31-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_8",
            "tgt_ix": "31-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_10",
            "tgt_ix": "31-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_11",
            "tgt_ix": "31-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_12",
            "tgt_ix": "31-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_14",
            "tgt_ix": "31-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_15",
            "tgt_ix": "31-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_9",
            "tgt_ix": "31-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_18",
            "tgt_ix": "31-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_19",
            "tgt_ix": "31-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_21",
            "tgt_ix": "31-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_22",
            "tgt_ix": "31-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_20",
            "tgt_ix": "31-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_24",
            "tgt_ix": "31-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_26",
            "tgt_ix": "31-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_28",
            "tgt_ix": "31-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_31",
            "tgt_ix": "31-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_32",
            "tgt_ix": "31-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_33",
            "tgt_ix": "31-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_34",
            "tgt_ix": "31-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_36",
            "tgt_ix": "31-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_44",
            "tgt_ix": "31-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_46",
            "tgt_ix": "31-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_47",
            "tgt_ix": "31-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_48",
            "tgt_ix": "31-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_50",
            "tgt_ix": "31-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_51",
            "tgt_ix": "31-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_49",
            "tgt_ix": "31-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_49",
            "tgt_ix": "31-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_49",
            "tgt_ix": "31-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_49",
            "tgt_ix": "31-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_52",
            "tgt_ix": "31-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_53",
            "tgt_ix": "31-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_53",
            "tgt_ix": "31-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_53",
            "tgt_ix": "31-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_56",
            "tgt_ix": "31-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_56",
            "tgt_ix": "31-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_56",
            "tgt_ix": "31-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_60",
            "tgt_ix": "31-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_59",
            "tgt_ix": "31-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_59",
            "tgt_ix": "31-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_59",
            "tgt_ix": "31-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_61",
            "tgt_ix": "31-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_65",
            "tgt_ix": "31-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_66",
            "tgt_ix": "31-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_68",
            "tgt_ix": "31-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_69",
            "tgt_ix": "31-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_72",
            "tgt_ix": "31-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_75",
            "tgt_ix": "31-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_76",
            "tgt_ix": "31-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_79",
            "tgt_ix": "31-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_77",
            "tgt_ix": "31-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_77",
            "tgt_ix": "31-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_77",
            "tgt_ix": "31-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_77",
            "tgt_ix": "31-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_83",
            "tgt_ix": "31-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_84",
            "tgt_ix": "31-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_85",
            "tgt_ix": "31-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "31-ARR_v2_0",
            "tgt_ix": "31-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_1",
            "tgt_ix": "31-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_2",
            "tgt_ix": "31-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_3",
            "tgt_ix": "31-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_3",
            "tgt_ix": "31-ARR_v2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_3",
            "tgt_ix": "31-ARR_v2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_3",
            "tgt_ix": "31-ARR_v2_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_4",
            "tgt_ix": "31-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_5",
            "tgt_ix": "31-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_5",
            "tgt_ix": "31-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_5",
            "tgt_ix": "31-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_6",
            "tgt_ix": "31-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_7",
            "tgt_ix": "31-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_8",
            "tgt_ix": "31-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_9",
            "tgt_ix": "31-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_10",
            "tgt_ix": "31-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_10",
            "tgt_ix": "31-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_11",
            "tgt_ix": "31-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_12",
            "tgt_ix": "31-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_13",
            "tgt_ix": "31-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_14",
            "tgt_ix": "31-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_14",
            "tgt_ix": "31-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_15",
            "tgt_ix": "31-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_16",
            "tgt_ix": "31-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_17",
            "tgt_ix": "31-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_18",
            "tgt_ix": "31-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_18",
            "tgt_ix": "31-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_19",
            "tgt_ix": "31-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_19",
            "tgt_ix": "31-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_20",
            "tgt_ix": "31-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_20",
            "tgt_ix": "31-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_20",
            "tgt_ix": "31-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_21",
            "tgt_ix": "31-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_21",
            "tgt_ix": "31-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_21",
            "tgt_ix": "31-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_21",
            "tgt_ix": "31-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_22",
            "tgt_ix": "31-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_22",
            "tgt_ix": "31-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_22",
            "tgt_ix": "31-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_23",
            "tgt_ix": "31-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_24",
            "tgt_ix": "31-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_24",
            "tgt_ix": "31-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_25",
            "tgt_ix": "31-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_26",
            "tgt_ix": "31-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_26",
            "tgt_ix": "31-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_26",
            "tgt_ix": "31-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_27",
            "tgt_ix": "31-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_28",
            "tgt_ix": "31-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_28",
            "tgt_ix": "31-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_28",
            "tgt_ix": "31-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_28",
            "tgt_ix": "31-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_29",
            "tgt_ix": "31-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_30",
            "tgt_ix": "31-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_31",
            "tgt_ix": "31-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_31",
            "tgt_ix": "31-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_31",
            "tgt_ix": "31-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_31",
            "tgt_ix": "31-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_32",
            "tgt_ix": "31-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_32",
            "tgt_ix": "31-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_32",
            "tgt_ix": "31-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_33",
            "tgt_ix": "31-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_33",
            "tgt_ix": "31-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_34",
            "tgt_ix": "31-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_34",
            "tgt_ix": "31-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_35",
            "tgt_ix": "31-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_36",
            "tgt_ix": "31-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_36",
            "tgt_ix": "31-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_36",
            "tgt_ix": "31-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_37",
            "tgt_ix": "31-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_38",
            "tgt_ix": "31-ARR_v2_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_39",
            "tgt_ix": "31-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_40",
            "tgt_ix": "31-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_41",
            "tgt_ix": "31-ARR_v2_41@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_42",
            "tgt_ix": "31-ARR_v2_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_43",
            "tgt_ix": "31-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_44",
            "tgt_ix": "31-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_44",
            "tgt_ix": "31-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_45",
            "tgt_ix": "31-ARR_v2_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_46",
            "tgt_ix": "31-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_46",
            "tgt_ix": "31-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_46",
            "tgt_ix": "31-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_46",
            "tgt_ix": "31-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_47",
            "tgt_ix": "31-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_47",
            "tgt_ix": "31-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_47",
            "tgt_ix": "31-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_47",
            "tgt_ix": "31-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_48",
            "tgt_ix": "31-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_48",
            "tgt_ix": "31-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_49",
            "tgt_ix": "31-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_50",
            "tgt_ix": "31-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_50",
            "tgt_ix": "31-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_50",
            "tgt_ix": "31-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_51",
            "tgt_ix": "31-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_51",
            "tgt_ix": "31-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_51",
            "tgt_ix": "31-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_51",
            "tgt_ix": "31-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_52",
            "tgt_ix": "31-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_52",
            "tgt_ix": "31-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_52",
            "tgt_ix": "31-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_52",
            "tgt_ix": "31-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_53",
            "tgt_ix": "31-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_54",
            "tgt_ix": "31-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_55",
            "tgt_ix": "31-ARR_v2_55@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_56",
            "tgt_ix": "31-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_57",
            "tgt_ix": "31-ARR_v2_57@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_58",
            "tgt_ix": "31-ARR_v2_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_59",
            "tgt_ix": "31-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_60",
            "tgt_ix": "31-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_60",
            "tgt_ix": "31-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_60",
            "tgt_ix": "31-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_60",
            "tgt_ix": "31-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_61",
            "tgt_ix": "31-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_61",
            "tgt_ix": "31-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_61",
            "tgt_ix": "31-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_61",
            "tgt_ix": "31-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_62",
            "tgt_ix": "31-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_63",
            "tgt_ix": "31-ARR_v2_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_64",
            "tgt_ix": "31-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_65",
            "tgt_ix": "31-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_65",
            "tgt_ix": "31-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_66",
            "tgt_ix": "31-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_67",
            "tgt_ix": "31-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_68",
            "tgt_ix": "31-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_68",
            "tgt_ix": "31-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_69",
            "tgt_ix": "31-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_70",
            "tgt_ix": "31-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_71",
            "tgt_ix": "31-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_72",
            "tgt_ix": "31-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_72",
            "tgt_ix": "31-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_72",
            "tgt_ix": "31-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_73",
            "tgt_ix": "31-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_74",
            "tgt_ix": "31-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_75",
            "tgt_ix": "31-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_75",
            "tgt_ix": "31-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_75",
            "tgt_ix": "31-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_75",
            "tgt_ix": "31-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_76",
            "tgt_ix": "31-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_76",
            "tgt_ix": "31-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_76",
            "tgt_ix": "31-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_77",
            "tgt_ix": "31-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_78",
            "tgt_ix": "31-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_79",
            "tgt_ix": "31-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_79",
            "tgt_ix": "31-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_80",
            "tgt_ix": "31-ARR_v2_80@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_81",
            "tgt_ix": "31-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_82",
            "tgt_ix": "31-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_83",
            "tgt_ix": "31-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_83",
            "tgt_ix": "31-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_83",
            "tgt_ix": "31-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_84",
            "tgt_ix": "31-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_84",
            "tgt_ix": "31-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_85",
            "tgt_ix": "31-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_86",
            "tgt_ix": "31-ARR_v2_86@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_87",
            "tgt_ix": "31-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_87",
            "tgt_ix": "31-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_87",
            "tgt_ix": "31-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_88",
            "tgt_ix": "31-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_89",
            "tgt_ix": "31-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_90",
            "tgt_ix": "31-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_91",
            "tgt_ix": "31-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_92",
            "tgt_ix": "31-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_93",
            "tgt_ix": "31-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_94",
            "tgt_ix": "31-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_95",
            "tgt_ix": "31-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_96",
            "tgt_ix": "31-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_97",
            "tgt_ix": "31-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_98",
            "tgt_ix": "31-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_99",
            "tgt_ix": "31-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_100",
            "tgt_ix": "31-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_101",
            "tgt_ix": "31-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_102",
            "tgt_ix": "31-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_103",
            "tgt_ix": "31-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_104",
            "tgt_ix": "31-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_105",
            "tgt_ix": "31-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_106",
            "tgt_ix": "31-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_107",
            "tgt_ix": "31-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_108",
            "tgt_ix": "31-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_109",
            "tgt_ix": "31-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_110",
            "tgt_ix": "31-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_111",
            "tgt_ix": "31-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_112",
            "tgt_ix": "31-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_113",
            "tgt_ix": "31-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_114",
            "tgt_ix": "31-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_115",
            "tgt_ix": "31-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_116",
            "tgt_ix": "31-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_117",
            "tgt_ix": "31-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_118",
            "tgt_ix": "31-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_119",
            "tgt_ix": "31-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_120",
            "tgt_ix": "31-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "31-ARR_v2_121",
            "tgt_ix": "31-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 957,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "31-ARR",
        "version": 2
    }
}