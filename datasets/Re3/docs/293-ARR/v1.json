{
    "nodes": [
        {
            "ix": "293-ARR_v1_0",
            "content": "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_2",
            "content": "We propose a framework for training nonautoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output. We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios. We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent. We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification (TS) and abstractive summarization. Our approach significantly improves output quality on both tasks and controls output complexity better on the controllable TS task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "293-ARR_v1_4",
            "content": "Neural sequence-to-sequence (seq2seq) models primarily developed and tested for machine translation (MT) Bahdanau et al. (2015); Vaswani et al. (2017); Gu et al. (2018) are increasingly used for other sequence transduction tasks. This paper focuses on editing tasks, such as post-editing of MT output Simard et al. (2007), style transfer (Jin et al., 2020), or text simplification (TS) Chandrasekar and Srinivas (1997); Xu et al. (2015), where systems directly edit the input sequence, instead of generating the output from scratch as in MT. As illustrated in Table 1, in these tasks, there might be substantial overlap in content between inputs and outputs, and also diverse rewrites, ranging from local substitutions to more complex restructuring.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_5",
            "content": "While dedicated architectures have been designed for these editing tasks, based on e.g., a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_6",
            "content": "Original: The Mauritshuis museum is staging an exhibition focusing on the 17th century selfportraits, highlighting the similarities and the differences between modern-day snapshots and historic works of art.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_7",
            "content": "Simplified: The Mauritshuis museum is now set to open an exhibit on the 17th century selfportraits. It shows the similarities and differences between modern photos and artworks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_8",
            "content": "Table 1: Text simplification is an editing task, where the output sequence overlaps with the input, while incorporating multiple types of rephrasings to restructure and simplify content. multistep, tag-then-edit approach (Alva-Manchego et al., 2017;Malmi et al., 2019;Dong et al., 2019;Mallinson et al., 2020), they can also be addressed with non-autoregressive (NAR) seq2seq models which generate their output by iteratively editing intermediate sequences (Lee et al., 2018;Gu et al., 2019;Awasthi et al., 2019;Stern et al., 2019;. NAR models hold the promise of providing a more generic solution, where the model does not need to be tailored to a given editing task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_9",
            "content": "This work is centered on the hypothesis that training NAR models for editing tasks using the same strategy as for MT leads to a mismatch between train and test settings that limits their generalization ability and output quality. Specifically, the learning algorithms designed for MT are aligned with inference strategies that generate output from an empty initial sequence. By contrast, in sequence editing tasks, the inference step is initialized instead with the original input sequence. In addition, since editing samples might range from limited lexical substitutions to more thorough rewrites, training samples cover a wide range of edit distances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_10",
            "content": "During training, the loss can thus be dominated by the more distant samples leading to undertrained models and poor generalization. By contrast, the distance between input and output samples in MT is more uniform, since it always involves at least lexical translation of the input tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_11",
            "content": "To address these issues, we introduce a new training framework, EDITING CURRICULUM, which dynamically exposes the model to more relevant edit actions during training and exploits the full spectrum of available training samples more effectively. First, we design a new roll-in strategy, EDITING roll-in, that exposes the model to intermediate sequences that it is more likely to encounter during inference. Second, we introduce a training curriculum to expose the model to training samples in order of increasing edit distance, thus gradually increasing the complexity of oracle edit operations that the model learns to imitate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_12",
            "content": "We show that our approach improves the quality of outputs on two challenging English text editing tasks: controllable TS and abstractive summarization. It also improves the degree of TS control by generating simplified outputs that match the target reading grade level better than the baselines. We conduct an extensive analysis which supports our hypothesis, and show that the sequences generated by our training policy improve exploration during training and are easier to learn from, leading to better generalization across samples with varying edit distances. Training with curriculum further improves output quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_13",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "293-ARR_v1_14",
            "content": "Model NAR edit-based models Gu et al., 2019;Stern et al., 2019;Xu and Carpuat, 2021) cast sequence editing as an iterative sequence refinement problem modeled by a Markov Decision Process Y, A, E, R, y 0 . A state y = (y 1 , y 2 , ..., y L ) \u2208 Y is a sequence of tokens where each y i represents a token from the vocabulary V, L is the sequence length and y 0 \u2208 Y is the initial sequence to be refined, using actions drawn from the set A. The reward R is based on the distance D between the generated output and the reference sequence y * \u2208 Y: R(y) = \u2212D(y, y * ). At each decoding iteration, the model takes an input y, chooses an action a \u2208 A to refine the sequence using a policy \u03c0, resulting in state E(y, a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_15",
            "content": "Models differ based on the nature of edit actions used. Models support different operations such as insertion, deletion, reposition and substitution and are trained with different types of roll-in policies (mixed/learned/expert). We select the operations from the EDITOR model based on its competitive performance on constrained decoding tasks that require editing non-empty initial sequences (Xu and Carpuat, 2021). It is a Transformer model that uses two types of actions or edits on sequences, y: (1) reposition and (2) insertion. The reposition operation, modeled by \u03c0 rps , predicts the new position of each token in the input sequence (including deletions). The insertion operation has two components: placeholder prediction, \u03c0 plh that predicts the number of placeholders to be inserted and token prediction, \u03c0 ins that generates the actual output tokens for each placeholder. At each decoding iteration, the model applies an action a that consists of a reposition and an insertion operation. This refinement process is repeated until two consecutive decoding iterations return the same output (Gu et al., 2019), or a preset maximum number of them is reached (Lee et al., 2018;Ghazvininejad et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_16",
            "content": "Training NAR models are trained via imitation learning that uses a roll-in policy to generate the sequences that the model learns to refine from and a roll-out policy to estimate the cost-to-go from the generated roll-in sequences to the desired output sequences. The latter is performed by comparing the model actions to oracle demonstrations. We summarize the policies of various NAR models proposed for MT in Table 2. For EDITOR, the roll-in sequences for the reposition (or insertion) module are stochastic mixtures of the output of the insertion(or reposition) module or a noised version of the output sequence. The oracle is the Levenshtein edit distance (Gu et al., 2019). The noisy sequence is generated by applying random word dropping (Gu et al., 2019) and random word shuffle (Lample et al., 2018) with a probability of 0.5 and maximum shuffle distance of 3. ated at inference time (Ross and Bagnell, 2010).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_17",
            "content": "Our Approach: EDITING CURRICULUM",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "293-ARR_v1_18",
            "content": "y ins = {y if u < \u03b1 else E(y, r), r \u223c \u03c0 rps } y rps = {y if u < \u03b2 else E(E(y, p * ), t), p * \u223c \u03c0 * plh , t \u223c \u03c0 ins }",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_19",
            "content": "While typically, the roll-in policy is a stochastic mixture of the model and the oracle demonstrations as described above, the noise incurred early on due to the large difference between the expert demonstration and the learner's policy actions may hurt overall performance (Brantley et al., 2019;He et al., 2012;Leblond et al., 2018). As we will see ( \u00a75), this is what happens on editing tasks when training the model to imitate experts using learned roll-in sequences. At the same time, rolling in with expert demonstrations raises its own issues, as it can limit the exploration of the search space. Motivated by these observations, we propose a new policy, EDITING, that allows exploration by injecting noise to the input sequence to generate new intermediate sequences for training. This lets the model learn to fix errors without deviating from learning the task at hand. We modify the roll-in policies to be aligned with the editing inference process, where the reposition operation is followed by insertion on the original input sequence: \u2022 The roll-in sequence for training the reposition policy, \u03c0 rps , is generated by applying noise to the original source sequence y s , i.e. y rps = {noise(y s )}; \u2022 The roll-in sequence for training the insertion policy, \u03c0 ins is an intermediate sequence generated by applying the expert reposition policy to y rps , i.e. y ins = {E(y rps , r * ), r * \u223c \u03c0 * rps }.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_20",
            "content": "To prevent undertraining when samples with large edit distances overwhelm the loss, we use a curriculum to expose the model to easy-to-learn actions first, then gradually increase the difficulty of the edit-operations performed as the learner becomes more compe-tent. Prior work on CL does not agree on standard measures of sample difficulty for seq2seq tasks (Kumar et al., 2019;Yao et al., 2021;Zhang et al., 2018; or apply curriculum learning (CL) for the different problem of shifting the training of a Transformer model from AR to NAR regimes (Guo et al., 2020;. By contrast, in our settings, the Levenshtein distance provides a measure of difficulty that directly aligns with the model design and the training oracle.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_21",
            "content": "Resulting Algorithm Given a training dataset D = {y s , y * } M i=1 consisting of M samples, the difficulty score for each sample s i = {y s i , y * i } \u2208 D is measured by the Levenshtein Distance between the input and the output sequence. The cumulative density function (CDF) of the difficulty scores is then computed, resulting in one difficulty CDF score per sample, d(s i ). At each training step t, we compute the current competence value c(t) \u2208 (0, 1], given by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_22",
            "content": "c sqrt (t) \u2208 (0, 1] = min \uf8eb \uf8ed 1, t 1 \u2212 c 2 0 \u03bb t + c 2 0 \uf8f6 \uf8f8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_23",
            "content": "where, \u03bb t defines the length of the curriculum 1 ; c 0 = 0.1 as in Platanios et al. (2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_24",
            "content": "Based on this competence value c(t), the model is then trained on all the samples whose difficulty as measured by the Levenshtein distance between the input and the output sequence is lower than that competence value, i.e. d(s i ) \u2264 c(t). The resulting algorithm is also shown in Algorithm 1. 12 Return best \u03c0 rps and \u03c0 ins evaluated on validation set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_25",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "293-ARR_v1_26",
            "content": "We evaluate our approach on Controllable TS and Abstractive Summarization, two challenging sequence editing tasks that are motivated by real world information access needs. They are challenging because they require learning to perform a wide range of rewrites (from local substitution to sentence restructuring).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_27",
            "content": "Controllable Text Simplification",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "293-ARR_v1_28",
            "content": "Task Definition Given a complex text and a target grade level, the goal is to generate a simplified output that is appropriate for the desired grade level. The type of operations performed across different grade levels span sentence splitting, paraphrasing, deletion, content elaboration and substitution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_29",
            "content": "Data We use English Newsela samples as extracted by Agrawal and Carpuat (2019) with 470k/2k/19k for training, development and test sets respectively. Grade side-constraints are defined using a distinct special token for each grade level (from 2 to 12) and are introduced as side constraints for both the input and the output grade levels Scarton and Specia (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_30",
            "content": "Evaluation Metrics We automatically evaluate truecased detokenized system outputs using: SARI (Xu et al., 2016), which measures the lexical simplicity based on the n-grams kept, added, and deleted by the system relative to the input and the output sequence. It computes the F1 score for the n-grams that are added (add-F1). The model's deletion capability is measured by the F1 score for n-grams that are kept (keep-F1) and precision for the n-grams that are deleted (del-P) 2 ; Pearson's correlation coefficient (PCC) between the complexity of the system and reference outputs as measured by Automatic Readability Index (ARI) (Senter and Smith, 1967) and ARI-Accuracy (Heilman et al., 2008) representing the percentage of sentences where the system output grade level is within 1 grade of the reference text according to the ARI.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_31",
            "content": "Abstractive Summarization",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "293-ARR_v1_32",
            "content": "Task Given a short paragraph (one or two sentences on average), the goal is to generate a concise summary that captures the salient ideas of the source text. It contains heavy deletions with moderate amounts of substitutions and frequent shifts caused by re-orderings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_33",
            "content": "Data We use the dataset from Toutanova et al. (2016), which contains 6K short input texts, with upto 5 summaries each. We use the same split as provided by the authors with 4937/448/786 unique input texts in the training, development and test sets respectively. The human experts were allowed to insert new words and reorder parts of the sentence when generating the summary, which makes this dataset particularly suited for abstractive summarization models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_34",
            "content": "Evaluation Metrics We automatically evaluate truecased detokenized system outputs using: Rouge-L 3 (Lin, 2004). Even though it is not a summarization metric, we also report SARI to track the nature and type of edit operations performed. Given multiple references for each input text, we define the corpus level score as the arithmetic mean of automated metrics at the instance level, which is further averaged across the multiple references.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_35",
            "content": "Model configurations",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "293-ARR_v1_36",
            "content": "Data Preprocessing We pre-process all data using Moses tools for normalization, and truecasing. We apply subword segmentation with a joint inputoutput byte pair encoding model with 32, 000 operations. We use ARI to compute the input grade level at the inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_37",
            "content": "Architecture We adopt the base Transformer architecture (Vaswani et al., 2017) with d model = 512, d hidden = 2048, n heads = 8, n layers = 6, and p dropout = 0.1 for all our models. We add dropout to embeddings (0.1) and label smoothing (0.1). The base EDITOR model is trained using Adam with initial learning rate of 0.0005 and a batch size of 16, 000 tokens. The model is further finetuned on the editing task with a learning rate of 0.0001. We train all our models on two GeForce GTX 1080Ti GPUs. The average training time for a single seed of AR model is \u223c8-9 hrs and for the EDITOR model is \u223c20-22 hrs. Fine-tuning EDI-TOR takes additional 5-6 hrs. Training stops after 8 checkpoints without improvement of validation perplexity. All models are implemented using the Fairseq toolkit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_38",
            "content": "Models We compare our proposed approaches against the following models trained from scratch in controlled conditions: 1) AR is a auto-regressive (AR) transformer model (Scarton and Specia, 2018). 2) We train EDITOR with the dual-path roll-in policy as in Xu and Carpuat (2021), refered to as From Reference. We fine-tune EDI-TOR with the following policy variants: 3) From Input replaces the reference with the input for generating the initial sequence as in Agrawal et al. (2021). 4) Editing is our proposed roll-in policy. 5) Editing Curriculum, EDITCL, refers to our approach as described in \u00a73. During inference, we start from the input sequence (y s ), which is refined iteratively by applying a sequence of actions, as described in \u00a72 until 1) the output sequences from two consecutive iterations are the same, or 2) the maximum number of decoding steps (N = 10) is reached. The edit distance between two sequences is measured by the Levenshtein edit distance (Levenshtein et al., 1966).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_39",
            "content": "Findings",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "293-ARR_v1_40",
            "content": "Controlled TS As can be seen in Table 3, our overall training framework, ED I TCL improves over the prior training strategy for EDI-TOR-From Reference -significantly for all metrics (SARI: +3.8, PCC: +0.091, ARI-Acc: +10.1%), and over the AR baseline. Ablations show that this is a combined effect of multiple factors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_41",
            "content": "Dual-path roll-in, From Input improves over From Reference as expected (SARI: +1.9, PCC: +0.077, ARI-Acc: +8.0%), as the roll-in sequences encountered during training are similar to those encountered during inference. Using expert roll-in (ED I T I N G) performs better than using learned roll-in (dual-path roll-in) across the board, with gains of up to 3 SARI points over From Reference. Training with CL (ED I TCL) improves over the best roll-in strategy 4 , improving the precision of deletions (+1.6) and leading to a significant improvement in SARI score (+0.7) over ED I T I N G with no significant change in gradespecific metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_42",
            "content": "We also report training and inference statistics. For training, we report the number of training updates to convergence, i.e. when the model achieves the best validation perplexity on the development dataset. For inference, we report the average number of actions taken by the model to generate the refined output counts. Each iteration encompasses a reposition operation followed by an insertion applied to the all the tokens in the input sequence in parallel. CL reduces the average number of actions needed to generate outputs compared to ED I T I N G, while taking only \u223c 2K more updates during training than From Input. These results show that our roll-in policy, EDITING and the curriculum play a complementary role in improving training for editing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_43",
            "content": "Summarization On the Abstractive Summarization task(Table 4), ED I TCL achieves the best performance across the board compared to alternative training strategies for EDITOR with gain of upto \u223c 4 SARI, and \u223c 3 ROUGE points. Our proposed approach improves the precision of the deletion operation (DEL-P, +7). It also preserves the tokens from the source sequence that are present in the reference suggested by the improvement in KEEP-F1(+3.9) over the EDITOR (From Reference) model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_44",
            "content": "For completeness, we also compare our approach with systems trained in prior work: (1) ILP (Clarke and Lapata, 2008), an integer linear programing approach for deletion-based compression, (2) T3 (Cohn and Lapata, 2008), a tree transducer-based model for abstractive compression, (3) SEQ2SEQ (Filippova et al., 2015), a neural network model for deletion-based compression, (4) NAMAS (Rush et al., 2015), a neural model for abstractive compression and summarization and ( 5) FELIX (Mallinson et al., 2020), a nonautoregressive approach to text editing. We use the outputs provided by Toutanova et al. (2016) for [1-4] and Mallinson et al. (2020) for [5]. We endeavored to make the comparison as fair as possible 5 , but it is not possible to have a fully controlled comparison. In particular, FELIX is trained on uncased data and generates uncased outputs, while we train and evaluate our models with truecasing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_45",
            "content": "When evaluated using our pipeline, our training strategy applied to generic NAR models achieve scores that are on par with or better than those 5 We detokenized and manually checked the outputs from Mallinson et al. (2020) and corrected for de-tokenization errors such as \"1. 23\" to \"1.23\" and \"wanda 's\" to \"wanda's\". obtained by dedicated summarization models (Table 5). However, this evaluation penalizes FELIX as it is trained to address the simpler problem of summarization on uncased text. On lower-cased outputs, our best model falls behind FELIX by \u223c 1.7 ROUGE points. However, FELIX has about twice as many parameters as our model and benefits from BERT pre-training (Devlin et al., 2019). So overall this comparison confirms the promise of our approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_46",
            "content": "Rouge-L P R F1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_47",
            "content": "ILP (Clarke and Lapata, 2008) 60.6 63.2 60.6 T3 (Cohn and Lapata, 2008) 48.3 20.0 26.8 NAMAS (Rush et al., 2015) 48.8 55.2 51.5 SEQ2SEQ (Filippova et al., 2015) 57.6 51.5 53.1 FELIX (Mallinson et al., 2020)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_48",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "293-ARR_v1_49",
            "content": "We conduct further experiments to better understand the factors that help our training strategies improve editing quality. First, we seek to measure whether our approach has the intended effect of bridging the gap between training and test for editing tasks. Figure 1 shows the distribution of oracle insertion and deletions observed when (a) training with EDITOR \u015b default roll-in policy; (b) refining an original input sequence and (c) exposed to the model with our EDITING roll-in policy for Controllable TS. The plots show that with the default learning policy of the Editor model, the model doesn't learn to perform complex deletion operation at inference time. By contrast, our proposed roll-in exposes the model to the distribution that has higher overlap with the inference distribution as as well as additional intermediate sequences that encourages exploration during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_50",
            "content": "Impact of Curriculum Controlled roll-out",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "293-ARR_v1_51",
            "content": "Training Dynamics To verify that curriculum learning helps our model better exploit its training data, we train EDITOR on x% \u2208 [0, 100] of the data, and compare using random samples with samples ranked by increasing edit distance. Figure 2 shows the number of updates to convergence on the development dataset for controlled TS with/without CL. Training converges early (70 iterations only) on 13% of the easiest samples with oracle edit distance between the input and the output sequence <= 2. This supports the hypothesis that despite adding noise, our approach yields easier examples to train on. The order in which samples are presented matters, as adding batches with larger edit distance (> 63% data) without maintaining the order of the samples converges early. By contrast, the curriculum pacing function adds samples in order of increasing difficulty, allowing the model sufficient training time to learn from new samples while improving overall performance across metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_52",
            "content": "We also report the learning curves when training EDITOR on the Newsela dataset in Figure 3. Training with curriculum reduces the overall loss consistently on the development dataset, leading to better generalization. Ranking Criteria We compare the use of editdistance to organize the curriculum with other curriculum criteria in Table 6 where the order of examples is a) random, b) controlled by the length ratio between source and target sequence (Length Ratio), c) governed by the difference between the source and target grade levels (Grade Difference). Our proposed criterion outperforms both task-specific (\"Grade Difference\") and task agnostic criteria (\"Length Ratio\") on the Newsela Grade development set across all the metrics. \"Length Ratio\" achieves better correlation with Edit distance than \"Grade Difference\" which is also reflected by its performance (SARI: +0.3, PCC: 0.032, ARI: 0.7) on the Controlled TS task. This might reflect the fact that higher grade differences do not necessarily require more edits to be performed, for instance when the sentence to be simplified is already relatively simple. These mismatches do not occur when the edit distance itself is used as the sample difficulty criterion. Curriculum Learning for Sequence Refinement While curriculum learning has been applied to many tasks such as MT (Haffari, 2009;Platanios et al., 2019;Kumar et al., 2019), sentiment analysis (Sido and Konop\u00edk, 2019), natural language understanding , reading comprehension (Tay et al., 2019), their application to sequence refinement tasks has not been explored yet. Various strategies have been proposed to control the sample difficulty like n-gram frequency (Haffari, 2009;Platanios et al., 2019), token rarity, and sentence length . Chang et al. (2021) use Levenshtein edit distance as a sample difficulty criteria to order the samples for the task of data-to-text generation where the training model using an AR seq2seq model. Instead, we focus on edit distance as a sample difficulty criteria that is directly tied to the training oracle and model design.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_53",
            "content": "Roll-in policies There has been a plethora of work in the Imitation learning landscape on algorithms that strike a balance between learned and expert roll-in policies (Ross et al., 2011;Venkatraman et al., 2015;Chang et al., 2015). However, large differences in expert and learner's policy action can hurt performance (Brantley et al., 2019;He et al., 2012;Leblond et al., 2018). In our work, we propose to roll-in with noised states instead, so that the model can be exposed to mimic expert demonstrations from states that the model is more likely to encounter during inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_54",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "293-ARR_v1_55",
            "content": "We propose two complementary strategies to address undertraining and poor generalization when adapting NAR models to editing tasks: 1) a new roll-in policy that generates intermediate sequences that the model is likely to encounter during inference and 2) a curriculum to control the difficulty of the roll-out policy throughout training. Together, these strategies improve output quality consistently on controlled TS and abstractive summarization. These results open space for further research to evaluate the potential of this approach for other editing tasks (e.g., post editing, style transfer), and to further tailor imitation learning policies and curriculum design to these tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "293-ARR_v1_56",
            "content": "Sweta Agrawal, Marine Carpuat, Controlling text complexity in neural machine translation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Sweta Agrawal",
                    "Marine Carpuat"
                ],
                "title": "Controlling text complexity in neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_57",
            "content": "Sweta Agrawal, Weijia Xu, Marine Carpuat, A non-autoregressive edit-based approach to controllable text simplification, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Sweta Agrawal",
                    "Weijia Xu",
                    "Marine Carpuat"
                ],
                "title": "A non-autoregressive edit-based approach to controllable text simplification",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_58",
            "content": "Fernando Alva-Manchego, Joachim Bingel, Gustavo Paetzold, Carolina Scarton, Lucia Specia, Learning how to simplify from explicit labeling of complex-simplified text pairs, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Fernando Alva-Manchego",
                    "Joachim Bingel",
                    "Gustavo Paetzold",
                    "Carolina Scarton",
                    "Lucia Specia"
                ],
                "title": "Learning how to simplify from explicit labeling of complex-simplified text pairs",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "293-ARR_v1_59",
            "content": "Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, Vihari Piratla, Parallel iterative edit models for local sequence transduction, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Abhijeet Awasthi",
                    "Sunita Sarawagi",
                    "Rasna Goyal",
                    "Sabyasachi Ghosh",
                    "Vihari Piratla"
                ],
                "title": "Parallel iterative edit models for local sequence transduction",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_60",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, 2015, International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Dzmitry Bahdanau",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
                "pub_date": "2015",
                "pub_title": "International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_61",
            "content": "Kiante Brantley, Kyunghyun Cho, Hal Daum\u00e9, Sean Welleck, Non-monotonic sequential text generation, 2019, Proceedings of the 2019 Workshop on Widening NLP, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Kiante Brantley",
                    "Kyunghyun Cho",
                    "Hal Daum\u00e9",
                    "Sean Welleck"
                ],
                "title": "Non-monotonic sequential text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Workshop on Widening NLP",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_62",
            "content": "William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly, Imputer: Sequence modelling via imputation and dynamic programming, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "William Chan",
                    "Chitwan Saharia",
                    "Geoffrey Hinton",
                    "Mohammad Norouzi",
                    "Navdeep Jaitly"
                ],
                "title": "Imputer: Sequence modelling via imputation and dynamic programming",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "293-ARR_v1_63",
            "content": "UNKNOWN, None, 1997, Automatic induction of rules for text simplification. Knowledge-Based Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "1997",
                "pub_title": "Automatic induction of rules for text simplification. Knowledge-Based Systems",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_64",
            "content": "Ernie Chang, Hui-Syuan Yeh, Vera Demberg, Does the order of training samples matter? improving neural data-to-text generation with curriculum learning, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Ernie Chang",
                    "Hui-Syuan Yeh",
                    "Vera Demberg"
                ],
                "title": "Does the order of training samples matter? improving neural data-to-text generation with curriculum learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_65",
            "content": "Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, John Langford, Learning to search better than your teacher, 2015, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Kai-Wei Chang",
                    "Akshay Krishnamurthy",
                    "Alekh Agarwal",
                    "Hal Daume",
                    "John Langford"
                ],
                "title": "Learning to search better than your teacher",
                "pub_date": "2015",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "293-ARR_v1_66",
            "content": "James Clarke, Mirella Lapata, Global inference for sentence compression: An integer linear programming approach, 2008, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "James Clarke",
                    "Mirella Lapata"
                ],
                "title": "Global inference for sentence compression: An integer linear programming approach",
                "pub_date": "2008",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_67",
            "content": "Trevor Cohn, Mirella Lapata, Sentence compression beyond word deletion, 2008, Proceedings of the 22nd International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Trevor Cohn",
                    "Mirella Lapata"
                ],
                "title": "Sentence compression beyond word deletion",
                "pub_date": "2008",
                "pub_title": "Proceedings of the 22nd International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_68",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT (1), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "NAACL-HLT (1)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_69",
            "content": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung, EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Yue Dong",
                    "Zichao Li",
                    "Mehdi Rezagholizadeh",
                    "Jackie Chi Kit Cheung"
                ],
                "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_70",
            "content": "Katja Filippova, Enrique Alfonseca, A Carlos, \u0141ukasz Colmenares, Oriol Kaiser,  Vinyals, Sentence compression by deletion with lstms, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Katja Filippova",
                    "Enrique Alfonseca",
                    "A Carlos",
                    "\u0141ukasz Colmenares",
                    "Oriol Kaiser",
                    " Vinyals"
                ],
                "title": "Sentence compression by deletion with lstms",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_71",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Omer Levy",
                    "Yinhan Liu",
                    "Luke Zettlemoyer"
                ],
                "title": "Mask-predict: Parallel decoding of conditional masked language models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_72",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Nonautoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jiatao Gu",
                    "James Bradbury",
                    "Caiming Xiong",
                    "O Victor",
                    "Richard Li",
                    " Socher"
                ],
                "title": "Nonautoregressive neural machine translation",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_73",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Jiatao Gu",
                    "Changhan Wang",
                    "Junbo Zhao"
                ],
                "title": "Levenshtein transformer",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "293-ARR_v1_74",
            "content": "Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong Chen, Tie-Yan Liu, Fine-tuning by curriculum learning for non-autoregressive neural machine translation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Junliang Guo",
                    "Xu Tan",
                    "Linli Xu",
                    "Tao Qin",
                    "Enhong Chen",
                    "Tie-Yan Liu"
                ],
                "title": "Fine-tuning by curriculum learning for non-autoregressive neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_75",
            "content": "UNKNOWN, None, 2009, Machine learning approaches for dealing with limited bilingual training data in statistical machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Machine learning approaches for dealing with limited bilingual training data in statistical machine translation",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_76",
            "content": "He He, Jason Eisner, Hal Daume, Imitation learning by coaching, 2012, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "He He",
                    "Jason Eisner",
                    "Hal Daume"
                ],
                "title": "Imitation learning by coaching",
                "pub_date": "2012",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_77",
            "content": "Michael Heilman, Kevyn Collins-Thompson, Maxine Eskenazi, An analysis of statistical models and features for reading difficulty prediction, 2008, Proceedings of the third workshop on innovative use of NLP for building educational applications, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Michael Heilman",
                    "Kevyn Collins-Thompson",
                    "Maxine Eskenazi"
                ],
                "title": "An analysis of statistical models and features for reading difficulty prediction",
                "pub_date": "2008",
                "pub_title": "Proceedings of the third workshop on innovative use of NLP for building educational applications",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_78",
            "content": "UNKNOWN, None, 2020, Deep learning for text style transfer: A survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Deep learning for text style transfer: A survey",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_79",
            "content": "Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun, Reinforcement learning based curriculum optimization for neural machine translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Gaurav Kumar",
                    "George Foster",
                    "Colin Cherry",
                    "Maxim Krikun"
                ],
                "title": "Reinforcement learning based curriculum optimization for neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "293-ARR_v1_80",
            "content": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'aurelio Ranzato, Unsupervised machine translation using monolingual corpora only, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Guillaume Lample",
                    "Alexis Conneau",
                    "Ludovic Denoyer",
                    "Marc'aurelio Ranzato"
                ],
                "title": "Unsupervised machine translation using monolingual corpora only",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_81",
            "content": "R\u00e9mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien, Searnn: Training rnns with global-local losses, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "R\u00e9mi Leblond",
                    "Jean-Baptiste Alayrac",
                    "Anton Osokin",
                    "Simon Lacoste-Julien"
                ],
                "title": "Searnn: Training rnns with global-local losses",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_82",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Jason Lee",
                    "Elman Mansimov",
                    "Kyunghyun Cho"
                ],
                "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_83",
            "content": "Vladimir I Levenshtein, Binary codes capable of correcting deletions, insertions, and reversals, 1966, Soviet physics doklady, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    " Vladimir I Levenshtein"
                ],
                "title": "Binary codes capable of correcting deletions, insertions, and reversals",
                "pub_date": "1966",
                "pub_title": "Soviet physics doklady",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_84",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text Summarization Branches Out",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_85",
            "content": "UNKNOWN, None, , Task-level curriculum learning for non-autoregressive neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Task-level curriculum learning for non-autoregressive neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_86",
            "content": "Xuebo Liu, Houtim Lai, Derek Wong, Lidia Chao, Norm-based curriculum learning for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Xuebo Liu",
                    "Houtim Lai",
                    "Derek Wong",
                    "Lidia Chao"
                ],
                "title": "Norm-based curriculum learning for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_87",
            "content": "Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, Guillermo Garrido, FELIX: Flexible text editing through tagging and insertion, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Jonathan Mallinson",
                    "Aliaksei Severyn",
                    "Eric Malmi",
                    "Guillermo Garrido"
                ],
                "title": "FELIX: Flexible text editing through tagging and insertion",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_88",
            "content": "Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn, Encode, tag, realize: High-precision text editing, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Eric Malmi",
                    "Sebastian Krause",
                    "Sascha Rothe",
                    "Daniil Mirylenka",
                    "Aliaksei Severyn"
                ],
                "title": "Encode, tag, realize: High-precision text editing",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_89",
            "content": "Otilia Emmanouil Antonios Platanios, Graham Stretcu, Barnab\u00e1s Neubig, Tom M P\u00f3czos,  Mitchell, Competence-based curriculum learning for neural machine translation, 2019, NAACL-HLT (1), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Otilia Emmanouil Antonios Platanios",
                    "Graham Stretcu",
                    "Barnab\u00e1s Neubig",
                    "Tom M P\u00f3czos",
                    " Mitchell"
                ],
                "title": "Competence-based curriculum learning for neural machine translation",
                "pub_date": "2019",
                "pub_title": "NAACL-HLT (1)",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_90",
            "content": "UNKNOWN, None, 2020, Glancing transformer for non-autoregressive neural machine translation. arXiv e-prints, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Glancing transformer for non-autoregressive neural machine translation. arXiv e-prints",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_91",
            "content": "Stephane Ross, Drew Bagnell, Efficient reductions for imitation learning, 2010, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Stephane Ross",
                    "Drew Bagnell"
                ],
                "title": "Efficient reductions for imitation learning",
                "pub_date": "2010",
                "pub_title": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_92",
            "content": "St\u00e9phane Ross, Geoffrey Gordon, Drew Bagnell, A reduction of imitation learning and structured prediction to no-regret online learning, 2011, Proceedings of the fourteenth international conference on artificial intelligence and statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "St\u00e9phane Ross",
                    "Geoffrey Gordon",
                    "Drew Bagnell"
                ],
                "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
                "pub_date": "2011",
                "pub_title": "Proceedings of the fourteenth international conference on artificial intelligence and statistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_93",
            "content": "Sumit Alexander M Rush, Jason Chopra,  Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Sumit Alexander M Rush",
                    "Jason Chopra",
                    " Weston"
                ],
                "title": "A neural attention model for abstractive sentence summarization",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_94",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Chitwan Saharia",
                    "William Chan",
                    "Saurabh Saxena",
                    "Mohammad Norouzi"
                ],
                "title": "Non-autoregressive machine translation with latent alignments",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "293-ARR_v1_95",
            "content": "Carolina Scarton, Lucia Specia, Learning Simplifications for Specific Target Audiences, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Carolina Scarton",
                    "Lucia Specia"
                ],
                "title": "Learning Simplifications for Specific Target Audiences",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "293-ARR_v1_96",
            "content": "UNKNOWN, None, 1967, Automated readability index, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "1967",
                "pub_title": "Automated readability index",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_97",
            "content": "Jakub Sido, Miloslav Konop\u00edk, Curriculum learning in sentiment analysis, 2019, International Conference on Speech and Computer, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Jakub Sido",
                    "Miloslav Konop\u00edk"
                ],
                "title": "Curriculum learning in sentiment analysis",
                "pub_date": "2019",
                "pub_title": "International Conference on Speech and Computer",
                "pub": "Springer"
            }
        },
        {
            "ix": "293-ARR_v1_98",
            "content": "Michel Simard, Cyril Goutte, Pierre Isabelle, Statistical phrase-based post-editing, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Michel Simard",
                    "Cyril Goutte",
                    "Pierre Isabelle"
                ],
                "title": "Statistical phrase-based post-editing",
                "pub_date": "2007",
                "pub_title": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_99",
            "content": "Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit, Insertion transformer: Flexible sequence generation via insertion operations, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Mitchell Stern",
                    "William Chan",
                    "Jamie Kiros",
                    "Jakob Uszkoreit"
                ],
                "title": "Insertion transformer: Flexible sequence generation via insertion operations",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "293-ARR_v1_100",
            "content": "Yi Tay, Shuohang Wang, Anh Luu, Jie Fu, Minh Phan, Xingdi Yuan, Jinfeng Rao, Siu Hui, Aston Zhang, Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Yi Tay",
                    "Shuohang Wang",
                    "Anh Luu",
                    "Jie Fu",
                    "Minh Phan",
                    "Xingdi Yuan",
                    "Jinfeng Rao",
                    "Siu Hui",
                    "Aston Zhang"
                ],
                "title": "Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_101",
            "content": "Kristina Toutanova, Chris Brockett, Ke Tran, Saleema Amershi, A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Kristina Toutanova",
                    "Chris Brockett",
                    "Ke Tran",
                    "Saleema Amershi"
                ],
                "title": "A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_102",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "NIPS",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_103",
            "content": "Arun Venkatraman, J Andrew Hebert,  Bagnell, Improving multi-step prediction of learned time series models, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Arun Venkatraman",
                    "J Andrew Hebert",
                    " Bagnell"
                ],
                "title": "Improving multi-step prediction of learned time series models",
                "pub_date": "2015",
                "pub_title": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_104",
            "content": "Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang, Curriculum learning for natural language understanding, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Benfeng Xu",
                    "Licheng Zhang",
                    "Zhendong Mao",
                    "Quan Wang",
                    "Hongtao Xie",
                    "Yongdong Zhang"
                ],
                "title": "Curriculum learning for natural language understanding",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_105",
            "content": "Wei Xu, Chris Callison-Burch, Courtney Napoles, Problems in current text simplification research: New data can help, 2015, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Wei Xu",
                    "Chris Callison-Burch",
                    "Courtney Napoles"
                ],
                "title": "Problems in current text simplification research: New data can help",
                "pub_date": "2015",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_106",
            "content": "Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, Chris Callison-Burch, Optimizing statistical machine translation for text simplification, 2016, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Wei Xu",
                    "Courtney Napoles",
                    "Ellie Pavlick",
                    "Quanze Chen",
                    "Chris Callison-Burch"
                ],
                "title": "Optimizing statistical machine translation for text simplification",
                "pub_date": "2016",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_107",
            "content": ", Editor: An editbased transformer with repositioning for neural machine translation with soft lexical constraints, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [],
                "title": "Editor: An editbased transformer with repositioning for neural machine translation with soft lexical constraints",
                "pub_date": null,
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_108",
            "content": "Ziyu Yao, Frank Xu, Pengcheng Yin, Huan Sun, Graham Neubig, Learning structural edits via incremental tree transformations, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Ziyu Yao",
                    "Frank Xu",
                    "Pengcheng Yin",
                    "Huan Sun",
                    "Graham Neubig"
                ],
                "title": "Learning structural edits via incremental tree transformations",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_109",
            "content": "UNKNOWN, None, 2018, An empirical exploration of curriculum learning for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "An empirical exploration of curriculum learning for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "293-ARR_v1_110",
            "content": "Yikai Zhou, Baosong Yang, Derek Wong, Yu Wan, Lidia Chao, Uncertainty-aware curriculum learning for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Yikai Zhou",
                    "Baosong Yang",
                    "Derek Wong",
                    "Yu Wan",
                    "Lidia Chao"
                ],
                "title": "Uncertainty-aware curriculum learning for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "293-ARR_v1_0@0",
            "content": "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_0",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_2@0",
            "content": "We propose a framework for training nonautoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_2",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_2@1",
            "content": "We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_2",
            "start": 180,
            "end": 405,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_2@2",
            "content": "We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_2",
            "start": 407,
            "end": 749,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_2@3",
            "content": "We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification (TS) and abstractive summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_2",
            "start": 751,
            "end": 901,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_2@4",
            "content": "Our approach significantly improves output quality on both tasks and controls output complexity better on the controllable TS task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_2",
            "start": 903,
            "end": 1033,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_4@0",
            "content": "Neural sequence-to-sequence (seq2seq) models primarily developed and tested for machine translation (MT) Bahdanau et al. (2015); Vaswani et al. (2017); Gu et al. (2018) are increasingly used for other sequence transduction tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_4",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_4@1",
            "content": "This paper focuses on editing tasks, such as post-editing of MT output Simard et al. (2007), style transfer (Jin et al., 2020), or text simplification (TS) Chandrasekar and Srinivas (1997); Xu et al. (2015), where systems directly edit the input sequence, instead of generating the output from scratch as in MT. As illustrated in Table 1, in these tasks, there might be substantial overlap in content between inputs and outputs, and also diverse rewrites, ranging from local substitutions to more complex restructuring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_4",
            "start": 230,
            "end": 748,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_5@0",
            "content": "While dedicated architectures have been designed for these editing tasks, based on e.g., a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_5",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_6@0",
            "content": "Original: The Mauritshuis museum is staging an exhibition focusing on the 17th century selfportraits, highlighting the similarities and the differences between modern-day snapshots and historic works of art.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_6",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_7@0",
            "content": "Simplified: The Mauritshuis museum is now set to open an exhibit on the 17th century selfportraits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_7",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_7@1",
            "content": "It shows the similarities and differences between modern photos and artworks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_7",
            "start": 100,
            "end": 176,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_8@0",
            "content": "Table 1: Text simplification is an editing task, where the output sequence overlaps with the input, while incorporating multiple types of rephrasings to restructure and simplify content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_8",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_8@1",
            "content": "multistep, tag-then-edit approach (Alva-Manchego et al., 2017;Malmi et al., 2019;Dong et al., 2019;Mallinson et al., 2020), they can also be addressed with non-autoregressive (NAR) seq2seq models which generate their output by iteratively editing intermediate sequences (Lee et al., 2018;Gu et al., 2019;Awasthi et al., 2019;Stern et al., 2019;. NAR models hold the promise of providing a more generic solution, where the model does not need to be tailored to a given editing task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_8",
            "start": 187,
            "end": 667,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_9@0",
            "content": "This work is centered on the hypothesis that training NAR models for editing tasks using the same strategy as for MT leads to a mismatch between train and test settings that limits their generalization ability and output quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_9",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_9@1",
            "content": "Specifically, the learning algorithms designed for MT are aligned with inference strategies that generate output from an empty initial sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_9",
            "start": 230,
            "end": 373,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_9@2",
            "content": "By contrast, in sequence editing tasks, the inference step is initialized instead with the original input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_9",
            "start": 375,
            "end": 489,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_9@3",
            "content": "In addition, since editing samples might range from limited lexical substitutions to more thorough rewrites, training samples cover a wide range of edit distances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_9",
            "start": 491,
            "end": 653,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_10@0",
            "content": "During training, the loss can thus be dominated by the more distant samples leading to undertrained models and poor generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_10",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_10@1",
            "content": "By contrast, the distance between input and output samples in MT is more uniform, since it always involves at least lexical translation of the input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_10",
            "start": 132,
            "end": 287,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_11@0",
            "content": "To address these issues, we introduce a new training framework, EDITING CURRICULUM, which dynamically exposes the model to more relevant edit actions during training and exploits the full spectrum of available training samples more effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_11",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_11@1",
            "content": "First, we design a new roll-in strategy, EDITING roll-in, that exposes the model to intermediate sequences that it is more likely to encounter during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_11",
            "start": 245,
            "end": 404,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_11@2",
            "content": "Second, we introduce a training curriculum to expose the model to training samples in order of increasing edit distance, thus gradually increasing the complexity of oracle edit operations that the model learns to imitate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_11",
            "start": 406,
            "end": 626,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_12@0",
            "content": "We show that our approach improves the quality of outputs on two challenging English text editing tasks: controllable TS and abstractive summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_12",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_12@1",
            "content": "It also improves the degree of TS control by generating simplified outputs that match the target reading grade level better than the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_12",
            "start": 152,
            "end": 294,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_12@2",
            "content": "We conduct an extensive analysis which supports our hypothesis, and show that the sequences generated by our training policy improve exploration during training and are easier to learn from, leading to better generalization across samples with varying edit distances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_12",
            "start": 296,
            "end": 562,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_12@3",
            "content": "Training with curriculum further improves output quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_12",
            "start": 564,
            "end": 620,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_13@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_13",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_14@0",
            "content": "Model NAR edit-based models Gu et al., 2019;Stern et al., 2019;Xu and Carpuat, 2021) cast sequence editing as an iterative sequence refinement problem modeled by a Markov Decision Process Y, A, E, R, y 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_14",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_14@1",
            "content": "A state y = (y 1 , y 2 , ..., y L ) \u2208 Y is a sequence of tokens where each y i represents a token from the vocabulary V, L is the sequence length and y 0 \u2208 Y is the initial sequence to be refined, using actions drawn from the set A. The reward R is based on the distance D between the generated output and the reference sequence y * \u2208 Y: R(y) = \u2212D(y, y * ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_14",
            "start": 206,
            "end": 562,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_14@2",
            "content": "At each decoding iteration, the model takes an input y, chooses an action a \u2208 A to refine the sequence using a policy \u03c0, resulting in state E(y, a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_14",
            "start": 564,
            "end": 711,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@0",
            "content": "Models differ based on the nature of edit actions used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@1",
            "content": "Models support different operations such as insertion, deletion, reposition and substitution and are trained with different types of roll-in policies (mixed/learned/expert).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 56,
            "end": 228,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@2",
            "content": "We select the operations from the EDITOR model based on its competitive performance on constrained decoding tasks that require editing non-empty initial sequences (Xu and Carpuat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 230,
            "end": 415,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@3",
            "content": "It is a Transformer model that uses two types of actions or edits on sequences, y: (1) reposition and (2) insertion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 417,
            "end": 532,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@4",
            "content": "The reposition operation, modeled by \u03c0 rps , predicts the new position of each token in the input sequence (including deletions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 534,
            "end": 662,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@5",
            "content": "The insertion operation has two components: placeholder prediction, \u03c0 plh that predicts the number of placeholders to be inserted and token prediction, \u03c0 ins that generates the actual output tokens for each placeholder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 664,
            "end": 882,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@6",
            "content": "At each decoding iteration, the model applies an action a that consists of a reposition and an insertion operation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 884,
            "end": 998,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_15@7",
            "content": "This refinement process is repeated until two consecutive decoding iterations return the same output (Gu et al., 2019), or a preset maximum number of them is reached (Lee et al., 2018;Ghazvininejad et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_15",
            "start": 1000,
            "end": 1211,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@0",
            "content": "Training NAR models are trained via imitation learning that uses a roll-in policy to generate the sequences that the model learns to refine from and a roll-out policy to estimate the cost-to-go from the generated roll-in sequences to the desired output sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@1",
            "content": "The latter is performed by comparing the model actions to oracle demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 264,
            "end": 343,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@2",
            "content": "We summarize the policies of various NAR models proposed for MT in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 345,
            "end": 419,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@3",
            "content": "For EDITOR, the roll-in sequences for the reposition (or insertion) module are stochastic mixtures of the output of the insertion(or reposition) module or a noised version of the output sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 421,
            "end": 615,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@4",
            "content": "The oracle is the Levenshtein edit distance (Gu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 617,
            "end": 678,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_16@5",
            "content": "The noisy sequence is generated by applying random word dropping (Gu et al., 2019) and random word shuffle (Lample et al., 2018) with a probability of 0.5 and maximum shuffle distance of 3. ated at inference time (Ross and Bagnell, 2010).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_16",
            "start": 680,
            "end": 917,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_17@0",
            "content": "Our Approach: EDITING CURRICULUM",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_17",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_18@0",
            "content": "y ins = {y if u < \u03b1 else E(y, r), r \u223c \u03c0 rps } y rps = {y if u < \u03b2 else E(E(y, p * ), t), p * \u223c \u03c0 * plh , t \u223c \u03c0 ins }",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_18",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@0",
            "content": "While typically, the roll-in policy is a stochastic mixture of the model and the oracle demonstrations as described above, the noise incurred early on due to the large difference between the expert demonstration and the learner's policy actions may hurt overall performance (Brantley et al., 2019;He et al., 2012;Leblond et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@1",
            "content": "As we will see ( \u00a75), this is what happens on editing tasks when training the model to imitate experts using learned roll-in sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 336,
            "end": 470,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@2",
            "content": "At the same time, rolling in with expert demonstrations raises its own issues, as it can limit the exploration of the search space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 472,
            "end": 602,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@3",
            "content": "Motivated by these observations, we propose a new policy, EDITING, that allows exploration by injecting noise to the input sequence to generate new intermediate sequences for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 604,
            "end": 787,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@4",
            "content": "This lets the model learn to fix errors without deviating from learning the task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 789,
            "end": 877,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_19@5",
            "content": "We modify the roll-in policies to be aligned with the editing inference process, where the reposition operation is followed by insertion on the original input sequence: \u2022 The roll-in sequence for training the reposition policy, \u03c0 rps , is generated by applying noise to the original source sequence y s , i.e. y rps = {noise(y s )}; \u2022 The roll-in sequence for training the insertion policy, \u03c0 ins is an intermediate sequence generated by applying the expert reposition policy to y rps , i.e. y ins = {E(y rps , r * ), r * \u223c \u03c0 * rps }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_19",
            "start": 879,
            "end": 1412,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_20@0",
            "content": "To prevent undertraining when samples with large edit distances overwhelm the loss, we use a curriculum to expose the model to easy-to-learn actions first, then gradually increase the difficulty of the edit-operations performed as the learner becomes more compe-tent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_20",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_20@1",
            "content": "Prior work on CL does not agree on standard measures of sample difficulty for seq2seq tasks (Kumar et al., 2019;Yao et al., 2021;Zhang et al., 2018; or apply curriculum learning (CL) for the different problem of shifting the training of a Transformer model from AR to NAR regimes (Guo et al., 2020;. By contrast, in our settings, the Levenshtein distance provides a measure of difficulty that directly aligns with the model design and the training oracle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_20",
            "start": 268,
            "end": 722,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_21@0",
            "content": "Resulting Algorithm Given a training dataset D = {y s , y * } M i=1 consisting of M samples, the difficulty score for each sample s i = {y s i , y * i } \u2208 D is measured by the Levenshtein Distance between the input and the output sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_21",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_21@1",
            "content": "The cumulative density function (CDF) of the difficulty scores is then computed, resulting in one difficulty CDF score per sample, d(s i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_21",
            "start": 240,
            "end": 378,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_21@2",
            "content": "At each training step t, we compute the current competence value c(t) \u2208 (0, 1], given by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_21",
            "start": 380,
            "end": 468,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_22@0",
            "content": "c sqrt (t) \u2208 (0, 1] = min \uf8eb \uf8ed 1, t 1 \u2212 c 2 0 \u03bb t + c 2 0 \uf8f6 \uf8f8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_22",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_23@0",
            "content": "where, \u03bb t defines the length of the curriculum 1 ; c 0 = 0.1 as in Platanios et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_23",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_24@0",
            "content": "Based on this competence value c(t), the model is then trained on all the samples whose difficulty as measured by the Levenshtein distance between the input and the output sequence is lower than that competence value, i.e. d(s i ) \u2264 c(t).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_24",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_24@1",
            "content": "The resulting algorithm is also shown in Algorithm 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_24",
            "start": 239,
            "end": 291,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_24@2",
            "content": "12 Return best \u03c0 rps and \u03c0 ins evaluated on validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_24",
            "start": 293,
            "end": 351,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_25@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_25",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_26@0",
            "content": "We evaluate our approach on Controllable TS and Abstractive Summarization, two challenging sequence editing tasks that are motivated by real world information access needs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_26",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_26@1",
            "content": "They are challenging because they require learning to perform a wide range of rewrites (from local substitution to sentence restructuring).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_26",
            "start": 173,
            "end": 311,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_27@0",
            "content": "Controllable Text Simplification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_27",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_28@0",
            "content": "Task Definition Given a complex text and a target grade level, the goal is to generate a simplified output that is appropriate for the desired grade level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_28",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_28@1",
            "content": "The type of operations performed across different grade levels span sentence splitting, paraphrasing, deletion, content elaboration and substitution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_28",
            "start": 156,
            "end": 304,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_29@0",
            "content": "Data We use English Newsela samples as extracted by Agrawal and Carpuat (2019) with 470k/2k/19k for training, development and test sets respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_29",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_29@1",
            "content": "Grade side-constraints are defined using a distinct special token for each grade level (from 2 to 12) and are introduced as side constraints for both the input and the output grade levels Scarton and Specia (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_29",
            "start": 150,
            "end": 363,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_30@0",
            "content": "Evaluation Metrics We automatically evaluate truecased detokenized system outputs using: SARI (Xu et al., 2016), which measures the lexical simplicity based on the n-grams kept, added, and deleted by the system relative to the input and the output sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_30",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_30@1",
            "content": "It computes the F1 score for the n-grams that are added (add-F1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_30",
            "start": 258,
            "end": 322,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_30@2",
            "content": "The model's deletion capability is measured by the F1 score for n-grams that are kept (keep-F1) and precision for the n-grams that are deleted (del-P) 2 ; Pearson's correlation coefficient (PCC) between the complexity of the system and reference outputs as measured by Automatic Readability Index (ARI) (Senter and Smith, 1967) and ARI-Accuracy (Heilman et al., 2008) representing the percentage of sentences where the system output grade level is within 1 grade of the reference text according to the ARI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_30",
            "start": 324,
            "end": 829,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_31@0",
            "content": "Abstractive Summarization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_31",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_32@0",
            "content": "Task Given a short paragraph (one or two sentences on average), the goal is to generate a concise summary that captures the salient ideas of the source text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_32",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_32@1",
            "content": "It contains heavy deletions with moderate amounts of substitutions and frequent shifts caused by re-orderings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_32",
            "start": 158,
            "end": 267,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_33@0",
            "content": "Data We use the dataset from Toutanova et al. (2016), which contains 6K short input texts, with upto 5 summaries each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_33",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_33@1",
            "content": "We use the same split as provided by the authors with 4937/448/786 unique input texts in the training, development and test sets respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_33",
            "start": 119,
            "end": 260,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_33@2",
            "content": "The human experts were allowed to insert new words and reorder parts of the sentence when generating the summary, which makes this dataset particularly suited for abstractive summarization models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_33",
            "start": 262,
            "end": 457,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_34@0",
            "content": "Evaluation Metrics We automatically evaluate truecased detokenized system outputs using: Rouge-L 3 (Lin, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_34",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_34@1",
            "content": "Even though it is not a summarization metric, we also report SARI to track the nature and type of edit operations performed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_34",
            "start": 112,
            "end": 235,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_34@2",
            "content": "Given multiple references for each input text, we define the corpus level score as the arithmetic mean of automated metrics at the instance level, which is further averaged across the multiple references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_34",
            "start": 237,
            "end": 440,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_35@0",
            "content": "Model configurations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_35",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_36@0",
            "content": "Data Preprocessing We pre-process all data using Moses tools for normalization, and truecasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_36",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_36@1",
            "content": "We apply subword segmentation with a joint inputoutput byte pair encoding model with 32, 000 operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_36",
            "start": 96,
            "end": 199,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_36@2",
            "content": "We use ARI to compute the input grade level at the inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_36",
            "start": 201,
            "end": 266,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@0",
            "content": "Architecture We adopt the base Transformer architecture (Vaswani et al., 2017) with d model = 512, d hidden = 2048, n heads = 8, n layers = 6, and p dropout = 0.1 for all our models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@1",
            "content": "We add dropout to embeddings (0.1) and label smoothing (0.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 183,
            "end": 243,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@2",
            "content": "The base EDITOR model is trained using Adam with initial learning rate of 0.0005 and a batch size of 16, 000 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 245,
            "end": 360,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@3",
            "content": "The model is further finetuned on the editing task with a learning rate of 0.0001.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 362,
            "end": 443,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@4",
            "content": "We train all our models on two GeForce GTX 1080Ti GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 445,
            "end": 499,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@5",
            "content": "The average training time for a single seed of AR model is \u223c8-9 hrs and for the EDITOR model is \u223c20-22 hrs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 501,
            "end": 607,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@6",
            "content": "Fine-tuning EDI-TOR takes additional 5-6 hrs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 609,
            "end": 653,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@7",
            "content": "Training stops after 8 checkpoints without improvement of validation perplexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 655,
            "end": 734,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_37@8",
            "content": "All models are implemented using the Fairseq toolkit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_37",
            "start": 736,
            "end": 788,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@0",
            "content": "Models We compare our proposed approaches against the following models trained from scratch in controlled conditions: 1) AR is a auto-regressive (AR) transformer model (Scarton and Specia, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@1",
            "content": "2) We train EDITOR with the dual-path roll-in policy as in Xu and Carpuat (2021), refered to as From Reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 196,
            "end": 306,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@2",
            "content": "We fine-tune EDI-TOR with the following policy variants: 3) From Input replaces the reference with the input for generating the initial sequence as in Agrawal et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 308,
            "end": 480,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@3",
            "content": "4) Editing is our proposed roll-in policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 482,
            "end": 523,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@4",
            "content": "5) Editing Curriculum, EDITCL, refers to our approach as described in \u00a73.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 525,
            "end": 597,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@5",
            "content": "During inference, we start from the input sequence (y s ), which is refined iteratively by applying a sequence of actions, as described in \u00a72 until 1) the output sequences from two consecutive iterations are the same, or 2) the maximum number of decoding steps (N = 10) is reached.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 599,
            "end": 879,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_38@6",
            "content": "The edit distance between two sequences is measured by the Levenshtein edit distance (Levenshtein et al., 1966).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_38",
            "start": 881,
            "end": 992,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_39@0",
            "content": "Findings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_39",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_40@0",
            "content": "Controlled TS As can be seen in Table 3, our overall training framework, ED I TCL improves over the prior training strategy for EDI-TOR-From Reference -significantly for all metrics (SARI: +3.8, PCC: +0.091, ARI-Acc: +10.1%), and over the AR baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_40",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_40@1",
            "content": "Ablations show that this is a combined effect of multiple factors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_40",
            "start": 252,
            "end": 317,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_41@0",
            "content": "Dual-path roll-in, From Input improves over From Reference as expected (SARI: +1.9, PCC: +0.077, ARI-Acc: +8.0%), as the roll-in sequences encountered during training are similar to those encountered during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_41",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_41@1",
            "content": "Using expert roll-in (ED I T I N G) performs better than using learned roll-in (dual-path roll-in) across the board, with gains of up to 3 SARI points over From Reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_41",
            "start": 218,
            "end": 388,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_41@2",
            "content": "Training with CL (ED I TCL) improves over the best roll-in strategy 4 , improving the precision of deletions (+1.6) and leading to a significant improvement in SARI score (+0.7) over ED I T I N G with no significant change in gradespecific metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_41",
            "start": 390,
            "end": 637,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@0",
            "content": "We also report training and inference statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@1",
            "content": "For training, we report the number of training updates to convergence, i.e. when the model achieves the best validation perplexity on the development dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 50,
            "end": 207,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@2",
            "content": "For inference, we report the average number of actions taken by the model to generate the refined output counts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 209,
            "end": 320,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@3",
            "content": "Each iteration encompasses a reposition operation followed by an insertion applied to the all the tokens in the input sequence in parallel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 322,
            "end": 460,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@4",
            "content": "CL reduces the average number of actions needed to generate outputs compared to ED I T I N G, while taking only \u223c 2K more updates during training than From Input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 462,
            "end": 623,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_42@5",
            "content": "These results show that our roll-in policy, EDITING and the curriculum play a complementary role in improving training for editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_42",
            "start": 625,
            "end": 755,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_43@0",
            "content": "Summarization On the Abstractive Summarization task(Table 4), ED I TCL achieves the best performance across the board compared to alternative training strategies for EDITOR with gain of upto \u223c 4 SARI, and \u223c 3 ROUGE points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_43",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_43@1",
            "content": "Our proposed approach improves the precision of the deletion operation (DEL-P, +7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_43",
            "start": 223,
            "end": 305,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_43@2",
            "content": "It also preserves the tokens from the source sequence that are present in the reference suggested by the improvement in KEEP-F1(+3.9) over the EDITOR (From Reference) model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_43",
            "start": 307,
            "end": 479,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_44@0",
            "content": "For completeness, we also compare our approach with systems trained in prior work: (1) ILP (Clarke and Lapata, 2008), an integer linear programing approach for deletion-based compression, (2) T3 (Cohn and Lapata, 2008), a tree transducer-based model for abstractive compression, (3) SEQ2SEQ (Filippova et al., 2015), a neural network model for deletion-based compression, (4) NAMAS (Rush et al., 2015), a neural model for abstractive compression and summarization and ( 5) FELIX (Mallinson et al., 2020), a nonautoregressive approach to text editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_44",
            "start": 0,
            "end": 549,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_44@1",
            "content": "We use the outputs provided by Toutanova et al. (2016) for [1-4] and Mallinson et al. (2020) for [5].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_44",
            "start": 551,
            "end": 651,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_44@2",
            "content": "We endeavored to make the comparison as fair as possible 5 , but it is not possible to have a fully controlled comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_44",
            "start": 653,
            "end": 774,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_44@3",
            "content": "In particular, FELIX is trained on uncased data and generates uncased outputs, while we train and evaluate our models with truecasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_44",
            "start": 776,
            "end": 909,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@0",
            "content": "When evaluated using our pipeline, our training strategy applied to generic NAR models achieve scores that are on par with or better than those 5 We detokenized and manually checked the outputs from Mallinson et al. (2020) and corrected for de-tokenization errors such as \"1. 23\" to \"1.23\" and \"wanda 's\" to \"wanda's\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@1",
            "content": "obtained by dedicated summarization models (Table 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 319,
            "end": 371,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@2",
            "content": "However, this evaluation penalizes FELIX as it is trained to address the simpler problem of summarization on uncased text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 373,
            "end": 494,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@3",
            "content": "On lower-cased outputs, our best model falls behind FELIX by \u223c 1.7 ROUGE points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 496,
            "end": 575,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@4",
            "content": "However, FELIX has about twice as many parameters as our model and benefits from BERT pre-training (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 577,
            "end": 697,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_45@5",
            "content": "So overall this comparison confirms the promise of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_45",
            "start": 699,
            "end": 762,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_46@0",
            "content": "Rouge-L P R F1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_46",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_47@0",
            "content": "ILP (Clarke and Lapata, 2008) 60.6 63.2 60.6 T3 (Cohn and Lapata, 2008) 48.3 20.0 26.8 NAMAS (Rush et al., 2015) 48.8 55.2 51.5 SEQ2SEQ (Filippova et al., 2015) 57.6 51.5 53.1 FELIX (Mallinson et al., 2020)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_47",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_48@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_48",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_49@0",
            "content": "We conduct further experiments to better understand the factors that help our training strategies improve editing quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_49",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_49@1",
            "content": "First, we seek to measure whether our approach has the intended effect of bridging the gap between training and test for editing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_49",
            "start": 123,
            "end": 257,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_49@2",
            "content": "Figure 1 shows the distribution of oracle insertion and deletions observed when (a) training with EDITOR \u015b default roll-in policy; (b) refining an original input sequence and (c) exposed to the model with our EDITING roll-in policy for Controllable TS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_49",
            "start": 259,
            "end": 510,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_49@3",
            "content": "The plots show that with the default learning policy of the Editor model, the model doesn't learn to perform complex deletion operation at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_49",
            "start": 512,
            "end": 665,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_49@4",
            "content": "By contrast, our proposed roll-in exposes the model to the distribution that has higher overlap with the inference distribution as as well as additional intermediate sequences that encourages exploration during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_49",
            "start": 667,
            "end": 886,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_50@0",
            "content": "Impact of Curriculum Controlled roll-out",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_50",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@0",
            "content": "Training Dynamics To verify that curriculum learning helps our model better exploit its training data, we train EDITOR on x% \u2208 [0, 100] of the data, and compare using random samples with samples ranked by increasing edit distance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@1",
            "content": "Figure 2 shows the number of updates to convergence on the development dataset for controlled TS with/without CL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 231,
            "end": 343,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@2",
            "content": "Training converges early (70 iterations only) on 13% of the easiest samples with oracle edit distance between the input and the output sequence <= 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 345,
            "end": 493,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@3",
            "content": "This supports the hypothesis that despite adding noise, our approach yields easier examples to train on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 495,
            "end": 598,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@4",
            "content": "The order in which samples are presented matters, as adding batches with larger edit distance (> 63% data) without maintaining the order of the samples converges early.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 600,
            "end": 767,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_51@5",
            "content": "By contrast, the curriculum pacing function adds samples in order of increasing difficulty, allowing the model sufficient training time to learn from new samples while improving overall performance across metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_51",
            "start": 769,
            "end": 981,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@0",
            "content": "We also report the learning curves when training EDITOR on the Newsela dataset in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@1",
            "content": "Training with curriculum reduces the overall loss consistently on the development dataset, leading to better generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 92,
            "end": 215,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@2",
            "content": "Ranking Criteria We compare the use of editdistance to organize the curriculum with other curriculum criteria in Table 6 where the order of examples is a) random, b) controlled by the length ratio between source and target sequence (Length Ratio), c) governed by the difference between the source and target grade levels (Grade Difference).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 217,
            "end": 556,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@3",
            "content": "Our proposed criterion outperforms both task-specific (\"Grade Difference\") and task agnostic criteria (\"Length Ratio\") on the Newsela Grade development set across all the metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 558,
            "end": 736,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@4",
            "content": "\"Length Ratio\" achieves better correlation with Edit distance than \"Grade Difference\" which is also reflected by its performance (SARI: +0.3, PCC: 0.032, ARI: 0.7) on the Controlled TS task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 738,
            "end": 927,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@5",
            "content": "This might reflect the fact that higher grade differences do not necessarily require more edits to be performed, for instance when the sentence to be simplified is already relatively simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 929,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@6",
            "content": "These mismatches do not occur when the edit distance itself is used as the sample difficulty criterion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 1120,
            "end": 1222,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@7",
            "content": "Curriculum Learning for Sequence Refinement While curriculum learning has been applied to many tasks such as MT (Haffari, 2009;Platanios et al., 2019;Kumar et al., 2019), sentiment analysis (Sido and Konop\u00edk, 2019), natural language understanding , reading comprehension (Tay et al., 2019), their application to sequence refinement tasks has not been explored yet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 1224,
            "end": 1587,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@8",
            "content": "Various strategies have been proposed to control the sample difficulty like n-gram frequency (Haffari, 2009;Platanios et al., 2019), token rarity, and sentence length .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 1589,
            "end": 1756,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@9",
            "content": "Chang et al. (2021) use Levenshtein edit distance as a sample difficulty criteria to order the samples for the task of data-to-text generation where the training model using an AR seq2seq model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 1758,
            "end": 1951,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_52@10",
            "content": "Instead, we focus on edit distance as a sample difficulty criteria that is directly tied to the training oracle and model design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_52",
            "start": 1953,
            "end": 2081,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_53@0",
            "content": "Roll-in policies There has been a plethora of work in the Imitation learning landscape on algorithms that strike a balance between learned and expert roll-in policies (Ross et al., 2011;Venkatraman et al., 2015;Chang et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_53",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_53@1",
            "content": "However, large differences in expert and learner's policy action can hurt performance (Brantley et al., 2019;He et al., 2012;Leblond et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_53",
            "start": 232,
            "end": 378,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_53@2",
            "content": "In our work, we propose to roll-in with noised states instead, so that the model can be exposed to mimic expert demonstrations from states that the model is more likely to encounter during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_53",
            "start": 380,
            "end": 578,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_54@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_54",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_55@0",
            "content": "We propose two complementary strategies to address undertraining and poor generalization when adapting NAR models to editing tasks: 1) a new roll-in policy that generates intermediate sequences that the model is likely to encounter during inference and 2) a curriculum to control the difficulty of the roll-out policy throughout training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_55",
            "start": 0,
            "end": 337,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_55@1",
            "content": "Together, these strategies improve output quality consistently on controlled TS and abstractive summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_55",
            "start": 339,
            "end": 448,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_55@2",
            "content": "These results open space for further research to evaluate the potential of this approach for other editing tasks (e.g., post editing, style transfer), and to further tailor imitation learning policies and curriculum design to these tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_55",
            "start": 450,
            "end": 687,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_56@0",
            "content": "Sweta Agrawal, Marine Carpuat, Controlling text complexity in neural machine translation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_56",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_57@0",
            "content": "Sweta Agrawal, Weijia Xu, Marine Carpuat, A non-autoregressive edit-based approach to controllable text simplification, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_57",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_58@0",
            "content": "Fernando Alva-Manchego, Joachim Bingel, Gustavo Paetzold, Carolina Scarton, Lucia Specia, Learning how to simplify from explicit labeling of complex-simplified text pairs, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_58",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_59@0",
            "content": "Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, Vihari Piratla, Parallel iterative edit models for local sequence transduction, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_59",
            "start": 0,
            "end": 329,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_60@0",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, 2015, International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_60",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_61@0",
            "content": "Kiante Brantley, Kyunghyun Cho, Hal Daum\u00e9, Sean Welleck, Non-monotonic sequential text generation, 2019, Proceedings of the 2019 Workshop on Widening NLP, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_61",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_62@0",
            "content": "William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly, Imputer: Sequence modelling via imputation and dynamic programming, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_62",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_63@0",
            "content": "UNKNOWN, None, 1997, Automatic induction of rules for text simplification. Knowledge-Based Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_63",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_64@0",
            "content": "Ernie Chang, Hui-Syuan Yeh, Vera Demberg, Does the order of training samples matter? improving neural data-to-text generation with curriculum learning, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_64",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_65@0",
            "content": "Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, John Langford, Learning to search better than your teacher, 2015, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_65",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_66@0",
            "content": "James Clarke, Mirella Lapata, Global inference for sentence compression: An integer linear programming approach, 2008, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_66",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_67@0",
            "content": "Trevor Cohn, Mirella Lapata, Sentence compression beyond word deletion, 2008, Proceedings of the 22nd International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_67",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_68@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT (1), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_68",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_69@0",
            "content": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung, EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_69",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_70@0",
            "content": "Katja Filippova, Enrique Alfonseca, A Carlos, \u0141ukasz Colmenares, Oriol Kaiser,  Vinyals, Sentence compression by deletion with lstms, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_70",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_71@0",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_71",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_72@0",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Nonautoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_72",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_73@0",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_73",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_74@0",
            "content": "Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong Chen, Tie-Yan Liu, Fine-tuning by curriculum learning for non-autoregressive neural machine translation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_74",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2009, Machine learning approaches for dealing with limited bilingual training data in statistical machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_75",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_76@0",
            "content": "He He, Jason Eisner, Hal Daume, Imitation learning by coaching, 2012, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_76",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_77@0",
            "content": "Michael Heilman, Kevyn Collins-Thompson, Maxine Eskenazi, An analysis of statistical models and features for reading difficulty prediction, 2008, Proceedings of the third workshop on innovative use of NLP for building educational applications, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_77",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_78@0",
            "content": "UNKNOWN, None, 2020, Deep learning for text style transfer: A survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_78",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_79@0",
            "content": "Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun, Reinforcement learning based curriculum optimization for neural machine translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_79",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_80@0",
            "content": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'aurelio Ranzato, Unsupervised machine translation using monolingual corpora only, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_80",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_81@0",
            "content": "R\u00e9mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien, Searnn: Training rnns with global-local losses, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_81",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_82@0",
            "content": "Jason Lee, Elman Mansimov, Kyunghyun Cho, Deterministic non-autoregressive neural sequence modeling by iterative refinement, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_82",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_83@0",
            "content": "Vladimir I Levenshtein, Binary codes capable of correcting deletions, insertions, and reversals, 1966, Soviet physics doklady, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_83",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_84@0",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_84",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_85@0",
            "content": "UNKNOWN, None, , Task-level curriculum learning for non-autoregressive neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_85",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_86@0",
            "content": "Xuebo Liu, Houtim Lai, Derek Wong, Lidia Chao, Norm-based curriculum learning for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_86",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_87@0",
            "content": "Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, Guillermo Garrido, FELIX: Flexible text editing through tagging and insertion, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_87",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_88@0",
            "content": "Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn, Encode, tag, realize: High-precision text editing, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_88",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_89@0",
            "content": "Otilia Emmanouil Antonios Platanios, Graham Stretcu, Barnab\u00e1s Neubig, Tom M P\u00f3czos,  Mitchell, Competence-based curriculum learning for neural machine translation, 2019, NAACL-HLT (1), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_89",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2020, Glancing transformer for non-autoregressive neural machine translation. arXiv e-prints, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_90",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_91@0",
            "content": "Stephane Ross, Drew Bagnell, Efficient reductions for imitation learning, 2010, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_91",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_92@0",
            "content": "St\u00e9phane Ross, Geoffrey Gordon, Drew Bagnell, A reduction of imitation learning and structured prediction to no-regret online learning, 2011, Proceedings of the fourteenth international conference on artificial intelligence and statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_92",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_93@0",
            "content": "Sumit Alexander M Rush, Jason Chopra,  Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_93",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_94@0",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi, Non-autoregressive machine translation with latent alignments, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_94",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_95@0",
            "content": "Carolina Scarton, Lucia Specia, Learning Simplifications for Specific Target Audiences, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_95",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_96@0",
            "content": "UNKNOWN, None, 1967, Automated readability index, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_96",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_97@0",
            "content": "Jakub Sido, Miloslav Konop\u00edk, Curriculum learning in sentiment analysis, 2019, International Conference on Speech and Computer, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_97",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_98@0",
            "content": "Michel Simard, Cyril Goutte, Pierre Isabelle, Statistical phrase-based post-editing, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_98",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_99@0",
            "content": "Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit, Insertion transformer: Flexible sequence generation via insertion operations, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_99",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_100@0",
            "content": "Yi Tay, Shuohang Wang, Anh Luu, Jie Fu, Minh Phan, Xingdi Yuan, Jinfeng Rao, Siu Hui, Aston Zhang, Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_100",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_101@0",
            "content": "Kristina Toutanova, Chris Brockett, Ke Tran, Saleema Amershi, A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_101",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_102@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_102",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_103@0",
            "content": "Arun Venkatraman, J Andrew Hebert,  Bagnell, Improving multi-step prediction of learned time series models, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_103",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_104@0",
            "content": "Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang, Curriculum learning for natural language understanding, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_104",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_105@0",
            "content": "Wei Xu, Chris Callison-Burch, Courtney Napoles, Problems in current text simplification research: New data can help, 2015, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_105",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_106@0",
            "content": "Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, Chris Callison-Burch, Optimizing statistical machine translation for text simplification, 2016, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_106",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_107@0",
            "content": ", Editor: An editbased transformer with repositioning for neural machine translation with soft lexical constraints, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_107",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_108@0",
            "content": "Ziyu Yao, Frank Xu, Pengcheng Yin, Huan Sun, Graham Neubig, Learning structural edits via incremental tree transformations, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_108",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2018, An empirical exploration of curriculum learning for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_109",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "293-ARR_v1_110@0",
            "content": "Yikai Zhou, Baosong Yang, Derek Wong, Yu Wan, Lidia Chao, Uncertainty-aware curriculum learning for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "293-ARR_v1_110",
            "start": 0,
            "end": 223,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_1",
            "tgt_ix": "293-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_1",
            "tgt_ix": "293-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_4",
            "tgt_ix": "293-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_5",
            "tgt_ix": "293-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_6",
            "tgt_ix": "293-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_7",
            "tgt_ix": "293-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_8",
            "tgt_ix": "293-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_9",
            "tgt_ix": "293-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_10",
            "tgt_ix": "293-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_11",
            "tgt_ix": "293-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_12",
            "tgt_ix": "293-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_14",
            "tgt_ix": "293-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_13",
            "tgt_ix": "293-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_13",
            "tgt_ix": "293-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_13",
            "tgt_ix": "293-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_13",
            "tgt_ix": "293-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_18",
            "tgt_ix": "293-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_20",
            "tgt_ix": "293-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_21",
            "tgt_ix": "293-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_22",
            "tgt_ix": "293-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_23",
            "tgt_ix": "293-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_24",
            "tgt_ix": "293-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_26",
            "tgt_ix": "293-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_28",
            "tgt_ix": "293-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_29",
            "tgt_ix": "293-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_27",
            "tgt_ix": "293-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_27",
            "tgt_ix": "293-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_27",
            "tgt_ix": "293-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_27",
            "tgt_ix": "293-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_30",
            "tgt_ix": "293-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_32",
            "tgt_ix": "293-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_33",
            "tgt_ix": "293-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_31",
            "tgt_ix": "293-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_31",
            "tgt_ix": "293-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_31",
            "tgt_ix": "293-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_31",
            "tgt_ix": "293-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_34",
            "tgt_ix": "293-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_36",
            "tgt_ix": "293-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_35",
            "tgt_ix": "293-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_35",
            "tgt_ix": "293-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_35",
            "tgt_ix": "293-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_35",
            "tgt_ix": "293-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_40",
            "tgt_ix": "293-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_41",
            "tgt_ix": "293-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_43",
            "tgt_ix": "293-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_44",
            "tgt_ix": "293-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_46",
            "tgt_ix": "293-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_47",
            "tgt_ix": "293-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_48",
            "tgt_ix": "293-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_48",
            "tgt_ix": "293-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_48",
            "tgt_ix": "293-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_50",
            "tgt_ix": "293-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_50",
            "tgt_ix": "293-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_50",
            "tgt_ix": "293-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_50",
            "tgt_ix": "293-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_53",
            "tgt_ix": "293-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_54",
            "tgt_ix": "293-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_54",
            "tgt_ix": "293-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "293-ARR_v1_0",
            "tgt_ix": "293-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_1",
            "tgt_ix": "293-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_2",
            "tgt_ix": "293-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_3",
            "tgt_ix": "293-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_4",
            "tgt_ix": "293-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_4",
            "tgt_ix": "293-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_5",
            "tgt_ix": "293-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_6",
            "tgt_ix": "293-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_7",
            "tgt_ix": "293-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_7",
            "tgt_ix": "293-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_8",
            "tgt_ix": "293-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_8",
            "tgt_ix": "293-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_9",
            "tgt_ix": "293-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_9",
            "tgt_ix": "293-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_9",
            "tgt_ix": "293-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_9",
            "tgt_ix": "293-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_10",
            "tgt_ix": "293-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_10",
            "tgt_ix": "293-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_11",
            "tgt_ix": "293-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_11",
            "tgt_ix": "293-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_11",
            "tgt_ix": "293-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_12",
            "tgt_ix": "293-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_12",
            "tgt_ix": "293-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_12",
            "tgt_ix": "293-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_12",
            "tgt_ix": "293-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_13",
            "tgt_ix": "293-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_14",
            "tgt_ix": "293-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_14",
            "tgt_ix": "293-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_14",
            "tgt_ix": "293-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_15",
            "tgt_ix": "293-ARR_v1_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_16",
            "tgt_ix": "293-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_17",
            "tgt_ix": "293-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_18",
            "tgt_ix": "293-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_19",
            "tgt_ix": "293-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_20",
            "tgt_ix": "293-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_20",
            "tgt_ix": "293-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_21",
            "tgt_ix": "293-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_21",
            "tgt_ix": "293-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_21",
            "tgt_ix": "293-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_22",
            "tgt_ix": "293-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_23",
            "tgt_ix": "293-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_24",
            "tgt_ix": "293-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_24",
            "tgt_ix": "293-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_24",
            "tgt_ix": "293-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_25",
            "tgt_ix": "293-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_26",
            "tgt_ix": "293-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_26",
            "tgt_ix": "293-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_27",
            "tgt_ix": "293-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_28",
            "tgt_ix": "293-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_28",
            "tgt_ix": "293-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_29",
            "tgt_ix": "293-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_29",
            "tgt_ix": "293-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_30",
            "tgt_ix": "293-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_30",
            "tgt_ix": "293-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_30",
            "tgt_ix": "293-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_31",
            "tgt_ix": "293-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_32",
            "tgt_ix": "293-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_32",
            "tgt_ix": "293-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_33",
            "tgt_ix": "293-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_33",
            "tgt_ix": "293-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_33",
            "tgt_ix": "293-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_34",
            "tgt_ix": "293-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_34",
            "tgt_ix": "293-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_34",
            "tgt_ix": "293-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_35",
            "tgt_ix": "293-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_36",
            "tgt_ix": "293-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_36",
            "tgt_ix": "293-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_36",
            "tgt_ix": "293-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_37",
            "tgt_ix": "293-ARR_v1_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_38",
            "tgt_ix": "293-ARR_v1_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_39",
            "tgt_ix": "293-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_40",
            "tgt_ix": "293-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_40",
            "tgt_ix": "293-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_41",
            "tgt_ix": "293-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_41",
            "tgt_ix": "293-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_41",
            "tgt_ix": "293-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_42",
            "tgt_ix": "293-ARR_v1_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_43",
            "tgt_ix": "293-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_43",
            "tgt_ix": "293-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_43",
            "tgt_ix": "293-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_44",
            "tgt_ix": "293-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_44",
            "tgt_ix": "293-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_44",
            "tgt_ix": "293-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_44",
            "tgt_ix": "293-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_45",
            "tgt_ix": "293-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_46",
            "tgt_ix": "293-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_47",
            "tgt_ix": "293-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_48",
            "tgt_ix": "293-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_49",
            "tgt_ix": "293-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_50",
            "tgt_ix": "293-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_51",
            "tgt_ix": "293-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_52",
            "tgt_ix": "293-ARR_v1_52@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_53",
            "tgt_ix": "293-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_53",
            "tgt_ix": "293-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_53",
            "tgt_ix": "293-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_54",
            "tgt_ix": "293-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_55",
            "tgt_ix": "293-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_55",
            "tgt_ix": "293-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_55",
            "tgt_ix": "293-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_56",
            "tgt_ix": "293-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_57",
            "tgt_ix": "293-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_58",
            "tgt_ix": "293-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_59",
            "tgt_ix": "293-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_60",
            "tgt_ix": "293-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_61",
            "tgt_ix": "293-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_62",
            "tgt_ix": "293-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_63",
            "tgt_ix": "293-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_64",
            "tgt_ix": "293-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_65",
            "tgt_ix": "293-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_66",
            "tgt_ix": "293-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_67",
            "tgt_ix": "293-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_68",
            "tgt_ix": "293-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_69",
            "tgt_ix": "293-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_70",
            "tgt_ix": "293-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_71",
            "tgt_ix": "293-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_72",
            "tgt_ix": "293-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_73",
            "tgt_ix": "293-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_74",
            "tgt_ix": "293-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_75",
            "tgt_ix": "293-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_76",
            "tgt_ix": "293-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_77",
            "tgt_ix": "293-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_78",
            "tgt_ix": "293-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_79",
            "tgt_ix": "293-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_80",
            "tgt_ix": "293-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_81",
            "tgt_ix": "293-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_82",
            "tgt_ix": "293-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_83",
            "tgt_ix": "293-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_84",
            "tgt_ix": "293-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_85",
            "tgt_ix": "293-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_86",
            "tgt_ix": "293-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_87",
            "tgt_ix": "293-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_88",
            "tgt_ix": "293-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_89",
            "tgt_ix": "293-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_90",
            "tgt_ix": "293-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_91",
            "tgt_ix": "293-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_92",
            "tgt_ix": "293-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_93",
            "tgt_ix": "293-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_94",
            "tgt_ix": "293-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_95",
            "tgt_ix": "293-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_96",
            "tgt_ix": "293-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_97",
            "tgt_ix": "293-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_98",
            "tgt_ix": "293-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_99",
            "tgt_ix": "293-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_100",
            "tgt_ix": "293-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_101",
            "tgt_ix": "293-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_102",
            "tgt_ix": "293-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_103",
            "tgt_ix": "293-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_104",
            "tgt_ix": "293-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_105",
            "tgt_ix": "293-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_106",
            "tgt_ix": "293-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_107",
            "tgt_ix": "293-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_108",
            "tgt_ix": "293-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_109",
            "tgt_ix": "293-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "293-ARR_v1_110",
            "tgt_ix": "293-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1078,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "293-ARR",
        "version": 1
    }
}