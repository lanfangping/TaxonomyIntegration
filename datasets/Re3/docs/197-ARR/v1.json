{
    "nodes": [
        {
            "ix": "197-ARR_v1_0",
            "content": "PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_2",
            "content": "BERT and other pre-trained language models (PLMs) are ubiquitous in the modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (Qiu et al., 2020), the significant latency during inference forbids more widely industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-theshelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer's prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can make achieve different speedup ratios by adjusting the patience parameter and the confidence threshold.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "197-ARR_v1_4",
            "content": "Since BERT (Devlin et al., 2018), the pre-trained language models (PLMs) become the default stateof-the-art (SOTA) models for natural language processing (NLP). The recent years have witnessed the rise of many PLMs, such as GPT (Radford et al., 2019), XLNet (Yang et al., 2019), and AL-BERT (Lan et al., 2020), and so forth. These BERTstyle models achieved considerable improvements in many Natural Language Processing (NLP) tasks by pre-training on the unlabeled corpus and finetuning on labeled tasks, such as text classification, natural language inference (NLI), sequence labeling. Despite their excellent performances, there are two issues for PLMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_5",
            "content": "First, previous studies show that PLMs such as BERT suffer from the over-thinking problem. Zhu et al., 2021) shows that in the sentence classification task, BERT's last few layers may be too deep for some samples. For a sentence classification task, if we insert a classifier on a certain intermediate layer and drop the deeper layers, these intermediate layers may outperform the last layer. The second drawback of PLMs is their high latency. Sentence classification (CLS) tasks play a central role in many application scenarios, such as dialogue systems, document analysis, content recommendation, etc. However, these applications are time-sensitive. For example, if a task-oriented dialogue (TOD) system takes a lot of time to respond, users will have no doubt stop using this system. User experience studies show that a response has to be made in between 0-100 ms. Thus, a CLS module should be efficient and accurate. In addition, a special feature of consumer queries is that there are times when the number of queries is extremely high. For example, during the flu season, online medical consultation will be used much often than usual. Thus, it is important for deployed models and our PCEE-BERT, a novel early exiting method that combines the score-based early exiting with the patience-based early exiting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_6",
            "content": "to adjust their latency dynamically. During peak hours, it switches to a low-latency mode to deal with more queries. And in other hours, it makes the best of itself to provide accurate answers. So how can we make model inference dynamically?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_7",
            "content": "The answer is adaptive inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_8",
            "content": "There exists a branch of literature focusing on making PLMs' inference more efficient via network pruning (Zhu and Gupta, 2018;Fan et al., 2020;Michel et al., 2019), knowledge distillation (Sun et al., 2019;Sanh et al., 2019;Jiao et al., 2020a), weight quantization Bai et al., 2020;Kim et al., 2021) and adaptive inference Xin et al., 2020;. The adaptive inference has drawn much attention. The idea of adaptive inference is to deal with simple examples with only shallow layers of BERT and process more difficult queries with deeper layers, thus significantly speeding up the inference time on average while maintaining high accuracy. The speed-up ratio can be easily controlled with certain hyper-parameters to process significant changes in query traffic without re-deploying the model services or maintaining a group of models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_9",
            "content": "Early exiting is one of the most important adaptive inference methods (Bolukbasi et al., 2017). As depicted in Figure 2(b), it implements adaptive inference by installing an early exit, i.e., an intermediate prediction layer, at each layer of BERT and early exiting \"easy\" samples to speed up inference. At the training stage, all the exits are jointly optimized with BERT's parameters. At the inference stage, there are two different settings. First, in budgeted exiting mode, the model makes a prediction with a fixed exit for all queries. This mode deals with heavy traffic by assigning a shallower exit for prediction. The other one is dynamic exiting mode. That is, some strategies for early exiting are designed to decide whether to exit at each layer given the currently obtained predictions (from previous and current layers) (Teerapittayanon et al., 2016;Kaya et al., 2019;Xin et al., 2020;. In this mode, different samples can exit at different depths.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_10",
            "content": "There are mainly three early exiting strategies for BERT dynamic exiting. The first one is scorebased early exiting. BranchyNet (Teerapittayanon et al., 2016), FastBERT , and Dee-BERT (Xin et al., 2020) calculated the entropy of the prediction probability distribution as an estimation for the confidence of exiting classifiers to enable dynamic early exiting. Shallow-Deep Nets (Kaya et al., 2019) and RightTool (Schwartz et al., 2020a) leveraged the maximum of the predicted distribution as the exiting signal. The second type is the learned exiting (Elbayad et al., 2020). In this type of work, an early exiting signal is generated by a learnable module in the neural network. For example, BERxiT (Xin et al., 2021) install a fully connected layer right after each transformer block of BERT to output a score that is used to decide whether the BERT should stop inference and exit early. The third type is patience-based early exiting, which relies on cross-layer comparison to formulate the exiting signal. PABEE propose a dynamic exiting strategy analogous to early stopping model training. That is, if the exits' predictions remain unchanged for a pre-defined number of times (patience), the model will stop inference and exit early. PABEE achieves SOTAs results for BERT early exiting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_11",
            "content": "Despite its state-of-the-art performances during early exiting, PABEE is inflexible in adjusting the speedup ratios. On a given task, once the multiexit BERT is fine-tuned and the patience parameter is fixed, PABEE can only achieve a fixed average speedup ratio. Thus, PABEE can not achieve speedup ratios of certain values. This drawback makes PABEE inconvenient to use in real industrial scenarios. Thus, it is of great importance to come up with a method that can flexibly adjust its speedup ratios and performs comparable to or better than PABEE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_12",
            "content": "In this work, we propose Patiently Confidently Early Exiting BERT (PCEE-BERT), a novel early exiting method that combines the advantage of score-based methods and the patience based early exiting method. A multi-exit BERT is adopted as the backbone model, and an intermediate classifier (i.e., an exit) is installed right after each transformer black. PCEE-BERT will early exit if there are enough numbers (i.e., the patience parameter) of consecutive exits being confident for their predicted distributions. We mainly use entropy as the confidence measure. Intuitively, our method requires patience and confidence. It will not rush into an early exiting if we only see a couple of intermediate layers being confident. In addition, it allows the next layer to modify the predictions. In this way, our PCEE-BERT can exit with higher accuracy while maintaining flexibility.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_13",
            "content": "Extensive experiments are conducted on the GLUE benchmark (Wang et al., 2018). The results show that our method outperforms the previous SOTA early exiting methods, especially in cases where the speedup ratio is large. In addition, one can adjust the patience and confidence threshold so that PCEE-BERT can arrive at different speedup ratios. A series of ablation studies are conducted, resulting in the following observations: (a) PCEE-BERT can work with different confidence measures; (b) our method performs consistently well on different PLMs, and can work alongside model compression methods to further speed up the BERT's inference; (c) our PCEE-BERT can also be applied to computer vision tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_14",
            "content": "The rest of the paper is organized as follows. First, we introduce the preliminaries for multi-exit BERT and early exiting. Second, we elaborate on our PCEE-BERT method. Third, we conduct experiments on the GLUE benchmark and conduct a series of ablations studies. Finally, we conclude with possible future works.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_15",
            "content": "Preliminaries",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "197-ARR_v1_16",
            "content": "In this section, we introduce the necessary background for BERT early exiting. Throughout this work, we consider the case of multi-class classification with samples {(x, y), x \u2208 X , y \u2208 Y, i = 1, 2, ..., N }, e.g., sentences, and the number of classes is K.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_17",
            "content": "Backbone models",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "197-ARR_v1_18",
            "content": "In this work, we adopt BERT as the backbone model. BERT is a multi-layer Transformer (Vaswani et al., 2017) network, which is pre-trained in a self-supervised manner on a large corpus. The number of transformer layers of our backbone is denoted as M , and the hidden dimension is d.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_19",
            "content": "Early-exiting Architecture",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "197-ARR_v1_20",
            "content": "As depicted in Figure 2, early exiting architectures are networks with exits at each transformer layer. With M exits, M classifiers f (m) (x; \u03b8 (m) ) : X \u2192 \u2206 K (m = 1, 2, ..., M ) are designated at M layers of BERT, each of which maps its input to p (m) (x; \u03b8 (m) ), a probability distribution over the K classes. All the parameters of the transformer layers and exits are denoted as \u0398.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_21",
            "content": "Training",
            "ntype": "title",
            "meta": {
                "section": "2.2.1"
            }
        },
        {
            "ix": "197-ARR_v1_22",
            "content": "At the training stage, all the exits are jointly optimized with a summed loss function. Following Huang et al. (2017) and , the loss function is the weighted average of the crossentropy (CE) losses given by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_23",
            "content": "L = M m=1 m * L (m) M m=1 m ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_24",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_25",
            "content": "L (m) = CE(y, p (m) (x; \u03b8 (m)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_26",
            "content": ")) denotes the cross-entropy loss of the m-th exit. Note that the weight m corresponds to the relative inference cost of exit m.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_27",
            "content": "Inference",
            "ntype": "title",
            "meta": {
                "section": "2.2.2"
            }
        },
        {
            "ix": "197-ARR_v1_28",
            "content": "During inference, the multi-exit BERT can exit early in two different modes, depending on whether the computational budget to classify an example is known or not. Budgeted Exiting. If the computational budget is known, we can directly appoint a suitable exit m * of BERT, f (m * ) (x; \u03b8 (m * ) ), to predict all queries. Dynamic Exiting. Under this mode, after receiving a query input x, the model starts to predict on the classifiers f (1) (x; \u03b8 (1) ), f (2) (x; \u03b8 (2) ), ..., in turn in a forward pass, reusing computation where possible. It will continue to do so until it receives a signal to stop early at an exit m * < M , or arrives at the last exit M . At this point, it will output the final predictions based on the current and previous predictions. Note that under this early exit setting, different samples might exit at different layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_29",
            "content": "PCEE-BERT",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "197-ARR_v1_30",
            "content": "Motivation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "197-ARR_v1_31",
            "content": "PABEE achieves the SOTA performances for BERT early exiting by applying an early exiting decisionmaking process that mimics the early stopping of model training. However, one drawback of PABEE is that it can not flexibly adjust the average inference layers (i.e., speed-ups) for a given dataset once its patience parameter is set. Table 1 shows PABEE can not achieve certain values for average inference layers, such as around 4.0, 6.0, or 9.0 on RTE. This drawback may limit the industrial usage of early exiting techniques. Thus, it is of great importance to develop a new method that performs comparably with PABEE and is more flexible than PABEE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_32",
            "content": "PCEE-BERT: a novel dynamic exiting method",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "197-ARR_v1_33",
            "content": "The inference process of PCEE-BERT is illustrated in Figure 2(b). Assume the feed forward process for predicting sample x has gone through layers 1, ..., m \u2212 1, and we are now at layer m. After going through the transformer layer m, the intermediate classifier",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_34",
            "content": "f (m) (x; \u03b8 (m) ) predicts a class label distri- bution p (m) (x; \u03b8 (m)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_35",
            "content": "). The confidence level of layer m is measured by the entropy value of distribution p (m) (x; \u03b8 (m) ):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_36",
            "content": "C (m) = K k=1 p (m) k log p (m) k log(1/K) ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_37",
            "content": "where p (m) k is the probability mass for k-th class label. If C (m) is smaller than a pre-defined threshold \u03c4 , the predictions of layer m is considered confident. Otherwise, it is considered in-confident.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_38",
            "content": "We use a patience counter pct to store the number of times that the predictions remain confident in consecutive layers. Formally, at layer m, pct (m) is calculated as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_39",
            "content": "pct (m) = pct (m\u22121) + 1, if C (m) < \u03c4, 0, otherwise.(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_40",
            "content": "We stop inference early at layer m when pct (m) reaches a predefined integer number t (the patience parameter). If this condition is never fulfilled, we use the final classifier M for prediction. In this way, the model can make an early exit without passing through all layers to make a prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_41",
            "content": "Our method draws advantages from the previous score-based early exiting method (Teerapittayanon et al., 2016) and patience-based method and overcomes their shortcomings. First, the score-based early exiting method relies on the confidence score from only the current layer. However, as revealed by Szegedy et al. (2014); Jiang et al. (2018), prediction of probability distributions (i.e., softmax scores) suffers from being over-confident to one class, making it an unreliable metric to represent confidence. In our method, early exiting occurs when a group of consecutive layers is confident, thus making the early exiting decision more reliable. Second, with a patiencebased early exiting method like PABEE, when a deeper layer tries to correct the predictions, the patience count resets to zero. As a result, PABEE is less efficient than our PCEE-BERT. Third, since our method is a combination of PABEE and the score-based method, one can conveniently adjust the threshold and patience parameters to control the speed-up ratios, which makes our method more flexible than PABEE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_42",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "197-ARR_v1_43",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "197-ARR_v1_44",
            "content": "We evaluate our proposed approach to the classification tasks on the GLUE benchmark. We only exclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2018;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_45",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "197-ARR_v1_46",
            "content": "We compare our approaches with three groups of baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_47",
            "content": "Backbone models: We mainly choose the BERT-base model open-sourced by Devlin et al. (2019) as the backbone model. We also investigate whether our method is applicable across different backbones, so we also run ablation experiments with ALBERT base (Lan et al., 2020) and TinyBERT 6 (Jiao et al., 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_48",
            "content": "Budgeted exiting: In the section 2.2 we have introduced how to train a multi-exit BERT. Once the multi-exit BERT, we can conduct budgeted early exiting, that is, asking a designated intermediate layer to encode and predict all the samples. Budgeted exiting is a direct way to speed up BERT's inference, but it is instance adaptive. Some of the samples may not need to go through many of the BERT's layers, and the others may be more difficult and require deeper feature encoding from the deeper layers of BERT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_49",
            "content": "Dynamic exiting: In this part, we compare our methods with a series of strong baselines, including BranchyNet (Teerapittayanon et al., 2016), Shallow-Deep (Kaya et al., 2019), BERxiT (Xin et al., 2021), and PABEE . Note that PABEE can not flexibly adjust the average inference layers on a task once the patience parameter is set. So we will adjust the thresholds in the other baselines and our PCEE-BERT so that all methods' number of average inference layers are close.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_50",
            "content": "Evaluation of early exiting method",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "197-ARR_v1_51",
            "content": "In this work, we strictly follow the GLUE benchmark to report the performances metrics on each task. Note that this work focuses on investigating the early exiting of PLMs. Thus we have to consider the trade-offs between performance and efficiency. Following PABEE , we mainly report the speedup ratio as the efficiency metric. Assume the PLM backbone has N layers in total. For each test sample x i (i \u2208 {0, 1, ..., N }), the early exiting layer is m i , then the average speedup ratio on the test set is calculated by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_52",
            "content": "Speedup = 1 \u2212 N 1 m i N 1 M . (4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_53",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_54",
            "content": "We choose this efficiency metric for the following reason: (1) it is linear w.r.t. the actual amount of computation;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_55",
            "content": "(2) according to our experiments, it is proportional to actual wall-clock runtime and is also more stable across different runs compared with actual runtime due to randomness by other processes on the same machine.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_56",
            "content": "Experimental settings",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "197-ARR_v1_57",
            "content": "Training We add a linear output layer after each intermediate layer of the pre-trained BERT or other backbone models as the internal classifiers. We perform grid search over batch sizes of 16, 32, 128, and learning rates of 1e-5, 2e-5, 3e-5, 5e-5 with an Adam optimizer. The hyper-parameters are selected via the 5-fold cross validation on the train set of GLUE tasks. We implement PCEE-BERT on the base of Hugging Face's Transformers (Wolf et al., 2020). Experiments are conducted on a single Nvidia V100 16GB GPU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_58",
            "content": "Inference Following prior work on inputadaptive inference (Teerapittayanon et al., 2016;Kaya et al., 2019), inference is on a per-instance basis, i.e., the batch size for inference is set to 1. This is a common scenario in the industry where individual requests from different users (Schwartz et al., 2020b) come at different time points. We report the median performance over five runs with different random seeds.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_59",
            "content": "Main results",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "197-ARR_v1_60",
            "content": "In Table 2, we report the performance comparisons of each method on the GLUE benchmark under three different speedup settings. The three speedup settings are: (1) 74% to 82% speedup; (2) 46% to 54% speedup; (3) 23% to 28% speedup. Since PABEE can not flexibly adjust the speedup ratios for a given patience parameter and a given task, we adjust the hyper-parameters (such as entropy threshold) of our PCEE-BERT and the other baselines to achieve similar speedups with PABEE. The results in table 2 clearly show that our PCEE-BERT method outperforms the baseline methods under different speedup ratios. Table 2 also shows that the PABEE method is the best performing baseline. Thus, in order to further analyze and better visualize the results, we draw the score-speedup curves (in Figure 3) for budgeted early exiting, PABEE and PCEE-BERT, on the QNLI and MRPC tasks. 1 With Table 2 and Figure 3, we can make the following observations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_61",
            "content": "\u2022 Although it is clear that PABEE performs better than the other baselines when the speedup ratio is around 50% or 25%, its advantages over the other baselines with the 75% speedup ratio is relatively small. With the 75% speedup ratio for seven GLUE tasks, it performs better than the score-based methods only on three tasks. This observation motivates us to improve PABEE by combining its patience-based early exiting mechanism with the score-based ones. \u2022 Our PCEE-BERT consistently performs better than the baseline methods, especially when the speedup ratio is large. Note that our PCEE-BERT also consistently outperforms the budgeted exiting speedup ratios, which the other baselines do not achieve. Figure 3(a) and 3(b) show that score-speedup curve for PABEE is interleaving with that of the budgeted exiting. However, the score-speedup curve for PCEE-BERT distances itself from the others for most of the GLUE tasks. \u2022 The overthinking problem is prevailing in the GLUE benchmark, and our PCEE-BERT early exiting can effectively take advantage of this phenomenon. For 6 of the GLUE tasks, PCEE-BERT can outperform BERT-base with a 25% (or more than) speedup ratio. And for 2 of the GLUE tasks, PCEE-BERT can outperform BERT-base with a 50% (or more than) speedup ratio.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_62",
            "content": "Putting performance comparisons aside, one benefit of PCEE-BERT is that it is flexible since by adjusting the threshold and the patience parameter, it can easily control the average inference layers and cover (or achieve values close to) any speedup ratios. 2) with that from Shallow-Deep (Kaya et al., 2019):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_63",
            "content": "C (M ) = Argmax k p (m) k ,(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_64",
            "content": "and we will call this version of PCEE-BERT as PCEE-BERT-v1. Note that PCEE-BERT-v1 does not require a newly fine-tuned model. With BERxiT, we can come up with PCEE-BERT-v2. Following BERxiT, PCEE-BERT-v2 fine-tunes the multi-exit BERT with a fully connected layer right after each transformer block designated to evaluate the confidence score C (M ) for early exiting at that layer. C (M ) is learned along with the training of intermediate classifiers. Note that PCEE-BERT-v2 can not reuse the fine-tuned checkpoints used in PCEE-BERT and requires one to fine-tune the BERT backbones on the task at hand.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_65",
            "content": "We conduct the experiments on the QNLI tasks, and the results are reported in Figure 4. We can see that PCEE-BERT-v1 and PCEE-BERT-v2 perform comparably to PCEE-BERT. The results show that the proposed PCEE-BERT early exiting mechanism is off-the-shelf, and the reason for the success of our PCEE-BERT is its early exiting mechanism, that is, early exit if a group of consecutive exits is confident for their predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_66",
            "content": "Ablation of PLM backbones",
            "ntype": "title",
            "meta": {
                "section": "4.6.2"
            }
        },
        {
            "ix": "197-ARR_v1_67",
            "content": "In the main experiments, we use BERT as the pretrained backbone model. However, PCEE-BERT can also work with the other types of pre-trained backbones, such as ALBERT base (Lan et al., 2020) and TinyBERT 6 (Jiao et al., 2020b). We conduct the experiments on the QNLI task with these two 2 See the Appendix for demonstration on MRPC. We can see that when using the other pre-trained backbones, PCEE-BERT also performs better than the baseline methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_68",
            "content": "The results for PCEE-BERT on the TinyBERT also convey an important message: as an inference speedup method, our PCEE method can work alongside the model compression methods to further reduce the latency of BERT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_69",
            "content": "Ablation of cross-layer ensemble",
            "ntype": "title",
            "meta": {
                "section": "4.6.3"
            }
        },
        {
            "ix": "197-ARR_v1_70",
            "content": "Since we have a prediction module at each layer of BERT, we can conduct model ensemble across layers that the forward pass has gone through already. In Figure 6, we conduct the ablation studies on the RTE and QNLI tasks. According to Figure 6, cross-layer ensemble leads to performance degradation when the speedup ratio is large, while when the average inference layers is close to the number of BERT's transformer blocks M , cross-layer ensemble results in slight improvements. In conclusion, the cross-layer ensemble does not result in consistent performance improvements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_71",
            "content": "A possible application of the above results is to apply the cross-layer ensemble when a low speedup ratio is applied. And when we ask the model to exit early in the shallow layers, the cross-layer ensemble is not used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_72",
            "content": "PCEE-BERT are effective for image classification",
            "ntype": "title",
            "meta": {
                "section": "4.6.4"
            }
        },
        {
            "ix": "197-ARR_v1_73",
            "content": "Our main experiments are conducted on BERT, a pre-trained language model, and the GLUE benchmark, a series of natural language understanding tasks. However, our PCEE-BERT method is a plugand-play early exiting and can be applied to models and tasks of different modalities. To demonstrate the effectiveness of PCEE-BERT on the image classification task, we follow the experimental settings in PABEE . We conduct experiments on two image classification datasets, CIFAR-10 and CIFAR-100 (Krizhevsky, 2009). The ResNet-56 model (He et al., 2016) serves as the backbone, and we compare PCEE-BERT with PABEE. We place an exiting classifier at every two convolutional layers. We set the batch size to 128 and use an SGD optimizer with a learning rate of 0.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_74",
            "content": "Table 3 reports the results. PCEE-BERT outperforms PABEE when early exiting at different speedup ratios. In addition, the performance advantages of PCEE-BERT are larger when the speedup ratio is large, which is also observed in the NLP tasks. And PCEE-BERT outperforms the original ResNet-56 on both tasks even when it provides around 25% speedup.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_75",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "197-ARR_v1_76",
            "content": "In this work, we propose PCEE-BERT, a novel efficient inference method that can yield a better performance-speed trade-off than the existing early exiting methods. PCEE-BERT adopts BERT as the backbone model and makes the exiting decision if there are enough intermediate layers to make confident predictions. The confidence level is measured by the entropy of the predicted distributions. Experiments on the GLUE benchmark demonstrate that our method outperforms the previous SOTA early exiting methods, especially when the speedup ratio is large. In addition, PCEE-BERT can achieve different speedup ratios by adjusting the patience parameter and the confidence threshold, which makes it more flexible in industrial usage. Ablation studies show that:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "197-ARR_v1_77",
            "content": "Haoli Bai, Wei Zhang, L Hou, L Shang, Jing Jin, X Jiang, Qun Liu, Michael Lyu, Irwin King, Binarybert: Pushing the limit of bert quantization, 2012, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Haoli Bai",
                    "Wei Zhang",
                    "L Hou",
                    "L Shang",
                    "Jing Jin",
                    "X Jiang",
                    "Qun Liu",
                    "Michael Lyu",
                    "Irwin King"
                ],
                "title": "Binarybert: Pushing the limit of bert quantization",
                "pub_date": "2012",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_78",
            "content": "Tolga Bolukbasi, J Wang, O Dekel, Venkatesh Saligrama, Adaptive neural networks for efficient inference, 2017, ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Tolga Bolukbasi",
                    "J Wang",
                    "O Dekel",
                    "Venkatesh Saligrama"
                ],
                "title": "Adaptive neural networks for efficient inference",
                "pub_date": "2017",
                "pub_title": "ICML",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_79",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_80",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "197-ARR_v1_81",
            "content": "UNKNOWN, None, 1910, Depth-adaptive transformer. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "1910",
                "pub_title": "Depth-adaptive transformer. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_82",
            "content": ", Angela Fan, Armand Grave,  Joulin, Reducing transformer depth on demand with structured dropout, 2020, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    ", Angela Fan",
                    "Armand Grave",
                    " Joulin"
                ],
                "title": "Reducing transformer depth on demand with structured dropout",
                "pub_date": "2020",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_83",
            "content": "X Kaiming He, Shaoqing Zhang, Jian Ren,  Sun, Deep residual learning for image recognition, 2016, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "X Kaiming He",
                    "Shaoqing Zhang",
                    "Jian Ren",
                    " Sun"
                ],
                "title": "Deep residual learning for image recognition",
                "pub_date": "2016",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_84",
            "content": "Gao Huang, Danlu Chen, T Li, Felix Wu, L Maaten, Kilian Weinberger, Multi-scale dense convolutional networks for efficient prediction, 2017, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Gao Huang",
                    "Danlu Chen",
                    "T Li",
                    "Felix Wu",
                    "L Maaten",
                    "Kilian Weinberger"
                ],
                "title": "Multi-scale dense convolutional networks for efficient prediction",
                "pub_date": "2017",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_85",
            "content": "UNKNOWN, None, 2018, To trust or not to trust a classifier, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "To trust or not to trust a classifier",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_86",
            "content": "Y Xiaoqi Jiao, L Yin, Xin Shang, X Jiang,  Chen, F Li, Qun Wang,  Liu, Tinybert: Distilling bert for natural language understanding, 1909, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Y Xiaoqi Jiao",
                    "L Yin",
                    "Xin Shang",
                    "X Jiang",
                    " Chen",
                    "F Li",
                    "Qun Wang",
                    " Liu"
                ],
                "title": "Tinybert: Distilling bert for natural language understanding",
                "pub_date": "1909",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_87",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, TinyBERT: Distilling BERT for natural language understanding, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Xiaoqi Jiao",
                    "Yichun Yin",
                    "Lifeng Shang",
                    "Xin Jiang",
                    "Xiao Chen",
                    "Linlin Li",
                    "Fang Wang",
                    "Qun Liu"
                ],
                "title": "TinyBERT: Distilling BERT for natural language understanding",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_88",
            "content": "Y Kaya, Sanghyun Hong, T Dumitras, Shallow-deep networks: Understanding and mitigating network overthinking, 2019, ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Y Kaya",
                    "Sanghyun Hong",
                    "T Dumitras"
                ],
                "title": "Shallow-deep networks: Understanding and mitigating network overthinking",
                "pub_date": "2019",
                "pub_title": "ICML",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_89",
            "content": "Se-Hoon Kim, Amir Gholami, Zhewei Yao, M Mahoney, K Keutzer, 2021. I-bert: Integer-only bert quantization, , ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Se-Hoon Kim",
                    "Amir Gholami",
                    "Zhewei Yao",
                    "M Mahoney",
                    "K Keutzer"
                ],
                "title": "2021. I-bert: Integer-only bert quantization",
                "pub_date": null,
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_90",
            "content": "UNKNOWN, None, 2009, Learning multiple layers of features from tiny images, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Learning multiple layers of features from tiny images",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_91",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, Albert: A lite bert for self-supervised learning of language representations, 2020, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Zhenzhong Lan",
                    "Mingda Chen",
                    "Sebastian Goodman",
                    "Kevin Gimpel",
                    "Piyush Sharma",
                    "Radu Soricut"
                ],
                "title": "Albert: A lite bert for self-supervised learning of language representations",
                "pub_date": "2020",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_92",
            "content": "Weijie Liu, P Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, Q Ju, Fastbert: a self-distilling bert with adaptive inference time, 2004, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Weijie Liu",
                    "P Zhou",
                    "Zhe Zhao",
                    "Zhiruo Wang",
                    "Haotang Deng",
                    "Q Ju"
                ],
                "title": "Fastbert: a self-distilling bert with adaptive inference time",
                "pub_date": "2004",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_93",
            "content": "UNKNOWN, None, 2019, Are sixteen heads really better than one?, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Are sixteen heads really better than one?",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_94",
            "content": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Pre-trained models for natural language processing: A survey, 2003, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Xipeng Qiu",
                    "Tianxiang Sun",
                    "Yige Xu",
                    "Yunfan Shao",
                    "Ning Dai",
                    "Xuanjing Huang"
                ],
                "title": "Pre-trained models for natural language processing: A survey",
                "pub_date": "2003",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_95",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Language models are unsupervised multitask learners",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_96",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Victor Sanh",
                    "Lysandre Debut",
                    "Julien Chaumond",
                    "Thomas Wolf"
                ],
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "pub_date": "2019",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_97",
            "content": "Roy Schwartz, Gabi Stanovsky, Swabha Swayamdipta, Jesse Dodge, N Smith, The right tool for the job: Matching model and instance complexities, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Roy Schwartz",
                    "Gabi Stanovsky",
                    "Swabha Swayamdipta",
                    "Jesse Dodge",
                    "N Smith"
                ],
                "title": "The right tool for the job: Matching model and instance complexities",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_98",
            "content": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah Smith, The right tool for the job: Matching model and instance complexities, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Roy Schwartz",
                    "Gabriel Stanovsky",
                    "Swabha Swayamdipta",
                    "Jesse Dodge",
                    "Noah Smith"
                ],
                "title": "The right tool for the job: Matching model and instance complexities",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "197-ARR_v1_99",
            "content": "S Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for bert model compression, 2019, EMNLP/IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "S Sun",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Jingjing Liu"
                ],
                "title": "Patient knowledge distillation for bert model compression",
                "pub_date": "2019",
                "pub_title": "EMNLP/IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_100",
            "content": "UNKNOWN, None, 2014, Intriguing properties of neural networks, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Intriguing properties of neural networks",
                "pub": "CoRR"
            }
        },
        {
            "ix": "197-ARR_v1_101",
            "content": "Surat Teerapittayanon, Bradley Mcdanel, H Kung, Branchynet: Fast inference via early exiting from deep neural networks, 2016, 23rd International Conference on Pattern Recognition (ICPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Surat Teerapittayanon",
                    "Bradley Mcdanel",
                    "H Kung"
                ],
                "title": "Branchynet: Fast inference via early exiting from deep neural networks",
                "pub_date": "2016",
                "pub_title": "23rd International Conference on Pattern Recognition (ICPR)",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_102",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_103",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_104",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "197-ARR_v1_105",
            "content": "UNKNOWN, None, 2004, Deebert: Dynamic early exiting for accelerating bert inference. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2004",
                "pub_title": "Deebert: Dynamic early exiting for accelerating bert inference. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_106",
            "content": "Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin, BERxiT: Early exiting for BERT with better fine-tuning and extension to regression, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Ji Xin",
                    "Raphael Tang",
                    "Yaoliang Yu",
                    "Jimmy Lin"
                ],
                "title": "BERxiT: Early exiting for BERT with better fine-tuning and extension to regression",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "197-ARR_v1_107",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, M Zhou, Bert-of-theseus: Compressing bert by progressive module replacing, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Canwen Xu",
                    "Wangchunshu Zhou",
                    "Tao Ge",
                    "Furu Wei",
                    "M Zhou"
                ],
                "title": "Bert-of-theseus: Compressing bert by progressive module replacing",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_108",
            "content": "Z Yang, Zihang Dai, Yiming Yang, J Carbonell, R Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Z Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "J Carbonell",
                    "R Salakhutdinov",
                    "V Quoc",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_109",
            "content": "UNKNOWN, None, 2009, Ternarybert: Distillation-aware ultra-low bit bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Ternarybert: Distillation-aware ultra-low bit bert",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_110",
            "content": "Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, Bert loses patience: Fast and robust inference with early exit, 2006, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Wangchunshu Zhou",
                    "Canwen Xu",
                    "Tao Ge",
                    "Julian Mcauley",
                    "Ke Xu",
                    "Furu Wei"
                ],
                "title": "Bert loses patience: Fast and robust inference with early exit",
                "pub_date": "2006",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_111",
            "content": "M Zhu, S Gupta, To prune, or not to prune: exploring the efficacy of pruning for model compression, 2018, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "M Zhu",
                    "S Gupta"
                ],
                "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
                "pub_date": "2018",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "197-ARR_v1_112",
            "content": "Wei Zhu, Xiaoling Wang, Yuan Ni, Guotong Xie, GAML-BERT: Improving BERT early exiting by gradient aligned mutual learning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Wei Zhu",
                    "Xiaoling Wang",
                    "Yuan Ni",
                    "Guotong Xie"
                ],
                "title": "GAML-BERT: Improving BERT early exiting by gradient aligned mutual learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "197-ARR_v1_0@0",
            "content": "PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_0",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@0",
            "content": "BERT and other pre-trained language models (PLMs) are ubiquitous in the modern NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@1",
            "content": "Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (Qiu et al., 2020), the significant latency during inference forbids more widely industrial usage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 84,
            "end": 263,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@2",
            "content": "In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-theshelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 265,
            "end": 494,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@3",
            "content": "With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 496,
            "end": 700,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@4",
            "content": "The entropy value measures the confidence level of an intermediate layer's prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 702,
            "end": 787,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@5",
            "content": "Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 789,
            "end": 898,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_2@6",
            "content": "Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can make achieve different speedup ratios by adjusting the patience parameter and the confidence threshold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_2",
            "start": 900,
            "end": 1134,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_4@0",
            "content": "Since BERT (Devlin et al., 2018), the pre-trained language models (PLMs) become the default stateof-the-art (SOTA) models for natural language processing (NLP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_4",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_4@1",
            "content": "The recent years have witnessed the rise of many PLMs, such as GPT (Radford et al., 2019), XLNet (Yang et al., 2019), and AL-BERT (Lan et al., 2020), and so forth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_4",
            "start": 161,
            "end": 323,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_4@2",
            "content": "These BERTstyle models achieved considerable improvements in many Natural Language Processing (NLP) tasks by pre-training on the unlabeled corpus and finetuning on labeled tasks, such as text classification, natural language inference (NLI), sequence labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_4",
            "start": 325,
            "end": 584,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_4@3",
            "content": "Despite their excellent performances, there are two issues for PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_4",
            "start": 586,
            "end": 653,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@0",
            "content": "First, previous studies show that PLMs such as BERT suffer from the over-thinking problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@1",
            "content": "Zhu et al., 2021) shows that in the sentence classification task, BERT's last few layers may be too deep for some samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 91,
            "end": 212,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@2",
            "content": "For a sentence classification task, if we insert a classifier on a certain intermediate layer and drop the deeper layers, these intermediate layers may outperform the last layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 214,
            "end": 391,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@3",
            "content": "The second drawback of PLMs is their high latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 393,
            "end": 442,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@4",
            "content": "Sentence classification (CLS) tasks play a central role in many application scenarios, such as dialogue systems, document analysis, content recommendation, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 444,
            "end": 603,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@5",
            "content": "However, these applications are time-sensitive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 605,
            "end": 651,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@6",
            "content": "For example, if a task-oriented dialogue (TOD) system takes a lot of time to respond, users will have no doubt stop using this system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 653,
            "end": 786,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@7",
            "content": "User experience studies show that a response has to be made in between 0-100 ms. Thus, a CLS module should be efficient and accurate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 788,
            "end": 920,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@8",
            "content": "In addition, a special feature of consumer queries is that there are times when the number of queries is extremely high.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 922,
            "end": 1041,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@9",
            "content": "For example, during the flu season, online medical consultation will be used much often than usual.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 1043,
            "end": 1141,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_5@10",
            "content": "Thus, it is important for deployed models and our PCEE-BERT, a novel early exiting method that combines the score-based early exiting with the patience-based early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_5",
            "start": 1143,
            "end": 1314,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_6@0",
            "content": "to adjust their latency dynamically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_6",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_6@1",
            "content": "During peak hours, it switches to a low-latency mode to deal with more queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_6",
            "start": 37,
            "end": 115,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_6@2",
            "content": "And in other hours, it makes the best of itself to provide accurate answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_6",
            "start": 117,
            "end": 192,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_6@3",
            "content": "So how can we make model inference dynamically?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_6",
            "start": 194,
            "end": 240,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_7@0",
            "content": "The answer is adaptive inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_7",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_8@0",
            "content": "There exists a branch of literature focusing on making PLMs' inference more efficient via network pruning (Zhu and Gupta, 2018;Fan et al., 2020;Michel et al., 2019), knowledge distillation (Sun et al., 2019;Sanh et al., 2019;Jiao et al., 2020a), weight quantization Bai et al., 2020;Kim et al., 2021) and adaptive inference Xin et al., 2020;. The adaptive inference has drawn much attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_8",
            "start": 0,
            "end": 390,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_8@1",
            "content": "The idea of adaptive inference is to deal with simple examples with only shallow layers of BERT and process more difficult queries with deeper layers, thus significantly speeding up the inference time on average while maintaining high accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_8",
            "start": 392,
            "end": 635,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_8@2",
            "content": "The speed-up ratio can be easily controlled with certain hyper-parameters to process significant changes in query traffic without re-deploying the model services or maintaining a group of models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_8",
            "start": 637,
            "end": 831,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@0",
            "content": "Early exiting is one of the most important adaptive inference methods (Bolukbasi et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@1",
            "content": "As depicted in Figure 2(b), it implements adaptive inference by installing an early exit, i.e., an intermediate prediction layer, at each layer of BERT and early exiting \"easy\" samples to speed up inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 96,
            "end": 302,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@2",
            "content": "At the training stage, all the exits are jointly optimized with BERT's parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 304,
            "end": 385,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@3",
            "content": "At the inference stage, there are two different settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 387,
            "end": 443,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@4",
            "content": "First, in budgeted exiting mode, the model makes a prediction with a fixed exit for all queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 445,
            "end": 540,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@5",
            "content": "This mode deals with heavy traffic by assigning a shallower exit for prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 542,
            "end": 621,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@6",
            "content": "The other one is dynamic exiting mode.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 623,
            "end": 660,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_9@7",
            "content": "That is, some strategies for early exiting are designed to decide whether to exit at each layer given the currently obtained predictions (from previous and current layers) (Teerapittayanon et al., 2016;Kaya et al., 2019;Xin et al., 2020;. In this mode, different samples can exit at different depths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_9",
            "start": 662,
            "end": 961,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@0",
            "content": "There are mainly three early exiting strategies for BERT dynamic exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@1",
            "content": "The first one is scorebased early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 74,
            "end": 115,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@2",
            "content": "BranchyNet (Teerapittayanon et al., 2016), FastBERT , and Dee-BERT (Xin et al., 2020) calculated the entropy of the prediction probability distribution as an estimation for the confidence of exiting classifiers to enable dynamic early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 117,
            "end": 359,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@3",
            "content": "Shallow-Deep Nets (Kaya et al., 2019) and RightTool (Schwartz et al., 2020a) leveraged the maximum of the predicted distribution as the exiting signal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 361,
            "end": 511,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@4",
            "content": "The second type is the learned exiting (Elbayad et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 513,
            "end": 574,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@5",
            "content": "In this type of work, an early exiting signal is generated by a learnable module in the neural network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 576,
            "end": 678,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@6",
            "content": "For example, BERxiT (Xin et al., 2021) install a fully connected layer right after each transformer block of BERT to output a score that is used to decide whether the BERT should stop inference and exit early.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 680,
            "end": 888,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@7",
            "content": "The third type is patience-based early exiting, which relies on cross-layer comparison to formulate the exiting signal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 890,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@8",
            "content": "PABEE propose a dynamic exiting strategy analogous to early stopping model training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 1010,
            "end": 1093,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@9",
            "content": "That is, if the exits' predictions remain unchanged for a pre-defined number of times (patience), the model will stop inference and exit early.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 1095,
            "end": 1237,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_10@10",
            "content": "PABEE achieves SOTAs results for BERT early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_10",
            "start": 1239,
            "end": 1290,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_11@0",
            "content": "Despite its state-of-the-art performances during early exiting, PABEE is inflexible in adjusting the speedup ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_11",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_11@1",
            "content": "On a given task, once the multiexit BERT is fine-tuned and the patience parameter is fixed, PABEE can only achieve a fixed average speedup ratio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_11",
            "start": 117,
            "end": 261,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_11@2",
            "content": "Thus, PABEE can not achieve speedup ratios of certain values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_11",
            "start": 263,
            "end": 323,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_11@3",
            "content": "This drawback makes PABEE inconvenient to use in real industrial scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_11",
            "start": 325,
            "end": 399,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_11@4",
            "content": "Thus, it is of great importance to come up with a method that can flexibly adjust its speedup ratios and performs comparable to or better than PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_11",
            "start": 401,
            "end": 549,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@0",
            "content": "In this work, we propose Patiently Confidently Early Exiting BERT (PCEE-BERT), a novel early exiting method that combines the advantage of score-based methods and the patience based early exiting method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@1",
            "content": "A multi-exit BERT is adopted as the backbone model, and an intermediate classifier (i.e., an exit) is installed right after each transformer black.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 204,
            "end": 350,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@2",
            "content": "PCEE-BERT will early exit if there are enough numbers (i.e., the patience parameter) of consecutive exits being confident for their predicted distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 352,
            "end": 507,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@3",
            "content": "We mainly use entropy as the confidence measure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 509,
            "end": 556,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@4",
            "content": "Intuitively, our method requires patience and confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 558,
            "end": 614,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@5",
            "content": "It will not rush into an early exiting if we only see a couple of intermediate layers being confident.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 616,
            "end": 717,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@6",
            "content": "In addition, it allows the next layer to modify the predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 719,
            "end": 782,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_12@7",
            "content": "In this way, our PCEE-BERT can exit with higher accuracy while maintaining flexibility.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_12",
            "start": 784,
            "end": 870,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_13@0",
            "content": "Extensive experiments are conducted on the GLUE benchmark (Wang et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_13",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_13@1",
            "content": "The results show that our method outperforms the previous SOTA early exiting methods, especially in cases where the speedup ratio is large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_13",
            "start": 79,
            "end": 217,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_13@2",
            "content": "In addition, one can adjust the patience and confidence threshold so that PCEE-BERT can arrive at different speedup ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_13",
            "start": 219,
            "end": 341,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_13@3",
            "content": "A series of ablation studies are conducted, resulting in the following observations: (a) PCEE-BERT can work with different confidence measures; (b) our method performs consistently well on different PLMs, and can work alongside model compression methods to further speed up the BERT's inference; (c) our PCEE-BERT can also be applied to computer vision tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_13",
            "start": 343,
            "end": 701,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_14@0",
            "content": "The rest of the paper is organized as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_14",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_14@1",
            "content": "First, we introduce the preliminaries for multi-exit BERT and early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_14",
            "start": 47,
            "end": 122,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_14@2",
            "content": "Second, we elaborate on our PCEE-BERT method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_14",
            "start": 124,
            "end": 168,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_14@3",
            "content": "Third, we conduct experiments on the GLUE benchmark and conduct a series of ablations studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_14",
            "start": 170,
            "end": 263,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_14@4",
            "content": "Finally, we conclude with possible future works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_14",
            "start": 265,
            "end": 312,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_15@0",
            "content": "Preliminaries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_15",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_16@0",
            "content": "In this section, we introduce the necessary background for BERT early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_16",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_16@1",
            "content": "Throughout this work, we consider the case of multi-class classification with samples {(x, y), x \u2208 X , y \u2208 Y, i = 1, 2, ..., N }, e.g., sentences, and the number of classes is K.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_16",
            "start": 79,
            "end": 256,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_17@0",
            "content": "Backbone models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_17",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_18@0",
            "content": "In this work, we adopt BERT as the backbone model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_18",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_18@1",
            "content": "BERT is a multi-layer Transformer (Vaswani et al., 2017) network, which is pre-trained in a self-supervised manner on a large corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_18",
            "start": 51,
            "end": 183,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_18@2",
            "content": "The number of transformer layers of our backbone is denoted as M , and the hidden dimension is d.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_18",
            "start": 185,
            "end": 281,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_19@0",
            "content": "Early-exiting Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_19",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_20@0",
            "content": "As depicted in Figure 2, early exiting architectures are networks with exits at each transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_20",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_20@1",
            "content": "With M exits, M classifiers f (m) (x; \u03b8 (m) ) : X \u2192 \u2206 K (m = 1, 2, ..., M ) are designated at M layers of BERT, each of which maps its input to p (m) (x; \u03b8 (m) ), a probability distribution over the K classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_20",
            "start": 104,
            "end": 312,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_20@2",
            "content": "All the parameters of the transformer layers and exits are denoted as \u0398.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_20",
            "start": 314,
            "end": 385,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_21@0",
            "content": "Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_21",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_22@0",
            "content": "At the training stage, all the exits are jointly optimized with a summed loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_22",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_22@1",
            "content": "Following Huang et al. (2017) and , the loss function is the weighted average of the crossentropy (CE) losses given by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_22",
            "start": 88,
            "end": 205,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_23@0",
            "content": "L = M m=1 m * L (m) M m=1 m ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_23",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_24@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_24",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_25@0",
            "content": "L (m) = CE(y, p (m) (x; \u03b8 (m)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_26@0",
            "content": ")) denotes the cross-entropy loss of the m-th exit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_26",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_26@1",
            "content": "Note that the weight m corresponds to the relative inference cost of exit m.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_26",
            "start": 52,
            "end": 127,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_27@0",
            "content": "Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_27",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@0",
            "content": "During inference, the multi-exit BERT can exit early in two different modes, depending on whether the computational budget to classify an example is known or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@1",
            "content": "Budgeted Exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 163,
            "end": 179,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@2",
            "content": "If the computational budget is known, we can directly appoint a suitable exit m * of BERT, f (m * ) (x; \u03b8 (m * ) ), to predict all queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 181,
            "end": 319,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@3",
            "content": "Dynamic Exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 321,
            "end": 336,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@4",
            "content": "Under this mode, after receiving a query input x, the model starts to predict on the classifiers f (1) (x; \u03b8 (1) ), f (2) (x; \u03b8 (2) ), ..., in turn in a forward pass, reusing computation where possible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 338,
            "end": 539,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@5",
            "content": "It will continue to do so until it receives a signal to stop early at an exit m * < M , or arrives at the last exit M .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 541,
            "end": 659,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@6",
            "content": "At this point, it will output the final predictions based on the current and previous predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 661,
            "end": 758,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_28@7",
            "content": "Note that under this early exit setting, different samples might exit at different layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_28",
            "start": 760,
            "end": 849,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_29@0",
            "content": "PCEE-BERT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_29",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_30@0",
            "content": "Motivation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_30",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_31@0",
            "content": "PABEE achieves the SOTA performances for BERT early exiting by applying an early exiting decisionmaking process that mimics the early stopping of model training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_31",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_31@1",
            "content": "However, one drawback of PABEE is that it can not flexibly adjust the average inference layers (i.e., speed-ups) for a given dataset once its patience parameter is set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_31",
            "start": 162,
            "end": 329,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_31@2",
            "content": "Table 1 shows PABEE can not achieve certain values for average inference layers, such as around 4.0, 6.0, or 9.0 on RTE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_31",
            "start": 331,
            "end": 450,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_31@3",
            "content": "This drawback may limit the industrial usage of early exiting techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_31",
            "start": 452,
            "end": 524,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_31@4",
            "content": "Thus, it is of great importance to develop a new method that performs comparably with PABEE and is more flexible than PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_31",
            "start": 526,
            "end": 649,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_32@0",
            "content": "PCEE-BERT: a novel dynamic exiting method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_32",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_33@0",
            "content": "The inference process of PCEE-BERT is illustrated in Figure 2(b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_33",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_33@1",
            "content": "Assume the feed forward process for predicting sample x has gone through layers 1, ..., m \u2212 1, and we are now at layer m. After going through the transformer layer m, the intermediate classifier",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_33",
            "start": 66,
            "end": 259,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_34@0",
            "content": "f (m) (x; \u03b8 (m) ) predicts a class label distri- bution p (m) (x; \u03b8 (m)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_34",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_35@0",
            "content": ").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_35",
            "start": 0,
            "end": 1,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_35@1",
            "content": "The confidence level of layer m is measured by the entropy value of distribution p (m) (x; \u03b8 (m) ):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_35",
            "start": 3,
            "end": 101,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_36@0",
            "content": "C (m) = K k=1 p (m) k log p (m) k log(1/K) ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_36",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_37@0",
            "content": "where p (m) k is the probability mass for k-th class label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_37",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_37@1",
            "content": "If C (m) is smaller than a pre-defined threshold \u03c4 , the predictions of layer m is considered confident.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_37",
            "start": 60,
            "end": 163,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_37@2",
            "content": "Otherwise, it is considered in-confident.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_37",
            "start": 165,
            "end": 205,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_38@0",
            "content": "We use a patience counter pct to store the number of times that the predictions remain confident in consecutive layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_38",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_38@1",
            "content": "Formally, at layer m, pct (m) is calculated as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_38",
            "start": 120,
            "end": 165,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_39@0",
            "content": "pct (m) = pct (m\u22121) + 1, if C (m) < \u03c4, 0, otherwise.(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_39",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_40@0",
            "content": "We stop inference early at layer m when pct (m) reaches a predefined integer number t (the patience parameter).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_40",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_40@1",
            "content": "If this condition is never fulfilled, we use the final classifier M for prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_40",
            "start": 112,
            "end": 194,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_40@2",
            "content": "In this way, the model can make an early exit without passing through all layers to make a prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_40",
            "start": 196,
            "end": 297,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@0",
            "content": "Our method draws advantages from the previous score-based early exiting method (Teerapittayanon et al., 2016) and patience-based method and overcomes their shortcomings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@1",
            "content": "First, the score-based early exiting method relies on the confidence score from only the current layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 170,
            "end": 272,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@2",
            "content": "However, as revealed by Szegedy et al. (2014); Jiang et al. (2018), prediction of probability distributions (i.e., softmax scores) suffers from being over-confident to one class, making it an unreliable metric to represent confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 274,
            "end": 507,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@3",
            "content": "In our method, early exiting occurs when a group of consecutive layers is confident, thus making the early exiting decision more reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 509,
            "end": 646,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@4",
            "content": "Second, with a patiencebased early exiting method like PABEE, when a deeper layer tries to correct the predictions, the patience count resets to zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 648,
            "end": 797,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@5",
            "content": "As a result, PABEE is less efficient than our PCEE-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 799,
            "end": 854,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_41@6",
            "content": "Third, since our method is a combination of PABEE and the score-based method, one can conveniently adjust the threshold and patience parameters to control the speed-up ratios, which makes our method more flexible than PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_41",
            "start": 856,
            "end": 1079,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_42@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_42",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_43@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_43",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_44@0",
            "content": "We evaluate our proposed approach to the classification tasks on the GLUE benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_44",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_44@1",
            "content": "We only exclude the STS-B task since it is a regression task, and we exclude the WNLI task following previous work (Devlin et al., 2018;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_44",
            "start": 85,
            "end": 221,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_45@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_45",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_46@0",
            "content": "We compare our approaches with three groups of baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_46",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_47@0",
            "content": "Backbone models: We mainly choose the BERT-base model open-sourced by Devlin et al. (2019) as the backbone model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_47",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_47@1",
            "content": "We also investigate whether our method is applicable across different backbones, so we also run ablation experiments with ALBERT base (Lan et al., 2020) and TinyBERT 6 (Jiao et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_47",
            "start": 114,
            "end": 302,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_48@0",
            "content": "Budgeted exiting: In the section 2.2 we have introduced how to train a multi-exit BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_48",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_48@1",
            "content": "Once the multi-exit BERT, we can conduct budgeted early exiting, that is, asking a designated intermediate layer to encode and predict all the samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_48",
            "start": 88,
            "end": 238,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_48@2",
            "content": "Budgeted exiting is a direct way to speed up BERT's inference, but it is instance adaptive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_48",
            "start": 240,
            "end": 330,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_48@3",
            "content": "Some of the samples may not need to go through many of the BERT's layers, and the others may be more difficult and require deeper feature encoding from the deeper layers of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_48",
            "start": 332,
            "end": 509,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_49@0",
            "content": "Dynamic exiting: In this part, we compare our methods with a series of strong baselines, including BranchyNet (Teerapittayanon et al., 2016), Shallow-Deep (Kaya et al., 2019), BERxiT (Xin et al., 2021), and PABEE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_49",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_49@1",
            "content": "Note that PABEE can not flexibly adjust the average inference layers on a task once the patience parameter is set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_49",
            "start": 215,
            "end": 328,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_49@2",
            "content": "So we will adjust the thresholds in the other baselines and our PCEE-BERT so that all methods' number of average inference layers are close.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_49",
            "start": 330,
            "end": 469,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_50@0",
            "content": "Evaluation of early exiting method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_50",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@0",
            "content": "In this work, we strictly follow the GLUE benchmark to report the performances metrics on each task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@1",
            "content": "Note that this work focuses on investigating the early exiting of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 101,
            "end": 171,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@2",
            "content": "Thus we have to consider the trade-offs between performance and efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 173,
            "end": 247,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@3",
            "content": "Following PABEE , we mainly report the speedup ratio as the efficiency metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 249,
            "end": 326,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@4",
            "content": "Assume the PLM backbone has N layers in total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 328,
            "end": 373,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_51@5",
            "content": "For each test sample x i (i \u2208 {0, 1, ..., N }), the early exiting layer is m i , then the average speedup ratio on the test set is calculated by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_51",
            "start": 375,
            "end": 518,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_52@0",
            "content": "Speedup = 1 \u2212 N 1 m i N 1 M . (4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_52",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_53@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_53",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_54@0",
            "content": "We choose this efficiency metric for the following reason: (1) it is linear w.r.t. the actual amount of computation;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_54",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_55@0",
            "content": "(2) according to our experiments, it is proportional to actual wall-clock runtime and is also more stable across different runs compared with actual runtime due to randomness by other processes on the same machine.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_55",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_56@0",
            "content": "Experimental settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_56",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_57@0",
            "content": "Training We add a linear output layer after each intermediate layer of the pre-trained BERT or other backbone models as the internal classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_57",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_57@1",
            "content": "We perform grid search over batch sizes of 16, 32, 128, and learning rates of 1e-5, 2e-5, 3e-5, 5e-5 with an Adam optimizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_57",
            "start": 146,
            "end": 269,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_57@2",
            "content": "The hyper-parameters are selected via the 5-fold cross validation on the train set of GLUE tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_57",
            "start": 271,
            "end": 367,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_57@3",
            "content": "We implement PCEE-BERT on the base of Hugging Face's Transformers (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_57",
            "start": 369,
            "end": 454,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_57@4",
            "content": "Experiments are conducted on a single Nvidia V100 16GB GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_57",
            "start": 456,
            "end": 514,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_58@0",
            "content": "Inference Following prior work on inputadaptive inference (Teerapittayanon et al., 2016;Kaya et al., 2019), inference is on a per-instance basis, i.e., the batch size for inference is set to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_58",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_58@1",
            "content": "This is a common scenario in the industry where individual requests from different users (Schwartz et al., 2020b) come at different time points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_58",
            "start": 194,
            "end": 337,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_58@2",
            "content": "We report the median performance over five runs with different random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_58",
            "start": 339,
            "end": 414,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_59@0",
            "content": "Main results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_59",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@0",
            "content": "In Table 2, we report the performance comparisons of each method on the GLUE benchmark under three different speedup settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@1",
            "content": "The three speedup settings are: (1) 74% to 82% speedup; (2) 46% to 54% speedup; (3) 23% to 28% speedup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 127,
            "end": 229,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@2",
            "content": "Since PABEE can not flexibly adjust the speedup ratios for a given patience parameter and a given task, we adjust the hyper-parameters (such as entropy threshold) of our PCEE-BERT and the other baselines to achieve similar speedups with PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 231,
            "end": 473,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@3",
            "content": "The results in table 2 clearly show that our PCEE-BERT method outperforms the baseline methods under different speedup ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 475,
            "end": 600,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@4",
            "content": "Table 2 also shows that the PABEE method is the best performing baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 602,
            "end": 674,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@5",
            "content": "Thus, in order to further analyze and better visualize the results, we draw the score-speedup curves (in Figure 3) for budgeted early exiting, PABEE and PCEE-BERT, on the QNLI and MRPC tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 676,
            "end": 866,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_60@6",
            "content": "1 With Table 2 and Figure 3, we can make the following observations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_60",
            "start": 868,
            "end": 935,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_61@0",
            "content": "\u2022 Although it is clear that PABEE performs better than the other baselines when the speedup ratio is around 50% or 25%, its advantages over the other baselines with the 75% speedup ratio is relatively small. With the 75% speedup ratio for seven GLUE tasks, it performs better than the score-based methods only on three tasks. This observation motivates us to improve PABEE by combining its patience-based early exiting mechanism with the score-based ones. \u2022 Our PCEE-BERT consistently performs better than the baseline methods, especially when the speedup ratio is large. Note that our PCEE-BERT also consistently outperforms the budgeted exiting speedup ratios, which the other baselines do not achieve. Figure 3(a) and 3(b) show that score-speedup curve for PABEE is interleaving with that of the budgeted exiting. However, the score-speedup curve for PCEE-BERT distances itself from the others for most of the GLUE tasks. \u2022 The overthinking problem is prevailing in the GLUE benchmark, and our PCEE-BERT early exiting can effectively take advantage of this phenomenon. For 6 of the GLUE tasks, PCEE-BERT can outperform BERT-base with a 25% (or more than) speedup ratio. And for 2 of the GLUE tasks, PCEE-BERT can outperform BERT-base with a 50% (or more than) speedup ratio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_61",
            "start": 0,
            "end": 1276,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_62@0",
            "content": "Putting performance comparisons aside, one benefit of PCEE-BERT is that it is flexible since by adjusting the threshold and the patience parameter, it can easily control the average inference layers and cover (or achieve values close to) any speedup ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_62",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_62@1",
            "content": "2) with that from Shallow-Deep (Kaya et al., 2019):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_62",
            "start": 258,
            "end": 308,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_63@0",
            "content": "C (M ) = Argmax k p (m) k ,(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_63",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@0",
            "content": "and we will call this version of PCEE-BERT as PCEE-BERT-v1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@1",
            "content": "Note that PCEE-BERT-v1 does not require a newly fine-tuned model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 60,
            "end": 124,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@2",
            "content": "With BERxiT, we can come up with PCEE-BERT-v2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 126,
            "end": 171,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@3",
            "content": "Following BERxiT, PCEE-BERT-v2 fine-tunes the multi-exit BERT with a fully connected layer right after each transformer block designated to evaluate the confidence score C (M ) for early exiting at that layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 173,
            "end": 381,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@4",
            "content": "C (M ) is learned along with the training of intermediate classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 383,
            "end": 452,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_64@5",
            "content": "Note that PCEE-BERT-v2 can not reuse the fine-tuned checkpoints used in PCEE-BERT and requires one to fine-tune the BERT backbones on the task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_64",
            "start": 454,
            "end": 604,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_65@0",
            "content": "We conduct the experiments on the QNLI tasks, and the results are reported in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_65",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_65@1",
            "content": "We can see that PCEE-BERT-v1 and PCEE-BERT-v2 perform comparably to PCEE-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_65",
            "start": 88,
            "end": 165,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_65@2",
            "content": "The results show that the proposed PCEE-BERT early exiting mechanism is off-the-shelf, and the reason for the success of our PCEE-BERT is its early exiting mechanism, that is, early exit if a group of consecutive exits is confident for their predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_65",
            "start": 167,
            "end": 420,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_66@0",
            "content": "Ablation of PLM backbones",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_66",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_67@0",
            "content": "In the main experiments, we use BERT as the pretrained backbone model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_67",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_67@1",
            "content": "However, PCEE-BERT can also work with the other types of pre-trained backbones, such as ALBERT base (Lan et al., 2020) and TinyBERT 6 (Jiao et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_67",
            "start": 71,
            "end": 225,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_67@2",
            "content": "We conduct the experiments on the QNLI task with these two 2 See the Appendix for demonstration on MRPC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_67",
            "start": 227,
            "end": 330,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_67@3",
            "content": "We can see that when using the other pre-trained backbones, PCEE-BERT also performs better than the baseline methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_67",
            "start": 332,
            "end": 448,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_68@0",
            "content": "The results for PCEE-BERT on the TinyBERT also convey an important message: as an inference speedup method, our PCEE method can work alongside the model compression methods to further reduce the latency of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_68",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_69@0",
            "content": "Ablation of cross-layer ensemble",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_69",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_70@0",
            "content": "Since we have a prediction module at each layer of BERT, we can conduct model ensemble across layers that the forward pass has gone through already.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_70",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_70@1",
            "content": "In Figure 6, we conduct the ablation studies on the RTE and QNLI tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_70",
            "start": 149,
            "end": 219,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_70@2",
            "content": "According to Figure 6, cross-layer ensemble leads to performance degradation when the speedup ratio is large, while when the average inference layers is close to the number of BERT's transformer blocks M , cross-layer ensemble results in slight improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_70",
            "start": 221,
            "end": 478,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_70@3",
            "content": "In conclusion, the cross-layer ensemble does not result in consistent performance improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_70",
            "start": 480,
            "end": 574,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_71@0",
            "content": "A possible application of the above results is to apply the cross-layer ensemble when a low speedup ratio is applied. And when we ask the model to exit early in the shallow layers, the cross-layer ensemble is not used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_71",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_72@0",
            "content": "PCEE-BERT are effective for image classification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_72",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@0",
            "content": "Our main experiments are conducted on BERT, a pre-trained language model, and the GLUE benchmark, a series of natural language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@1",
            "content": "However, our PCEE-BERT method is a plugand-play early exiting and can be applied to models and tasks of different modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 148,
            "end": 272,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@2",
            "content": "To demonstrate the effectiveness of PCEE-BERT on the image classification task, we follow the experimental settings in PABEE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 274,
            "end": 399,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@3",
            "content": "We conduct experiments on two image classification datasets, CIFAR-10 and CIFAR-100 (Krizhevsky, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 401,
            "end": 503,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@4",
            "content": "The ResNet-56 model (He et al., 2016) serves as the backbone, and we compare PCEE-BERT with PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 505,
            "end": 602,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@5",
            "content": "We place an exiting classifier at every two convolutional layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 604,
            "end": 668,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_73@6",
            "content": "We set the batch size to 128 and use an SGD optimizer with a learning rate of 0.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_73",
            "start": 670,
            "end": 751,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_74@0",
            "content": "Table 3 reports the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_74",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_74@1",
            "content": "PCEE-BERT outperforms PABEE when early exiting at different speedup ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_74",
            "start": 29,
            "end": 103,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_74@2",
            "content": "In addition, the performance advantages of PCEE-BERT are larger when the speedup ratio is large, which is also observed in the NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_74",
            "start": 105,
            "end": 241,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_74@3",
            "content": "And PCEE-BERT outperforms the original ResNet-56 on both tasks even when it provides around 25% speedup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_74",
            "start": 243,
            "end": 346,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_75@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_75",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@0",
            "content": "In this work, we propose PCEE-BERT, a novel efficient inference method that can yield a better performance-speed trade-off than the existing early exiting methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@1",
            "content": "PCEE-BERT adopts BERT as the backbone model and makes the exiting decision if there are enough intermediate layers to make confident predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 164,
            "end": 308,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@2",
            "content": "The confidence level is measured by the entropy of the predicted distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 310,
            "end": 388,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@3",
            "content": "Experiments on the GLUE benchmark demonstrate that our method outperforms the previous SOTA early exiting methods, especially when the speedup ratio is large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 390,
            "end": 547,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@4",
            "content": "In addition, PCEE-BERT can achieve different speedup ratios by adjusting the patience parameter and the confidence threshold, which makes it more flexible in industrial usage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 549,
            "end": 723,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_76@5",
            "content": "Ablation studies show that:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_76",
            "start": 725,
            "end": 751,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_77@0",
            "content": "Haoli Bai, Wei Zhang, L Hou, L Shang, Jing Jin, X Jiang, Qun Liu, Michael Lyu, Irwin King, Binarybert: Pushing the limit of bert quantization, 2012, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_77",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_78@0",
            "content": "Tolga Bolukbasi, J Wang, O Dekel, Venkatesh Saligrama, Adaptive neural networks for efficient inference, 2017, ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_78",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_79",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_80@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_80",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_81@0",
            "content": "UNKNOWN, None, 1910, Depth-adaptive transformer. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_81",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_82@0",
            "content": ", Angela Fan, Armand Grave,  Joulin, Reducing transformer depth on demand with structured dropout, 2020, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_82",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_83@0",
            "content": "X Kaiming He, Shaoqing Zhang, Jian Ren,  Sun, Deep residual learning for image recognition, 2016, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_83",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_84@0",
            "content": "Gao Huang, Danlu Chen, T Li, Felix Wu, L Maaten, Kilian Weinberger, Multi-scale dense convolutional networks for efficient prediction, 2017, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_84",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2018, To trust or not to trust a classifier, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_85",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_86@0",
            "content": "Y Xiaoqi Jiao, L Yin, Xin Shang, X Jiang,  Chen, F Li, Qun Wang,  Liu, Tinybert: Distilling bert for natural language understanding, 1909, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_86",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_87@0",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, TinyBERT: Distilling BERT for natural language understanding, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_87",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_88@0",
            "content": "Y Kaya, Sanghyun Hong, T Dumitras, Shallow-deep networks: Understanding and mitigating network overthinking, 2019, ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_88",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_89@0",
            "content": "Se-Hoon Kim, Amir Gholami, Zhewei Yao, M Mahoney, K Keutzer, 2021. I-bert: Integer-only bert quantization, , ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_89",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2009, Learning multiple layers of features from tiny images, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_90",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_91@0",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, Albert: A lite bert for self-supervised learning of language representations, 2020, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_91",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_92@0",
            "content": "Weijie Liu, P Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, Q Ju, Fastbert: a self-distilling bert with adaptive inference time, 2004, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_92",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2019, Are sixteen heads really better than one?, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_93",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_94@0",
            "content": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Pre-trained models for natural language processing: A survey, 2003, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_94",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_95",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_96@0",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_96",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_97@0",
            "content": "Roy Schwartz, Gabi Stanovsky, Swabha Swayamdipta, Jesse Dodge, N Smith, The right tool for the job: Matching model and instance complexities, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_97",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_98@0",
            "content": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah Smith, The right tool for the job: Matching model and instance complexities, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_98",
            "start": 0,
            "end": 292,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_99@0",
            "content": "S Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for bert model compression, 2019, EMNLP/IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_99",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2014, Intriguing properties of neural networks, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_100",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_101@0",
            "content": "Surat Teerapittayanon, Bradley Mcdanel, H Kung, Branchynet: Fast inference via early exiting from deep neural networks, 2016, 23rd International Conference on Pattern Recognition (ICPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_101",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_102",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_103",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_104@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_104",
            "start": 0,
            "end": 536,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2004, Deebert: Dynamic early exiting for accelerating bert inference. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_105",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_106@0",
            "content": "Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin, BERxiT: Early exiting for BERT with better fine-tuning and extension to regression, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_106",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_107@0",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, M Zhou, Bert-of-theseus: Compressing bert by progressive module replacing, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_107",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_108@0",
            "content": "Z Yang, Zihang Dai, Yiming Yang, J Carbonell, R Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_108",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2009, Ternarybert: Distillation-aware ultra-low bit bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_109",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_110@0",
            "content": "Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, Bert loses patience: Fast and robust inference with early exit, 2006, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_110",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_111@0",
            "content": "M Zhu, S Gupta, To prune, or not to prune: exploring the efficacy of pruning for model compression, 2018, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_111",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "197-ARR_v1_112@0",
            "content": "Wei Zhu, Xiaoling Wang, Yuan Ni, Guotong Xie, GAML-BERT: Improving BERT early exiting by gradient aligned mutual learning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "197-ARR_v1_112",
            "start": 0,
            "end": 258,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_1",
            "tgt_ix": "197-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_1",
            "tgt_ix": "197-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_4",
            "tgt_ix": "197-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_6",
            "tgt_ix": "197-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_7",
            "tgt_ix": "197-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_8",
            "tgt_ix": "197-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_13",
            "tgt_ix": "197-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_16",
            "tgt_ix": "197-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_17",
            "tgt_ix": "197-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_17",
            "tgt_ix": "197-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_18",
            "tgt_ix": "197-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_19",
            "tgt_ix": "197-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_19",
            "tgt_ix": "197-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_20",
            "tgt_ix": "197-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_22",
            "tgt_ix": "197-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_23",
            "tgt_ix": "197-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_24",
            "tgt_ix": "197-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_25",
            "tgt_ix": "197-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_26",
            "tgt_ix": "197-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_27",
            "tgt_ix": "197-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_27",
            "tgt_ix": "197-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_29",
            "tgt_ix": "197-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_29",
            "tgt_ix": "197-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_30",
            "tgt_ix": "197-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_30",
            "tgt_ix": "197-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_29",
            "tgt_ix": "197-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_33",
            "tgt_ix": "197-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_34",
            "tgt_ix": "197-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_35",
            "tgt_ix": "197-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_37",
            "tgt_ix": "197-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_38",
            "tgt_ix": "197-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_39",
            "tgt_ix": "197-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_40",
            "tgt_ix": "197-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_43",
            "tgt_ix": "197-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_43",
            "tgt_ix": "197-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_44",
            "tgt_ix": "197-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_46",
            "tgt_ix": "197-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_47",
            "tgt_ix": "197-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_48",
            "tgt_ix": "197-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_49",
            "tgt_ix": "197-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_52",
            "tgt_ix": "197-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_53",
            "tgt_ix": "197-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_54",
            "tgt_ix": "197-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_55",
            "tgt_ix": "197-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_56",
            "tgt_ix": "197-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_56",
            "tgt_ix": "197-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_56",
            "tgt_ix": "197-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_58",
            "tgt_ix": "197-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_62",
            "tgt_ix": "197-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_63",
            "tgt_ix": "197-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_65",
            "tgt_ix": "197-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_67",
            "tgt_ix": "197-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_66",
            "tgt_ix": "197-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_66",
            "tgt_ix": "197-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_66",
            "tgt_ix": "197-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_68",
            "tgt_ix": "197-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_70",
            "tgt_ix": "197-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_69",
            "tgt_ix": "197-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_69",
            "tgt_ix": "197-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_69",
            "tgt_ix": "197-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_71",
            "tgt_ix": "197-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_72",
            "tgt_ix": "197-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_72",
            "tgt_ix": "197-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_72",
            "tgt_ix": "197-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_74",
            "tgt_ix": "197-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_75",
            "tgt_ix": "197-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_75",
            "tgt_ix": "197-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "197-ARR_v1_0",
            "tgt_ix": "197-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_1",
            "tgt_ix": "197-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_2",
            "tgt_ix": "197-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_3",
            "tgt_ix": "197-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_4",
            "tgt_ix": "197-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_4",
            "tgt_ix": "197-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_4",
            "tgt_ix": "197-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_4",
            "tgt_ix": "197-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_5",
            "tgt_ix": "197-ARR_v1_5@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_6",
            "tgt_ix": "197-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_6",
            "tgt_ix": "197-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_6",
            "tgt_ix": "197-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_6",
            "tgt_ix": "197-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_7",
            "tgt_ix": "197-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_8",
            "tgt_ix": "197-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_8",
            "tgt_ix": "197-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_8",
            "tgt_ix": "197-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_9",
            "tgt_ix": "197-ARR_v1_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_10",
            "tgt_ix": "197-ARR_v1_10@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_11",
            "tgt_ix": "197-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_12",
            "tgt_ix": "197-ARR_v1_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_13",
            "tgt_ix": "197-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_13",
            "tgt_ix": "197-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_13",
            "tgt_ix": "197-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_13",
            "tgt_ix": "197-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_14",
            "tgt_ix": "197-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_15",
            "tgt_ix": "197-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_16",
            "tgt_ix": "197-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_16",
            "tgt_ix": "197-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_17",
            "tgt_ix": "197-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_18",
            "tgt_ix": "197-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_18",
            "tgt_ix": "197-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_18",
            "tgt_ix": "197-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_19",
            "tgt_ix": "197-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_20",
            "tgt_ix": "197-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_20",
            "tgt_ix": "197-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_20",
            "tgt_ix": "197-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_21",
            "tgt_ix": "197-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_22",
            "tgt_ix": "197-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_22",
            "tgt_ix": "197-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_23",
            "tgt_ix": "197-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_24",
            "tgt_ix": "197-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_25",
            "tgt_ix": "197-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_26",
            "tgt_ix": "197-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_26",
            "tgt_ix": "197-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_27",
            "tgt_ix": "197-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_28",
            "tgt_ix": "197-ARR_v1_28@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_29",
            "tgt_ix": "197-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_30",
            "tgt_ix": "197-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_31",
            "tgt_ix": "197-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_32",
            "tgt_ix": "197-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_33",
            "tgt_ix": "197-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_33",
            "tgt_ix": "197-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_34",
            "tgt_ix": "197-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_35",
            "tgt_ix": "197-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_35",
            "tgt_ix": "197-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_36",
            "tgt_ix": "197-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_37",
            "tgt_ix": "197-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_37",
            "tgt_ix": "197-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_37",
            "tgt_ix": "197-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_38",
            "tgt_ix": "197-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_38",
            "tgt_ix": "197-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_39",
            "tgt_ix": "197-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_40",
            "tgt_ix": "197-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_40",
            "tgt_ix": "197-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_40",
            "tgt_ix": "197-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_41",
            "tgt_ix": "197-ARR_v1_41@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_42",
            "tgt_ix": "197-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_43",
            "tgt_ix": "197-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_44",
            "tgt_ix": "197-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_44",
            "tgt_ix": "197-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_45",
            "tgt_ix": "197-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_46",
            "tgt_ix": "197-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_47",
            "tgt_ix": "197-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_47",
            "tgt_ix": "197-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_48",
            "tgt_ix": "197-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_48",
            "tgt_ix": "197-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_48",
            "tgt_ix": "197-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_48",
            "tgt_ix": "197-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_49",
            "tgt_ix": "197-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_49",
            "tgt_ix": "197-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_49",
            "tgt_ix": "197-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_50",
            "tgt_ix": "197-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_51",
            "tgt_ix": "197-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_52",
            "tgt_ix": "197-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_53",
            "tgt_ix": "197-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_54",
            "tgt_ix": "197-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_55",
            "tgt_ix": "197-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_56",
            "tgt_ix": "197-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_57",
            "tgt_ix": "197-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_58",
            "tgt_ix": "197-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_58",
            "tgt_ix": "197-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_58",
            "tgt_ix": "197-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_59",
            "tgt_ix": "197-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_60",
            "tgt_ix": "197-ARR_v1_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_61",
            "tgt_ix": "197-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_62",
            "tgt_ix": "197-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_62",
            "tgt_ix": "197-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_63",
            "tgt_ix": "197-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_64",
            "tgt_ix": "197-ARR_v1_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_65",
            "tgt_ix": "197-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_65",
            "tgt_ix": "197-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_65",
            "tgt_ix": "197-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_66",
            "tgt_ix": "197-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_67",
            "tgt_ix": "197-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_67",
            "tgt_ix": "197-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_67",
            "tgt_ix": "197-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_67",
            "tgt_ix": "197-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_68",
            "tgt_ix": "197-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_69",
            "tgt_ix": "197-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_70",
            "tgt_ix": "197-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_70",
            "tgt_ix": "197-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_70",
            "tgt_ix": "197-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_70",
            "tgt_ix": "197-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_71",
            "tgt_ix": "197-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_72",
            "tgt_ix": "197-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_73",
            "tgt_ix": "197-ARR_v1_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_74",
            "tgt_ix": "197-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_74",
            "tgt_ix": "197-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_74",
            "tgt_ix": "197-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_74",
            "tgt_ix": "197-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_75",
            "tgt_ix": "197-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_76",
            "tgt_ix": "197-ARR_v1_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_77",
            "tgt_ix": "197-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_78",
            "tgt_ix": "197-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_79",
            "tgt_ix": "197-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_80",
            "tgt_ix": "197-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_81",
            "tgt_ix": "197-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_82",
            "tgt_ix": "197-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_83",
            "tgt_ix": "197-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_84",
            "tgt_ix": "197-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_85",
            "tgt_ix": "197-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_86",
            "tgt_ix": "197-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_87",
            "tgt_ix": "197-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_88",
            "tgt_ix": "197-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_89",
            "tgt_ix": "197-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_90",
            "tgt_ix": "197-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_91",
            "tgt_ix": "197-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_92",
            "tgt_ix": "197-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_93",
            "tgt_ix": "197-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_94",
            "tgt_ix": "197-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_95",
            "tgt_ix": "197-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_96",
            "tgt_ix": "197-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_97",
            "tgt_ix": "197-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_98",
            "tgt_ix": "197-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_99",
            "tgt_ix": "197-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_100",
            "tgt_ix": "197-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_101",
            "tgt_ix": "197-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_102",
            "tgt_ix": "197-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_103",
            "tgt_ix": "197-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_104",
            "tgt_ix": "197-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_105",
            "tgt_ix": "197-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_106",
            "tgt_ix": "197-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_107",
            "tgt_ix": "197-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_108",
            "tgt_ix": "197-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_109",
            "tgt_ix": "197-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_110",
            "tgt_ix": "197-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_111",
            "tgt_ix": "197-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "197-ARR_v1_112",
            "tgt_ix": "197-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1255,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "197-ARR",
        "version": 1
    }
}