{
    "nodes": [
        {
            "ix": "431-ARR_v1_0",
            "content": "MultiSpanQA: A Dataset for Multi-Span Question Answering",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_2",
            "content": "Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common in real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on questions with multi-span answers. Raw questions and contexts are extracted from the Natural Questions (Kwiatkowski et al., 2019) dataset. After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multispan answers in the expanded version. We introduce new metrics for the purposes of multispan question answering evaluation, and establish several baselines using advanced models. Finally, we propose a new model which beats all baselines and achieves the state-of-the-art on our dataset. Dataset and code will be released on acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "431-ARR_v1_4",
            "content": "The task of reading comprehension, where models are required to process a text and answer questions about it, has seen rapid progress in recent years. As systems have increasingly matched humans on popular datasets (Rajpurkar et al., 2016(Rajpurkar et al., , 2018, researchers have developed newer, more complex formulations of the task, such as very long contexts and answers (Kwiatkowski et al., 2019), multi-hop reasoning (Yang et al., 2018), and discrete operations over the content of paragraphs (Dua et al., 2019). One thing these datasets have in common is that the answer is constrained to be a single span that can be extracted or computed from the context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_5",
            "content": "However, in practice, the answer to a question will often consist of multiple parts. As in the example in Figure 1, the answer set contains 10 countries, Question: Which countries does the Danube River flow through? Passage: ... Originating in Germany, the Danube flows southeast for 2,850 km (1,770 mi), passing through or bordering Austria, Slovakia, Hungary, Croatia, Serbia, Romania, Bulgaria, Moldova and Ukraine before draining into the Black Sea. ... Answer set: {Germany, Austria, Slovakia, Hungary, Croatia, Serbia, Romania, Bulgaria, Moldova, Ukraine } some of which are discontiguous in the passage. Such cases are largely ignored in existing reading comprehension research, in part because there are no datasets of multi-span questions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_6",
            "content": "In this paper, we introduce MultiSpanQA, a new reading comprehension dataset consistinga of 6,465 multi-span examples. The raw questions and passages are extracted from Natural Questions (\"NQ\": Kwiatkowski et al. (2019)), a large-scale open-domain QA dataset. Trained annotators were asked to identify question-passage pairs where the answer was multi-span, and annotate the spans. In addition to the basic version of the dataset consisting entirely of multi-span answers, we also prepare an expanded version with a selection of unanswerable questions, and questions with single-and multi-span answers, intended to reflect a more realistic QA setup.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_7",
            "content": "We further classify answer semantics into 5 categories, and manually label the logical structure of the answer spans. We introduce metrics to evaluate multi-span QA systems across these different tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_8",
            "content": "We propose several baselines, and a new model which casts the task as a sequence tagging problem. The proposed model combines a sequence tagger with a span number predictor, span structure predictor, and span adjustment module. Experimental results show that the proposed model surpasses all baselines and achieves 62.58% exact-match F1 score and 77.46% overlapping F1 score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_9",
            "content": "To summarize, our contributions are:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_10",
            "content": "\u2022 A new reading comprehension dataset containing 6.5k high-quality multi-span answers, along with analysis and metrics for multi-span QA. \u2022 A novel label set for capturing the semantics of multi-span answers, with annotations. \u2022 A new model for multi-span reading comprehension which achieves state-of-the-art results on our dataset.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_11",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_12",
            "content": "Question Answering Datasets",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "431-ARR_v1_13",
            "content": "Extractive QA Most existing extractive QA datasets such as SQuAD (Rajpurkar et al., 2016), SQuAD2.0 (Rajpurkar et al., 2018), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) restrict the answer passage to a single span of text. SQuAD and SQuAD 2.0 limit the answer passage to a short paragraph from Wikipedia; the bestperforming systems have now exceeded human performance on these datasets. QuAC frames the task in a dialogue setting by introducing a teacher and student, where the student repeatedly asks the teacher questions about a topic and the teacher tries to find answers from the given passage. That is, it supports information seeking through multi-turn conversation. TriviaQA (Joshi et al., 2017) and HothopQA (Yang et al., 2018) extend the answer context from single passage to multiple passages, while HotpotQA further requires reasoning over multiple passages to answer the question. However, all of these datasets limit the answer to a single text span from the provided answer context. DROP (Dua et al., 2019) requires systems to resolve (possibly multiple) references in a question, and perform discrete operations (such as addition, sorting, or counting) over them. However, because these operations are mostly numeric, the spans are almost exclusively semantically homogeneous and related to numeric values. MASH-QA (Dua et al., 2019) extends the answer space to texts that span across a longer document, but this dataset is highly domain-specific, in the healthcare domain. Quoref and Natural Questions (\"NQ\": Kwiatkowski et al. (2019)) both contain multi-span answers. Quoref requires systems to resolve coreference among entities, to aid in span-selection. NQ is a large-scale dataset that provides questions with very long answer contexts. The proportion of multispan answers is around 10% and 2% in Quoref and NQ, respectively. However in each case, multispan answers are captured as a single span, with no annotation of the internal structure of the component spans. WikiHowQA and WebQA (Cui et al., 2021) both focus on non-factoid (e.g., how, why) questions, with answers mostly being long spans or full sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_14",
            "content": "Generative QA Generative QA datasets usually require systems to answer questions in the form of several sentences, either selected from the provided answer context or generated based on it. Wik-iQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) are two open-domain generative QA datasets, where answers in WikiQA are mostly sentences from the answer passage, while answers in MS Marco are free-form sentences generated by crowd workers. NarrativeQA (Kocisk\u00fd et al., 2018) is a dataset of movie and book summaries. SearchQA (Dunn et al., 2017), ELI5 (Fan et al., 2019), and CoQA (Reddy et al., 2019) are three multiple-document datasets. SearchQA is constructed from question-answer pairs crawled from Jeopardy!, and most questions can be answered with a short (99% less than 5 tokens) extractive span from a single document. ELI5 requires systems to generate paragraph-length answers by summarizing information from multiple documents. CoQA contains conversational questions, with freeform text as answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_15",
            "content": "Cloze style Cloze datasets such as CNN/Daily Mail (Hermann et al., 2015), Children's Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) require systems to predict a missing word from a passage. However, researchers have shown that this task is artificial, and can be largely solved with simple methods and relatively little reasoning (Chen et al., 2016). Dua et al. (2019) proposed to predict the number of output spans for each question, by applying a single-span predictor recursively, making training complex. Segal et al. (2020) first proposed to treat multi-span QA as a sequence tagging task, in the form of a multi-head architecture (Dua et al., 2019) to perform arithmetic operations between the predicted spans. Hu et al. (2019) applied the non-maximum suppression (NMS) algorithm (Rosenfeld and Thurston, 1971) to prune redundant bounding boxes from the top-k predicted spans of a single-span predictor. Pang et al. (2019) proposed HAS-QA, which supports multi-span prediction by computing answer probabilities at the question, paragraph and span levels. A common feature of these works is that the predicted spans are fed into an aggregation module, and the answers are usually a single span chosen from the prediction, or a number computed from them. Cui et al. (2021) proposed a model which can extract list-form answers across multiple spans. Their work mainly focuses on capturing the sequential and progressive relationships between long-span descriptions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_16",
            "content": "Multi-span Models",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "431-ARR_v1_17",
            "content": "Dataset Construction and Composition",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "431-ARR_v1_18",
            "content": "In this section, we describe how we construct Mul-tiSpanQA, and provide a statistical breakdown of its composition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_19",
            "content": "Data Collection and Preprocessing",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "431-ARR_v1_20",
            "content": "The question-passage pairs were selected from Natural Questions (NQ: Kwiatkowski et al. ( 2019)), a large-scale open-domain QA dataset made up of (question, passage, long answer, short answer) quadruples where: the questions are real queries issued to the Google search engine; the passage is a Wikipedia page which may or may not contain the information required to answer the question; the long answer is a paragraph from the page containing all information required to infer the answer; and the short answer is one or more text spans that answer the question. Both long and short answers can be NULL if no viable answer candidate exists on the page.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_21",
            "content": "To create MultiSpanQA, we first extract NQ questions annotated with multiple short answers, and consider the long answer to be the answer passage. We then remove paragraphs that don't contain any question part, to eliminate the informationretrieval component of NQ and focus more on the short answer extraction problem. To make the dataset easy to use, we strip HTML from the passages, so that they only contain plain text. As table structure cannot be captured in the plain text after removing HTML, we remove the passages that contain tables. Ultimately, around 6700 candidates remain where each candidate is a triple of (question, passage, set of answer spans).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_22",
            "content": "To aid the annotation process, we classifies the samples into 5 categories according to the expected answer type of questions using a BERT-based classifier trained on the TREC Question Classification dataset (Li and Roth, 2002). The classes are DE-SCRIPTION, LOCATION, HUMAN, NUMERIC, and OTHER ENTITY. Table 1 shows the breakdown and an example of each answer type class.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_23",
            "content": "Issues in Existing Dataset",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "431-ARR_v1_24",
            "content": "NQ was originally annotated by around 50 annotators, with an average annotation time of 80 seconds per instance. However, we found a number of issues with the dataset: (1) grammatical errors in questions, due to them being actual queries submitted to the Google search engine by real users;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_25",
            "content": "(2) answer boundary inconsistencies or errors, such as the entity University of Michigan being annotated as an answer in one example but The University of Michigan being annotated in another; and (3) wrong or incomplete answer: some questions are not answered or are answered incompletely in the annotated answer span, for example, to answer the question Which countries does the River Danube flow through?, 10 countries should be included in the answer span while only 9 are annotated. These issues are relatively uncommon overall in the dataset, but occur disproportionately in multi-span answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_26",
            "content": "High Quality Re-annotation",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "431-ARR_v1_27",
            "content": "We (re-)annotated all the data using the Brat annotation tool (Stenetorp et al., 2012). 1 Three annotators were provided with a category-specific annotation guide (broken down across the 5 predicted answer types), and annotated the data on a per-category basis. 2 For each annotation instance, we show the question, passage, and the original multiple answer spans to the annotator. The first-pass annotation was according to the following four categories: \u2022 Good example: the question is clear, and the answer spans are labelled consistent with the annotation guide, in which case accept the instance as is. \u2022 Bad question: the question is ungrammatical or not aligned with the passage content, in which case rewrite the question while preserving its original intended meaning where possible (otherwise reject). \u2022 Bad answer span(s): the answer span(s) are incorrect or incomplete, in which case remove the inappropriate spans and select the correct spans. \u2022 Bad question-answer pair: the question doesn't align with the passage content (e.g.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_28",
            "content": "there is no answer there) or there are not multiple answer spans in the passage (e.g. there is only a single answer span), in which case reject the instance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_29",
            "content": "Although all examples in our dataset contain multiple answer spans, the semantic structure varies considerably. We hand-annotate this via a novel 5-way annotation scheme, as follows (see Table 2 for examples):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_30",
            "content": "\u2022 1. CONJUNCTION: Each span is part of the answer, and the answer is complete only when all of the spans are combined \u2022 MULTI-PART-DISJUNCTION: Each span is a complete (but independent) answer to the question, with one of the following structures:",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_31",
            "content": "-2. REDUNDANT: the multiple spans refer to the same concept or entity. For example, in the example in Table 2, each span is a full answer to the question, specified using different temporal reference points. -3. NON-REDUNDANT: the different spans refer to different concepts or entities, each of which is independently correct in its respective context. For example, in the example in Table 2 each span is independently correct in the context of a particular national market. \u2022 4. COMPLEX: The question is complex (made up of multiple sub-parts), and each span is an answer to a different sub-part, the internal logic of which is not enumeration. For example, in the example in Table 2, the two spans are independent answers to the two subquestions in the original question. \u2022 5. SHARED STRUCTURE: Spans are enumerated in the form of a syntactically-coordinated structure, sharing either a modifier or a head (i.e. the first word(s) of the first span or last word(s) of the last span). For example, in the example in Table 2, the three spans share the syntactic head bus service, and the full answer is equivalent to scheduled bus service + fixed-route regional bus service + commuter bus service. Table 3: Number of answer spans in MultiSpanQA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_32",
            "content": "Dataset Statistics",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "431-ARR_v1_33",
            "content": "The annotation was performed by three trained annotators with an average annotation time of 70 seconds per instance. To test the inter-annotator agreement (IAA), we randomly selected 100 instances for each pairing of the three annotators to anntotate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_34",
            "content": "The same annotation (of all spans) of an instance is considered as an agreement, and any difference in one instance is considered as a disagreement. The average pairwise IAA is 0.86 for answer spans and 0.94 for answer structures (both based on macroaveraged exact match F1 score), with some disagreements between CONJUNCTION and MULTI-PART-DISJUNCTION (NON-REDUNDANT). To better understand the composition of MultiSpanQA, we compare our annotations with those in NQ, and provide some basic statistics. Compared to the original annotations in NQ, the annotators rejected 3.1% of instances, re-wrote the question for 5.6% of instances, and modified the answer span annotations for 22% of instances. MultiSpanQA contains 6,465 instances with 5,173 for training, 646 for validation, and 646 for test. Table 3 provides the distribution of the number of answer spans in the dataset, from which we see the number of spans ranges from 2 to 21, but 80% of instances contain 2 or 3 spans, and only about 1% of instances contain more than 9 spans.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_35",
            "content": "Dataset Expansion",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "431-ARR_v1_36",
            "content": "In its basic form, the MultiSpanQA dataset contains only multiple-span answers, and the correct answer can always be located in the passage (in the form of multiple answer spans). However, in a real-word QA scenario, single-span answer questions and unanswerable questions (i.e. the answer is not contained in the passage) would realistically exist. To create a more realistic and challenging variant of the dataset, we add a comparable number of single-span question-answer pairs and unanswerable instances to MultiSpanQA, by randomly sampling from NQ and applying the same preprocessing. The total size of the expanded dataset is 19,395 instances (three times the basic version, partitioned similarly to the basic version).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_37",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "431-ARR_v1_38",
            "content": "Formally, given a question and passage pair <q,p>, the task of multi-span QA involves finding all answer spans s 1 , s 2 , ...s n , which are neither duplicated nor overlap with each other, as well as predict the answer structures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_39",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "431-ARR_v1_40",
            "content": "Single-span Baseline Because most existing reading comprehension datasets only have singlespan answers, single-span architectures are widely used in reading comprehension research. Usually, a pre-trained model is used to encode the question and passage, and output a contextualised representation for all input tokens. Then two feed-forward networks are used to compute a score for each token which indicates whether the token is the start or end of the answer. Finally, a softmax layer followed by an argmax function is used to produce the start and end positions of the answer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_41",
            "content": "To make MultiSpanQA trainable for a singlespan architecture, we experimented with two preprocessing methods, and created two baselines accordingly:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_42",
            "content": "1. Mark the start of the answer as the start position of the first answer span and mark the end of the answer as the end position of the last answer span. In this way, the model can learn to find the shortest span that includes all answer spans. We select the best prediction for evaluation. 2. Suppose an instance has n answer spans, we replace the instance with n instances, one for each span with a single-span answer. In this way, we can apply single-span answer models to our dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_43",
            "content": "For evaluation, to enable multi-span prediction, we output the 20 highest-scoring predictions, and tune a threshold t to select the answer spans with a confidence score larger than t that optimises performance on the training set. We remove overlapping predictions based on confidence scores, rejecting predictions with lower confidence scores. Note that for both baselines, we apply the pre-processing to the training data only.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_44",
            "content": "Sequence Tagging Baseline Following Segal et al. (2020), we cast question answering as a sequence tagging task, predicting for each token whether it is part of an answer. In our experiments, we use the popular IOB tagging scheme to mark answer spans in the passage where B denotes the first token of an answer span, I denotes subsequent tokens within a span, and O denotes tokens that are not part of an answer span.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_45",
            "content": "Proposed Model",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "431-ARR_v1_46",
            "content": "By investigating the failures of sequence tagging baseline, we find there is an issue that the global information is hard to be captured during tagging. For example, the number of answer spans may be specified in the question, but cannot be imposed as a constraint on the tagger. To better use such global information, we propose a span number predictor, an answer structure predictor, and a span adjustment module (as in Figure 2), which can be combined with any on-the-fly sequence tagger (encoder). Given a pair of question q and passage p, we first encode the question and context together using an sequence pair encoder as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_47",
            "content": "H = Encoder(< q, p >) \u2208 R l\u00d7h(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_48",
            "content": "where H = [H [CLS] ; H q ; H p ] is the contextualised token representation of all input tokens with a pooled global token [CLS], h is the hidden-layer size, and l is the input length.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_49",
            "content": "After encoding, we fetch the hidden states of the context tokens and input them to a linear classifier to perform a preliminary answer span prediction, as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_50",
            "content": "U p = F F N (H p ) \u2208 R lp\u00d7t (2) O p = CRF (U p ) \u2208 R lp\u00d7t(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_51",
            "content": "where l p denotes the length of the passage, and t denotes the number of labels (t = 3 for the IOB tagging scheme).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_52",
            "content": "We then fetch the hidden state of [CLS] token H [CLS] and input it to two feed-forward networks to predict the number of answer spans and the answer structure, respectively, as below:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_53",
            "content": "P num = F F N (H [CLS] )(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_54",
            "content": "P structure = F F N (H [CLS] )(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_55",
            "content": "We use cross-entropy loss for answer span and structure prediction, and mean-square loss for span number regression. For training, we use the weighted sum of the three losses:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_56",
            "content": "L = L spans + \u03bb 1 L num + \u03bb 2 L structure(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_57",
            "content": "Finally, a span adjustment module is used to explicitly combine the predicted span number with the span texts. We first assign a confidence score to each label of the preliminary classification using a softmax layer:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_58",
            "content": "\u03b1 conf = softmax(U p ) \u2208 R lp\u00d7t (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_59",
            "content": "The confidence of a predicted answer span a i is defined as the maximum confidence of the tokens within a i . Suppose there are k spans that been tagged as answers and the predicted number of span is n, if n < k, we rank the predicted spans by confidence score, and keep the top-n answer spans as answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_60",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "431-ARR_v1_61",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "431-ARR_v1_62",
            "content": "For all baselines and our model, we use the Hug-gingFace implementation of BERT Base (Wolf et al., 2019; as our encoder with max_sequence_length = 384 and doc_stride = 128 to deal with long passages. For training, we use the BertAdam optimizer with default hyperparameters and learning rate of 5e-5. All models are trained with a batch size of 8 for 3,000 steps. We use a two-layer feed-forward network with a ReLu activation function for all linear layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_63",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "431-ARR_v1_64",
            "content": "We evaluate in terms of exact match and partial match performance, where an exact match occurs when a prediction fully matches one of the groundtruth answers, and the F1 score is computed by treating the predicted and ground-truth answer spans as a set of spans. By taking positional information into account, a partial match occurs when a prediction overlaps with a ground-truth span. We use micro-averaged precision, recall, and F1 score for evaluation based on the standard formulation of Precision = T P/(T P + F P ), Recall = T P/(T P + F N ), and F1 = 2 * Precision * Recall /(Precision + Recall), where T P (True Positive) is the number of answer spans correctly predicted by the model, F P (False Positive) is the number of spans incorrectly predicted by the model, and F N (False Negative) is the number of answer spans not predicted by the model. In the case of an unanswerable question with the expanded dataset, we use a virtual span which indicates no answer. For answer structure prediction, we use accuracy to evaluate the model performance. only (the basic dataset), and the right part is the results on the expanded dataset (including singlespan answers and unanswerable questions).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_65",
            "content": "Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "431-ARR_v1_66",
            "content": "Single-span model From the table, we see that single-span (v1) gets very low exact match scores but higher overlapping scores (compared to exact match), as it is trained to find a single long span that overlaps with all answer spans. By comparison, single-span (v2) improves the exact match scores on the basic dataset because it is trained on independent single-span answers. An interesting finding is that single-span (v2) does not drop in the overlapping scores, which seems to be because the tuned threshold t (see Section 4.1) works in this setting. The overall performance of the single-span baselines is relatively low, simply because the models can only predict a single-span answer, which is incompatible with the MultiSpanQA dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_67",
            "content": "Sequence tagging model Compared to the single-span baselines, the sequence tagging models perform much better. Without changing the encoder, there is an improvement of more than 30 absolute points on the exact match metrics, and about 3 for the overlapping F1 metric. Performance is boosted using joint training with span number prediction and answer semantics prediction. Our proposed model achieves the best F1 score in most settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_68",
            "content": "Another interesting finding is that single-span models usually attain higher precision, while sequence tagging models attain higher recall. This demonstrates that single-span models are more accurate in the single-span answer they predict, while sequence tagging models predictably tend to make more predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_69",
            "content": "Comparing the two datasets Comparing results on the two datasets, we see that single-span baselines are boosted over the expanded dataset (where we add single-span answers and unanswerable questions), as single-span answers are more tractable for these simpler models. The relative improvements for sequence tagging models are more modest, but they still have a clear advantage over the single-span baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_70",
            "content": "Difficulty analysis To explore the difficulty of the MultiSpanQA dataset, we report the dev set results categorised by answer type in Table 5 and categorised by the number of spans in Table 6. From the answer type perspective, the model performs best on HUMAN questions, followed by OTHER ENTITY and LOCATION (largely following the natural distribution of the respective classes in the dataset). There is quite a drop for the NUMERIC class, and a big drop again for the DESCRIPTION class, which was also the class our annotators found most difficulty with.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_71",
            "content": "From the perspective of the number of spans, the model performs best on questions with many (> 7) answers. We think this is because the answers are usually a list of spans with similar semantics, often structured as a simple coordination. The performance drops as the answer number decreases because the syntactic pattern in which answer spans occurs is less predictable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_72",
            "content": "Answer Semantics From the answer type perspective, LOCATION answers usually have easily predictable structure, while the structure of NU-MERIC answers is the most difficult to predict. From the perspective of the number of spans, answers consisting of 4-7 spans are relatively easy to predict and there is no significant difference between answers contain few (2 or 3) spans or many (> 7) spans. Figure 3 shows the confusion matrix of the answer structure predictions. We can see that our model tends to predict CONJUNCTION and NON-REDUNDANT, and there are no REDUNDANT and SHARE predictions. The overall answer structure accuracy is 84.38%, which is slightly higher than the proportion of CONJUNCTION (the majority class) in the dataset. This suggests that directly applying a simple feedforward network to the pooled encoder output is ineffective for answer semantics prediction, and that this should be an area for future model refinement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_73",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "431-ARR_v1_74",
            "content": "We present MultiSpanQA, a reading comprehension dataset where answers consist of multiple discrete spans. As part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans. We also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic. We additionally presented a number of models for multi-span QA extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "431-ARR_v1_75",
            "content": "UNKNOWN, None, 2016, Embracing data abundance: Booktest dataset for reading comprehension, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Embracing data abundance: Booktest dataset for reading comprehension",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_76",
            "content": "Danqi Chen, Jason Bolton, Christopher Manning, A thorough examination of the CNN/daily mail reading comprehension task, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Danqi Chen",
                    "Jason Bolton",
                    "Christopher Manning"
                ],
                "title": "A thorough examination of the CNN/daily mail reading comprehension task",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "431-ARR_v1_77",
            "content": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, Quac: Question answering in context, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Eunsol Choi",
                    "He He",
                    "Mohit Iyyer",
                    "Mark Yatskar",
                    "Wentau Yih",
                    "Yejin Choi",
                    "Percy Liang",
                    "Luke Zettlemoyer"
                ],
                "title": "Quac: Question answering in context",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_78",
            "content": "UNKNOWN, None, 2021, Listreader: Extracting list-form answers for opinion questions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Listreader: Extracting list-form answers for opinion questions",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_79",
            "content": "Pradeep Dasigi, Nelson Liu, Ana Marasovic, Noah Smith, Matt Gardner, Quoref: A reading comprehension dataset with questions requiring coreferential reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Pradeep Dasigi",
                    "Nelson Liu",
                    "Ana Marasovic",
                    "Noah Smith",
                    "Matt Gardner"
                ],
                "title": "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_80",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_81",
            "content": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Dheeru Dua",
                    "Yizhong Wang",
                    "Pradeep Dasigi",
                    "Gabriel Stanovsky",
                    "Sameer Singh",
                    "Matt Gardner"
                ],
                "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_82",
            "content": "UNKNOWN, None, 2017, Searchqa: A new q&a dataset augmented with context from a search engine, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Searchqa: A new q&a dataset augmented with context from a search engine",
                "pub": "CoRR"
            }
        },
        {
            "ix": "431-ARR_v1_83",
            "content": "Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, ELI5: long form question answering, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Angela Fan",
                    "Yacine Jernite",
                    "Ethan Perez",
                    "David Grangier",
                    "Jason Weston",
                    "Michael Auli"
                ],
                "title": "ELI5: long form question answering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_84",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Karl Moritz Hermann",
                    "Tom\u00e1s Kocisk\u00fd",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom"
                ],
                "title": "Teaching machines to read and comprehend",
                "pub_date": "2015",
                "pub_title": "Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_85",
            "content": "Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston, The goldilocks principle: Reading children's books with explicit memory representations, 2016, 4th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Felix Hill",
                    "Antoine Bordes",
                    "Sumit Chopra",
                    "Jason Weston"
                ],
                "title": "The goldilocks principle: Reading children's books with explicit memory representations",
                "pub_date": "2016",
                "pub_title": "4th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_86",
            "content": "Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, A multi-type multi-span network for reading comprehension that requires discrete reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Minghao Hu",
                    "Yuxing Peng",
                    "Zhen Huang",
                    "Dongsheng Li"
                ],
                "title": "A multi-type multi-span network for reading comprehension that requires discrete reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_87",
            "content": "Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Mandar Joshi",
                    "Eunsol Choi",
                    "Daniel Weld",
                    "Luke Zettlemoyer"
                ],
                "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Vancouver"
            }
        },
        {
            "ix": "431-ARR_v1_88",
            "content": "Tom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Hermann, G\u00e1bor Melis, Edward Grefenstette, The narrativeqa reading comprehension challenge, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Tom\u00e1s Kocisk\u00fd",
                    "Jonathan Schwarz",
                    "Phil Blunsom",
                    "Chris Dyer",
                    "Karl Hermann",
                    "G\u00e1bor Melis",
                    "Edward Grefenstette"
                ],
                "title": "The narrativeqa reading comprehension challenge",
                "pub_date": "2018",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_89",
            "content": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Natural questions: a benchmark for question answering research, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Tom Kwiatkowski",
                    "Jennimaria Palomaki",
                    "Olivia Redfield",
                    "Michael Collins",
                    "Ankur Parikh",
                    "Chris Alberti",
                    "Danielle Epstein",
                    "Illia Polosukhin",
                    "Jacob Devlin",
                    "Kenton Lee",
                    "Kristina Toutanova",
                    "Llion Jones",
                    "Matthew Kelcey",
                    "Ming-Wei Chang",
                    "Andrew Dai",
                    "Jakob Uszkoreit",
                    "Quoc Le",
                    "Slav Petrov"
                ],
                "title": "Natural questions: a benchmark for question answering research",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_90",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, 19th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xin Li",
                    "Dan Roth"
                ],
                "title": "Learning question classifiers",
                "pub_date": "2002",
                "pub_title": "19th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_91",
            "content": "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, MS MARCO: A human generated machine reading comprehension dataset, 2016, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Tri Nguyen",
                    "Mir Rosenberg",
                    "Xia Song",
                    "Jianfeng Gao",
                    "Saurabh Tiwary",
                    "Rangan Majumder",
                    "Li Deng"
                ],
                "title": "MS MARCO: A human generated machine reading comprehension dataset",
                "pub_date": "2016",
                "pub_title": "Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_92",
            "content": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Lixin Su, Xueqi Cheng, HAS-QA: hierarchical answer spans model for open-domain question answering, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Liang Pang",
                    "Yanyan Lan",
                    "Jiafeng Guo",
                    "Jun Xu",
                    "Lixin Su",
                    "Xueqi Cheng"
                ],
                "title": "HAS-QA: hierarchical answer spans model for open-domain question answering",
                "pub_date": "2019",
                "pub_title": "The Thirty-Third AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_93",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for squad, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Pranav Rajpurkar",
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Know what you don't know: Unanswerable questions for squad",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_94",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100, 000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "Squad: 100, 000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_95",
            "content": "Siva Reddy, Danqi Chen, Christopher Manning, Coqa: A conversational question answering challenge, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Siva Reddy",
                    "Danqi Chen",
                    "Christopher Manning"
                ],
                "title": "Coqa: A conversational question answering challenge",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_96",
            "content": "Azriel Rosenfeld, Mark Thurston, Edge and curve detection for visual scene analysis, 1971, IEEE Transactions on Computers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Azriel Rosenfeld",
                    "Mark Thurston"
                ],
                "title": "Edge and curve detection for visual scene analysis",
                "pub_date": "1971",
                "pub_title": "IEEE Transactions on Computers",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_97",
            "content": "Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, Jonathan Berant, A simple and effective model for answering multi-span questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Elad Segal",
                    "Avia Efrat",
                    "Mor Shoham",
                    "Amir Globerson",
                    "Jonathan Berant"
                ],
                "title": "A simple and effective model for answering multi-span questions",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_98",
            "content": "Pontus Stenetorp, Sampo Pyysalo, Goran Topi\u0107, Tomoko Ohta, Sophia Ananiadou, Jun'ichi Tsujii, brat: a web-based tool for NLP-assisted text annotation, 2012, Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Pontus Stenetorp",
                    "Sampo Pyysalo",
                    "Goran Topi\u0107",
                    "Tomoko Ohta",
                    "Sophia Ananiadou",
                    "Jun'ichi Tsujii"
                ],
                "title": "brat: a web-based tool for NLP-assisted text annotation",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "431-ARR_v1_99",
            "content": "UNKNOWN, None, 1910, Huggingface's transformers: State-of-the-art natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "1910",
                "pub_title": "Huggingface's transformers: State-of-the-art natural language processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_100",
            "content": "Yi Yang, Yih Wen-Tau, Christopher Meek, Wikiqa: A challenge dataset for open-domain question answering, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yi Yang",
                    "Yih Wen-Tau",
                    "Christopher Meek"
                ],
                "title": "Wikiqa: A challenge dataset for open-domain question answering",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "431-ARR_v1_101",
            "content": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher Manning, Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Zhilin Yang",
                    "Peng Qi",
                    "Saizheng Zhang",
                    "Yoshua Bengio",
                    "William Cohen",
                    "Ruslan Salakhutdinov",
                    "Christopher Manning"
                ],
                "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "431-ARR_v1_0@0",
            "content": "MultiSpanQA: A Dataset for Multi-Span Question Answering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_0",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@0",
            "content": "Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@1",
            "content": "Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common in real life but are less studied.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 153,
            "end": 305,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@2",
            "content": "In this paper, we present MultiSpanQA, a new dataset that focuses on questions with multi-span answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 307,
            "end": 409,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@3",
            "content": "Raw questions and contexts are extracted from the Natural Questions (Kwiatkowski et al., 2019) dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 411,
            "end": 513,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@4",
            "content": "After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multispan answers in the expanded version.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 515,
            "end": 763,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@5",
            "content": "We introduce new metrics for the purposes of multispan question answering evaluation, and establish several baselines using advanced models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 765,
            "end": 904,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@6",
            "content": "Finally, we propose a new model which beats all baselines and achieves the state-of-the-art on our dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 906,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_2@7",
            "content": "Dataset and code will be released on acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_2",
            "start": 1014,
            "end": 1061,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_4@0",
            "content": "The task of reading comprehension, where models are required to process a text and answer questions about it, has seen rapid progress in recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_4",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_4@1",
            "content": "As systems have increasingly matched humans on popular datasets (Rajpurkar et al., 2016(Rajpurkar et al., , 2018, researchers have developed newer, more complex formulations of the task, such as very long contexts and answers (Kwiatkowski et al., 2019), multi-hop reasoning (Yang et al., 2018), and discrete operations over the content of paragraphs (Dua et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_4",
            "start": 151,
            "end": 519,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_4@2",
            "content": "One thing these datasets have in common is that the answer is constrained to be a single span that can be extracted or computed from the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_4",
            "start": 521,
            "end": 665,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_5@0",
            "content": "However, in practice, the answer to a question will often consist of multiple parts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_5",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_5@1",
            "content": "As in the example in Figure 1, the answer set contains 10 countries, Question: Which countries does the Danube River flow through? Passage: ... Originating in Germany, the Danube flows southeast for 2,850 km (1,770 mi), passing through or bordering Austria, Slovakia, Hungary, Croatia, Serbia, Romania, Bulgaria, Moldova and Ukraine before draining into the Black Sea. ... Answer set: {Germany, Austria, Slovakia, Hungary, Croatia, Serbia, Romania, Bulgaria, Moldova, Ukraine } some of which are discontiguous in the passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_5",
            "start": 85,
            "end": 609,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_5@2",
            "content": "Such cases are largely ignored in existing reading comprehension research, in part because there are no datasets of multi-span questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_5",
            "start": 611,
            "end": 747,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_6@0",
            "content": "In this paper, we introduce MultiSpanQA, a new reading comprehension dataset consistinga of 6,465 multi-span examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_6",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_6@1",
            "content": "The raw questions and passages are extracted from Natural Questions (\"NQ\": Kwiatkowski et al. (2019)), a large-scale open-domain QA dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_6",
            "start": 119,
            "end": 258,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_6@2",
            "content": "Trained annotators were asked to identify question-passage pairs where the answer was multi-span, and annotate the spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_6",
            "start": 260,
            "end": 380,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_6@3",
            "content": "In addition to the basic version of the dataset consisting entirely of multi-span answers, we also prepare an expanded version with a selection of unanswerable questions, and questions with single-and multi-span answers, intended to reflect a more realistic QA setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_6",
            "start": 382,
            "end": 648,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_7@0",
            "content": "We further classify answer semantics into 5 categories, and manually label the logical structure of the answer spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_7",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_7@1",
            "content": "We introduce metrics to evaluate multi-span QA systems across these different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_7",
            "start": 118,
            "end": 201,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_8@0",
            "content": "We propose several baselines, and a new model which casts the task as a sequence tagging problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_8",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_8@1",
            "content": "The proposed model combines a sequence tagger with a span number predictor, span structure predictor, and span adjustment module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_8",
            "start": 98,
            "end": 226,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_8@2",
            "content": "Experimental results show that the proposed model surpasses all baselines and achieves 62.58% exact-match F1 score and 77.46% overlapping F1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_8",
            "start": 228,
            "end": 374,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_9@0",
            "content": "To summarize, our contributions are:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_9",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_10@0",
            "content": "\u2022 A new reading comprehension dataset containing 6.5k high-quality multi-span answers, along with analysis and metrics for multi-span QA. \u2022 A novel label set for capturing the semantics of multi-span answers, with annotations. \u2022 A new model for multi-span reading comprehension which achieves state-of-the-art results on our dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_10",
            "start": 0,
            "end": 332,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_11@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_11",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_12@0",
            "content": "Question Answering Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_12",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@0",
            "content": "Extractive QA Most existing extractive QA datasets such as SQuAD (Rajpurkar et al., 2016), SQuAD2.0 (Rajpurkar et al., 2018), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) restrict the answer passage to a single span of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@1",
            "content": "SQuAD and SQuAD 2.0 limit the answer passage to a short paragraph from Wikipedia; the bestperforming systems have now exceeded human performance on these datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 239,
            "end": 401,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@2",
            "content": "QuAC frames the task in a dialogue setting by introducing a teacher and student, where the student repeatedly asks the teacher questions about a topic and the teacher tries to find answers from the given passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 403,
            "end": 614,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@3",
            "content": "That is, it supports information seeking through multi-turn conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 616,
            "end": 688,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@4",
            "content": "TriviaQA (Joshi et al., 2017) and HothopQA (Yang et al., 2018) extend the answer context from single passage to multiple passages, while HotpotQA further requires reasoning over multiple passages to answer the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 690,
            "end": 908,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@5",
            "content": "However, all of these datasets limit the answer to a single text span from the provided answer context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 910,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@6",
            "content": "DROP (Dua et al., 2019) requires systems to resolve (possibly multiple) references in a question, and perform discrete operations (such as addition, sorting, or counting) over them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1014,
            "end": 1194,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@7",
            "content": "However, because these operations are mostly numeric, the spans are almost exclusively semantically homogeneous and related to numeric values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1196,
            "end": 1337,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@8",
            "content": "MASH-QA (Dua et al., 2019) extends the answer space to texts that span across a longer document, but this dataset is highly domain-specific, in the healthcare domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1339,
            "end": 1504,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@9",
            "content": "Quoref and Natural Questions (\"NQ\": Kwiatkowski et al. (2019)) both contain multi-span answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1506,
            "end": 1600,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@10",
            "content": "Quoref requires systems to resolve coreference among entities, to aid in span-selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1602,
            "end": 1689,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@11",
            "content": "NQ is a large-scale dataset that provides questions with very long answer contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1691,
            "end": 1773,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@12",
            "content": "The proportion of multispan answers is around 10% and 2% in Quoref and NQ, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1775,
            "end": 1862,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@13",
            "content": "However in each case, multispan answers are captured as a single span, with no annotation of the internal structure of the component spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 1864,
            "end": 2002,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_13@14",
            "content": "WikiHowQA and WebQA (Cui et al., 2021) both focus on non-factoid (e.g., how, why) questions, with answers mostly being long spans or full sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_13",
            "start": 2004,
            "end": 2151,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@0",
            "content": "Generative QA Generative QA datasets usually require systems to answer questions in the form of several sentences, either selected from the provided answer context or generated based on it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@1",
            "content": "Wik-iQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) are two open-domain generative QA datasets, where answers in WikiQA are mostly sentences from the answer passage, while answers in MS Marco are free-form sentences generated by crowd workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 190,
            "end": 443,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@2",
            "content": "NarrativeQA (Kocisk\u00fd et al., 2018) is a dataset of movie and book summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 445,
            "end": 520,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@3",
            "content": "SearchQA (Dunn et al., 2017), ELI5 (Fan et al., 2019), and CoQA (Reddy et al., 2019) are three multiple-document datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 522,
            "end": 643,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@4",
            "content": "SearchQA is constructed from question-answer pairs crawled from Jeopardy!, and most questions can be answered with a short (99% less than 5 tokens) extractive span from a single document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 645,
            "end": 831,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@5",
            "content": "ELI5 requires systems to generate paragraph-length answers by summarizing information from multiple documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 833,
            "end": 942,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_14@6",
            "content": "CoQA contains conversational questions, with freeform text as answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_14",
            "start": 944,
            "end": 1013,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@0",
            "content": "Cloze style Cloze datasets such as CNN/Daily Mail (Hermann et al., 2015), Children's Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) require systems to predict a missing word from a passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@1",
            "content": "However, researchers have shown that this task is artificial, and can be largely solved with simple methods and relatively little reasoning (Chen et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 215,
            "end": 374,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@2",
            "content": "Dua et al. (2019) proposed to predict the number of output spans for each question, by applying a single-span predictor recursively, making training complex.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 376,
            "end": 532,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@3",
            "content": "Segal et al. (2020) first proposed to treat multi-span QA as a sequence tagging task, in the form of a multi-head architecture (Dua et al., 2019) to perform arithmetic operations between the predicted spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 534,
            "end": 740,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@4",
            "content": "Hu et al. (2019) applied the non-maximum suppression (NMS) algorithm (Rosenfeld and Thurston, 1971) to prune redundant bounding boxes from the top-k predicted spans of a single-span predictor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 742,
            "end": 933,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@5",
            "content": "Pang et al. (2019) proposed HAS-QA, which supports multi-span prediction by computing answer probabilities at the question, paragraph and span levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 935,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@6",
            "content": "A common feature of these works is that the predicted spans are fed into an aggregation module, and the answers are usually a single span chosen from the prediction, or a number computed from them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 1086,
            "end": 1282,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@7",
            "content": "Cui et al. (2021) proposed a model which can extract list-form answers across multiple spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 1284,
            "end": 1376,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_15@8",
            "content": "Their work mainly focuses on capturing the sequential and progressive relationships between long-span descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_15",
            "start": 1378,
            "end": 1492,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_16@0",
            "content": "Multi-span Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_16",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_17@0",
            "content": "Dataset Construction and Composition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_17",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_18@0",
            "content": "In this section, we describe how we construct Mul-tiSpanQA, and provide a statistical breakdown of its composition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_18",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_19@0",
            "content": "Data Collection and Preprocessing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_19",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_20@0",
            "content": "The question-passage pairs were selected from Natural Questions (NQ: Kwiatkowski et al. ( 2019)), a large-scale open-domain QA dataset made up of (question, passage, long answer, short answer) quadruples where: the questions are real queries issued to the Google search engine; the passage is a Wikipedia page which may or may not contain the information required to answer the question; the long answer is a paragraph from the page containing all information required to infer the answer; and the short answer is one or more text spans that answer the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_20",
            "start": 0,
            "end": 561,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_20@1",
            "content": "Both long and short answers can be NULL if no viable answer candidate exists on the page.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_20",
            "start": 563,
            "end": 651,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_21@0",
            "content": "To create MultiSpanQA, we first extract NQ questions annotated with multiple short answers, and consider the long answer to be the answer passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_21",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_21@1",
            "content": "We then remove paragraphs that don't contain any question part, to eliminate the informationretrieval component of NQ and focus more on the short answer extraction problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_21",
            "start": 147,
            "end": 318,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_21@2",
            "content": "To make the dataset easy to use, we strip HTML from the passages, so that they only contain plain text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_21",
            "start": 320,
            "end": 422,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_21@3",
            "content": "As table structure cannot be captured in the plain text after removing HTML, we remove the passages that contain tables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_21",
            "start": 424,
            "end": 543,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_21@4",
            "content": "Ultimately, around 6700 candidates remain where each candidate is a triple of (question, passage, set of answer spans).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_21",
            "start": 545,
            "end": 663,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_22@0",
            "content": "To aid the annotation process, we classifies the samples into 5 categories according to the expected answer type of questions using a BERT-based classifier trained on the TREC Question Classification dataset (Li and Roth, 2002).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_22",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_22@1",
            "content": "The classes are DE-SCRIPTION, LOCATION, HUMAN, NUMERIC, and OTHER ENTITY.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_22",
            "start": 229,
            "end": 301,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_22@2",
            "content": "Table 1 shows the breakdown and an example of each answer type class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_22",
            "start": 303,
            "end": 371,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_23@0",
            "content": "Issues in Existing Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_23",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_24@0",
            "content": "NQ was originally annotated by around 50 annotators, with an average annotation time of 80 seconds per instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_24",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_24@1",
            "content": "However, we found a number of issues with the dataset: (1) grammatical errors in questions, due to them being actual queries submitted to the Google search engine by real users;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_24",
            "start": 113,
            "end": 289,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_25@0",
            "content": "(2) answer boundary inconsistencies or errors, such as the entity University of Michigan being annotated as an answer in one example but The University of Michigan being annotated in another; and (3) wrong or incomplete answer: some questions are not answered or are answered incompletely in the annotated answer span, for example, to answer the question Which countries does the River Danube flow through?, 10 countries should be included in the answer span while only 9 are annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_25",
            "start": 0,
            "end": 485,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_25@1",
            "content": "These issues are relatively uncommon overall in the dataset, but occur disproportionately in multi-span answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_25",
            "start": 487,
            "end": 598,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_26@0",
            "content": "High Quality Re-annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_26",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@0",
            "content": "We (re-)annotated all the data using the Brat annotation tool (Stenetorp et al., 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@1",
            "content": "1 Three annotators were provided with a category-specific annotation guide (broken down across the 5 predicted answer types), and annotated the data on a per-category basis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 88,
            "end": 260,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@2",
            "content": "2 For each annotation instance, we show the question, passage, and the original multiple answer spans to the annotator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 262,
            "end": 380,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@3",
            "content": "The first-pass annotation was according to the following four categories: \u2022 Good example: the question is clear, and the answer spans are labelled consistent with the annotation guide, in which case accept the instance as is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 382,
            "end": 606,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@4",
            "content": "\u2022 Bad question: the question is ungrammatical or not aligned with the passage content, in which case rewrite the question while preserving its original intended meaning where possible (otherwise reject).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 608,
            "end": 810,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@5",
            "content": "\u2022 Bad answer span(s): the answer span(s) are incorrect or incomplete, in which case remove the inappropriate spans and select the correct spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 812,
            "end": 955,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_27@6",
            "content": "\u2022 Bad question-answer pair: the question doesn't align with the passage content (e.g.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_27",
            "start": 957,
            "end": 1041,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_28@0",
            "content": "there is no answer there) or there are not multiple answer spans in the passage (e.g. there is only a single answer span), in which case reject the instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_28",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_29@0",
            "content": "Although all examples in our dataset contain multiple answer spans, the semantic structure varies considerably.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_29",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_29@1",
            "content": "We hand-annotate this via a novel 5-way annotation scheme, as follows (see Table 2 for examples):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_29",
            "start": 112,
            "end": 208,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_30@0",
            "content": "\u2022 1. CONJUNCTION: Each span is part of the answer, and the answer is complete only when all of the spans are combined \u2022 MULTI-PART-DISJUNCTION: Each span is a complete (but independent) answer to the question, with one of the following structures:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_30",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@0",
            "content": "-2. REDUNDANT: the multiple spans refer to the same concept or entity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@1",
            "content": "For example, in the example in Table 2, each span is a full answer to the question, specified using different temporal reference points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 71,
            "end": 206,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@2",
            "content": "-3. NON-REDUNDANT: the different spans refer to different concepts or entities, each of which is independently correct in its respective context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 208,
            "end": 352,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@3",
            "content": "For example, in the example in Table 2 each span is independently correct in the context of a particular national market.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 354,
            "end": 474,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@4",
            "content": "\u2022 4. COMPLEX: The question is complex (made up of multiple sub-parts), and each span is an answer to a different sub-part, the internal logic of which is not enumeration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 476,
            "end": 645,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@5",
            "content": "For example, in the example in Table 2, the two spans are independent answers to the two subquestions in the original question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 647,
            "end": 773,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@6",
            "content": "\u2022 5. SHARED STRUCTURE: Spans are enumerated in the form of a syntactically-coordinated structure, sharing either a modifier or a head (i.e. the first word(s) of the first span or last word(s) of the last span).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 775,
            "end": 984,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@7",
            "content": "For example, in the example in Table 2, the three spans share the syntactic head bus service, and the full answer is equivalent to scheduled bus service + fixed-route regional bus service + commuter bus service.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 986,
            "end": 1196,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_31@8",
            "content": "Table 3: Number of answer spans in MultiSpanQA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_31",
            "start": 1198,
            "end": 1244,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_32@0",
            "content": "Dataset Statistics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_32",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_33@0",
            "content": "The annotation was performed by three trained annotators with an average annotation time of 70 seconds per instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_33",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_33@1",
            "content": "To test the inter-annotator agreement (IAA), we randomly selected 100 instances for each pairing of the three annotators to anntotate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_33",
            "start": 117,
            "end": 250,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@0",
            "content": "The same annotation (of all spans) of an instance is considered as an agreement, and any difference in one instance is considered as a disagreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@1",
            "content": "The average pairwise IAA is 0.86 for answer spans and 0.94 for answer structures (both based on macroaveraged exact match F1 score), with some disagreements between CONJUNCTION and MULTI-PART-DISJUNCTION (NON-REDUNDANT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 149,
            "end": 368,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@2",
            "content": "To better understand the composition of MultiSpanQA, we compare our annotations with those in NQ, and provide some basic statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 370,
            "end": 501,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@3",
            "content": "Compared to the original annotations in NQ, the annotators rejected 3.1% of instances, re-wrote the question for 5.6% of instances, and modified the answer span annotations for 22% of instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 503,
            "end": 696,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@4",
            "content": "MultiSpanQA contains 6,465 instances with 5,173 for training, 646 for validation, and 646 for test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 698,
            "end": 796,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_34@5",
            "content": "Table 3 provides the distribution of the number of answer spans in the dataset, from which we see the number of spans ranges from 2 to 21, but 80% of instances contain 2 or 3 spans, and only about 1% of instances contain more than 9 spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_34",
            "start": 798,
            "end": 1036,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_35@0",
            "content": "Dataset Expansion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_35",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_36@0",
            "content": "In its basic form, the MultiSpanQA dataset contains only multiple-span answers, and the correct answer can always be located in the passage (in the form of multiple answer spans).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_36",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_36@1",
            "content": "However, in a real-word QA scenario, single-span answer questions and unanswerable questions (i.e. the answer is not contained in the passage) would realistically exist.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_36",
            "start": 180,
            "end": 348,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_36@2",
            "content": "To create a more realistic and challenging variant of the dataset, we add a comparable number of single-span question-answer pairs and unanswerable instances to MultiSpanQA, by randomly sampling from NQ and applying the same preprocessing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_36",
            "start": 350,
            "end": 588,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_36@3",
            "content": "The total size of the expanded dataset is 19,395 instances (three times the basic version, partitioned similarly to the basic version).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_36",
            "start": 590,
            "end": 724,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_37@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_37",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_38@0",
            "content": "Formally, given a question and passage pair <q,p>, the task of multi-span QA involves finding all answer spans s 1 , s 2 , ...s n , which are neither duplicated nor overlap with each other, as well as predict the answer structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_38",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_39@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_39",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_40@0",
            "content": "Single-span Baseline Because most existing reading comprehension datasets only have singlespan answers, single-span architectures are widely used in reading comprehension research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_40",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_40@1",
            "content": "Usually, a pre-trained model is used to encode the question and passage, and output a contextualised representation for all input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_40",
            "start": 181,
            "end": 317,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_40@2",
            "content": "Then two feed-forward networks are used to compute a score for each token which indicates whether the token is the start or end of the answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_40",
            "start": 319,
            "end": 460,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_40@3",
            "content": "Finally, a softmax layer followed by an argmax function is used to produce the start and end positions of the answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_40",
            "start": 462,
            "end": 578,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_41@0",
            "content": "To make MultiSpanQA trainable for a singlespan architecture, we experimented with two preprocessing methods, and created two baselines accordingly:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_41",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_42@0",
            "content": "1. Mark the start of the answer as the start position of the first answer span and mark the end of the answer as the end position of the last answer span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_42",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_42@1",
            "content": "In this way, the model can learn to find the shortest span that includes all answer spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_42",
            "start": 155,
            "end": 244,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_42@2",
            "content": "We select the best prediction for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_42",
            "start": 246,
            "end": 290,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_42@3",
            "content": "2. Suppose an instance has n answer spans, we replace the instance with n instances, one for each span with a single-span answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_42",
            "start": 292,
            "end": 420,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_42@4",
            "content": "In this way, we can apply single-span answer models to our dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_42",
            "start": 422,
            "end": 488,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_43@0",
            "content": "For evaluation, to enable multi-span prediction, we output the 20 highest-scoring predictions, and tune a threshold t to select the answer spans with a confidence score larger than t that optimises performance on the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_43",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_43@1",
            "content": "We remove overlapping predictions based on confidence scores, rejecting predictions with lower confidence scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_43",
            "start": 231,
            "end": 343,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_43@2",
            "content": "Note that for both baselines, we apply the pre-processing to the training data only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_43",
            "start": 345,
            "end": 428,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_44@0",
            "content": "Sequence Tagging Baseline Following Segal et al. (2020), we cast question answering as a sequence tagging task, predicting for each token whether it is part of an answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_44",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_44@1",
            "content": "In our experiments, we use the popular IOB tagging scheme to mark answer spans in the passage where B denotes the first token of an answer span, I denotes subsequent tokens within a span, and O denotes tokens that are not part of an answer span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_44",
            "start": 171,
            "end": 415,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_45@0",
            "content": "Proposed Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_45",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_46@0",
            "content": "By investigating the failures of sequence tagging baseline, we find there is an issue that the global information is hard to be captured during tagging.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_46",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_46@1",
            "content": "For example, the number of answer spans may be specified in the question, but cannot be imposed as a constraint on the tagger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_46",
            "start": 153,
            "end": 278,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_46@2",
            "content": "To better use such global information, we propose a span number predictor, an answer structure predictor, and a span adjustment module (as in Figure 2), which can be combined with any on-the-fly sequence tagger (encoder).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_46",
            "start": 280,
            "end": 500,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_46@3",
            "content": "Given a pair of question q and passage p, we first encode the question and context together using an sequence pair encoder as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_46",
            "start": 502,
            "end": 627,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_47@0",
            "content": "H = Encoder(< q, p >) \u2208 R l\u00d7h(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_47",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_48@0",
            "content": "where H = [H [CLS] ; H q ; H p ] is the contextualised token representation of all input tokens with a pooled global token [CLS], h is the hidden-layer size, and l is the input length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_48",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_49@0",
            "content": "After encoding, we fetch the hidden states of the context tokens and input them to a linear classifier to perform a preliminary answer span prediction, as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_49",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_50@0",
            "content": "U p = F F N (H p ) \u2208 R lp\u00d7t (2) O p = CRF (U p ) \u2208 R lp\u00d7t(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_50",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_51@0",
            "content": "where l p denotes the length of the passage, and t denotes the number of labels (t = 3 for the IOB tagging scheme).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_51",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_52@0",
            "content": "We then fetch the hidden state of [CLS] token H [CLS] and input it to two feed-forward networks to predict the number of answer spans and the answer structure, respectively, as below:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_52",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_53@0",
            "content": "P num = F F N (H [CLS] )(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_53",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_54@0",
            "content": "P structure = F F N (H [CLS] )(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_54",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_55@0",
            "content": "We use cross-entropy loss for answer span and structure prediction, and mean-square loss for span number regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_55",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_55@1",
            "content": "For training, we use the weighted sum of the three losses:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_55",
            "start": 117,
            "end": 174,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_56@0",
            "content": "L = L spans + \u03bb 1 L num + \u03bb 2 L structure(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_56",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_57@0",
            "content": "Finally, a span adjustment module is used to explicitly combine the predicted span number with the span texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_57",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_57@1",
            "content": "We first assign a confidence score to each label of the preliminary classification using a softmax layer:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_57",
            "start": 111,
            "end": 215,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_58@0",
            "content": "\u03b1 conf = softmax(U p ) \u2208 R lp\u00d7t (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_58",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_59@0",
            "content": "The confidence of a predicted answer span a i is defined as the maximum confidence of the tokens within a i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_59",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_59@1",
            "content": "Suppose there are k spans that been tagged as answers and the predicted number of span is n, if n < k, we rank the predicted spans by confidence score, and keep the top-n answer spans as answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_59",
            "start": 110,
            "end": 304,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_60@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_60",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_61@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_61",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_62@0",
            "content": "For all baselines and our model, we use the Hug-gingFace implementation of BERT Base (Wolf et al., 2019; as our encoder with max_sequence_length = 384 and doc_stride = 128 to deal with long passages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_62",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_62@1",
            "content": "For training, we use the BertAdam optimizer with default hyperparameters and learning rate of 5e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_62",
            "start": 200,
            "end": 298,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_62@2",
            "content": "All models are trained with a batch size of 8 for 3,000 steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_62",
            "start": 300,
            "end": 361,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_62@3",
            "content": "We use a two-layer feed-forward network with a ReLu activation function for all linear layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_62",
            "start": 363,
            "end": 456,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_63@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_63",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@0",
            "content": "We evaluate in terms of exact match and partial match performance, where an exact match occurs when a prediction fully matches one of the groundtruth answers, and the F1 score is computed by treating the predicted and ground-truth answer spans as a set of spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@1",
            "content": "By taking positional information into account, a partial match occurs when a prediction overlaps with a ground-truth span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 263,
            "end": 384,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@2",
            "content": "We use micro-averaged precision, recall, and F1 score for evaluation based on the standard formulation of Precision = T P/(T P + F P ), Recall = T P/(T P + F N ), and F1 = 2 * Precision * Recall /(Precision + Recall), where T P (True Positive) is the number of answer spans correctly predicted by the model, F P (False Positive) is the number of spans incorrectly predicted by the model, and F N (False Negative) is the number of answer spans not predicted by the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 386,
            "end": 855,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@3",
            "content": "In the case of an unanswerable question with the expanded dataset, we use a virtual span which indicates no answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 857,
            "end": 971,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@4",
            "content": "For answer structure prediction, we use accuracy to evaluate the model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 973,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_64@5",
            "content": "only (the basic dataset), and the right part is the results on the expanded dataset (including singlespan answers and unanswerable questions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_64",
            "start": 1057,
            "end": 1198,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_65@0",
            "content": "Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_65",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_66@0",
            "content": "Single-span model From the table, we see that single-span (v1) gets very low exact match scores but higher overlapping scores (compared to exact match), as it is trained to find a single long span that overlaps with all answer spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_66",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_66@1",
            "content": "By comparison, single-span (v2) improves the exact match scores on the basic dataset because it is trained on independent single-span answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_66",
            "start": 234,
            "end": 375,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_66@2",
            "content": "An interesting finding is that single-span (v2) does not drop in the overlapping scores, which seems to be because the tuned threshold t (see Section 4.1) works in this setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_66",
            "start": 377,
            "end": 553,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_66@3",
            "content": "The overall performance of the single-span baselines is relatively low, simply because the models can only predict a single-span answer, which is incompatible with the MultiSpanQA dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_66",
            "start": 555,
            "end": 742,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_67@0",
            "content": "Sequence tagging model Compared to the single-span baselines, the sequence tagging models perform much better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_67",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_67@1",
            "content": "Without changing the encoder, there is an improvement of more than 30 absolute points on the exact match metrics, and about 3 for the overlapping F1 metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_67",
            "start": 111,
            "end": 266,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_67@2",
            "content": "Performance is boosted using joint training with span number prediction and answer semantics prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_67",
            "start": 268,
            "end": 371,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_67@3",
            "content": "Our proposed model achieves the best F1 score in most settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_67",
            "start": 373,
            "end": 435,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_68@0",
            "content": "Another interesting finding is that single-span models usually attain higher precision, while sequence tagging models attain higher recall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_68",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_68@1",
            "content": "This demonstrates that single-span models are more accurate in the single-span answer they predict, while sequence tagging models predictably tend to make more predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_68",
            "start": 140,
            "end": 311,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_69@0",
            "content": "Comparing the two datasets Comparing results on the two datasets, we see that single-span baselines are boosted over the expanded dataset (where we add single-span answers and unanswerable questions), as single-span answers are more tractable for these simpler models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_69",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_69@1",
            "content": "The relative improvements for sequence tagging models are more modest, but they still have a clear advantage over the single-span baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_69",
            "start": 269,
            "end": 408,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_70@0",
            "content": "Difficulty analysis To explore the difficulty of the MultiSpanQA dataset, we report the dev set results categorised by answer type in Table 5 and categorised by the number of spans in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_70",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_70@1",
            "content": "From the answer type perspective, the model performs best on HUMAN questions, followed by OTHER ENTITY and LOCATION (largely following the natural distribution of the respective classes in the dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_70",
            "start": 193,
            "end": 394,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_70@2",
            "content": "There is quite a drop for the NUMERIC class, and a big drop again for the DESCRIPTION class, which was also the class our annotators found most difficulty with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_70",
            "start": 396,
            "end": 555,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_71@0",
            "content": "From the perspective of the number of spans, the model performs best on questions with many (> 7) answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_71",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_71@1",
            "content": "We think this is because the answers are usually a list of spans with similar semantics, often structured as a simple coordination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_71",
            "start": 107,
            "end": 237,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_71@2",
            "content": "The performance drops as the answer number decreases because the syntactic pattern in which answer spans occurs is less predictable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_71",
            "start": 239,
            "end": 370,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@0",
            "content": "Answer Semantics From the answer type perspective, LOCATION answers usually have easily predictable structure, while the structure of NU-MERIC answers is the most difficult to predict.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@1",
            "content": "From the perspective of the number of spans, answers consisting of 4-7 spans are relatively easy to predict and there is no significant difference between answers contain few (2 or 3) spans or many (> 7) spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 185,
            "end": 394,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@2",
            "content": "Figure 3 shows the confusion matrix of the answer structure predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 396,
            "end": 467,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@3",
            "content": "We can see that our model tends to predict CONJUNCTION and NON-REDUNDANT, and there are no REDUNDANT and SHARE predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 469,
            "end": 591,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@4",
            "content": "The overall answer structure accuracy is 84.38%, which is slightly higher than the proportion of CONJUNCTION (the majority class) in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 593,
            "end": 737,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_72@5",
            "content": "This suggests that directly applying a simple feedforward network to the pooled encoder output is ineffective for answer semantics prediction, and that this should be an area for future model refinement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_72",
            "start": 739,
            "end": 941,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_73@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_73",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_74@0",
            "content": "We present MultiSpanQA, a reading comprehension dataset where answers consist of multiple discrete spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_74",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_74@1",
            "content": "As part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_74",
            "start": 106,
            "end": 246,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_74@2",
            "content": "We also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_74",
            "start": 248,
            "end": 417,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_74@3",
            "content": "We additionally presented a number of models for multi-span QA extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_74",
            "start": 419,
            "end": 634,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2016, Embracing data abundance: Booktest dataset for reading comprehension, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_75",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_76@0",
            "content": "Danqi Chen, Jason Bolton, Christopher Manning, A thorough examination of the CNN/daily mail reading comprehension task, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_76",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_77@0",
            "content": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, Quac: Question answering in context, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_77",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_78@0",
            "content": "UNKNOWN, None, 2021, Listreader: Extracting list-form answers for opinion questions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_78",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_79@0",
            "content": "Pradeep Dasigi, Nelson Liu, Ana Marasovic, Noah Smith, Matt Gardner, Quoref: A reading comprehension dataset with questions requiring coreferential reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_79",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_80@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_80",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_81@0",
            "content": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_81",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_82@0",
            "content": "UNKNOWN, None, 2017, Searchqa: A new q&a dataset augmented with context from a search engine, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_82",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_83@0",
            "content": "Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, ELI5: long form question answering, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_83",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_84@0",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_84",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_85@0",
            "content": "Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston, The goldilocks principle: Reading children's books with explicit memory representations, 2016, 4th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_85",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_86@0",
            "content": "Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, A multi-type multi-span network for reading comprehension that requires discrete reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_86",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_87@0",
            "content": "Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_87",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_88@0",
            "content": "Tom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Hermann, G\u00e1bor Melis, Edward Grefenstette, The narrativeqa reading comprehension challenge, 2018, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_88",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_89@0",
            "content": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Natural questions: a benchmark for question answering research, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_89",
            "start": 0,
            "end": 412,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_90@0",
            "content": "Xin Li, Dan Roth, Learning question classifiers, 2002, 19th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_90",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_91@0",
            "content": "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, MS MARCO: A human generated machine reading comprehension dataset, 2016, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_91",
            "start": 0,
            "end": 352,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_92@0",
            "content": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Lixin Su, Xueqi Cheng, HAS-QA: hierarchical answer spans model for open-domain question answering, 2019, The Thirty-Third AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_92",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_93@0",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for squad, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_93",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_94@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100, 000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_94",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_95@0",
            "content": "Siva Reddy, Danqi Chen, Christopher Manning, Coqa: A conversational question answering challenge, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_95",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_96@0",
            "content": "Azriel Rosenfeld, Mark Thurston, Edge and curve detection for visual scene analysis, 1971, IEEE Transactions on Computers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_96",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_97@0",
            "content": "Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, Jonathan Berant, A simple and effective model for answering multi-span questions, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_97",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_98@0",
            "content": "Pontus Stenetorp, Sampo Pyysalo, Goran Topi\u0107, Tomoko Ohta, Sophia Ananiadou, Jun'ichi Tsujii, brat: a web-based tool for NLP-assisted text annotation, 2012, Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_98",
            "start": 0,
            "end": 329,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_99@0",
            "content": "UNKNOWN, None, 1910, Huggingface's transformers: State-of-the-art natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_99",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_100@0",
            "content": "Yi Yang, Yih Wen-Tau, Christopher Meek, Wikiqa: A challenge dataset for open-domain question answering, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_100",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "431-ARR_v1_101@0",
            "content": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher Manning, Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "431-ARR_v1_101",
            "start": 0,
            "end": 280,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_1",
            "tgt_ix": "431-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_1",
            "tgt_ix": "431-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_4",
            "tgt_ix": "431-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_5",
            "tgt_ix": "431-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_6",
            "tgt_ix": "431-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_7",
            "tgt_ix": "431-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_8",
            "tgt_ix": "431-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_9",
            "tgt_ix": "431-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_11",
            "tgt_ix": "431-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_12",
            "tgt_ix": "431-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_12",
            "tgt_ix": "431-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_12",
            "tgt_ix": "431-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_12",
            "tgt_ix": "431-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_16",
            "tgt_ix": "431-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_18",
            "tgt_ix": "431-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_20",
            "tgt_ix": "431-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_19",
            "tgt_ix": "431-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_19",
            "tgt_ix": "431-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_19",
            "tgt_ix": "431-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_19",
            "tgt_ix": "431-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_22",
            "tgt_ix": "431-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_24",
            "tgt_ix": "431-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_23",
            "tgt_ix": "431-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_23",
            "tgt_ix": "431-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_23",
            "tgt_ix": "431-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_25",
            "tgt_ix": "431-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_28",
            "tgt_ix": "431-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_29",
            "tgt_ix": "431-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_33",
            "tgt_ix": "431-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_32",
            "tgt_ix": "431-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_32",
            "tgt_ix": "431-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_32",
            "tgt_ix": "431-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_35",
            "tgt_ix": "431-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_35",
            "tgt_ix": "431-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_36",
            "tgt_ix": "431-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_37",
            "tgt_ix": "431-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_37",
            "tgt_ix": "431-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_37",
            "tgt_ix": "431-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_38",
            "tgt_ix": "431-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_40",
            "tgt_ix": "431-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_41",
            "tgt_ix": "431-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_43",
            "tgt_ix": "431-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_37",
            "tgt_ix": "431-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_44",
            "tgt_ix": "431-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_46",
            "tgt_ix": "431-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_47",
            "tgt_ix": "431-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_48",
            "tgt_ix": "431-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_49",
            "tgt_ix": "431-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_50",
            "tgt_ix": "431-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_51",
            "tgt_ix": "431-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_52",
            "tgt_ix": "431-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_53",
            "tgt_ix": "431-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_54",
            "tgt_ix": "431-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_55",
            "tgt_ix": "431-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_56",
            "tgt_ix": "431-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_57",
            "tgt_ix": "431-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_58",
            "tgt_ix": "431-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_59",
            "tgt_ix": "431-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_60",
            "tgt_ix": "431-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_60",
            "tgt_ix": "431-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_61",
            "tgt_ix": "431-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_61",
            "tgt_ix": "431-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_60",
            "tgt_ix": "431-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_62",
            "tgt_ix": "431-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_63",
            "tgt_ix": "431-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_63",
            "tgt_ix": "431-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_60",
            "tgt_ix": "431-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_66",
            "tgt_ix": "431-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_67",
            "tgt_ix": "431-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_68",
            "tgt_ix": "431-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_69",
            "tgt_ix": "431-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_70",
            "tgt_ix": "431-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_71",
            "tgt_ix": "431-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_73",
            "tgt_ix": "431-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_73",
            "tgt_ix": "431-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "431-ARR_v1_0",
            "tgt_ix": "431-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_1",
            "tgt_ix": "431-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_2",
            "tgt_ix": "431-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_3",
            "tgt_ix": "431-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_4",
            "tgt_ix": "431-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_4",
            "tgt_ix": "431-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_4",
            "tgt_ix": "431-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_5",
            "tgt_ix": "431-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_5",
            "tgt_ix": "431-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_5",
            "tgt_ix": "431-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_6",
            "tgt_ix": "431-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_6",
            "tgt_ix": "431-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_6",
            "tgt_ix": "431-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_6",
            "tgt_ix": "431-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_7",
            "tgt_ix": "431-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_7",
            "tgt_ix": "431-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_8",
            "tgt_ix": "431-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_8",
            "tgt_ix": "431-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_8",
            "tgt_ix": "431-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_9",
            "tgt_ix": "431-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_10",
            "tgt_ix": "431-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_11",
            "tgt_ix": "431-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_12",
            "tgt_ix": "431-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_13",
            "tgt_ix": "431-ARR_v1_13@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_14",
            "tgt_ix": "431-ARR_v1_14@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_15",
            "tgt_ix": "431-ARR_v1_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_16",
            "tgt_ix": "431-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_17",
            "tgt_ix": "431-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_18",
            "tgt_ix": "431-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_19",
            "tgt_ix": "431-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_20",
            "tgt_ix": "431-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_20",
            "tgt_ix": "431-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_21",
            "tgt_ix": "431-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_22",
            "tgt_ix": "431-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_22",
            "tgt_ix": "431-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_22",
            "tgt_ix": "431-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_23",
            "tgt_ix": "431-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_24",
            "tgt_ix": "431-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_24",
            "tgt_ix": "431-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_25",
            "tgt_ix": "431-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_25",
            "tgt_ix": "431-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_26",
            "tgt_ix": "431-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_27",
            "tgt_ix": "431-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_28",
            "tgt_ix": "431-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_29",
            "tgt_ix": "431-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_29",
            "tgt_ix": "431-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_30",
            "tgt_ix": "431-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_31",
            "tgt_ix": "431-ARR_v1_31@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_32",
            "tgt_ix": "431-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_33",
            "tgt_ix": "431-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_33",
            "tgt_ix": "431-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_34",
            "tgt_ix": "431-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_35",
            "tgt_ix": "431-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_36",
            "tgt_ix": "431-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_36",
            "tgt_ix": "431-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_36",
            "tgt_ix": "431-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_36",
            "tgt_ix": "431-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_37",
            "tgt_ix": "431-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_38",
            "tgt_ix": "431-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_39",
            "tgt_ix": "431-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_40",
            "tgt_ix": "431-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_40",
            "tgt_ix": "431-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_40",
            "tgt_ix": "431-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_40",
            "tgt_ix": "431-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_41",
            "tgt_ix": "431-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_42",
            "tgt_ix": "431-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_43",
            "tgt_ix": "431-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_43",
            "tgt_ix": "431-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_43",
            "tgt_ix": "431-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_44",
            "tgt_ix": "431-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_44",
            "tgt_ix": "431-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_45",
            "tgt_ix": "431-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_46",
            "tgt_ix": "431-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_46",
            "tgt_ix": "431-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_46",
            "tgt_ix": "431-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_46",
            "tgt_ix": "431-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_47",
            "tgt_ix": "431-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_48",
            "tgt_ix": "431-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_49",
            "tgt_ix": "431-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_50",
            "tgt_ix": "431-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_51",
            "tgt_ix": "431-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_52",
            "tgt_ix": "431-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_53",
            "tgt_ix": "431-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_54",
            "tgt_ix": "431-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_55",
            "tgt_ix": "431-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_55",
            "tgt_ix": "431-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_56",
            "tgt_ix": "431-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_57",
            "tgt_ix": "431-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_57",
            "tgt_ix": "431-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_58",
            "tgt_ix": "431-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_59",
            "tgt_ix": "431-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_59",
            "tgt_ix": "431-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_60",
            "tgt_ix": "431-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_61",
            "tgt_ix": "431-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_62",
            "tgt_ix": "431-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_62",
            "tgt_ix": "431-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_62",
            "tgt_ix": "431-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_62",
            "tgt_ix": "431-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_63",
            "tgt_ix": "431-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_64",
            "tgt_ix": "431-ARR_v1_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_65",
            "tgt_ix": "431-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_66",
            "tgt_ix": "431-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_66",
            "tgt_ix": "431-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_66",
            "tgt_ix": "431-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_66",
            "tgt_ix": "431-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_67",
            "tgt_ix": "431-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_67",
            "tgt_ix": "431-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_67",
            "tgt_ix": "431-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_67",
            "tgt_ix": "431-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_68",
            "tgt_ix": "431-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_68",
            "tgt_ix": "431-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_69",
            "tgt_ix": "431-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_69",
            "tgt_ix": "431-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_70",
            "tgt_ix": "431-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_70",
            "tgt_ix": "431-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_70",
            "tgt_ix": "431-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_71",
            "tgt_ix": "431-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_71",
            "tgt_ix": "431-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_71",
            "tgt_ix": "431-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_72",
            "tgt_ix": "431-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_73",
            "tgt_ix": "431-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_74",
            "tgt_ix": "431-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_74",
            "tgt_ix": "431-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_74",
            "tgt_ix": "431-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_74",
            "tgt_ix": "431-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_75",
            "tgt_ix": "431-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_76",
            "tgt_ix": "431-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_77",
            "tgt_ix": "431-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_78",
            "tgt_ix": "431-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_79",
            "tgt_ix": "431-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_80",
            "tgt_ix": "431-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_81",
            "tgt_ix": "431-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_82",
            "tgt_ix": "431-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_83",
            "tgt_ix": "431-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_84",
            "tgt_ix": "431-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_85",
            "tgt_ix": "431-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_86",
            "tgt_ix": "431-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_87",
            "tgt_ix": "431-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_88",
            "tgt_ix": "431-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_89",
            "tgt_ix": "431-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_90",
            "tgt_ix": "431-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_91",
            "tgt_ix": "431-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_92",
            "tgt_ix": "431-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_93",
            "tgt_ix": "431-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_94",
            "tgt_ix": "431-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_95",
            "tgt_ix": "431-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_96",
            "tgt_ix": "431-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_97",
            "tgt_ix": "431-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_98",
            "tgt_ix": "431-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_99",
            "tgt_ix": "431-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_100",
            "tgt_ix": "431-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "431-ARR_v1_101",
            "tgt_ix": "431-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1228,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "431-ARR",
        "version": 1
    }
}