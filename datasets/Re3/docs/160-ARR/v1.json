{
    "nodes": [
        {
            "ix": "160-ARR_v1_0",
            "content": "Benchmarking Intersectional Biases in NLP",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_2",
            "content": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in stateof-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias -fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairnessaccuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across demographic dimensions. We conclude with implications for future NLP debiasing research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "160-ARR_v1_4",
            "content": "As state-of-the-art natural language processing (NLP) language models become increasingly powerful and pervasive, recent progress in NLP has underscored the need for deeper analyses of how such models perform with respect to underrepresented groups. Research on fairness in NLP has shown that distributed representations of words often encode stereotypes -particularly towards different demographic groups (Blodgett et al., 2020;Bender et al., 2021). There is a growing stream of research that looks at mitigating these biases, especially when it manifests in the learned embedding state (Bolukbasi et al., 2016;Zmigrod et al., 2019;Kaneko and Bollegala, 2021). While prior work has undoubtedly moved the needle, recent surveys and research articles have identified several important gaps and issues (Blodgett et al., 2020;Tan and Celis, 2019). First, much of the current work on examining NLP bias (and proposing debiasing strategies) has focused on representational harm -how a model describes certain groups, including stereotyping and other misrepresentations (Blodgett et al., 2020;Suresh and Guttag, 2019). Conversely, there has been far less work exploring allocational harm in downstream NLP prediction tasks -when a system distributes resources or opportunities differently (Blodgett et al., 2020;Suresh and Guttag, 2019). Downstream tasks, such as sequence classification, also affect underrepresented groups, as these models show disparate impact on various demographic subsets, including women, African Americans, and the elderly (Blodgett et al., 2020;Bender et al., 2021;Shah et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_5",
            "content": "Second, there has been limited work that examines intersectional bias across a wide array of relevant charactersitics, including several demographic dimensions, for a variety of non-debiased and debiased embeddings, on a multitude of downstream tasks. Some work has studied demographic intersections such as young men and old women from a theoretical perspective (e.g., Kearns et al., 2018). Other recent studies have empirically shown that the biases inherent in language models for gender and race intersections might exceed those observed for gender and race alone (Tan and Celis, 2019), and that only debiasing along a single dimension can be problematic (Subramanian et al., 2021). Based on these two gaps, there is a need for a more systematic analysis of how current state-ofthe-art language models and mitigation strategies perform with regards to intersectional bias in downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_6",
            "content": "Accordingly, in this study we perform a broad benchmark analysis of intersectional bias (Figure 1) encompassing the following key characteristics:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_7",
            "content": "Figure 1: Overview of our fairness benchmarking analyses. We benchmark performance across datasets, models, and debiasing methods for tasks involving multiple demographic variables.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_8",
            "content": "\u2022 Benchmark analysis on ten downstream sequence classification tasks related to five datasets that span common modes of usergenerated content: Twitter, forums, Reddit, and survey responses. For these tasks, we also note the allocational harm implications of disparate impact, namely the harm associated with biased NLP-guided interventions. \u2022 Inclusion of five demographic dimensions: gender, race, age, education, and income.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_9",
            "content": "Having three or more dimensions on many of the tasks affords opportunities to examine bias for various demographic intersection subgroups in a more in-depth manner. On four of the datasets, these demographics are selfreported as opposed to being algorithmically or heuristically inferred -an important consideration for debiasing research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_10",
            "content": "\u2022 Evaluation of three prominent word embeddings, BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GloVe (Pennington et al., 2014), and four state-of-the-art model debiasing methods (Ravfogel et al., 2020;Kaneko and Bollegala, 2021;Zmigrod et al., 2019;Webster et al., 2020). This allows us to draw empirical insights regarding the effectiveness of mitigation strategies for downstream tasks.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_11",
            "content": "Our results show that existing debiasing methods are generally very adept at preserving predictive power in downstream tasks. However, their ability to mitigate intersectional bias in such tasks is limited. In general, debiasing BERT/RoBERTa only incrementally alleviates disparate impact of model classifications. Further, while gender bias alone has disparate impact rates of 5-10% or less on most tasks, the range of bias is amplified for intersections -with unfairness rates often being 20 to 50% higher. On tasks such as inferring personality traits, literacy, or numeracy of users, these debiased models are still outside the fairness ranges recommended by governing bodies (Barocas and Selbst, 2016). Interestingly, these biases are more pronounced in models using GloVe, suggesting that debiased transformer-based models generally have better predictive power, and are fairer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_12",
            "content": "Our main contributions are two-fold. First, we perform a large-scale examination of intersectional bias across an array of downstream tasks. Our benchmark evaluation offers empirical evidence that the concerns voiced in recent critical surveys about too much emphasis on representational debiasing devoid of explicit normative goals (Blodgett et al., 2020), relative to mitigation of downstream allocational harm, are well-founded. Second, we quantify the size and scope of the intersectional bias problem, and the risks it can introduce for select underprivileged sub-groups when deploying NLP models for sequence prediction tasks. We are hopeful our work will spur future research that further sheds light on intersectional biases in downstream tasks, as well as mitigation strategies for alleviating allocational harm. Towards this goal, we have included code/data and will make them publicly available via GitHub upon publication.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_13",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "160-ARR_v1_14",
            "content": "Allocational and Representational Harms",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "160-ARR_v1_15",
            "content": "In their survey on bias in NLP, Blodgett et al. (2020) drew a distinction between allocational and representational harms. They found that most papers in NLP describe methods for measuring and mitigating representational harms -when \"a system (e.g., a search engine) represents some social groups in a less favorable light than others, demeans them, or fails to recognize their existence altogether\" (Blodgett et al., 2020). One well-known example are stereotypes in word embeddings, such as certain ethnic groups being more closely associated with \"housekeeper\" (Garg et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_16",
            "content": "In contrast, (Blodgett et al., 2020) only found four papers in their survey that were classified as having techniques for measuring/mitigating allocational harms -these \"arise when an automated system allocates resources (e.g., credit) or opportunities (e.g., jobs) unfairly to different social groups.\" Allocational harm is often aligned with downstream tasks/interventions guided by the NLP model. For instance, all four of the aforementioned allocational harm papers measure and/or mitigate gender bias with respect to an NLP-based occupation classifier Prost et al., 2019;Zhao et al., 2020). More specifically, these studies examine the allocational harm of biased occupation classification predictions on decisions that affect humans, specifically whether an HR NLP system scraping web bios classifies individuals as relevant or not for a position. Our work builds on the nascent allocational harm literature by examining ten downstream tasks related to five data sets spanning Twitter, Reddit, forum, and survey response text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_17",
            "content": "Intersectional Biases",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "160-ARR_v1_18",
            "content": "Intersectional biases arising as a result of interacting demographics have been studied in the broader machine learning literature, either from a theoretical perspective (Kearns et al., 2018;Yang et al., 2020), or in the context of facial recognition (Buolamwini and Gebru, 2018). In NLP, Tan and Celis (2019) evaluate and reveal important intersectional biases in contextualized word embedding models such as BERT and GPT-2. However, in their study, intersectional biases are evaluated using the word association test with an emphasis on representational harm -it remains unclear how intersectional biases affect allocational harm in downstream NLP tasks. Subramanian et al. (2021) looked at intersectional biases of classification models specifically designed for unbiased prediction, but do not evaluate embedding debiasing techniques. We build on the emergent literature on intersectional biases by assessing datasets encompassing up to five demographic dimensions, in conjunction with state-ofthe-art word embeddings and debiasing methods, on downstream tasks where biased predictions can lead to allocational harm ( \u00a73.1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_19",
            "content": "Debiasing",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "160-ARR_v1_20",
            "content": "Pretrained word embeddings, including static word embeddings such as GloVe and contexualized word embeddings such as BERT, contain human-like biases and stereotypical associations (Caliskan et al., 2017;Garg et al., 2018;May et al., 2019). A burgeoning body of NLP work has explored debiasing techniques to mitigate biases in pretrained word embeddings. One body of work has focused on debiasing static word embeddings (Bolukbasi et al., 2016;Zhao et al., 2020Zhao et al., , 2018Kaneko and Bollegala, 2019;Ravfogel et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_21",
            "content": "Given the wide adoption of transformer-based contextualized embedding models, recent research has investigated bias mitigation in models such as BERT and RoBERTa (Zmigrod et al., 2019;Webster et al., 2020;Garimella et al., 2021;Kaneko and Bollegala, 2021). Existing methods for debiasing static and contextualized embeddings have undoubtedly moved the needle on alleviating representational harm along demographic dimensions such as gender. However, Gonen and Goldberg (2019) raised the concern that some debiasing strategies geared towards static word embeddings simply cover up the biases -which can resurface. Moreover, the seemingly debiased static embeddings often do not alleviate biases in downstream NLP prediction tasks (Goldfarb-Tarrant et al., 2021). The extent to which state-of-the-art debiasing methods can mitigate downstream intersectional biases remains unclear. This is precisely one of the gaps our study attempts to shed light on.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_22",
            "content": "Data, Models, Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "160-ARR_v1_23",
            "content": "As previously depicted in Figure 1, our experimental setup is as follows. We assess predictive performance and fairness across five datasets spanning ten dependent variables/tasks and five demographic dimensions. We train three models (GloVe, BERT, and RoBERTa) as our prediction and fairness base- lines. We then debias the input embeddings for these models (Ravfogel et al., 2020;Zmigrod et al., 2019;Kaneko and Bollegala, 2021) and re-train them to compare the performance. Details of the data, models, and evaluation metrics are below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_24",
            "content": "Data",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "160-ARR_v1_25",
            "content": "We examine five datasets (Table 1) across several NLP tasks: psychometric dimension prediction, hate speech identification, personality detection, and sentiment analysis. The psychometric data set (Abbasi et al., 2021) consists of free-text responses on four psychometric dimensions: subjective health literacy, numeracy, anxiety, and trust in doctors. These free-text responses were then linked to survey-based psychometric scores also provided by the participants (serving as gold-standard numeric response labels). The data also includes selfreported demographics for each individual: age, race, gender, income, and education level. This data set was collected using crowd workers from Amazon Mechanical Turk and Qualtrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_26",
            "content": "Similarly, the Five Item Personality Inventory (FIPI) and Myers-Briggs Type Indicator (MBTI) datasets include free text responses to estimate one of the FIPI or MBTI personality traits (Gjurkovi\u0107 et al., 2021). In particular, due to space constraints, we focus on the MBTI traits of perceiving and thinking, and the FIPI traits of extraverted and stable. For FIPI, available demographics are gender, race, age, income, and education. For MBTI, selfreported gender and age are available. The AskAPatient dataset (Limsopatham and Collier, 2016) is taken from web forums and has labeled sentiment, along with gender and age information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_27",
            "content": "The Multilingual Twitter Corpus (MTC) hatespeech dataset contains labeled Twitter messages for the task of hate speech detection (Huang et al., 2020). The dataset also contains inferred author demographic factors. We use three demographics: gender, race, and age.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_28",
            "content": "The Psychometrics, FIPI, AskAPatient, and MBTI tasks are all relevant from an allocational harms perspective. Biases in predictions for healthcare-related variables (Psychometrics), or personality type variables (MBTI, FIPI) can affect an individual's health care plan, personalized interventions, job prospects, etc. Biased predictions for drug rating sentiment can affect which drugs a future user chooses to take.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_29",
            "content": "Models and Debiasing Methods",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "160-ARR_v1_30",
            "content": "In the experiments, we considered several different text classification models. We used a word convolutional neural network (CNN) model, initialized with GloVe embeddings. We also considered two transformer-based contexualized embedding models: BERT and RoBERTa. For each dataset we trained using five-fold cross validation, so that for each example in each dataset, we could generate predictions as unseen test data. 1 Each test fold was then concatenated for a given model for fairness calculations. All models were trained on the same data with hyperparameter tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_31",
            "content": "Debiasing Strategy Static word embeddings (GloVe, Pennington et al., 2014) were debiased using WordED (Ravfogel et al., 2020) 2 . This method iteratively learns a projection of embeddings that removes the bias information with minimal impact on embedding distances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_32",
            "content": "Contextualized word embedding models BERT and RoBERTa were debiased using ContextED (Kaneko and Bollegala, 2021) 3 , which has been shown to work well at removing gender-bias encoded in embeddings. This method uses pre-defined word lists to identify sentences that contain the gendered or stereotype words, and then fine-tunes the pretrained model parameters by encouraging gendered and stereotype words to have orthogonal representations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_33",
            "content": "We also assessed two alternative debiasing methods for the contextualized word embedding models: counterfactual data augmentation (CDA) (Zmigrod et al., 2019) and Dropout (Webster et al., 2020). 4 CDA augments the training corpora with counterfactual data so that the language model is pretrained on gender-balanced text. Dropout mitigates gender biases by increasing the dropout rate in the pretrained models. Therefore, the debiasing methods in our experiments represent different ways of mitigating biases: dataset level (CDA), debiasing during pretraining (ContextED and Dropout), and post-tuning debiasing (WordED).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_34",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "160-ARR_v1_35",
            "content": "There are several definitions of fairness in the literature (Mehrabi et al., 2021), each with corresponding methods of assessment. In this work we rely on two prior metrics from the literature, and also present a new metric, adjusted disparate impact, to account for base rates in the dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_36",
            "content": "Disparate Impact One of the most common fairness assessments is disparate impact (DI, Friedler et al., 2019). DI measures the inequality of positive cases between privileged and non-privileged groups for a particular demographic. DI comes from from the legal field, where certain regulations require DI be above a threshold of 0.8 (or below 1.2 in the inverse case). For true labels y, predicted labels \u0177, and relevant demographic group A:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_37",
            "content": "DI = p(\u0177 = 1|A = 0) p(\u0177 = 1|A = 1)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_38",
            "content": "Where A = 0 refers to the protected group and A = 1 refers to the privileged group. A DI ratio of 1 indicates demographic parity, where the rates of positive predictions are consistent across demographic classes: P (\u0177 = 1|A = 0) = P (\u0177 = 1|A = 1) (Mehrabi et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_39",
            "content": "Statistical Parity (SP) Subgroup Fairness Recent theoretical work on intersectional biases also assesses demographic parity, where the score compares group-specific rates to the global rate in the dataset instead of a comparison between privileged and protected classes (Kearns et al., 2018):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_40",
            "content": "p(A = g) \u00d7 |p(\u0177 = 1) \u2212 p(\u0177 = 1|A = g)| (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_41",
            "content": "This value is compared to an acceptability parameter \u03bb to assess fairness. As this method was proposed for the intersectional case, it gives a way to identify the upper-bound of the fairness violation in a dataset (Yang et al., 2020):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_42",
            "content": "F V = max g\u2208G f |T P R g \u2212 T P R D | (3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_43",
            "content": "Where G f is the set of demographic groups under consideration for analysis, T P R g is the true positive rate of the classifier on the instances in g, and T P R D is the overall true positive rate for the classifier on the dataset. Prior work considered the average violation across groups (Subramanian et al., 2021), but for the purposes of this study we are interested in a worst case analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_44",
            "content": "Adjusted Disparate Impact We propose reweighting DI to account for differences in base rates. Adjusted DI (ADI) divides DI by the base rate ratio for the protected and privileged groups:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_45",
            "content": "DI * = p(y=1|A=0) p(y=1|A=1) , ADI = DI DI *",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_46",
            "content": "Note that the disparate impact metrics are not defined for cases where there are no positive instances for either the protected or privilege classes in the data, or when there are no positive predictions for the privileged class (due to zero division). Therefore, we use additive smoothing when calculating DI and adjusted DI (Zhai and Lafferty, 2004).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_47",
            "content": "Intersectional Fairness To assess intersectional fairness we enumerated all combinations for each n-demographic scenario (e.g., 2-demographic, 3demographic, etc.). We set a reference demographic, specifically gender, because of the prior work on debiasing word embeddings for gender (Bolukbasi et al., 2016;Gonen and Goldberg, 2019;Kaneko and Bollegala, 2021). For intersectional cases, we calculated DI and FV for all possible combinations of demographics that included gender. For example, the 2-demographic case for the psychometrics dataset involves calculating DI and FV for the following protected groups: older women, lower education women, lower income women, and non-white women. Our privileged groups are the negations of the protected groups, e.g., for the above case they are younger men, higher education men, higher income men, and white men. By considering disjoint demographic groups, we avoid cumulative effects of merging fairness results from individual demographics during the intersectional phase. We follow the same procedure for enumerating protected groups for the 3-and 4-demographic cases. For 5-demographics we consider all demographics together. For all models and datasets, we calculated fairness and performance metrics. For performance, we report mean squared error (MSE), Pearson's r, 5 F1, and area under the receiver operating curve (AUC). For fairness, we report adjusted DI and fairness violation (FV, \u00a73.3). 6",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_48",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "160-ARR_v1_49",
            "content": "Figure 2 shows the ADI results for BERT and GloVe using ContextED and WordED for debiasing, respectively. In most cases, particularly for BERT, disparate impact scores for gender alone are 5 MSE and Pearson's r were calculated for datasets where continuous gold standard values were available 6 Standard disparate impact results were consistent with ADI and are not included due to space considerations. in a reasonable range (within 10%). For GloVe, we do observe high gender ADI on Anxiety and Thinking. However, as the number of demographics under consideration grows, the range of ADI scores widens. While debiasing the word embeddings typically helps to reduce the unfairness for the target demographic (e.g., gender), in the intersectional cases the model still performs poorly. There are similar trends in FV scores as the number of demographics increases, with the extent of violations often increasing by a factor of 3x to 10x as intersections increase (Table 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_50",
            "content": "In some cases the intersectional disparities are extreme. On the BERT models, the ratio of positive Numeracy predictions for the protected class is three-to-one compared to the privileged class. In the other direction, for 3-demographics, hatespeech detection positive predictions are significantly less likely for the protected group than the privileged group. This is consistent with prior hatespeech detection work that has shown large (absolute value) fairness gaps between protected and privileged groups (e.g., Liu et al., 2021). In most cases, trends are consistent between the BERT and GloVe models (e.g., Extraverted, Numeracy, Perceiving). Some counterexamples are the Trust and Anxiety tasks. Here model choice impacts the direction of bias. As more demographics are considered, the GloVe model skews more unfair against the protected group, while the BERT model remains mostly fair, skewing slightly unfair against the privileged group. Higher trust in physicians is associated with better well-being and lower anxiety when visiting a doctor (Netemeyer et al., 2020); disparate predictions can lead to missed interventions for trust-increase and anxiety reduction across demographic groups. Though not depicted in the main paper, plots for RoBERTa show similar trends to those observed for BERT while debiasing with ContextED (see Appendix B).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_51",
            "content": "Results are similar when looking at alternate BERT debiasing methods beyond ContextED, namely CDA and Dropout (Figure 3). These findings on the Anxiety, Literacy, Numeracy, and Trust tasks suggest that debiasing at the dataset, embedding pretraining, and post-tuning levels leads to similar increases in unfairness as the number of demographic intersections considered increases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_52",
            "content": "Collectively, the results underscore the allocational harm implications of NLP models on several downstream tasks -ones that even well-designed and well-intentioned debiasing strategies cannot overcome. This can be problematic in the era of personalized marketing and precision health, with NLP-based persona-generation playing a bigger role. For tasks like numeracy and literacy, this can affect how a patient is treated by a medical staff during a hospital visit (i.e., a false positive high literacy prediction for a person who has trouble understanding his or her medical record). For the personality indicators, inconsistent predictions may lead to biased decisions in the workplace (e.g., a manager looking to form a team of extroverts).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_53",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "160-ARR_v1_54",
            "content": "In this work we present a comprehensive benchmarking analysis of fairness for sequence prediction models. We also look at known debiasing methods for these models and show that while the debiased versions maintain predictive performance (as expected), they do not help with mitigating biases. While most models are relatively fair when looking at a single demographic characteristic, accounting for intersectional groups leads to less fair models and wider ranges of bias because of the combinatorial considerations of the intersectional groups. It is our hope that this benchmarking encourages future work into mitigating intersectional biases, and also to collect more demographic information when creating new datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "160-ARR_v1_55",
            "content": "Ahmed Abbasi, David Dobolyi, John Lalor, Richard Netemeyer, Kendall Smith, Yi Yang, Constructing a psychometric testbed for fair natural language processing, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Ahmed Abbasi",
                    "David Dobolyi",
                    "John Lalor",
                    "Richard Netemeyer",
                    "Kendall Smith",
                    "Yi Yang"
                ],
                "title": "Constructing a psychometric testbed for fair natural language processing",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "160-ARR_v1_56",
            "content": "Solon Barocas, D Andrew,  Selbst, Big data's disparate impact, 2016, Calif. L. Rev, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Solon Barocas",
                    "D Andrew",
                    " Selbst"
                ],
                "title": "Big data's disparate impact",
                "pub_date": "2016",
                "pub_title": "Calif. L. Rev",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_57",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Emily Bender",
                    "Timnit Gebru",
                    "Angelina Mcmillan-Major",
                    "Shmargaret Shmitchell"
                ],
                "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "160-ARR_v1_58",
            "content": "Su Blodgett, Solon Barocas, Hal Daum\u00e9, Iii , Hanna Wallach, Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Su Blodgett",
                    "Solon Barocas",
                    "Hal Daum\u00e9",
                    "Iii ",
                    "Hanna Wallach"
                ],
                "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_59",
            "content": "Tolga Bolukbasi, Kai-Wei Chang, Y James, Venkatesh Zou, Adam Saligrama,  Kalai, Man is to computer programmer as woman is to homemaker? debiasing word embeddings, 2016, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tolga Bolukbasi",
                    "Kai-Wei Chang",
                    "Y James",
                    "Venkatesh Zou",
                    "Adam Saligrama",
                    " Kalai"
                ],
                "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
                "pub_date": "2016",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_60",
            "content": "Joy Buolamwini, Timnit Gebru, Gender shades: Intersectional accuracy disparities in commercial gender classification, 2018, Conference on fairness, accountability and transparency, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Joy Buolamwini",
                    "Timnit Gebru"
                ],
                "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
                "pub_date": "2018",
                "pub_title": "Conference on fairness, accountability and transparency",
                "pub": "PMLR"
            }
        },
        {
            "ix": "160-ARR_v1_61",
            "content": "Aylin Caliskan, Joanna Bryson, Arvind Narayanan, Semantics derived automatically from language corpora contain human-like biases, 2017, Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Aylin Caliskan",
                    "Joanna Bryson",
                    "Arvind Narayanan"
                ],
                "title": "Semantics derived automatically from language corpora contain human-like biases",
                "pub_date": "2017",
                "pub_title": "Science",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_62",
            "content": "Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Adam Tauman Kalai, Bias in bios: A case study of semantic representation bias in a high-stakes setting, 2019, proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Maria De-Arteaga",
                    "Alexey Romanov",
                    "Hanna Wallach",
                    "Jennifer Chayes",
                    "Christian Borgs",
                    "Alexandra Chouldechova",
                    "Sahin Geyik",
                    "Krishnaram Kenthapadi",
                    "Adam Tauman Kalai"
                ],
                "title": "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
                "pub_date": "2019",
                "pub_title": "proceedings of the Conference on Fairness, Accountability, and Transparency",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_63",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "160-ARR_v1_64",
            "content": "A Sorelle, Carlos Friedler, Suresh Scheidegger, Sonam Venkatasubramanian, Evan Choudhary, Derek Hamilton,  Roth, A comparative study of fairness-enhancing interventions in machine learning, 2019, Proceedings of the conference on fairness, accountability, and transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "A Sorelle",
                    "Carlos Friedler",
                    "Suresh Scheidegger",
                    "Sonam Venkatasubramanian",
                    "Evan Choudhary",
                    "Derek Hamilton",
                    " Roth"
                ],
                "title": "A comparative study of fairness-enhancing interventions in machine learning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the conference on fairness, accountability, and transparency",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_65",
            "content": "Nikhil Garg, Londa Schiebinger, Dan Jurafsky, James Zou, Word embeddings quantify 100 years of gender and ethnic stereotypes, 2018, Proceedings of the National Academy of Sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Nikhil Garg",
                    "Londa Schiebinger",
                    "Dan Jurafsky",
                    "James Zou"
                ],
                "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
                "pub_date": "2018",
                "pub_title": "Proceedings of the National Academy of Sciences",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_66",
            "content": "Aparna Garimella, Akhash Amarnath, Kiran Kumar, Akash Pramod Yalla, N Anandhavelu, Niyati Chhaya, Balaji Vasan Srinivasan, He is very intelligent, she is very beautiful? on mitigating social biases in language modelling and generation, 2021, Findings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Aparna Garimella",
                    "Akhash Amarnath",
                    "Kiran Kumar",
                    "Akash Pramod Yalla",
                    "N Anandhavelu",
                    "Niyati Chhaya",
                    "Balaji Vasan Srinivasan"
                ],
                "title": "He is very intelligent, she is very beautiful? on mitigating social biases in language modelling and generation",
                "pub_date": "2021",
                "pub_title": "Findings of ACL",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_67",
            "content": "Matej Gjurkovi\u0107, Mladen Karan, Iva Vukojevi\u0107, Mihaela Bo\u0161njak, PANDORA talks: Personality and demographics on Reddit, 2021, Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Matej Gjurkovi\u0107",
                    "Mladen Karan",
                    "Iva Vukojevi\u0107",
                    "Mihaela Bo\u0161njak"
                ],
                "title": "PANDORA talks: Personality and demographics on Reddit",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_68",
            "content": "Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Sanchez, Mugdha Pandya, Adam Lopez, Intrinsic bias metrics do not correlate with application bias, 2021, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Seraphina Goldfarb-Tarrant",
                    "Rebecca Marchant",
                    "Ricardo Sanchez",
                    "Mugdha Pandya",
                    "Adam Lopez"
                ],
                "title": "Intrinsic bias metrics do not correlate with application bias",
                "pub_date": "2021",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_69",
            "content": "Hila Gonen, Yoav Goldberg, Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Hila Gonen",
                    "Yoav Goldberg"
                ],
                "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_70",
            "content": "Xiaolei Huang, Linzi Xing, Franck Dernoncourt, Michael Paul, Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xiaolei Huang",
                    "Linzi Xing",
                    "Franck Dernoncourt",
                    "Michael Paul"
                ],
                "title": "Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_71",
            "content": "Masahiro Kaneko, Danushka Bollegala, Gender-preserving debiasing for pre-trained word embeddings, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Masahiro Kaneko",
                    "Danushka Bollegala"
                ],
                "title": "Gender-preserving debiasing for pre-trained word embeddings",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_72",
            "content": "Masahiro Kaneko, Danushka Bollegala, Debiasing pre-trained contextualised embeddings, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Masahiro Kaneko",
                    "Danushka Bollegala"
                ],
                "title": "Debiasing pre-trained contextualised embeddings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_73",
            "content": "Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu, Preventing fairness gerrymandering: Auditing and learning for subgroup fairness, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Michael Kearns",
                    "Seth Neel",
                    "Aaron Roth",
                    "Zhiwei Steven Wu"
                ],
                "title": "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "160-ARR_v1_74",
            "content": "Nut Limsopatham, Nigel Collier, Normalising medical concepts in social media texts by learning semantic representation, 2016, Proceedings of the 54th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Nut Limsopatham",
                    "Nigel Collier"
                ],
                "title": "Normalising medical concepts in social media texts by learning semantic representation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_75",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "160-ARR_v1_76",
            "content": "Haochen Liu, Wei Jin, Hamid Karimi, Zitao Liu, Jiliang Tang, The authors matter: Understanding and mitigating implicit bias in deep text classification, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Haochen Liu",
                    "Wei Jin",
                    "Hamid Karimi",
                    "Zitao Liu",
                    "Jiliang Tang"
                ],
                "title": "The authors matter: Understanding and mitigating implicit bias in deep text classification",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "160-ARR_v1_77",
            "content": "UNKNOWN, None, , A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_78",
            "content": "Chandler May, Alex Wang, Shikha Bordia, On measuring social biases in sentence encoders, 2019, Proceedings of NAACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Chandler May",
                    "Alex Wang",
                    "Shikha Bordia"
                ],
                "title": "On measuring social biases in sentence encoders",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL)",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_79",
            "content": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan, A survey on bias and fairness in machine learning, 2021, ACM Computing Surveys (CSUR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ninareh Mehrabi",
                    "Fred Morstatter",
                    "Nripsuta Saxena",
                    "Kristina Lerman",
                    "Aram Galstyan"
                ],
                "title": "A survey on bias and fairness in machine learning",
                "pub_date": "2021",
                "pub_title": "ACM Computing Surveys (CSUR)",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_80",
            "content": "G Richard,  Netemeyer, G David, Ahmed Dobolyi, Gari Abbasi, Herman Clifford,  Taylor, Health literacy, health numeracy, and trust in doctor: effects on key patient health outcomes, , Journal of Consumer Affairs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "G Richard",
                    " Netemeyer",
                    "G David",
                    "Ahmed Dobolyi",
                    "Gari Abbasi",
                    "Herman Clifford",
                    " Taylor"
                ],
                "title": "Health literacy, health numeracy, and trust in doctor: effects on key patient health outcomes",
                "pub_date": null,
                "pub_title": "Journal of Consumer Affairs",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_81",
            "content": "Jeffrey Pennington, Richard Socher, Christopher D Manning, Glove: Global vectors for word representation, 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher D Manning"
                ],
                "title": "Glove: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_82",
            "content": "Flavien Prost, Nithum Thain, Tolga Bolukbasi, Debiasing embeddings for reduced gender bias in text classification, 2019, Proceedings of the First Workshop on Gender Bias in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Flavien Prost",
                    "Nithum Thain",
                    "Tolga Bolukbasi"
                ],
                "title": "Debiasing embeddings for reduced gender bias in text classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_83",
            "content": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg, Null it out: Guarding protected attributes by iterative nullspace projection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Shauli Ravfogel",
                    "Yanai Elazar",
                    "Hila Gonen",
                    "Michael Twiton",
                    "Yoav Goldberg"
                ],
                "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_84",
            "content": "Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, Adam Kalai, What's in a name? reducing bias in bios without access to protected attributes, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Alexey Romanov",
                    "Maria De-Arteaga",
                    "Hanna Wallach",
                    "Jennifer Chayes",
                    "Christian Borgs",
                    "Alexandra Chouldechova",
                    "Sahin Geyik",
                    "Krishnaram Kenthapadi",
                    "Anna Rumshisky",
                    "Adam Kalai"
                ],
                "title": "What's in a name? reducing bias in bios without access to protected attributes",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_85",
            "content": "Andrew Deven Santosh Shah, Dirk Schwartz,  Hovy, Predictive biases in natural language processing models: A conceptual framework and overview, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Andrew Deven Santosh Shah",
                    "Dirk Schwartz",
                    " Hovy"
                ],
                "title": "Predictive biases in natural language processing models: A conceptual framework and overview",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_86",
            "content": "Shivashankar Subramanian, Xudong Han, Timothy Baldwin, Trevor Cohn, Lea Frermann, Evaluating debiasing techniques for intersectional biases, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Shivashankar Subramanian",
                    "Xudong Han",
                    "Timothy Baldwin",
                    "Trevor Cohn",
                    "Lea Frermann"
                ],
                "title": "Evaluating debiasing techniques for intersectional biases",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_87",
            "content": "UNKNOWN, None, 2019, A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_88",
            "content": "Yi Chern Tan, L Celis, Assessing social and intersectional biases in contextualized word representations, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Yi Chern Tan",
                    "L Celis"
                ],
                "title": "Assessing social and intersectional biases in contextualized word representations",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "160-ARR_v1_89",
            "content": "UNKNOWN, None, , Slav Petrov. 2020. Measuring and reducing gendered correlations in pre-trained models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Slav Petrov. 2020. Measuring and reducing gendered correlations in pre-trained models",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_90",
            "content": "Forest Yang, Mouhamadou Cisse, Oluwasanmi O Koyejo, Fairness with overlapping groups; a probabilistic perspective, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Forest Yang",
                    "Mouhamadou Cisse",
                    "Oluwasanmi O Koyejo"
                ],
                "title": "Fairness with overlapping groups; a probabilistic perspective",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_91",
            "content": "Chengxiang Zhai, John Lafferty, A study of smoothing methods for language models applied to information retrieval, 2004, ACM Transactions on Information Systems (TOIS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Chengxiang Zhai",
                    "John Lafferty"
                ],
                "title": "A study of smoothing methods for language models applied to information retrieval",
                "pub_date": "2004",
                "pub_title": "ACM Transactions on Information Systems (TOIS)",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_92",
            "content": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Awadallah, Gender bias in multilingual embeddings and crosslingual transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Jieyu Zhao",
                    "Subhabrata Mukherjee",
                    "Saghar Hosseini",
                    "Kai-Wei Chang",
                    "Ahmed Awadallah"
                ],
                "title": "Gender bias in multilingual embeddings and crosslingual transfer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_93",
            "content": "Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, Kai-Wei Chang Chang, Learning gender-neutral word embeddings, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Jieyu Zhao",
                    "Yichao Zhou",
                    "Zeyu Li",
                    "Wei Wang",
                    "Kai-Wei Chang Chang"
                ],
                "title": "Learning gender-neutral word embeddings",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "160-ARR_v1_94",
            "content": "Ran Zmigrod, J Sabrina, Hanna Mielke, Ryan Wallach,  Cotterell, Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Ran Zmigrod",
                    "J Sabrina",
                    "Hanna Mielke",
                    "Ryan Wallach",
                    " Cotterell"
                ],
                "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "160-ARR_v1_0@0",
            "content": "Benchmarking Intersectional Biases in NLP",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_0",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@0",
            "content": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@1",
            "content": "While much work has highlighted biases embedded in stateof-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 207,
            "end": 467,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@2",
            "content": "Moreover, most prior work has emphasized bias along a single dimension such as gender or race.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 469,
            "end": 562,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@3",
            "content": "In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 564,
            "end": 698,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@4",
            "content": "In particular, we assess intersectional bias -fairness across multiple demographic dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 700,
            "end": 793,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@5",
            "content": "The results show that while current debiasing strategies fare well in terms of the fairnessaccuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 795,
            "end": 1030,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@6",
            "content": "Furthermore, this bias is often amplified across demographic dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 1032,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_2@7",
            "content": "We conclude with implications for future NLP debiasing research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_2",
            "start": 1105,
            "end": 1168,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@0",
            "content": "As state-of-the-art natural language processing (NLP) language models become increasingly powerful and pervasive, recent progress in NLP has underscored the need for deeper analyses of how such models perform with respect to underrepresented groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@1",
            "content": "Research on fairness in NLP has shown that distributed representations of words often encode stereotypes -particularly towards different demographic groups (Blodgett et al., 2020;Bender et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 250,
            "end": 449,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@2",
            "content": "There is a growing stream of research that looks at mitigating these biases, especially when it manifests in the learned embedding state (Bolukbasi et al., 2016;Zmigrod et al., 2019;Kaneko and Bollegala, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 451,
            "end": 660,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@3",
            "content": "While prior work has undoubtedly moved the needle, recent surveys and research articles have identified several important gaps and issues (Blodgett et al., 2020;Tan and Celis, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 662,
            "end": 843,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@4",
            "content": "First, much of the current work on examining NLP bias (and proposing debiasing strategies) has focused on representational harm -how a model describes certain groups, including stereotyping and other misrepresentations (Blodgett et al., 2020;Suresh and Guttag, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 845,
            "end": 1111,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@5",
            "content": "Conversely, there has been far less work exploring allocational harm in downstream NLP prediction tasks -when a system distributes resources or opportunities differently (Blodgett et al., 2020;Suresh and Guttag, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 1113,
            "end": 1330,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_4@6",
            "content": "Downstream tasks, such as sequence classification, also affect underrepresented groups, as these models show disparate impact on various demographic subsets, including women, African Americans, and the elderly (Blodgett et al., 2020;Bender et al., 2021;Shah et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_4",
            "start": 1332,
            "end": 1603,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_5@0",
            "content": "Second, there has been limited work that examines intersectional bias across a wide array of relevant charactersitics, including several demographic dimensions, for a variety of non-debiased and debiased embeddings, on a multitude of downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_5",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_5@1",
            "content": "Some work has studied demographic intersections such as young men and old women from a theoretical perspective (e.g., Kearns et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_5",
            "start": 252,
            "end": 390,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_5@2",
            "content": "Other recent studies have empirically shown that the biases inherent in language models for gender and race intersections might exceed those observed for gender and race alone (Tan and Celis, 2019), and that only debiasing along a single dimension can be problematic (Subramanian et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_5",
            "start": 392,
            "end": 685,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_5@3",
            "content": "Based on these two gaps, there is a need for a more systematic analysis of how current state-ofthe-art language models and mitigation strategies perform with regards to intersectional bias in downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_5",
            "start": 687,
            "end": 895,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_6@0",
            "content": "Accordingly, in this study we perform a broad benchmark analysis of intersectional bias (Figure 1) encompassing the following key characteristics:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_6",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_7@0",
            "content": "Figure 1: Overview of our fairness benchmarking analyses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_7",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_7@1",
            "content": "We benchmark performance across datasets, models, and debiasing methods for tasks involving multiple demographic variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_7",
            "start": 58,
            "end": 180,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_8@0",
            "content": "\u2022 Benchmark analysis on ten downstream sequence classification tasks related to five datasets that span common modes of usergenerated content: Twitter, forums, Reddit, and survey responses. For these tasks, we also note the allocational harm implications of disparate impact, namely the harm associated with biased NLP-guided interventions. \u2022 Inclusion of five demographic dimensions: gender, race, age, education, and income.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_8",
            "start": 0,
            "end": 425,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_9@0",
            "content": "Having three or more dimensions on many of the tasks affords opportunities to examine bias for various demographic intersection subgroups in a more in-depth manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_9",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_9@1",
            "content": "On four of the datasets, these demographics are selfreported as opposed to being algorithmically or heuristically inferred -an important consideration for debiasing research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_9",
            "start": 165,
            "end": 338,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_10@0",
            "content": "\u2022 Evaluation of three prominent word embeddings, BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GloVe (Pennington et al., 2014), and four state-of-the-art model debiasing methods (Ravfogel et al., 2020;Kaneko and Bollegala, 2021;Zmigrod et al., 2019;Webster et al., 2020). This allows us to draw empirical insights regarding the effectiveness of mitigation strategies for downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_10",
            "start": 0,
            "end": 401,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@0",
            "content": "Our results show that existing debiasing methods are generally very adept at preserving predictive power in downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@1",
            "content": "However, their ability to mitigate intersectional bias in such tasks is limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 126,
            "end": 205,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@2",
            "content": "In general, debiasing BERT/RoBERTa only incrementally alleviates disparate impact of model classifications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 207,
            "end": 313,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@3",
            "content": "Further, while gender bias alone has disparate impact rates of 5-10% or less on most tasks, the range of bias is amplified for intersections -with unfairness rates often being 20 to 50% higher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 315,
            "end": 507,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@4",
            "content": "On tasks such as inferring personality traits, literacy, or numeracy of users, these debiased models are still outside the fairness ranges recommended by governing bodies (Barocas and Selbst, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 509,
            "end": 706,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_11@5",
            "content": "Interestingly, these biases are more pronounced in models using GloVe, suggesting that debiased transformer-based models generally have better predictive power, and are fairer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_11",
            "start": 708,
            "end": 883,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@0",
            "content": "Our main contributions are two-fold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@1",
            "content": "First, we perform a large-scale examination of intersectional bias across an array of downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 37,
            "end": 139,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@2",
            "content": "Our benchmark evaluation offers empirical evidence that the concerns voiced in recent critical surveys about too much emphasis on representational debiasing devoid of explicit normative goals (Blodgett et al., 2020), relative to mitigation of downstream allocational harm, are well-founded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 141,
            "end": 430,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@3",
            "content": "Second, we quantify the size and scope of the intersectional bias problem, and the risks it can introduce for select underprivileged sub-groups when deploying NLP models for sequence prediction tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 432,
            "end": 631,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@4",
            "content": "We are hopeful our work will spur future research that further sheds light on intersectional biases in downstream tasks, as well as mitigation strategies for alleviating allocational harm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 633,
            "end": 820,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_12@5",
            "content": "Towards this goal, we have included code/data and will make them publicly available via GitHub upon publication.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_12",
            "start": 822,
            "end": 933,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_13@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_13",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_14@0",
            "content": "Allocational and Representational Harms",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_14",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_15@0",
            "content": "In their survey on bias in NLP, Blodgett et al. (2020) drew a distinction between allocational and representational harms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_15",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_15@1",
            "content": "They found that most papers in NLP describe methods for measuring and mitigating representational harms -when \"a system (e.g., a search engine) represents some social groups in a less favorable light than others, demeans them, or fails to recognize their existence altogether\" (Blodgett et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_15",
            "start": 123,
            "end": 423,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_15@2",
            "content": "One well-known example are stereotypes in word embeddings, such as certain ethnic groups being more closely associated with \"housekeeper\" (Garg et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_15",
            "start": 425,
            "end": 582,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_16@0",
            "content": "In contrast, (Blodgett et al., 2020) only found four papers in their survey that were classified as having techniques for measuring/mitigating allocational harms -these \"arise when an automated system allocates resources (e.g., credit) or opportunities (e.g., jobs) unfairly to different social groups.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_16",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_16@1",
            "content": "Allocational harm is often aligned with downstream tasks/interventions guided by the NLP model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_16",
            "start": 304,
            "end": 398,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_16@2",
            "content": "For instance, all four of the aforementioned allocational harm papers measure and/or mitigate gender bias with respect to an NLP-based occupation classifier Prost et al., 2019;Zhao et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_16",
            "start": 400,
            "end": 594,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_16@3",
            "content": "More specifically, these studies examine the allocational harm of biased occupation classification predictions on decisions that affect humans, specifically whether an HR NLP system scraping web bios classifies individuals as relevant or not for a position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_16",
            "start": 596,
            "end": 852,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_16@4",
            "content": "Our work builds on the nascent allocational harm literature by examining ten downstream tasks related to five data sets spanning Twitter, Reddit, forum, and survey response text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_16",
            "start": 854,
            "end": 1031,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_17@0",
            "content": "Intersectional Biases",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_17",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_18@0",
            "content": "Intersectional biases arising as a result of interacting demographics have been studied in the broader machine learning literature, either from a theoretical perspective (Kearns et al., 2018;Yang et al., 2020), or in the context of facial recognition (Buolamwini and Gebru, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_18",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_18@1",
            "content": "In NLP, Tan and Celis (2019) evaluate and reveal important intersectional biases in contextualized word embedding models such as BERT and GPT-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_18",
            "start": 281,
            "end": 424,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_18@2",
            "content": "However, in their study, intersectional biases are evaluated using the word association test with an emphasis on representational harm -it remains unclear how intersectional biases affect allocational harm in downstream NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_18",
            "start": 426,
            "end": 655,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_18@3",
            "content": "Subramanian et al. (2021) looked at intersectional biases of classification models specifically designed for unbiased prediction, but do not evaluate embedding debiasing techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_18",
            "start": 657,
            "end": 837,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_18@4",
            "content": "We build on the emergent literature on intersectional biases by assessing datasets encompassing up to five demographic dimensions, in conjunction with state-ofthe-art word embeddings and debiasing methods, on downstream tasks where biased predictions can lead to allocational harm ( \u00a73.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_18",
            "start": 839,
            "end": 1127,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_19@0",
            "content": "Debiasing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_19",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_20@0",
            "content": "Pretrained word embeddings, including static word embeddings such as GloVe and contexualized word embeddings such as BERT, contain human-like biases and stereotypical associations (Caliskan et al., 2017;Garg et al., 2018;May et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_20",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_20@1",
            "content": "A burgeoning body of NLP work has explored debiasing techniques to mitigate biases in pretrained word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_20",
            "start": 240,
            "end": 352,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_20@2",
            "content": "One body of work has focused on debiasing static word embeddings (Bolukbasi et al., 2016;Zhao et al., 2020Zhao et al., , 2018Kaneko and Bollegala, 2019;Ravfogel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_20",
            "start": 354,
            "end": 528,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@0",
            "content": "Given the wide adoption of transformer-based contextualized embedding models, recent research has investigated bias mitigation in models such as BERT and RoBERTa (Zmigrod et al., 2019;Webster et al., 2020;Garimella et al., 2021;Kaneko and Bollegala, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@1",
            "content": "Existing methods for debiasing static and contextualized embeddings have undoubtedly moved the needle on alleviating representational harm along demographic dimensions such as gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 257,
            "end": 439,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@2",
            "content": "However, Gonen and Goldberg (2019) raised the concern that some debiasing strategies geared towards static word embeddings simply cover up the biases -which can resurface.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 441,
            "end": 611,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@3",
            "content": "Moreover, the seemingly debiased static embeddings often do not alleviate biases in downstream NLP prediction tasks (Goldfarb-Tarrant et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 613,
            "end": 760,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@4",
            "content": "The extent to which state-of-the-art debiasing methods can mitigate downstream intersectional biases remains unclear.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 762,
            "end": 878,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_21@5",
            "content": "This is precisely one of the gaps our study attempts to shed light on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_21",
            "start": 880,
            "end": 949,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_22@0",
            "content": "Data, Models, Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_22",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_23@0",
            "content": "As previously depicted in Figure 1, our experimental setup is as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_23",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_23@1",
            "content": "We assess predictive performance and fairness across five datasets spanning ten dependent variables/tasks and five demographic dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_23",
            "start": 74,
            "end": 211,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_23@2",
            "content": "We train three models (GloVe, BERT, and RoBERTa) as our prediction and fairness base- lines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_23",
            "start": 213,
            "end": 304,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_23@3",
            "content": "We then debias the input embeddings for these models (Ravfogel et al., 2020;Zmigrod et al., 2019;Kaneko and Bollegala, 2021) and re-train them to compare the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_23",
            "start": 306,
            "end": 475,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_23@4",
            "content": "Details of the data, models, and evaluation metrics are below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_23",
            "start": 477,
            "end": 538,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_24@0",
            "content": "Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_24",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_25@0",
            "content": "We examine five datasets (Table 1) across several NLP tasks: psychometric dimension prediction, hate speech identification, personality detection, and sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_25",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_25@1",
            "content": "The psychometric data set (Abbasi et al., 2021) consists of free-text responses on four psychometric dimensions: subjective health literacy, numeracy, anxiety, and trust in doctors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_25",
            "start": 171,
            "end": 351,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_25@2",
            "content": "These free-text responses were then linked to survey-based psychometric scores also provided by the participants (serving as gold-standard numeric response labels).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_25",
            "start": 353,
            "end": 516,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_25@3",
            "content": "The data also includes selfreported demographics for each individual: age, race, gender, income, and education level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_25",
            "start": 518,
            "end": 634,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_25@4",
            "content": "This data set was collected using crowd workers from Amazon Mechanical Turk and Qualtrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_25",
            "start": 636,
            "end": 725,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_26@0",
            "content": "Similarly, the Five Item Personality Inventory (FIPI) and Myers-Briggs Type Indicator (MBTI) datasets include free text responses to estimate one of the FIPI or MBTI personality traits (Gjurkovi\u0107 et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_26",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_26@1",
            "content": "In particular, due to space constraints, we focus on the MBTI traits of perceiving and thinking, and the FIPI traits of extraverted and stable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_26",
            "start": 211,
            "end": 353,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_26@2",
            "content": "For FIPI, available demographics are gender, race, age, income, and education.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_26",
            "start": 355,
            "end": 432,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_26@3",
            "content": "For MBTI, selfreported gender and age are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_26",
            "start": 434,
            "end": 485,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_26@4",
            "content": "The AskAPatient dataset (Limsopatham and Collier, 2016) is taken from web forums and has labeled sentiment, along with gender and age information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_26",
            "start": 487,
            "end": 632,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_27@0",
            "content": "The Multilingual Twitter Corpus (MTC) hatespeech dataset contains labeled Twitter messages for the task of hate speech detection (Huang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_27",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_27@1",
            "content": "The dataset also contains inferred author demographic factors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_27",
            "start": 151,
            "end": 212,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_27@2",
            "content": "We use three demographics: gender, race, and age.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_27",
            "start": 214,
            "end": 262,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_28@0",
            "content": "The Psychometrics, FIPI, AskAPatient, and MBTI tasks are all relevant from an allocational harms perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_28",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_28@1",
            "content": "Biases in predictions for healthcare-related variables (Psychometrics), or personality type variables (MBTI, FIPI) can affect an individual's health care plan, personalized interventions, job prospects, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_28",
            "start": 110,
            "end": 316,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_28@2",
            "content": "Biased predictions for drug rating sentiment can affect which drugs a future user chooses to take.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_28",
            "start": 318,
            "end": 415,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_29@0",
            "content": "Models and Debiasing Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_29",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@0",
            "content": "In the experiments, we considered several different text classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@1",
            "content": "We used a word convolutional neural network (CNN) model, initialized with GloVe embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 80,
            "end": 170,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@2",
            "content": "We also considered two transformer-based contexualized embedding models: BERT and RoBERTa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 172,
            "end": 261,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@3",
            "content": "For each dataset we trained using five-fold cross validation, so that for each example in each dataset, we could generate predictions as unseen test data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 263,
            "end": 416,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@4",
            "content": "1 Each test fold was then concatenated for a given model for fairness calculations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 418,
            "end": 500,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_30@5",
            "content": "All models were trained on the same data with hyperparameter tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_30",
            "start": 502,
            "end": 569,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_31@0",
            "content": "Debiasing Strategy Static word embeddings (GloVe, Pennington et al., 2014) were debiased using WordED (Ravfogel et al., 2020) 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_31",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_31@1",
            "content": "This method iteratively learns a projection of embeddings that removes the bias information with minimal impact on embedding distances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_31",
            "start": 130,
            "end": 264,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_32@0",
            "content": "Contextualized word embedding models BERT and RoBERTa were debiased using ContextED (Kaneko and Bollegala, 2021) 3 , which has been shown to work well at removing gender-bias encoded in embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_32",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_32@1",
            "content": "This method uses pre-defined word lists to identify sentences that contain the gendered or stereotype words, and then fine-tunes the pretrained model parameters by encouraging gendered and stereotype words to have orthogonal representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_32",
            "start": 198,
            "end": 438,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_33@0",
            "content": "We also assessed two alternative debiasing methods for the contextualized word embedding models: counterfactual data augmentation (CDA) (Zmigrod et al., 2019) and Dropout (Webster et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_33",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_33@1",
            "content": "4 CDA augments the training corpora with counterfactual data so that the language model is pretrained on gender-balanced text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_33",
            "start": 195,
            "end": 320,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_33@2",
            "content": "Dropout mitigates gender biases by increasing the dropout rate in the pretrained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_33",
            "start": 322,
            "end": 409,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_33@3",
            "content": "Therefore, the debiasing methods in our experiments represent different ways of mitigating biases: dataset level (CDA), debiasing during pretraining (ContextED and Dropout), and post-tuning debiasing (WordED).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_33",
            "start": 411,
            "end": 619,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_34@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_34",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_35@0",
            "content": "There are several definitions of fairness in the literature (Mehrabi et al., 2021), each with corresponding methods of assessment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_35",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_35@1",
            "content": "In this work we rely on two prior metrics from the literature, and also present a new metric, adjusted disparate impact, to account for base rates in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_35",
            "start": 131,
            "end": 292,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_36@0",
            "content": "Disparate Impact One of the most common fairness assessments is disparate impact (DI, Friedler et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_36",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_36@1",
            "content": "DI measures the inequality of positive cases between privileged and non-privileged groups for a particular demographic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_36",
            "start": 110,
            "end": 228,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_36@2",
            "content": "DI comes from from the legal field, where certain regulations require DI be above a threshold of 0.8 (or below 1.2 in the inverse case).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_36",
            "start": 230,
            "end": 365,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_36@3",
            "content": "For true labels y, predicted labels \u0177, and relevant demographic group A:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_36",
            "start": 367,
            "end": 438,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_37@0",
            "content": "DI = p(\u0177 = 1|A = 0) p(\u0177 = 1|A = 1)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_37",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_38@0",
            "content": "Where A = 0 refers to the protected group and A = 1 refers to the privileged group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_38",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_38@1",
            "content": "A DI ratio of 1 indicates demographic parity, where the rates of positive predictions are consistent across demographic classes: P (\u0177 = 1|A = 0) = P (\u0177 = 1|A = 1) (Mehrabi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_38",
            "start": 84,
            "end": 269,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_39@0",
            "content": "Statistical Parity (SP) Subgroup Fairness Recent theoretical work on intersectional biases also assesses demographic parity, where the score compares group-specific rates to the global rate in the dataset instead of a comparison between privileged and protected classes (Kearns et al., 2018):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_39",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_40@0",
            "content": "p(A = g) \u00d7 |p(\u0177 = 1) \u2212 p(\u0177 = 1|A = g)| (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_40",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_41@0",
            "content": "This value is compared to an acceptability parameter \u03bb to assess fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_41",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_41@1",
            "content": "As this method was proposed for the intersectional case, it gives a way to identify the upper-bound of the fairness violation in a dataset (Yang et al., 2020):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_41",
            "start": 75,
            "end": 233,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_42@0",
            "content": "F V = max g\u2208G f |T P R g \u2212 T P R D | (3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_42",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_43@0",
            "content": "Where G f is the set of demographic groups under consideration for analysis, T P R g is the true positive rate of the classifier on the instances in g, and T P R D is the overall true positive rate for the classifier on the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_43",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_43@1",
            "content": "Prior work considered the average violation across groups (Subramanian et al., 2021), but for the purposes of this study we are interested in a worst case analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_43",
            "start": 233,
            "end": 396,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_44@0",
            "content": "Adjusted Disparate Impact We propose reweighting DI to account for differences in base rates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_44",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_44@1",
            "content": "Adjusted DI (ADI) divides DI by the base rate ratio for the protected and privileged groups:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_44",
            "start": 94,
            "end": 185,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_45@0",
            "content": "DI * = p(y=1|A=0) p(y=1|A=1) , ADI = DI DI *",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_45",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_46@0",
            "content": "Note that the disparate impact metrics are not defined for cases where there are no positive instances for either the protected or privilege classes in the data, or when there are no positive predictions for the privileged class (due to zero division).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_46",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_46@1",
            "content": "Therefore, we use additive smoothing when calculating DI and adjusted DI (Zhai and Lafferty, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_46",
            "start": 253,
            "end": 351,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@0",
            "content": "Intersectional Fairness To assess intersectional fairness we enumerated all combinations for each n-demographic scenario (e.g., 2-demographic, 3demographic, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@1",
            "content": "We set a reference demographic, specifically gender, because of the prior work on debiasing word embeddings for gender (Bolukbasi et al., 2016;Gonen and Goldberg, 2019;Kaneko and Bollegala, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 164,
            "end": 359,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@2",
            "content": "For intersectional cases, we calculated DI and FV for all possible combinations of demographics that included gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 361,
            "end": 477,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@3",
            "content": "For example, the 2-demographic case for the psychometrics dataset involves calculating DI and FV for the following protected groups: older women, lower education women, lower income women, and non-white women.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 479,
            "end": 687,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@4",
            "content": "Our privileged groups are the negations of the protected groups, e.g., for the above case they are younger men, higher education men, higher income men, and white men.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 689,
            "end": 855,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@5",
            "content": "By considering disjoint demographic groups, we avoid cumulative effects of merging fairness results from individual demographics during the intersectional phase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 857,
            "end": 1017,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@6",
            "content": "We follow the same procedure for enumerating protected groups for the 3-and 4-demographic cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1019,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@7",
            "content": "For 5-demographics we consider all demographics together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1116,
            "end": 1172,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@8",
            "content": "For all models and datasets, we calculated fairness and performance metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1174,
            "end": 1249,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@9",
            "content": "For performance, we report mean squared error (MSE), Pearson's r, 5 F1, and area under the receiver operating curve (AUC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1251,
            "end": 1372,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@10",
            "content": "For fairness, we report adjusted DI and fairness violation (FV, \u00a73.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1374,
            "end": 1443,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_47@11",
            "content": "6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_47",
            "start": 1445,
            "end": 1445,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_48@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_48",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@0",
            "content": "Figure 2 shows the ADI results for BERT and GloVe using ContextED and WordED for debiasing, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@1",
            "content": "In most cases, particularly for BERT, disparate impact scores for gender alone are 5 MSE and Pearson's r were calculated for datasets where continuous gold standard values were available 6 Standard disparate impact results were consistent with ADI and are not included due to space considerations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 106,
            "end": 402,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@2",
            "content": "in a reasonable range (within 10%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 404,
            "end": 438,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@3",
            "content": "For GloVe, we do observe high gender ADI on Anxiety and Thinking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 440,
            "end": 504,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@4",
            "content": "However, as the number of demographics under consideration grows, the range of ADI scores widens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 506,
            "end": 602,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@5",
            "content": "While debiasing the word embeddings typically helps to reduce the unfairness for the target demographic (e.g., gender), in the intersectional cases the model still performs poorly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 604,
            "end": 783,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_49@6",
            "content": "There are similar trends in FV scores as the number of demographics increases, with the extent of violations often increasing by a factor of 3x to 10x as intersections increase (Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_49",
            "start": 785,
            "end": 971,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@0",
            "content": "In some cases the intersectional disparities are extreme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@1",
            "content": "On the BERT models, the ratio of positive Numeracy predictions for the protected class is three-to-one compared to the privileged class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 58,
            "end": 193,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@2",
            "content": "In the other direction, for 3-demographics, hatespeech detection positive predictions are significantly less likely for the protected group than the privileged group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 195,
            "end": 360,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@3",
            "content": "This is consistent with prior hatespeech detection work that has shown large (absolute value) fairness gaps between protected and privileged groups (e.g., Liu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 362,
            "end": 534,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@4",
            "content": "In most cases, trends are consistent between the BERT and GloVe models (e.g., Extraverted, Numeracy, Perceiving).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 536,
            "end": 648,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@5",
            "content": "Some counterexamples are the Trust and Anxiety tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 650,
            "end": 702,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@6",
            "content": "Here model choice impacts the direction of bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 704,
            "end": 751,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@7",
            "content": "As more demographics are considered, the GloVe model skews more unfair against the protected group, while the BERT model remains mostly fair, skewing slightly unfair against the privileged group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 753,
            "end": 947,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@8",
            "content": "Higher trust in physicians is associated with better well-being and lower anxiety when visiting a doctor (Netemeyer et al., 2020); disparate predictions can lead to missed interventions for trust-increase and anxiety reduction across demographic groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 949,
            "end": 1201,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_50@9",
            "content": "Though not depicted in the main paper, plots for RoBERTa show similar trends to those observed for BERT while debiasing with ContextED (see Appendix B).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_50",
            "start": 1203,
            "end": 1354,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_51@0",
            "content": "Results are similar when looking at alternate BERT debiasing methods beyond ContextED, namely CDA and Dropout (Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_51",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_51@1",
            "content": "These findings on the Anxiety, Literacy, Numeracy, and Trust tasks suggest that debiasing at the dataset, embedding pretraining, and post-tuning levels leads to similar increases in unfairness as the number of demographic intersections considered increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_51",
            "start": 122,
            "end": 378,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_52@0",
            "content": "Collectively, the results underscore the allocational harm implications of NLP models on several downstream tasks -ones that even well-designed and well-intentioned debiasing strategies cannot overcome.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_52",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_52@1",
            "content": "This can be problematic in the era of personalized marketing and precision health, with NLP-based persona-generation playing a bigger role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_52",
            "start": 203,
            "end": 341,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_52@2",
            "content": "For tasks like numeracy and literacy, this can affect how a patient is treated by a medical staff during a hospital visit (i.e., a false positive high literacy prediction for a person who has trouble understanding his or her medical record).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_52",
            "start": 343,
            "end": 583,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_52@3",
            "content": "For the personality indicators, inconsistent predictions may lead to biased decisions in the workplace (e.g., a manager looking to form a team of extroverts).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_52",
            "start": 585,
            "end": 742,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_53@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_53",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_54@0",
            "content": "In this work we present a comprehensive benchmarking analysis of fairness for sequence prediction models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_54",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_54@1",
            "content": "We also look at known debiasing methods for these models and show that while the debiased versions maintain predictive performance (as expected), they do not help with mitigating biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_54",
            "start": 106,
            "end": 291,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_54@2",
            "content": "While most models are relatively fair when looking at a single demographic characteristic, accounting for intersectional groups leads to less fair models and wider ranges of bias because of the combinatorial considerations of the intersectional groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_54",
            "start": 293,
            "end": 544,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_54@3",
            "content": "It is our hope that this benchmarking encourages future work into mitigating intersectional biases, and also to collect more demographic information when creating new datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_54",
            "start": 546,
            "end": 721,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_55@0",
            "content": "Ahmed Abbasi, David Dobolyi, John Lalor, Richard Netemeyer, Kendall Smith, Yi Yang, Constructing a psychometric testbed for fair natural language processing, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_55",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_56@0",
            "content": "Solon Barocas, D Andrew,  Selbst, Big data's disparate impact, 2016, Calif. L. Rev, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_56",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_57@0",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_57",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_58@0",
            "content": "Su Blodgett, Solon Barocas, Hal Daum\u00e9, Iii , Hanna Wallach, Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_58",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_59@0",
            "content": "Tolga Bolukbasi, Kai-Wei Chang, Y James, Venkatesh Zou, Adam Saligrama,  Kalai, Man is to computer programmer as woman is to homemaker? debiasing word embeddings, 2016, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_59",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_60@0",
            "content": "Joy Buolamwini, Timnit Gebru, Gender shades: Intersectional accuracy disparities in commercial gender classification, 2018, Conference on fairness, accountability and transparency, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_60",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_61@0",
            "content": "Aylin Caliskan, Joanna Bryson, Arvind Narayanan, Semantics derived automatically from language corpora contain human-like biases, 2017, Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_61",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_62@0",
            "content": "Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Adam Tauman Kalai, Bias in bios: A case study of semantic representation bias in a high-stakes setting, 2019, proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_62",
            "start": 0,
            "end": 330,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_63@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_63",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_64@0",
            "content": "A Sorelle, Carlos Friedler, Suresh Scheidegger, Sonam Venkatasubramanian, Evan Choudhary, Derek Hamilton,  Roth, A comparative study of fairness-enhancing interventions in machine learning, 2019, Proceedings of the conference on fairness, accountability, and transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_64",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_65@0",
            "content": "Nikhil Garg, Londa Schiebinger, Dan Jurafsky, James Zou, Word embeddings quantify 100 years of gender and ethnic stereotypes, 2018, Proceedings of the National Academy of Sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_65",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_66@0",
            "content": "Aparna Garimella, Akhash Amarnath, Kiran Kumar, Akash Pramod Yalla, N Anandhavelu, Niyati Chhaya, Balaji Vasan Srinivasan, He is very intelligent, she is very beautiful? on mitigating social biases in language modelling and generation, 2021, Findings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_66",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_67@0",
            "content": "Matej Gjurkovi\u0107, Mladen Karan, Iva Vukojevi\u0107, Mihaela Bo\u0161njak, PANDORA talks: Personality and demographics on Reddit, 2021, Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_67",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_68@0",
            "content": "Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Sanchez, Mugdha Pandya, Adam Lopez, Intrinsic bias metrics do not correlate with application bias, 2021, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_68",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_69@0",
            "content": "Hila Gonen, Yoav Goldberg, Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_69",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_70@0",
            "content": "Xiaolei Huang, Linzi Xing, Franck Dernoncourt, Michael Paul, Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_70",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_71@0",
            "content": "Masahiro Kaneko, Danushka Bollegala, Gender-preserving debiasing for pre-trained word embeddings, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_71",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_72@0",
            "content": "Masahiro Kaneko, Danushka Bollegala, Debiasing pre-trained contextualised embeddings, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_72",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_73@0",
            "content": "Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu, Preventing fairness gerrymandering: Auditing and learning for subgroup fairness, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_73",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_74@0",
            "content": "Nut Limsopatham, Nigel Collier, Normalising medical concepts in social media texts by learning semantic representation, 2016, Proceedings of the 54th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_74",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_75@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_75",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_76@0",
            "content": "Haochen Liu, Wei Jin, Hamid Karimi, Zitao Liu, Jiliang Tang, The authors matter: Understanding and mitigating implicit bias in deep text classification, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_76",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_77@0",
            "content": "UNKNOWN, None, , A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_77",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_78@0",
            "content": "Chandler May, Alex Wang, Shikha Bordia, On measuring social biases in sentence encoders, 2019, Proceedings of NAACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_78",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_79@0",
            "content": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan, A survey on bias and fairness in machine learning, 2021, ACM Computing Surveys (CSUR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_79",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_80@0",
            "content": "G Richard,  Netemeyer, G David, Ahmed Dobolyi, Gari Abbasi, Herman Clifford,  Taylor, Health literacy, health numeracy, and trust in doctor: effects on key patient health outcomes, , Journal of Consumer Affairs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_80",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_81@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher D Manning, Glove: Global vectors for word representation, 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_81",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_82@0",
            "content": "Flavien Prost, Nithum Thain, Tolga Bolukbasi, Debiasing embeddings for reduced gender bias in text classification, 2019, Proceedings of the First Workshop on Gender Bias in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_82",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_83@0",
            "content": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg, Null it out: Guarding protected attributes by iterative nullspace projection, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_83",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_84@0",
            "content": "Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, Adam Kalai, What's in a name? reducing bias in bios without access to protected attributes, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_84",
            "start": 0,
            "end": 401,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_85@0",
            "content": "Andrew Deven Santosh Shah, Dirk Schwartz,  Hovy, Predictive biases in natural language processing models: A conceptual framework and overview, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_85",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_86@0",
            "content": "Shivashankar Subramanian, Xudong Han, Timothy Baldwin, Trevor Cohn, Lea Frermann, Evaluating debiasing techniques for intersectional biases, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_86",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2019, A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_87",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_88@0",
            "content": "Yi Chern Tan, L Celis, Assessing social and intersectional biases in contextualized word representations, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_88",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_89@0",
            "content": "UNKNOWN, None, , Slav Petrov. 2020. Measuring and reducing gendered correlations in pre-trained models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_89",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_90@0",
            "content": "Forest Yang, Mouhamadou Cisse, Oluwasanmi O Koyejo, Fairness with overlapping groups; a probabilistic perspective, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_90",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_91@0",
            "content": "Chengxiang Zhai, John Lafferty, A study of smoothing methods for language models applied to information retrieval, 2004, ACM Transactions on Information Systems (TOIS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_91",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_92@0",
            "content": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Awadallah, Gender bias in multilingual embeddings and crosslingual transfer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_92",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_93@0",
            "content": "Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, Kai-Wei Chang Chang, Learning gender-neutral word embeddings, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_93",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "160-ARR_v1_94@0",
            "content": "Ran Zmigrod, J Sabrina, Hanna Mielke, Ryan Wallach,  Cotterell, Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "160-ARR_v1_94",
            "start": 0,
            "end": 261,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_1",
            "tgt_ix": "160-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_1",
            "tgt_ix": "160-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_5",
            "tgt_ix": "160-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_6",
            "tgt_ix": "160-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_7",
            "tgt_ix": "160-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_9",
            "tgt_ix": "160-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_13",
            "tgt_ix": "160-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_13",
            "tgt_ix": "160-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_15",
            "tgt_ix": "160-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_14",
            "tgt_ix": "160-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_14",
            "tgt_ix": "160-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_14",
            "tgt_ix": "160-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_13",
            "tgt_ix": "160-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_17",
            "tgt_ix": "160-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_17",
            "tgt_ix": "160-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_13",
            "tgt_ix": "160-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_20",
            "tgt_ix": "160-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_19",
            "tgt_ix": "160-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_19",
            "tgt_ix": "160-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_19",
            "tgt_ix": "160-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_27",
            "tgt_ix": "160-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_28",
            "tgt_ix": "160-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_31",
            "tgt_ix": "160-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_32",
            "tgt_ix": "160-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_33",
            "tgt_ix": "160-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_35",
            "tgt_ix": "160-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_36",
            "tgt_ix": "160-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_37",
            "tgt_ix": "160-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_38",
            "tgt_ix": "160-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_39",
            "tgt_ix": "160-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_40",
            "tgt_ix": "160-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_41",
            "tgt_ix": "160-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_42",
            "tgt_ix": "160-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_43",
            "tgt_ix": "160-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_44",
            "tgt_ix": "160-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_45",
            "tgt_ix": "160-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_46",
            "tgt_ix": "160-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_51",
            "tgt_ix": "160-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_52",
            "tgt_ix": "160-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_53",
            "tgt_ix": "160-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_53",
            "tgt_ix": "160-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "160-ARR_v1_0",
            "tgt_ix": "160-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_1",
            "tgt_ix": "160-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_2",
            "tgt_ix": "160-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_3",
            "tgt_ix": "160-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_4",
            "tgt_ix": "160-ARR_v1_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_5",
            "tgt_ix": "160-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_5",
            "tgt_ix": "160-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_5",
            "tgt_ix": "160-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_5",
            "tgt_ix": "160-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_6",
            "tgt_ix": "160-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_7",
            "tgt_ix": "160-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_7",
            "tgt_ix": "160-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_8",
            "tgt_ix": "160-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_9",
            "tgt_ix": "160-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_9",
            "tgt_ix": "160-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_10",
            "tgt_ix": "160-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_11",
            "tgt_ix": "160-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_12",
            "tgt_ix": "160-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_13",
            "tgt_ix": "160-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_14",
            "tgt_ix": "160-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_15",
            "tgt_ix": "160-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_15",
            "tgt_ix": "160-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_15",
            "tgt_ix": "160-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_16",
            "tgt_ix": "160-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_17",
            "tgt_ix": "160-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_18",
            "tgt_ix": "160-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_19",
            "tgt_ix": "160-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_20",
            "tgt_ix": "160-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_20",
            "tgt_ix": "160-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_20",
            "tgt_ix": "160-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_21",
            "tgt_ix": "160-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_22",
            "tgt_ix": "160-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_23",
            "tgt_ix": "160-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_24",
            "tgt_ix": "160-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_25",
            "tgt_ix": "160-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_26",
            "tgt_ix": "160-ARR_v1_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_27",
            "tgt_ix": "160-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_27",
            "tgt_ix": "160-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_27",
            "tgt_ix": "160-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_28",
            "tgt_ix": "160-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_28",
            "tgt_ix": "160-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_28",
            "tgt_ix": "160-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_29",
            "tgt_ix": "160-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_30",
            "tgt_ix": "160-ARR_v1_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_31",
            "tgt_ix": "160-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_31",
            "tgt_ix": "160-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_32",
            "tgt_ix": "160-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_32",
            "tgt_ix": "160-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_33",
            "tgt_ix": "160-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_33",
            "tgt_ix": "160-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_33",
            "tgt_ix": "160-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_33",
            "tgt_ix": "160-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_34",
            "tgt_ix": "160-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_35",
            "tgt_ix": "160-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_35",
            "tgt_ix": "160-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_36",
            "tgt_ix": "160-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_36",
            "tgt_ix": "160-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_36",
            "tgt_ix": "160-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_36",
            "tgt_ix": "160-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_37",
            "tgt_ix": "160-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_38",
            "tgt_ix": "160-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_38",
            "tgt_ix": "160-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_39",
            "tgt_ix": "160-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_40",
            "tgt_ix": "160-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_41",
            "tgt_ix": "160-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_41",
            "tgt_ix": "160-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_42",
            "tgt_ix": "160-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_43",
            "tgt_ix": "160-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_43",
            "tgt_ix": "160-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_44",
            "tgt_ix": "160-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_44",
            "tgt_ix": "160-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_45",
            "tgt_ix": "160-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_46",
            "tgt_ix": "160-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_46",
            "tgt_ix": "160-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_47",
            "tgt_ix": "160-ARR_v1_47@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_48",
            "tgt_ix": "160-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_49",
            "tgt_ix": "160-ARR_v1_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_50",
            "tgt_ix": "160-ARR_v1_50@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_51",
            "tgt_ix": "160-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_51",
            "tgt_ix": "160-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_52",
            "tgt_ix": "160-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_52",
            "tgt_ix": "160-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_52",
            "tgt_ix": "160-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_52",
            "tgt_ix": "160-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_53",
            "tgt_ix": "160-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_54",
            "tgt_ix": "160-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_54",
            "tgt_ix": "160-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_54",
            "tgt_ix": "160-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_54",
            "tgt_ix": "160-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_55",
            "tgt_ix": "160-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_56",
            "tgt_ix": "160-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_57",
            "tgt_ix": "160-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_58",
            "tgt_ix": "160-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_59",
            "tgt_ix": "160-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_60",
            "tgt_ix": "160-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_61",
            "tgt_ix": "160-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_62",
            "tgt_ix": "160-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_63",
            "tgt_ix": "160-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_64",
            "tgt_ix": "160-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_65",
            "tgt_ix": "160-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_66",
            "tgt_ix": "160-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_67",
            "tgt_ix": "160-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_68",
            "tgt_ix": "160-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_69",
            "tgt_ix": "160-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_70",
            "tgt_ix": "160-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_71",
            "tgt_ix": "160-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_72",
            "tgt_ix": "160-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_73",
            "tgt_ix": "160-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_74",
            "tgt_ix": "160-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_75",
            "tgt_ix": "160-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_76",
            "tgt_ix": "160-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_77",
            "tgt_ix": "160-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_78",
            "tgt_ix": "160-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_79",
            "tgt_ix": "160-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_80",
            "tgt_ix": "160-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_81",
            "tgt_ix": "160-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_82",
            "tgt_ix": "160-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_83",
            "tgt_ix": "160-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_84",
            "tgt_ix": "160-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_85",
            "tgt_ix": "160-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_86",
            "tgt_ix": "160-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_87",
            "tgt_ix": "160-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_88",
            "tgt_ix": "160-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_89",
            "tgt_ix": "160-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_90",
            "tgt_ix": "160-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_91",
            "tgt_ix": "160-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_92",
            "tgt_ix": "160-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_93",
            "tgt_ix": "160-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "160-ARR_v1_94",
            "tgt_ix": "160-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1109,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "160-ARR",
        "version": 1
    }
}