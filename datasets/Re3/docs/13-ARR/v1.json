{
    "nodes": [
        {
            "ix": "13-ARR_v1_0",
            "content": "Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_2",
            "content": "Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment. To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets. However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments, which hinders their generalization of unseen scenes. To improve the ability of fast cross-domain adaptation, we propose Prompt-based Environmental Self-exploration (ProbES), which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model (CLIP). Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling. Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge. By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE. Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "13-ARR_v1_4",
            "content": "Teaching a robot to navigate following a natural language instruction has a broad impact in the field of human-robotic interaction. Many related tasks have been proposed to delve into this problem. The vision-language navigation (VLN) task is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_5",
            "content": "To solve a more practical problem, the REVERIE task focuses on target objects localization that asks an agent to identify an object in an unseen room.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_6",
            "content": "Solving these tasks requires an agent to obtain a vision-text alignment ability that locates related objects and executes corrective actions according to the instruction. However, collecting a large-scale VLN dataset is difficult and laborious since annotating the semantic of a trajectory within a sentence costs times of labor than annotating an image. Existing navigation datasets are relatively small-scale, and learning on such datasets hinders the agent to obtain a good generalization ability. To solve this problem, EnvDrop ) uses a speaker model to generate instructions for sampled trajectories in unseen environments, but the generalization ability is not strong with limited vision-language understanding ability. Recently, VLN-BERT (Majumdar et al., 2020) introduces a visio-linguistic model pretrained on Conceptual Captions (Sharma et al., 2018) dataset to learn from image-caption pairs, which are quite different from trajectoryinstruction pairs from VLN. To address this, Airbert (Guhur et al., 2021) constructs a large-scale in-domain pretraining dataset with image-caption pairs collected from online marketplaces such as Airbnb to finetune ViLBERT. However, Airbert collects image captioning data on websites, which are still far from the scenario of vision-language navigation. Different from previous methods that collect human-labeled data to train a navigation model, we suggest that automatically generating instruction-trajectory pairs by self-exploration for pretraining not only helps the model obtain better generalization ability but also achieves fast adaptation to downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_7",
            "content": "In this paper, we propose a method named prompt-based environmental self-exploration (ProbES) that generates navigation data with prior knowledge automatically and adapts pretrained model quickly to VLN tasks. An overview of our proposed framework is shown in Figure 1. By using this method, a pretrained visio-linguistic model is able to adapt to the VLN task automatically and efficiently. Specifically, we build an in-domain dataset by self-exploration without labeling or crawler. To build such a dataset. we first generate templates by masking visual and action words in labeled instructions. Then, we sample trajectories in the training environment. A pretrained CLIP (Radford et al., 2021) model is used to recognize rooms and objects in the sampled trajectories and match described phrases with them. We construct instructions by filling the matched phrases into sampled templates. By leveraging the prior knowledge learned by CLIP, we are able to build a dataset automatically with rich semantic information. Meanwhile, finetuning the whole pretrained model is time-consuming, we adopt prompt tuning (Li and Liang, 2021;Liu et al., 2021c,b), a lightweight alternative to finetuning. Our prompt-based method can distill task-relevant knowledge from pretrained model and achieve fast adaption to downstream tasks. We evaluate ProbES on R2R and REVERIE datasets by discriminative and generative settings. Results show that ProbES can match or surpass the performance of finetuning with substantially less training time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_8",
            "content": "To sum up, our main contributions are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_9",
            "content": "(1) We propose ProbES, a novel self-exploration method to automatically build an in-domain dataset that reduces the domain gap between the pretraining dataset and VLN tasks without human labeling; (2) Compared with finetuning large pretrained model, our proposed prompt tuning can achieve fast adaptation; (3) Experiments are conducted on R2R and REVERIE datasets with generative and discriminative settings, and results indicate that our proposed ProbES can achieve better or comparable performance. Besides, our generated data can be used as augmented data which improves the generalization ability of the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "13-ARR_v1_11",
            "content": "Vision-and-Language Navigation. proposed the first Vision-Language Navigation (VLN) benchmark combining real imagery (Chang et al., 2017) and natural language navigation instructions. To solve this task, proposed a novel SERL model to learn reward functions from the expert distribution. And combining imitation learning and reinforcement learning (Wang et al., 2019) has been proved to be beneficial for VLN. Since the VLN dataset is relatively small-scale, some works propose augmentation approaches (Fried et al., 2018;Liu et al., 2021a) to improve robustness. Auxiliary losses (Majumdar et al., 2020;Zhu et al., 2020) is used to take advantage of the additional training signals derived from the semantic information. Some pretraining methods Hao et al., 2020) have been proposed to learn generic cross-modal representations. This is further extended to a recurrent model that significantly improves sequential action prediction (Hong et al., 2021). However, the limited number of environments in pretraining constrain the generalization ability to unseen scenarios. Most related to this work, VLN-BERT (Majumdar et al., 2020) transfers knowledge from abundant, but out-of-domain image-text data to improve path-instruction matching. In contrast, we not only propose an effective method to build an in-domain dataset by sampling trajectory and generating instructions with templates, but also present a promptbased pretraining strategy to improve VLN.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_12",
            "content": "Vision-and-Language Pretraining. Vision-andlanguage pretraining has made great progress in recent years. Inspired by BERT (Devlin et al., 2018), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations. Previous research introduces one-stream BERT models and two-stream BERT models. The former directly perform inter-modal grounding (Li et al., 2019;Su et al., 2019;Alberti et al., 2019;Li et al., 2020a;Zhou et al., 2020;Li et al., 2020b), while two-stream models process both visual and textual inputs in separate streams, and then fuse the two modalities in a later stage . These models are often pretrained with self-supervised objectives akin to those in BERT: masked language modeling, masked object classification, and sentence-image alignment. In this work, the architecture of the ProbES model is structural similar to ViLBERT . We make several VLN-specific adaptations to ViLBERT so that pretrained weights can be transferred to initialize large portions of the model. Different from VLN-BERT which finetunes a ViLBERT on instruction-trajectory pairs to measure their compatibility in beam search setting, we introduce prompt tuning, which only tunes the continuous prompts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_13",
            "content": "Prompting. Natural language prompting freezes pretrained models and reformats the natural language input with example prompts. GPT-3 (Brown et al., 2020) introduces in-context learning, using manually designed and discrete text prompts. Sun et al. (Sun and Lai, 2020) also leverage prompts as keywords to control the sentiment or topic of the generated sentence. AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from a masked LM. Different from the discrete text prompt, some methods examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Prefix-Tuning (Li and Liang, 2021) prepends a sequence of continuous task-specific vectors as virtual tokens to the input. (Zhong et al., 2021;Qin and Eisner, 2021;Hambardzumyan et al., 2021) introduce continuous templates following manual prompt templates. Ptuning (Liu et al., 2021c) uses continuous prompts which are learned by inserting trainable variables into the embedded input. Ptr (Han et al., 2021) adopts manually crafted sub-templates and generates complete templates by logic rules. In ProbES, we prepend continuous task-specific vectors to the embedding of the input instruction and directly tune the embeddings of these vectors. After prompt tuning, the model can be adapted to VLN and REVERIE tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_14",
            "content": "Prompt-based Environmental",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "13-ARR_v1_15",
            "content": "Self-Exploration (ProbES)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_16",
            "content": "Vision-Language Navigation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "13-ARR_v1_17",
            "content": "The Vision-and-Language Navigation (VLN) task gives a global natural sentence I = {w 0 , ..., w l } as an instruction, where w i is a word token while the l is the length of the sentence. The instruction consists of step-by-step guidance toward the goal. At step t, the agent observes a panoramic view O t = {o t,i } 36 i=1 as the vision input, which is composed of 36 RGB image views. Each of these views consists of image feature v i and an orientation description (sin \u03b8 t,i , cos \u03b8 t,i , sin \u03d5 t,i , cos \u03d5 t,i ). Candidates in the panoramic action space consist of k neighbours of the current node in the navigation graph and a stop action.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_18",
            "content": "Instruction Generation with Templates",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "13-ARR_v1_19",
            "content": "We first generate templates from instructions in the R2R dataset. Then we sample trajectories in the training environment. We generate the candidate noun phrases and actionable verbs for the sampled trajectories and full-fill the templates by the above words. A detailed demonstration of our instruction generation module is shown in Fig. 2. Generating Templates We collect phrases and replace these phrases in human-annotated navigation instruction with blank masks to generate templates. Different from the Airbert (Guhur et al., 2021) that only extracts noun phrases, we also mask action words like 'left', 'right', 'forward', and 'around'. We denote the O mask as the mask for an object 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_20",
            "content": "We first sample the trajectories in the Matterport (Chang et al., 2017) Environment. We randomly sample the starting and ending positions, and collect tracks with lengths of less than 8 hops. Then we obtain the corresponding actions of each trajectory by firstperson movement. If the agent chooses the front navigable position to move, we generate a 'forward' action. If the agent chooses the back navigable position to move, we generate an 'around' action. Otherwise, if the agent selects the right front navigable position to move for the next step, we generate an action sequence like {'right', 'forward'}, which is used to fill actionable verbs during instruction generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_21",
            "content": "Full-filling Template with Prior Knowledge Prior knowledge is the key to generating high-quality data without human labeling. ProbES introduces CLIP, a powerful vision-language alignment model learned from a large-scale image-caption dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_22",
            "content": "To generate structured augmentation data, we fullfill the templates with phrases that describe the sampled trajectory and actions. A trajectory is denoted as {v 1 , v 2 , ..., v n }, where v i represents an observation viewpoint. We introduce CLIP (Radford et al., 2021) We randomly select a template with the same or a close number of O mask as the number of viewpoints in the sampled trajectory. The template has a sequence of object masks {O mask,1 , O mask,2 , ..., O mask,i } and a sequence of action masks {A mask,1 , A mask,2 , ..., A mask,j }. Lengths of object masks and action masks are denoted as l and n respectively. The number of object masks and action masks is roughly balanced. Let n v be the number of viewpoints in a sampled trajectory. Then the generated captions of this trajectory is written as {c 1 , c 2 , ..., c nv }. We full-fill the templates by the following rules: 1) if n v \u2265 l, we randomly sample l captions and fill the O mask in the template sequentially; 2) if n v < l, we randomly sample the O mask and use all the caption phrases to fill them. After filling phrases, we can identify which viewpoint A mask,i may appear since viewpoints of O mask,j near it are already known. For example, if the template is like 'O mask,1 A mask,1 O mask,2 ' and captions of v 1 and v 2 are used to fill O mask,1 and O mask,2 respectively, then A mask,1 is the sampled action between v 1 and v 2 . In this way, we use generated actionable verbs to full-fill the templates and get final instructions. By the above method, we can generate diverse instructions without human labeling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_23",
            "content": "Prompt-based Architecture",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "13-ARR_v1_24",
            "content": "Prompt tuning has been found effective on many natural language understanding (NLU) tasks. Motivated by this, we introduce a prompt-based architecture to achieve fast adaptation on the selfexploration dataset (e.g., Conceptual Captions) and downstream tasks. The architecture is ViLBERTlike and equipped with a prompt encoder for prompt tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_25",
            "content": "Given an instruction-trajectory pair, the visual and textual features can be extracted by the visual encoder E v and textual encoder E x in ViL-BERT respectively. Especially, the textual input has two parts: prompt sequence {p 1 , ..., p n } and word sequence {x 1 , ..., x m }, where p and x indicate a pseudo prompt token and a word token of a generated instruction respectively. n and m represent lengths of the prompt sequence and word sequence respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_26",
            "content": "We embed prompt sequence by the prompt encoder E p and embed word sequence by the textual encoder E x as follows: e p,1 , ..., e p,n = E p (p 1 , ..., p n ) e x,1 , ..., e x,m = E x (x 1 ), ..., E x (x m ), where E p is composed of a LSTM head followed by a MLP head. Then the textual embedding is mapped to e t = {e p,1 , ..., e p,n , e x,1 , ..., e x,m }, where e p,1 , ..., e p,n are trainable embedding tensors and enable us to find better continous prompts. Let e v be denoted as visual embedding produced by visual encoder E v . e t and e v are then passed to the co-attention transformer similar to ViLBERT. Then in the prompt tuning process, we only train E p and fix the parameters of E x . Similar to VLN-Bert (Devlin et al., 2018), we sample 3 hard negative paths using beam search for an instruction-trajectory pair, and the model is trained to choose the best path among them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_27",
            "content": "Downstream Tasks Adaptation",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "13-ARR_v1_28",
            "content": "Our model can adapt to diverse downstream navigation tasks, including VLN, a step-by-step navigation task, and REVERIE, an object-oriented navigation task. In the step-by-step navigation task, our model receives an instruction sentence and navigates following the commands in the instruction sequentially. In the object navigation task, our model receives an object description and explores the house to find an object. Also, our model can be adapted to both discriminative and generative navigation settings. In the discriminative setting, our model receives both an instruction and the observation sequence to represent a navigation trajectory and then output a score. In the generative setting, our model receives instruction and predicts actions sequentially.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_29",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "13-ARR_v1_30",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "13-ARR_v1_31",
            "content": "We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R ), and objectoriented navigation task (REVERIE ). ProbES can be easily applied to discriminative and generative models for these two tasks. Evaluation Metrics A large number of metrics are used to evaluate models in VLN, such as Trajectory Length (TL), the trajectory length in meters, Navigation Error (NE), the navigation error in meters, Oracle Success Rate (OR), the rate if . VLN task regard SR and SPL as the primary metric, and the REVERIE task regard RGS and RGSPL as the primary metric. Implementation Details Our training process is divided into two steps: Firstly, we pretrain our model on our generated self-exploration training set with prompt tuning for only 10 epochs. After that, we adapt our model to the downstream discriminative VLN task with only ranking loss for 20 epochs. The batch size is set as 64 and the learning rate is 4 \u00d7 10 \u22125 . The generative navigation settings are the same as Recurrent VLN-BERT on both R2R and REVERIE. During pretraining, we use ProbES to 50k instruction-trajectory pairs. We use 32 NVIDIA V100 GPUs for pretraining and 8 GPUs for adaptation. Experiments with generative settings are conducted on a V100 GPU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_32",
            "content": "Comparison to state-of-the-art Methods",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "13-ARR_v1_33",
            "content": "In this section, we compare our model with previous state-of-the-art methods. We compare the ProbES with two baselines (ViLBERT and VLN-BERT built on Recurrent VLN-Bert) and five other methods. A brief description of previous models is as followed: 1) Seq2Seq: A sequence to sequence model reported in ; 2) Speaker-Follower (Fried et al., 2018): a method introduces a data augmentation approach and panoramic action space; 3) PRESS (Li et al., 2019): a conventional fine-tuning method with stochastic instruction sampling; 4) EnvDrop : a method augment data with environmental dropout; 5) Recurrent VLN-Bert (Hong et al., 2021) on three different settings: OSCAR and ViLBERT pretrained on out-of-domain data, VLN-BERT pretrained on R2R. We compare the models on three splits in the R2R dataset: validation seen house, validation unseen house, and testing (where the houses are also unseen). We also compare ProbES with Seq2Seq, RCM (Wang et al., 2019), SMNA (Ma et al., 2019), FAST-MATTN , Recurrent VLN-Bert (Hong et al., 2021) on OSCAR on REVERIE dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_34",
            "content": "We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, as shown in Table 2. In the validation seen split, compared to VLN-BERT under the same setting, our ProbES achieves 5% improvement on SR and 5% improvement on SPL. In the validation unseen split, we achieve 1% improvement on SR compared to VLN-BERT. In the testing split, ProbES shows competitive results. Note that the PREVALENT backbone is pretrained on an in-domain R2R dataset with scene features and fine-tuned with an additional action prediction task in a generative setting while ProbES does not use labeled R2R data or augmented data generated by speaker (Fried et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_35",
            "content": "We compare ProbES with VLN-BERT in the discriminative setting as in Table 4. In the validation unseen split, our method outperforms VLN-BERT, which indicates ProbES is able to improve the generalization ability for unseen scenes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_36",
            "content": "Results on REVERIE We compare ProbES with previous state-of-the-art methods on the REVERIE dataset, as shown in Table 3. In the validation unseen split, we achieve 0.42% improvement on RGS and 0.65% improvement on RGSPL. In the testing split, ProbES achieves 0.87% improvement on RGS and 0.69% improvement on RGSPL. We can see that ProbES benefits from prompt tuning with our generated instruction-trajectory pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_37",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "13-ARR_v1_38",
            "content": "Ablation of Learning Strategies. In Table 5, we ablate the performance gains from different learning strategies. PT and FT represent prompt tuning and fine-tuning respectively. Mask and Rank stand for masked multi-modal modeling loss and the ranking loss for path-selection task. We regard the model finetuned by ranking loss as our baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_39",
            "content": "The masked multi-modal modeling loss on our data and R2R data are able to improve the performance. And finetuning on our data is able to improve generalization ability since the success rate in the validation unseen split gets 1.1% improvement and achieves 59.0%. At last, we discover that pre- As shown in Table 6, randomly selecting template without considering the number of masked tokens degrades the performance and introduces more noise in the data. Results show that equipped with our generated data (Row 3) improves the performance by a large margin. The model of using the rooms and objects from Places365 (Zhou et al., 2017) and Objects365 (Shao et al., 2019) (Row 4) performs worse than which uses the rooms and objects from Matterport. We infer from that Places365 and Objects365 contain many outdoor scenes and objects which are not suitable for VLN.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_40",
            "content": "Qualititiva Analysis",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "13-ARR_v1_41",
            "content": "Visualization of Data Distribution Figure 3 presents a statistical analysis of our generated instructions. We can see from the left figure that the number of object masks are larger than that of action masks, indicating that instructions contain more rich information generated by CLIP from sampled observations. The right figure shows the distribution of the instruction lengths. The lengths of most of the instructions range from 10 to 30, which matches the R2R dataset. The easy samples and hard samples in our generated instructions are balanced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_42",
            "content": "Here we provide visualization of the data generated by ProbES. Figure 4 shows the instructiontrajectory samples generated with our strategy. For each sample, we visualize observations of the trajectory, captions generated with CLIP, the selected template, and the final instruction generated by ProbES. Generated object classes fit observed scenes well, thus we can infer that CLIP is able to extract key information from the observation. Also, our method can select a suitable template and generate diverse instructions that describe observations of trajectories correctly. The length of our generated instruction ranges from 1 to 3 sentences, which matches the data distribution of the R2R dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_43",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "13-ARR_v1_44",
            "content": "In this work, we first introduce an effective way to generate in-domain data for pretraining the VLN model: leveraging a large pretrained CLIP model to generate captions for each viewpoint and sampling actions in the environment. Experiments show that the domain gap between pretraining data and VLN tasks can be mitigated. We also propose a promptbased architecture, which introduces prompt tuning to adapt the pretrained model fastly. Our proposed ProbES achieves better results compared to baseline on both R2R and REVERIE datasets, and ablations show the contribution of each module and the effectiveness of the generated data. Walk past family room with mirror on your left, walk to dining room with mirror, wait at dining room.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "13-ARR_v1_45",
            "content": "Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter, Fusion of detected objects in text for visual question answering, 2019, EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Chris Alberti",
                    "Jeffrey Ling",
                    "Michael Collins",
                    "David Reitter"
                ],
                "title": "Fusion of detected objects in text for visual question answering",
                "pub_date": "2019",
                "pub_title": "EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_46",
            "content": "Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir, 2018, On evaluation of embodied navigation agents, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Peter Anderson",
                    "Angel Chang",
                    "Devendra Singh Chaplot",
                    "Alexey Dosovitskiy",
                    "Saurabh Gupta",
                    "Vladlen Koltun",
                    "Jana Kosecka",
                    "Jitendra Malik"
                ],
                "title": "Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir",
                "pub_date": "2018",
                "pub_title": "On evaluation of embodied navigation agents",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_47",
            "content": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, Anton Van Den,  Hengel, Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments, 2018, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Peter Anderson",
                    "Qi Wu",
                    "Damien Teney",
                    "Jake Bruce",
                    "Mark Johnson",
                    "Niko S\u00fcnderhauf",
                    "Ian Reid",
                    "Stephen Gould",
                    "Anton Van Den",
                    " Hengel"
                ],
                "title": "Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments",
                "pub_date": "2018",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_48",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Language models are few-shot learners, 2020, Ilya Sutskever, and Dario Amodei, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Tom Brown",
                    "Benjamin Mann",
                    "Nick Ryder",
                    "Melanie Subbiah",
                    "Jared Kaplan",
                    "Prafulla Dhariwal",
                    "Arvind Neelakantan",
                    "Pranav Shyam",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Sandhini Agarwal",
                    "Ariel Herbert-Voss",
                    "Gretchen Krueger",
                    "Tom Henighan",
                    "Rewon Child",
                    "Aditya Ramesh",
                    "Daniel Ziegler",
                    "Jeffrey Wu",
                    "Clemens Winter",
                    "Christopher Hesse",
                    "Mark Chen",
                    "Eric Sigler",
                    "Mateusz Litwin"
                ],
                "title": "Language models are few-shot learners",
                "pub_date": "2020",
                "pub_title": "Ilya Sutskever, and Dario Amodei",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_49",
            "content": "UNKNOWN, None, 2017, Matter-port3d: Learning from rgb-d data in indoor environments, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Matter-port3d: Learning from rgb-d data in indoor environments",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_50",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "Uniter: Universal image-text representation learning",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_51",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_52",
            "content": "Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Speaker-follower models for vision-and-language navigation, 2018, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Daniel Fried",
                    "Ronghang Hu",
                    "Volkan Cirik",
                    "Anna Rohrbach",
                    "Jacob Andreas",
                    "Louis-Philippe Morency",
                    "Taylor Berg-Kirkpatrick",
                    "Kate Saenko",
                    "Dan Klein",
                    "Trevor Darrell"
                ],
                "title": "Speaker-follower models for vision-and-language navigation",
                "pub_date": "2018",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_53",
            "content": "Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid, Airbert: In-domain pretraining for vision-and-language navigation, 2021, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Pierre-Louis Guhur",
                    "Makarand Tapaswi",
                    "Shizhe Chen",
                    "Ivan Laptev",
                    "Cordelia Schmid"
                ],
                "title": "Airbert: In-domain pretraining for vision-and-language navigation",
                "pub_date": "2021",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_54",
            "content": "UNKNOWN, None, 2021-05, Warp: Word-level adversarial reprogramming, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2021-05",
                "pub_title": "Warp: Word-level adversarial reprogramming",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_55",
            "content": "UNKNOWN, None, 2021, Ptr: Prompt tuning with rules for text classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Ptr: Prompt tuning with rules for text classification",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_56",
            "content": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, Towards learning a generic agent for vision-and-language navigation via pretraining, 2020, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Weituo Hao",
                    "Chunyuan Li",
                    "Xiujun Li",
                    "Lawrence Carin",
                    "Jianfeng Gao"
                ],
                "title": "Towards learning a generic agent for vision-and-language navigation via pretraining",
                "pub_date": "2020",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_57",
            "content": "Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Vln bert: A recurrent vision-and-language bert for navigation, 2021, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yicong Hong",
                    "Qi Wu",
                    "Yuankai Qi",
                    "Cristian Rodriguez-Opazo",
                    "Stephen Gould"
                ],
                "title": "Vln bert: A recurrent vision-and-language bert for navigation",
                "pub_date": "2021",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_58",
            "content": "Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie, Transferable representation learning in vision-and-language navigation, 2019, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Haoshuo Huang",
                    "Vihan Jain",
                    "Harsh Mehta",
                    "Alexander Ku",
                    "Gabriel Magalhaes",
                    "Jason Baldridge",
                    "Eugene Ie"
                ],
                "title": "Transferable representation learning in vision-and-language navigation",
                "pub_date": "2019",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_59",
            "content": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining, 2020, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Gen Li",
                    "Nan Duan",
                    "Yuejian Fang",
                    "Ming Gong",
                    "Daxin Jiang"
                ],
                "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
                "pub_date": "2020",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_60",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visualbert: A simple and performant baseline for vision and language",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_61",
            "content": "UNKNOWN, None, 2021, Prefix-tuning: Optimizing continuous prompts for generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Prefix-tuning: Optimizing continuous prompts for generation",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_62",
            "content": "Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, Yejin Choi, Robust navigation with language pretraining and stochastic sampling, 2019, EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Xiujun Li",
                    "Chunyuan Li",
                    "Qiaolin Xia",
                    "Yonatan Bisk",
                    "Asli Celikyilmaz",
                    "Jianfeng Gao",
                    "Noah Smith",
                    "Yejin Choi"
                ],
                "title": "Robust navigation with language pretraining and stochastic sampling",
                "pub_date": "2019",
                "pub_title": "EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_63",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Oscar: Objectsemantics aligned pre-training for vision-language tasks, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Xiujun Li",
                    "Xi Yin",
                    "Chunyuan Li",
                    "Pengchuan Zhang",
                    "Xiaowei Hu",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Houdong Hu",
                    "Li Dong",
                    "Furu Wei"
                ],
                "title": "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_64",
            "content": "Chong Liu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang, Zongyuan Ge, Yi-Dong Shen, Vision-language navigation with random environmental mixup, 2021, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Chong Liu",
                    "Fengda Zhu",
                    "Xiaojun Chang",
                    "Xiaodan Liang",
                    "Zongyuan Ge",
                    "Yi-Dong Shen"
                ],
                "title": "Vision-language navigation with random environmental mixup",
                "pub_date": "2021",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_65",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021b. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Zhilin Yang, and Jie Tang. 2021b. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_66",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021c. Gpt understands, too, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Zhilin Yang, and Jie Tang. 2021c. Gpt understands, too",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_67",
            "content": "UNKNOWN, None, 2019, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_68",
            "content": "UNKNOWN, None, 2019, Self-monitoring navigation agent via auxiliary progress estimation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Self-monitoring navigation agent via auxiliary progress estimation",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_69",
            "content": "Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, Improving vision-and-language navigation with imagetext pairs from the web, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Arjun Majumdar",
                    "Ayush Shrivastava",
                    "Stefan Lee",
                    "Peter Anderson",
                    "Devi Parikh",
                    "Dhruv Batra"
                ],
                "title": "Improving vision-and-language navigation with imagetext pairs from the web",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_70",
            "content": "Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Wang, Chunhua Shen, Anton Van Den,  Hengel, Reverie: Remote embodied visual referring expression in real indoor environments, 2020, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yuankai Qi",
                    "Qi Wu",
                    "Peter Anderson",
                    "Xin Wang",
                    "William Wang",
                    "Chunhua Shen",
                    "Anton Van Den",
                    " Hengel"
                ],
                "title": "Reverie: Remote embodied visual referring expression in real indoor environments",
                "pub_date": "2020",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_71",
            "content": "UNKNOWN, None, 2021, Learning how to ask: Querying lms with mixtures of soft prompts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Learning how to ask: Querying lms with mixtures of soft prompts",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_72",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision, , ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal"
                ],
                "title": "Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision",
                "pub_date": null,
                "pub_title": "ICML",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_73",
            "content": "Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, Objects365: A large-scale, high-quality dataset for object detection, 2019, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Shuai Shao",
                    "Zeming Li",
                    "Tianyuan Zhang",
                    "Chao Peng",
                    "Gang Yu",
                    "Xiangyu Zhang",
                    "Jing Li",
                    "Jian Sun"
                ],
                "title": "Objects365: A large-scale, high-quality dataset for object detection",
                "pub_date": "2019",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_74",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Piyush Sharma",
                    "Nan Ding",
                    "Sebastian Goodman",
                    "Radu Soricut"
                ],
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "pub_date": "2018",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_75",
            "content": "UNKNOWN, None, 2020, Autoprompt: Eliciting knowledge from language models with automatically generated prompts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_76",
            "content": "UNKNOWN, None, 2019, Vl-bert: Pre-training of generic visual-linguistic representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Vl-bert: Pre-training of generic visual-linguistic representations",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_77",
            "content": "UNKNOWN, None, 2020, Conditioned natural language generation using only unconditioned language model: An exploration, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Conditioned natural language generation using only unconditioned language model: An exploration",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_78",
            "content": "UNKNOWN, None, 2019, Lxmert: Learning cross-modality encoder representations from transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Lxmert: Learning cross-modality encoder representations from transformers",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_79",
            "content": "UNKNOWN, None, 2019, Learning to navigate unseen environments: Back translation with environmental dropout, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Learning to navigate unseen environments: Back translation with environmental dropout",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_80",
            "content": "Hu Wang, Qi Wu, Chunhua Shen, Soft expert reward learning for vision-and-language navigation, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Hu Wang",
                    "Qi Wu",
                    "Chunhua Shen"
                ],
                "title": "Soft expert reward learning for vision-and-language navigation",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_81",
            "content": "Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Wang, Lei Zhang, Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation, 2019, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Xin Wang",
                    "Qiuyuan Huang",
                    "Asli Celikyilmaz",
                    "Jianfeng Gao",
                    "Dinghan Shen",
                    "Yuan-Fang Wang",
                    "William Wang",
                    "Lei Zhang"
                ],
                "title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
                "pub_date": "2019",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_82",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is, 2021, Learning vs. learning to recall, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Zexuan Zhong",
                    "Dan Friedman",
                    "Danqi Chen"
                ],
                "title": "Factual probing is",
                "pub_date": "2021",
                "pub_title": "Learning vs. learning to recall",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_83",
            "content": "UNKNOWN, None, 2017, Places: A 10 million image database for scene recognition, PAMI.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Places: A 10 million image database for scene recognition",
                "pub": "PAMI"
            }
        },
        {
            "ix": "13-ARR_v1_84",
            "content": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao, Unified visionlanguage pre-training for image captioning and vqa, 2020, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Luowei Zhou",
                    "Hamid Palangi",
                    "Lei Zhang",
                    "Houdong Hu",
                    "Jason Corso",
                    "Jianfeng Gao"
                ],
                "title": "Unified visionlanguage pre-training for image captioning and vqa",
                "pub_date": "2020",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "13-ARR_v1_85",
            "content": "Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, Vision-language navigation with selfsupervised auxiliary reasoning tasks, 2020, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Fengda Zhu",
                    "Yi Zhu",
                    "Xiaojun Chang",
                    "Xiaodan Liang"
                ],
                "title": "Vision-language navigation with selfsupervised auxiliary reasoning tasks",
                "pub_date": "2020",
                "pub_title": "CVPR",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "13-ARR_v1_0@0",
            "content": "Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_0",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@0",
            "content": "Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@1",
            "content": "To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 108,
            "end": 243,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@2",
            "content": "However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments, which hinders their generalization of unseen scenes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 245,
            "end": 442,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@3",
            "content": "To improve the ability of fast cross-domain adaptation, we propose Prompt-based Environmental Self-exploration (ProbES), which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model (CLIP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 444,
            "end": 735,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@4",
            "content": "Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 737,
            "end": 867,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@5",
            "content": "Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 869,
            "end": 1079,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@6",
            "content": "By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 1081,
            "end": 1304,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_2@7",
            "content": "Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_2",
            "start": 1306,
            "end": 1442,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_4@0",
            "content": "Teaching a robot to navigate following a natural language instruction has a broad impact in the field of human-robotic interaction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_4",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_4@1",
            "content": "Many related tasks have been proposed to delve into this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_4",
            "start": 132,
            "end": 196,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_4@2",
            "content": "The vision-language navigation (VLN) task is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_4",
            "start": 198,
            "end": 376,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_5@0",
            "content": "To solve a more practical problem, the REVERIE task focuses on target objects localization that asks an agent to identify an object in an unseen room.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_5",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@0",
            "content": "Solving these tasks requires an agent to obtain a vision-text alignment ability that locates related objects and executes corrective actions according to the instruction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@1",
            "content": "However, collecting a large-scale VLN dataset is difficult and laborious since annotating the semantic of a trajectory within a sentence costs times of labor than annotating an image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 171,
            "end": 353,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@2",
            "content": "Existing navigation datasets are relatively small-scale, and learning on such datasets hinders the agent to obtain a good generalization ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 355,
            "end": 499,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@3",
            "content": "To solve this problem, EnvDrop ) uses a speaker model to generate instructions for sampled trajectories in unseen environments, but the generalization ability is not strong with limited vision-language understanding ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 501,
            "end": 724,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@4",
            "content": "Recently, VLN-BERT (Majumdar et al., 2020) introduces a visio-linguistic model pretrained on Conceptual Captions (Sharma et al., 2018) dataset to learn from image-caption pairs, which are quite different from trajectoryinstruction pairs from VLN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 726,
            "end": 971,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@5",
            "content": "To address this, Airbert (Guhur et al., 2021) constructs a large-scale in-domain pretraining dataset with image-caption pairs collected from online marketplaces such as Airbnb to finetune ViLBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 973,
            "end": 1168,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@6",
            "content": "However, Airbert collects image captioning data on websites, which are still far from the scenario of vision-language navigation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 1170,
            "end": 1298,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_6@7",
            "content": "Different from previous methods that collect human-labeled data to train a navigation model, we suggest that automatically generating instruction-trajectory pairs by self-exploration for pretraining not only helps the model obtain better generalization ability but also achieves fast adaptation to downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_6",
            "start": 1300,
            "end": 1614,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@0",
            "content": "In this paper, we propose a method named prompt-based environmental self-exploration (ProbES) that generates navigation data with prior knowledge automatically and adapts pretrained model quickly to VLN tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@1",
            "content": "An overview of our proposed framework is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 210,
            "end": 268,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@2",
            "content": "By using this method, a pretrained visio-linguistic model is able to adapt to the VLN task automatically and efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 270,
            "end": 390,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@3",
            "content": "Specifically, we build an in-domain dataset by self-exploration without labeling or crawler.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 392,
            "end": 483,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@4",
            "content": "To build such a dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 485,
            "end": 508,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@5",
            "content": "we first generate templates by masking visual and action words in labeled instructions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 510,
            "end": 596,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@6",
            "content": "Then, we sample trajectories in the training environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 598,
            "end": 654,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@7",
            "content": "A pretrained CLIP (Radford et al., 2021) model is used to recognize rooms and objects in the sampled trajectories and match described phrases with them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 656,
            "end": 807,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@8",
            "content": "We construct instructions by filling the matched phrases into sampled templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 809,
            "end": 888,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@9",
            "content": "By leveraging the prior knowledge learned by CLIP, we are able to build a dataset automatically with rich semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 890,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@10",
            "content": "Meanwhile, finetuning the whole pretrained model is time-consuming, we adopt prompt tuning (Li and Liang, 2021;Liu et al., 2021c,b), a lightweight alternative to finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 1018,
            "end": 1190,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@11",
            "content": "Our prompt-based method can distill task-relevant knowledge from pretrained model and achieve fast adaption to downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 1192,
            "end": 1319,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@12",
            "content": "We evaluate ProbES on R2R and REVERIE datasets by discriminative and generative settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 1321,
            "end": 1409,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_7@13",
            "content": "Results show that ProbES can match or surpass the performance of finetuning with substantially less training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_7",
            "start": 1411,
            "end": 1524,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_8@0",
            "content": "To sum up, our main contributions are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_8",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_9@0",
            "content": "(1) We propose ProbES, a novel self-exploration method to automatically build an in-domain dataset that reduces the domain gap between the pretraining dataset and VLN tasks without human labeling; (2) Compared with finetuning large pretrained model, our proposed prompt tuning can achieve fast adaptation; (3) Experiments are conducted on R2R and REVERIE datasets with generative and discriminative settings, and results indicate that our proposed ProbES can achieve better or comparable performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_9",
            "start": 0,
            "end": 499,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_9@1",
            "content": "Besides, our generated data can be used as augmented data which improves the generalization ability of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_9",
            "start": 501,
            "end": 613,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@0",
            "content": "Vision-and-Language Navigation. proposed the first Vision-Language Navigation (VLN) benchmark combining real imagery (Chang et al., 2017) and natural language navigation instructions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@1",
            "content": "To solve this task, proposed a novel SERL model to learn reward functions from the expert distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 184,
            "end": 286,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@2",
            "content": "And combining imitation learning and reinforcement learning (Wang et al., 2019) has been proved to be beneficial for VLN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 288,
            "end": 408,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@3",
            "content": "Since the VLN dataset is relatively small-scale, some works propose augmentation approaches (Fried et al., 2018;Liu et al., 2021a) to improve robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 410,
            "end": 562,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@4",
            "content": "Auxiliary losses (Majumdar et al., 2020;Zhu et al., 2020) is used to take advantage of the additional training signals derived from the semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 564,
            "end": 720,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@5",
            "content": "Some pretraining methods Hao et al., 2020) have been proposed to learn generic cross-modal representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 722,
            "end": 828,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@6",
            "content": "This is further extended to a recurrent model that significantly improves sequential action prediction (Hong et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 830,
            "end": 952,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@7",
            "content": "However, the limited number of environments in pretraining constrain the generalization ability to unseen scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 954,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@8",
            "content": "Most related to this work, VLN-BERT (Majumdar et al., 2020) transfers knowledge from abundant, but out-of-domain image-text data to improve path-instruction matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 1071,
            "end": 1236,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_11@9",
            "content": "In contrast, we not only propose an effective method to build an in-domain dataset by sampling trajectory and generating instructions with templates, but also present a promptbased pretraining strategy to improve VLN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_11",
            "start": 1238,
            "end": 1454,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@0",
            "content": "Vision-and-Language Pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@1",
            "content": "Vision-andlanguage pretraining has made great progress in recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 33,
            "end": 103,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@2",
            "content": "Inspired by BERT (Devlin et al., 2018), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 105,
            "end": 294,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@3",
            "content": "Previous research introduces one-stream BERT models and two-stream BERT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 296,
            "end": 374,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@4",
            "content": "The former directly perform inter-modal grounding (Li et al., 2019;Su et al., 2019;Alberti et al., 2019;Li et al., 2020a;Zhou et al., 2020;Li et al., 2020b), while two-stream models process both visual and textual inputs in separate streams, and then fuse the two modalities in a later stage .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 376,
            "end": 668,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@5",
            "content": "These models are often pretrained with self-supervised objectives akin to those in BERT: masked language modeling, masked object classification, and sentence-image alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 670,
            "end": 843,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@6",
            "content": "In this work, the architecture of the ProbES model is structural similar to ViLBERT .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 845,
            "end": 929,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@7",
            "content": "We make several VLN-specific adaptations to ViLBERT so that pretrained weights can be transferred to initialize large portions of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 931,
            "end": 1070,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_12@8",
            "content": "Different from VLN-BERT which finetunes a ViLBERT on instruction-trajectory pairs to measure their compatibility in beam search setting, we introduce prompt tuning, which only tunes the continuous prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_12",
            "start": 1072,
            "end": 1276,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@0",
            "content": "Prompting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@1",
            "content": "Natural language prompting freezes pretrained models and reformats the natural language input with example prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 11,
            "end": 125,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@2",
            "content": "GPT-3 (Brown et al., 2020) introduces in-context learning, using manually designed and discrete text prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 127,
            "end": 235,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@3",
            "content": "Sun et al. (Sun and Lai, 2020) also leverage prompts as keywords to control the sentiment or topic of the generated sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 237,
            "end": 361,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@4",
            "content": "AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from a masked LM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 363,
            "end": 537,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@5",
            "content": "Different from the discrete text prompt, some methods examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 539,
            "end": 709,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@6",
            "content": "Prefix-Tuning (Li and Liang, 2021) prepends a sequence of continuous task-specific vectors as virtual tokens to the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 711,
            "end": 832,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@7",
            "content": "(Zhong et al., 2021;Qin and Eisner, 2021;Hambardzumyan et al., 2021) introduce continuous templates following manual prompt templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 834,
            "end": 967,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@8",
            "content": "Ptuning (Liu et al., 2021c) uses continuous prompts which are learned by inserting trainable variables into the embedded input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 969,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@9",
            "content": "Ptr (Han et al., 2021) adopts manually crafted sub-templates and generates complete templates by logic rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 1097,
            "end": 1205,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@10",
            "content": "In ProbES, we prepend continuous task-specific vectors to the embedding of the input instruction and directly tune the embeddings of these vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 1207,
            "end": 1353,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_13@11",
            "content": "After prompt tuning, the model can be adapted to VLN and REVERIE tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_13",
            "start": 1355,
            "end": 1425,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_14@0",
            "content": "Prompt-based Environmental",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_14",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_15@0",
            "content": "Self-Exploration (ProbES)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_15",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_16@0",
            "content": "Vision-Language Navigation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_16",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_17@0",
            "content": "The Vision-and-Language Navigation (VLN) task gives a global natural sentence I = {w 0 , ..., w l } as an instruction, where w i is a word token while the l is the length of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_17",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_17@1",
            "content": "The instruction consists of step-by-step guidance toward the goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_17",
            "start": 188,
            "end": 253,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_17@2",
            "content": "At step t, the agent observes a panoramic view O t = {o t,i } 36 i=1 as the vision input, which is composed of 36 RGB image views.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_17",
            "start": 255,
            "end": 384,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_17@3",
            "content": "Each of these views consists of image feature v i and an orientation description (sin \u03b8 t,i , cos \u03b8 t,i , sin \u03d5 t,i , cos \u03d5 t,i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_17",
            "start": 386,
            "end": 515,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_17@4",
            "content": "Candidates in the panoramic action space consist of k neighbours of the current node in the navigation graph and a stop action.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_17",
            "start": 517,
            "end": 643,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_18@0",
            "content": "Instruction Generation with Templates",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_18",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@0",
            "content": "We first generate templates from instructions in the R2R dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@1",
            "content": "Then we sample trajectories in the training environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 66,
            "end": 121,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@2",
            "content": "We generate the candidate noun phrases and actionable verbs for the sampled trajectories and full-fill the templates by the above words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 123,
            "end": 258,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@3",
            "content": "A detailed demonstration of our instruction generation module is shown in Fig. 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 260,
            "end": 340,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@4",
            "content": "Generating Templates We collect phrases and replace these phrases in human-annotated navigation instruction with blank masks to generate templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 342,
            "end": 488,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@5",
            "content": "Different from the Airbert (Guhur et al., 2021) that only extracts noun phrases, we also mask action words like 'left', 'right', 'forward', and 'around'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 490,
            "end": 642,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_19@6",
            "content": "We denote the O mask as the mask for an object 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_19",
            "start": 644,
            "end": 692,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@0",
            "content": "We first sample the trajectories in the Matterport (Chang et al., 2017) Environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@1",
            "content": "We randomly sample the starting and ending positions, and collect tracks with lengths of less than 8 hops.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 85,
            "end": 190,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@2",
            "content": "Then we obtain the corresponding actions of each trajectory by firstperson movement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 192,
            "end": 275,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@3",
            "content": "If the agent chooses the front navigable position to move, we generate a 'forward' action.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 277,
            "end": 366,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@4",
            "content": "If the agent chooses the back navigable position to move, we generate an 'around' action.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 368,
            "end": 456,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_20@5",
            "content": "Otherwise, if the agent selects the right front navigable position to move for the next step, we generate an action sequence like {'right', 'forward'}, which is used to fill actionable verbs during instruction generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_20",
            "start": 458,
            "end": 678,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_21@0",
            "content": "Full-filling Template with Prior Knowledge Prior knowledge is the key to generating high-quality data without human labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_21",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_21@1",
            "content": "ProbES introduces CLIP, a powerful vision-language alignment model learned from a large-scale image-caption dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_21",
            "start": 126,
            "end": 241,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@0",
            "content": "To generate structured augmentation data, we fullfill the templates with phrases that describe the sampled trajectory and actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@1",
            "content": "A trajectory is denoted as {v 1 , v 2 , ..., v n }, where v i represents an observation viewpoint.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 131,
            "end": 228,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@2",
            "content": "We introduce CLIP (Radford et al., 2021) We randomly select a template with the same or a close number of O mask as the number of viewpoints in the sampled trajectory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 230,
            "end": 396,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@3",
            "content": "The template has a sequence of object masks {O mask,1 , O mask,2 , ..., O mask,i } and a sequence of action masks {A mask,1 , A mask,2 , ..., A mask,j }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 398,
            "end": 550,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@4",
            "content": "Lengths of object masks and action masks are denoted as l and n respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 552,
            "end": 628,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@5",
            "content": "The number of object masks and action masks is roughly balanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 630,
            "end": 693,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@6",
            "content": "Let n v be the number of viewpoints in a sampled trajectory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 695,
            "end": 754,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@7",
            "content": "Then the generated captions of this trajectory is written as {c 1 , c 2 , ..., c nv }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 756,
            "end": 841,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@8",
            "content": "We full-fill the templates by the following rules: 1) if n v \u2265 l, we randomly sample l captions and fill the O mask in the template sequentially; 2) if n v < l, we randomly sample the O mask and use all the caption phrases to fill them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 843,
            "end": 1078,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@9",
            "content": "After filling phrases, we can identify which viewpoint A mask,i may appear since viewpoints of O mask,j near it are already known.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 1080,
            "end": 1209,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@10",
            "content": "For example, if the template is like 'O mask,1 A mask,1 O mask,2 ' and captions of v 1 and v 2 are used to fill O mask,1 and O mask,2 respectively, then A mask,1 is the sampled action between v 1 and v 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 1211,
            "end": 1415,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@11",
            "content": "In this way, we use generated actionable verbs to full-fill the templates and get final instructions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 1417,
            "end": 1517,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_22@12",
            "content": "By the above method, we can generate diverse instructions without human labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_22",
            "start": 1519,
            "end": 1599,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_23@0",
            "content": "Prompt-based Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_23",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_24@0",
            "content": "Prompt tuning has been found effective on many natural language understanding (NLU) tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_24",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_24@1",
            "content": "Motivated by this, we introduce a prompt-based architecture to achieve fast adaptation on the selfexploration dataset (e.g., Conceptual Captions) and downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_24",
            "start": 91,
            "end": 257,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_24@2",
            "content": "The architecture is ViLBERTlike and equipped with a prompt encoder for prompt tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_24",
            "start": 259,
            "end": 343,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_25@0",
            "content": "Given an instruction-trajectory pair, the visual and textual features can be extracted by the visual encoder E v and textual encoder E x in ViL-BERT respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_25",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_25@1",
            "content": "Especially, the textual input has two parts: prompt sequence {p 1 , ..., p n } and word sequence {x 1 , ..., x m }, where p and x indicate a pseudo prompt token and a word token of a generated instruction respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_25",
            "start": 163,
            "end": 380,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_25@2",
            "content": "n and m represent lengths of the prompt sequence and word sequence respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_25",
            "start": 382,
            "end": 461,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@0",
            "content": "We embed prompt sequence by the prompt encoder E p and embed word sequence by the textual encoder E x as follows: e p,1 , ..., e p,n = E p (p 1 , ..., p n ) e x,1 , ..., e x,m = E x (x 1 ), ..., E x (x m ), where E p is composed of a LSTM head followed by a MLP head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@1",
            "content": "Then the textual embedding is mapped to e t = {e p,1 , ..., e p,n , e x,1 , ..., e x,m }, where e p,1 , ..., e p,n are trainable embedding tensors and enable us to find better continous prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 268,
            "end": 461,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@2",
            "content": "Let e v be denoted as visual embedding produced by visual encoder E v .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 463,
            "end": 533,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@3",
            "content": "e t and e v are then passed to the co-attention transformer similar to ViLBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 535,
            "end": 613,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@4",
            "content": "Then in the prompt tuning process, we only train E p and fix the parameters of E x .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 615,
            "end": 698,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_26@5",
            "content": "Similar to VLN-Bert (Devlin et al., 2018), we sample 3 hard negative paths using beam search for an instruction-trajectory pair, and the model is trained to choose the best path among them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_26",
            "start": 700,
            "end": 888,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_27@0",
            "content": "Downstream Tasks Adaptation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_27",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@0",
            "content": "Our model can adapt to diverse downstream navigation tasks, including VLN, a step-by-step navigation task, and REVERIE, an object-oriented navigation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@1",
            "content": "In the step-by-step navigation task, our model receives an instruction sentence and navigates following the commands in the instruction sequentially.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 156,
            "end": 304,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@2",
            "content": "In the object navigation task, our model receives an object description and explores the house to find an object.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 306,
            "end": 418,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@3",
            "content": "Also, our model can be adapted to both discriminative and generative navigation settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 420,
            "end": 508,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@4",
            "content": "In the discriminative setting, our model receives both an instruction and the observation sequence to represent a navigation trajectory and then output a score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 510,
            "end": 669,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_28@5",
            "content": "In the generative setting, our model receives instruction and predicts actions sequentially.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_28",
            "start": 671,
            "end": 762,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_29@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_29",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_30@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_30",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@0",
            "content": "We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R ), and objectoriented navigation task (REVERIE ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@1",
            "content": "ProbES can be easily applied to discriminative and generative models for these two tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 149,
            "end": 237,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@2",
            "content": "Evaluation Metrics A large number of metrics are used to evaluate models in VLN, such as Trajectory Length (TL), the trajectory length in meters, Navigation Error (NE), the navigation error in meters, Oracle Success Rate (OR), the rate if .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 239,
            "end": 478,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@3",
            "content": "VLN task regard SR and SPL as the primary metric, and the REVERIE task regard RGS and RGSPL as the primary metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 480,
            "end": 593,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@4",
            "content": "Implementation Details Our training process is divided into two steps: Firstly, we pretrain our model on our generated self-exploration training set with prompt tuning for only 10 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 595,
            "end": 781,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@5",
            "content": "After that, we adapt our model to the downstream discriminative VLN task with only ranking loss for 20 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 783,
            "end": 892,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@6",
            "content": "The batch size is set as 64 and the learning rate is 4 \u00d7 10 \u22125 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 894,
            "end": 957,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@7",
            "content": "The generative navigation settings are the same as Recurrent VLN-BERT on both R2R and REVERIE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 959,
            "end": 1052,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@8",
            "content": "During pretraining, we use ProbES to 50k instruction-trajectory pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 1054,
            "end": 1123,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@9",
            "content": "We use 32 NVIDIA V100 GPUs for pretraining and 8 GPUs for adaptation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 1125,
            "end": 1193,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_31@10",
            "content": "Experiments with generative settings are conducted on a V100 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_31",
            "start": 1195,
            "end": 1259,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_32@0",
            "content": "Comparison to state-of-the-art Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_32",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_33@0",
            "content": "In this section, we compare our model with previous state-of-the-art methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_33",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_33@1",
            "content": "We compare the ProbES with two baselines (ViLBERT and VLN-BERT built on Recurrent VLN-Bert) and five other methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_33",
            "start": 78,
            "end": 192,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_33@2",
            "content": "A brief description of previous models is as followed: 1) Seq2Seq: A sequence to sequence model reported in ; 2) Speaker-Follower (Fried et al., 2018): a method introduces a data augmentation approach and panoramic action space; 3) PRESS (Li et al., 2019): a conventional fine-tuning method with stochastic instruction sampling; 4) EnvDrop : a method augment data with environmental dropout; 5) Recurrent VLN-Bert (Hong et al., 2021) on three different settings: OSCAR and ViLBERT pretrained on out-of-domain data, VLN-BERT pretrained on R2R.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_33",
            "start": 194,
            "end": 735,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_33@3",
            "content": "We compare the models on three splits in the R2R dataset: validation seen house, validation unseen house, and testing (where the houses are also unseen).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_33",
            "start": 737,
            "end": 889,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_33@4",
            "content": "We also compare ProbES with Seq2Seq, RCM (Wang et al., 2019), SMNA (Ma et al., 2019), FAST-MATTN , Recurrent VLN-Bert (Hong et al., 2021) on OSCAR on REVERIE dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_33",
            "start": 891,
            "end": 1056,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_34@0",
            "content": "We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, as shown in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_34",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_34@1",
            "content": "In the validation seen split, compared to VLN-BERT under the same setting, our ProbES achieves 5% improvement on SR and 5% improvement on SPL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_34",
            "start": 124,
            "end": 265,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_34@2",
            "content": "In the validation unseen split, we achieve 1% improvement on SR compared to VLN-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_34",
            "start": 267,
            "end": 351,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_34@3",
            "content": "In the testing split, ProbES shows competitive results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_34",
            "start": 353,
            "end": 407,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_34@4",
            "content": "Note that the PREVALENT backbone is pretrained on an in-domain R2R dataset with scene features and fine-tuned with an additional action prediction task in a generative setting while ProbES does not use labeled R2R data or augmented data generated by speaker (Fried et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_34",
            "start": 409,
            "end": 687,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_35@0",
            "content": "We compare ProbES with VLN-BERT in the discriminative setting as in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_35",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_35@1",
            "content": "In the validation unseen split, our method outperforms VLN-BERT, which indicates ProbES is able to improve the generalization ability for unseen scenes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_35",
            "start": 77,
            "end": 228,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_36@0",
            "content": "Results on REVERIE We compare ProbES with previous state-of-the-art methods on the REVERIE dataset, as shown in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_36",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_36@1",
            "content": "In the validation unseen split, we achieve 0.42% improvement on RGS and 0.65% improvement on RGSPL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_36",
            "start": 121,
            "end": 219,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_36@2",
            "content": "In the testing split, ProbES achieves 0.87% improvement on RGS and 0.69% improvement on RGSPL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_36",
            "start": 221,
            "end": 314,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_36@3",
            "content": "We can see that ProbES benefits from prompt tuning with our generated instruction-trajectory pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_36",
            "start": 316,
            "end": 414,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_37@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_37",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_38@0",
            "content": "Ablation of Learning Strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_38",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_38@1",
            "content": "In Table 5, we ablate the performance gains from different learning strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_38",
            "start": 33,
            "end": 111,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_38@2",
            "content": "PT and FT represent prompt tuning and fine-tuning respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_38",
            "start": 113,
            "end": 175,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_38@3",
            "content": "Mask and Rank stand for masked multi-modal modeling loss and the ranking loss for path-selection task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_38",
            "start": 177,
            "end": 278,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_38@4",
            "content": "We regard the model finetuned by ranking loss as our baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_38",
            "start": 280,
            "end": 341,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_39@0",
            "content": "The masked multi-modal modeling loss on our data and R2R data are able to improve the performance. And finetuning on our data is able to improve generalization ability since the success rate in the validation unseen split gets 1.1% improvement and achieves 59.0%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_39",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_39@1",
            "content": "At last, we discover that pre- As shown in Table 6, randomly selecting template without considering the number of masked tokens degrades the performance and introduces more noise in the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_39",
            "start": 264,
            "end": 454,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_39@2",
            "content": "Results show that equipped with our generated data (Row 3) improves the performance by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_39",
            "start": 456,
            "end": 557,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_39@3",
            "content": "The model of using the rooms and objects from Places365 (Zhou et al., 2017) and Objects365 (Shao et al., 2019) (Row 4) performs worse than which uses the rooms and objects from Matterport.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_39",
            "start": 559,
            "end": 746,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_39@4",
            "content": "We infer from that Places365 and Objects365 contain many outdoor scenes and objects which are not suitable for VLN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_39",
            "start": 748,
            "end": 862,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_40@0",
            "content": "Qualititiva Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_40",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_41@0",
            "content": "Visualization of Data Distribution Figure 3 presents a statistical analysis of our generated instructions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_41",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_41@1",
            "content": "We can see from the left figure that the number of object masks are larger than that of action masks, indicating that instructions contain more rich information generated by CLIP from sampled observations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_41",
            "start": 107,
            "end": 311,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_41@2",
            "content": "The right figure shows the distribution of the instruction lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_41",
            "start": 313,
            "end": 379,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_41@3",
            "content": "The lengths of most of the instructions range from 10 to 30, which matches the R2R dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_41",
            "start": 381,
            "end": 471,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_41@4",
            "content": "The easy samples and hard samples in our generated instructions are balanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_41",
            "start": 473,
            "end": 549,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@0",
            "content": "Here we provide visualization of the data generated by ProbES.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@1",
            "content": "Figure 4 shows the instructiontrajectory samples generated with our strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 63,
            "end": 139,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@2",
            "content": "For each sample, we visualize observations of the trajectory, captions generated with CLIP, the selected template, and the final instruction generated by ProbES.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 141,
            "end": 301,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@3",
            "content": "Generated object classes fit observed scenes well, thus we can infer that CLIP is able to extract key information from the observation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 303,
            "end": 437,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@4",
            "content": "Also, our method can select a suitable template and generate diverse instructions that describe observations of trajectories correctly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 439,
            "end": 573,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_42@5",
            "content": "The length of our generated instruction ranges from 1 to 3 sentences, which matches the data distribution of the R2R dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_42",
            "start": 575,
            "end": 699,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_43@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_43",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_44@0",
            "content": "In this work, we first introduce an effective way to generate in-domain data for pretraining the VLN model: leveraging a large pretrained CLIP model to generate captions for each viewpoint and sampling actions in the environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_44",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_44@1",
            "content": "Experiments show that the domain gap between pretraining data and VLN tasks can be mitigated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_44",
            "start": 230,
            "end": 322,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_44@2",
            "content": "We also propose a promptbased architecture, which introduces prompt tuning to adapt the pretrained model fastly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_44",
            "start": 324,
            "end": 435,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_44@3",
            "content": "Our proposed ProbES achieves better results compared to baseline on both R2R and REVERIE datasets, and ablations show the contribution of each module and the effectiveness of the generated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_44",
            "start": 437,
            "end": 630,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_44@4",
            "content": "Walk past family room with mirror on your left, walk to dining room with mirror, wait at dining room.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_44",
            "start": 632,
            "end": 732,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_45@0",
            "content": "Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter, Fusion of detected objects in text for visual question answering, 2019, EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_45",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_46@0",
            "content": "Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir, 2018, On evaluation of embodied navigation agents, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_46",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_47@0",
            "content": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, Anton Van Den,  Hengel, Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments, 2018, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_47",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_48@0",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Language models are few-shot learners, 2020, Ilya Sutskever, and Dario Amodei, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_48",
            "start": 0,
            "end": 432,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_49@0",
            "content": "UNKNOWN, None, 2017, Matter-port3d: Learning from rgb-d data in indoor environments, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_49",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_50@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_50",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_51@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_51",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_52@0",
            "content": "Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Speaker-follower models for vision-and-language navigation, 2018, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_52",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_53@0",
            "content": "Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid, Airbert: In-domain pretraining for vision-and-language navigation, 2021, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_53",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_54@0",
            "content": "UNKNOWN, None, 2021-05, Warp: Word-level adversarial reprogramming, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_54",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_55@0",
            "content": "UNKNOWN, None, 2021, Ptr: Prompt tuning with rules for text classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_55",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_56@0",
            "content": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, Towards learning a generic agent for vision-and-language navigation via pretraining, 2020, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_56",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_57@0",
            "content": "Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Vln bert: A recurrent vision-and-language bert for navigation, 2021, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_57",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_58@0",
            "content": "Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie, Transferable representation learning in vision-and-language navigation, 2019, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_58",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_59@0",
            "content": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining, 2020, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_59",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_60@0",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_60",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_61@0",
            "content": "UNKNOWN, None, 2021, Prefix-tuning: Optimizing continuous prompts for generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_61",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_62@0",
            "content": "Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, Yejin Choi, Robust navigation with language pretraining and stochastic sampling, 2019, EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_62",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_63@0",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Oscar: Objectsemantics aligned pre-training for vision-language tasks, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_63",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_64@0",
            "content": "Chong Liu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang, Zongyuan Ge, Yi-Dong Shen, Vision-language navigation with random environmental mixup, 2021, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_64",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_65@0",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021b. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_65",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_66@0",
            "content": "UNKNOWN, None, , Zhilin Yang, and Jie Tang. 2021c. Gpt understands, too, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_66",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2019, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_67",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_68@0",
            "content": "UNKNOWN, None, 2019, Self-monitoring navigation agent via auxiliary progress estimation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_68",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_69@0",
            "content": "Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, Improving vision-and-language navigation with imagetext pairs from the web, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_69",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_70@0",
            "content": "Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Wang, Chunhua Shen, Anton Van Den,  Hengel, Reverie: Remote embodied visual referring expression in real indoor environments, 2020, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_70",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_71@0",
            "content": "UNKNOWN, None, 2021, Learning how to ask: Querying lms with mixtures of soft prompts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_71",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_72@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision, , ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_72",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_73@0",
            "content": "Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, Objects365: A large-scale, high-quality dataset for object detection, 2019, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_73",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_74@0",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_74",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2020, Autoprompt: Eliciting knowledge from language models with automatically generated prompts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_75",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_76@0",
            "content": "UNKNOWN, None, 2019, Vl-bert: Pre-training of generic visual-linguistic representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_76",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_77@0",
            "content": "UNKNOWN, None, 2020, Conditioned natural language generation using only unconditioned language model: An exploration, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_77",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_78@0",
            "content": "UNKNOWN, None, 2019, Lxmert: Learning cross-modality encoder representations from transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_78",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2019, Learning to navigate unseen environments: Back translation with environmental dropout, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_79",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_80@0",
            "content": "Hu Wang, Qi Wu, Chunhua Shen, Soft expert reward learning for vision-and-language navigation, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_80",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_81@0",
            "content": "Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Wang, Lei Zhang, Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation, 2019, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_81",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_82@0",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is, 2021, Learning vs. learning to recall, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_82",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_83@0",
            "content": "UNKNOWN, None, 2017, Places: A 10 million image database for scene recognition, PAMI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_83",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_84@0",
            "content": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao, Unified visionlanguage pre-training for image captioning and vqa, 2020, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_84",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "13-ARR_v1_85@0",
            "content": "Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, Vision-language navigation with selfsupervised auxiliary reasoning tasks, 2020, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "13-ARR_v1_85",
            "start": 0,
            "end": 136,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_1",
            "tgt_ix": "13-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_1",
            "tgt_ix": "13-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_4",
            "tgt_ix": "13-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_5",
            "tgt_ix": "13-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_8",
            "tgt_ix": "13-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_9",
            "tgt_ix": "13-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_10",
            "tgt_ix": "13-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_10",
            "tgt_ix": "13-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_10",
            "tgt_ix": "13-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_10",
            "tgt_ix": "13-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_15",
            "tgt_ix": "13-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_16",
            "tgt_ix": "13-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_16",
            "tgt_ix": "13-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_21",
            "tgt_ix": "13-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_24",
            "tgt_ix": "13-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_23",
            "tgt_ix": "13-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_23",
            "tgt_ix": "13-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_23",
            "tgt_ix": "13-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_23",
            "tgt_ix": "13-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_27",
            "tgt_ix": "13-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_27",
            "tgt_ix": "13-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_30",
            "tgt_ix": "13-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_30",
            "tgt_ix": "13-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_35",
            "tgt_ix": "13-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_36",
            "tgt_ix": "13-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_37",
            "tgt_ix": "13-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_37",
            "tgt_ix": "13-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_37",
            "tgt_ix": "13-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_40",
            "tgt_ix": "13-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_40",
            "tgt_ix": "13-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_40",
            "tgt_ix": "13-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_43",
            "tgt_ix": "13-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_43",
            "tgt_ix": "13-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "13-ARR_v1_0",
            "tgt_ix": "13-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_1",
            "tgt_ix": "13-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_2",
            "tgt_ix": "13-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_3",
            "tgt_ix": "13-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_4",
            "tgt_ix": "13-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_4",
            "tgt_ix": "13-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_4",
            "tgt_ix": "13-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_5",
            "tgt_ix": "13-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_6",
            "tgt_ix": "13-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_7",
            "tgt_ix": "13-ARR_v1_7@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_8",
            "tgt_ix": "13-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_9",
            "tgt_ix": "13-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_9",
            "tgt_ix": "13-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_10",
            "tgt_ix": "13-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_11",
            "tgt_ix": "13-ARR_v1_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_12",
            "tgt_ix": "13-ARR_v1_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_13",
            "tgt_ix": "13-ARR_v1_13@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_14",
            "tgt_ix": "13-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_15",
            "tgt_ix": "13-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_16",
            "tgt_ix": "13-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_17",
            "tgt_ix": "13-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_18",
            "tgt_ix": "13-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_19",
            "tgt_ix": "13-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_20",
            "tgt_ix": "13-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_21",
            "tgt_ix": "13-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_21",
            "tgt_ix": "13-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_22",
            "tgt_ix": "13-ARR_v1_22@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_23",
            "tgt_ix": "13-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_24",
            "tgt_ix": "13-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_24",
            "tgt_ix": "13-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_24",
            "tgt_ix": "13-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_25",
            "tgt_ix": "13-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_25",
            "tgt_ix": "13-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_25",
            "tgt_ix": "13-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_26",
            "tgt_ix": "13-ARR_v1_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_27",
            "tgt_ix": "13-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_28",
            "tgt_ix": "13-ARR_v1_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_29",
            "tgt_ix": "13-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_30",
            "tgt_ix": "13-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_31",
            "tgt_ix": "13-ARR_v1_31@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_32",
            "tgt_ix": "13-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_33",
            "tgt_ix": "13-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_34",
            "tgt_ix": "13-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_35",
            "tgt_ix": "13-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_35",
            "tgt_ix": "13-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_36",
            "tgt_ix": "13-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_36",
            "tgt_ix": "13-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_36",
            "tgt_ix": "13-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_36",
            "tgt_ix": "13-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_37",
            "tgt_ix": "13-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_38",
            "tgt_ix": "13-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_39",
            "tgt_ix": "13-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_40",
            "tgt_ix": "13-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_41",
            "tgt_ix": "13-ARR_v1_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_42",
            "tgt_ix": "13-ARR_v1_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_43",
            "tgt_ix": "13-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_44",
            "tgt_ix": "13-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_44",
            "tgt_ix": "13-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_44",
            "tgt_ix": "13-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_44",
            "tgt_ix": "13-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_44",
            "tgt_ix": "13-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_45",
            "tgt_ix": "13-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_46",
            "tgt_ix": "13-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_47",
            "tgt_ix": "13-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_48",
            "tgt_ix": "13-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_49",
            "tgt_ix": "13-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_50",
            "tgt_ix": "13-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_51",
            "tgt_ix": "13-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_52",
            "tgt_ix": "13-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_53",
            "tgt_ix": "13-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_54",
            "tgt_ix": "13-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_55",
            "tgt_ix": "13-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_56",
            "tgt_ix": "13-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_57",
            "tgt_ix": "13-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_58",
            "tgt_ix": "13-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_59",
            "tgt_ix": "13-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_60",
            "tgt_ix": "13-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_61",
            "tgt_ix": "13-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_62",
            "tgt_ix": "13-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_63",
            "tgt_ix": "13-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_64",
            "tgt_ix": "13-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_65",
            "tgt_ix": "13-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_66",
            "tgt_ix": "13-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_67",
            "tgt_ix": "13-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_68",
            "tgt_ix": "13-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_69",
            "tgt_ix": "13-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_70",
            "tgt_ix": "13-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_71",
            "tgt_ix": "13-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_72",
            "tgt_ix": "13-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_73",
            "tgt_ix": "13-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_74",
            "tgt_ix": "13-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_75",
            "tgt_ix": "13-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_76",
            "tgt_ix": "13-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_77",
            "tgt_ix": "13-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_78",
            "tgt_ix": "13-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_79",
            "tgt_ix": "13-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_80",
            "tgt_ix": "13-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_81",
            "tgt_ix": "13-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_82",
            "tgt_ix": "13-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_83",
            "tgt_ix": "13-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_84",
            "tgt_ix": "13-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "13-ARR_v1_85",
            "tgt_ix": "13-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1099,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "13-ARR",
        "version": 1
    }
}