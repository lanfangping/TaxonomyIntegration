{
    "nodes": [
        {
            "ix": "133-ARR_v2_0",
            "content": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_2",
            "content": "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and Hete-GCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an O(N 2 ) graph, where N is the vocabulary plus corpus size. Finally, since Transformers need to compute O(L 2 ) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "133-ARR_v2_4",
            "content": "Text categorization is the task of assigning topical categories to text units such as documents, social media postings, or news articles. Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_5",
            "content": "There are approaches based on a Bag of Words (BoW) that perform text categorization purely on the basis of a multiset of tokens. Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embed-dings. These models count the occurrence of all tokens in the input sequence, while disregarding word position and order, and then rely on word embeddings and fully connected feedforward layer(s). We call these BoW-based models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_6",
            "content": "Among the most popular recent methods for text categorization are graph-based models such as TextGCN (Yao et al., 2019) that first induce a synthetic word-document co-occurence graph over the corpus and subsequently apply a graph neural network (GNN) to perform the classification task. Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_7",
            "content": "Finally, there is the well-known Transformer (Vaswani et al., 2017) universe with models such as BERT (Devlin et al., 2019) and its sizereduced variants such as DistilBERT (Sanh et al., 2019). Here, the input is a (fixed-length) sequence of tokens, which is then fed into multiple layers of self-attention. Lightweight versions such as Distil-BERT and others (Tay et al., 2020;Fournier et al., 2021) use less parameters but operate on the same type of input. Together with recurrent models such as LSTMs, we call these sequence-based models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_8",
            "content": "In this paper, we hypothesize that text categorization can be very well conducted by simple but effective BoW-based models. We investigate this research question in three steps: First, we conduct an in-depth analysis of the literature. We review the key research in the field of text categorization. From this analysis, we derive the different families of methods, the established benchmark datasets, and identify the top performing methods. We decide for which models we report numbers from the literature and which models we run on our own. Overall, we compare 16 different methods from the families of BoW-based models (8 methods), sequence-based models (3 methods), and graphbased models (5 methods). We run our own experiments for 7 of these methods on 5 text categorization datasets, while we report the results from the literature for the remaining methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_9",
            "content": "The result is surprising: Our own BoW-based MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019;Ragesh et al., 2021). Moreover, we did not find any reported scores for BERT-based methods from the sequence-based family. Thus, we fine-tuned our own BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019). These models set a new state of the art. On a metalevel, our study shows that MLPs have largely been ignored as competitor methods in experiments. It seems as if MLPs have been forgotten as baseline in the literature, which instead is focusing mostly on other advanced Deep Learning architectures. Considering strong baselines is, however, an important means to argue about true scientific advancement (Shen et al., 2018;Dacrema et al., 2019). Simple models are also often preferred in industry due to lower operational and maintenance costs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_10",
            "content": "Below, we introduce our methodology and results from the literature study. Subsequently, we introduce the families of models in Section 3. Thereafter, we describe the experimental procedure in Section 4. We present the results of our experiments in Section 5 and discuss our findings in Section 6, before we conclude.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_11",
            "content": "Literature on Text Categorization",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "133-ARR_v2_12",
            "content": "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Minaee et al., 2021;Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019;Galke et al., 2017;. These cover the range from shallow to deep classification models. Second, we have screened for literature in key NLP and AI venues. Finally, we have complemented our search by checking results and papers on paperswithcode.com. On the basis of this input, we have determined three families of methods and benchmark datasets (see Table 2). We focus our analysis on identifying models per family showing strong performance and select the methods to include in our study. For all models, we have verified that the same train-test split is used. We check whether modified versions of the datasets have been used (e. g., fewer classes), to avoid bias and wrongfully giving advantages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_13",
            "content": "BoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys (Kowsari et al., 2019;Kadhim, 2019) and other comparison studies (Galke et al., 2017). Iyyer et al. (2015 proposed DAN, which combine word embeddings and deep feedforward networks. It is an MLP with 1-6 hidden layers, non-linear activation, dropout, and Ada-Grad as optimization method. The results suggest to use pretrained embeddings such as GloVe (Pennington et al., 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al., 2014) as input. In fastText (Bojanowski et al., 2017;Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification. Furthermore, Shen et al. (2018) explore embedding pooling variants and find that SWEM can rival approaches based on recurrent (RNN) and convolutional neural networks (CNN). We consider fastText, SWEM, and a DAN-like deeper MLP in our comparison.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_14",
            "content": "Note that those approaches that rely on logistic regression on top of pretrained word embeddings, e. g., fastText, share a similar architecture as an MLP with one hidden layer. However, the standard training protocol involves pretraining the word embedding on large amounts of unlabeled text and then freezing the word embeddings while training the logistic regression (Mikolov et al., 2013).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_15",
            "content": "Graph-based Models Using graphs induced from text for the task of text categorization has a long history in the community. An early work is the term co-occurrence graph of the KeyGraph algorithm (Ohsawa et al., 1998). The graph is split into segments, representing the key concepts in the document. Co-occurence graphs have also been used for automatic keyword extraction such as in RAKE (Rose et al., 2010) and can be also used for classification (Zhang et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_16",
            "content": "Modern approaches exploit this idea in combination with graph neural networks (GNN) (Hamilton, 2020). Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al., 2020) as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), andDADGNN (Liu et al., 2020). We briefly discuss these models: In TextGCN, the authors set up a graph based on word-word connections given by window-based pointwise mutual information and word-document TF-IDF scores. They use a one-hot encoding as node features and apply a two-layer graph convolutional network (Kipf and Welling, 2017) on the graph to carry out the node classification task. HeteGCN combines ideas from Predictive Text Embedding (Tang et al., 2015) and TextGCN and split the adjacency matrix into its word-document and word-word sub-matrices and fuse the different layers' representations when required. TensorGCN uses multiple ways of converting text data into graph data including a semantic graph created with an LSTM, a syntactic graph created by dependency parsing, and a sequential graph based on word co-occurrence. HyperGAT extended the idea of text-induced graphs for text classification to hypergraphs. The model uses graph attention and two kinds of hyperedges. Sequential hyperedges represent the relation between sentences and their words. Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2001). Finally, DADGNN is a graph-based approach that uses attention diffusion and decoupling techniques to tackle oversmoothing of the GNN and to be able to stack more layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_17",
            "content": "In TextGCN's original transductive formulation, the entire graph including the test set needs to be known for training. This may be prohibitive in practical applications as each batch of new documents would require retraining the model. When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores (Ragesh et al., 2021). GNNs for text classification use corpus statistics, e. g., pointwise mutual information (PMI), to connect related words in a graph (Yao et al., 2019). When these were omitted, the GNNs would collapse to bag-of-words MLPs. Thus, GNNs have access to more information than BoW-MLPs. GloVe (Pennington et al., 2014) also captures PMI corpus statistics, which is why we include an MLP on GloVe input representations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_18",
            "content": "Sequence models: RNN and CNN Recurrent neural networks (RNN) are a natural choice for any NLP task. However, it turned out to be challenging to find numbers reported on text categorization in the literature that can be used as references. The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN has been applied on a stripped-down to 4 classes version of the 20ng dataset. Thus, the high score of 96.5 reported for 4ng cannot be compared with papers applied on the full 20ng dataset. Also Text-RCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset. The results of Text-RCNN are identical with BLSTM-2DCNN. For the MR dataset, BLSTM-2DCNN provides no information on the specific split of the dataset. RNN-Capsule is a sentiment analysis method reaching an accuracy of 83.8 on the MR dataset, but with a different train-test split. Lyu and Liu (2020) combine a 2D-CNN with bidirectional RNN. Another work applying a combination of a convolutional layer and an LSTM layer is by Wang et al. (2019b). The authors experiment with five English and two Chinese datasets, which are not in the set of representative datasets we identified. The authors report that their approach outperforms existing models like fastText on two of the five English datasets and both Chinese datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_19",
            "content": "Sequence models: Transformers Surprisingly, only few works consider Transformer models for text categorization. A recent work shows that BERT outperforms classic TF-IDF BoW approaches on English, Chinese, and Portuguese text classification datasets (Gonz\u00e1lez-Carvajal and Garrido-Merch\u00e1n, 2020). We have not found any results of transformer-based models reported on those text categorization datasets that are commonly used in the graph-based approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_20",
            "content": "Therefore, we fine-tune BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019) on those datasets ourselves. BERT is a large pretrained language model on the basis of Transformers. Dis-tilBERT (Sanh et al., 2019) is a distilled version of BERT with 40% reduced parameters while retaining 97% of BERT's language understanding capabilities. TinyBERT (Jiao et al., 2020) and Mo-bileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others. We chose Dis-tilBERT because it can be fine-tuned independently from the BERT teacher. Its inference times are 60% faster than BERT, which makes it more likely to be reusable by labs with limited resources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_21",
            "content": "Summary From our literature survey, we see that all recent methods are based on graphs. BoW-based methods are hardly found in experiments, while, likewise surprisingly, Transformer-based sequence models are extremely scarce in the literature on topical text categorization. The recent surveys on text categorization include both classical and Deep Learning models, but none considered a simple MLP except for the inclusion of DAN (Iyyer et al., 2015) in Li et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_22",
            "content": "Models for Text Categorization",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "133-ARR_v2_23",
            "content": "We formally introduce the three families of models for text categorization, namely the BoW-based, graph-based, and sequence-based models. Table 1 summarizes the key properties of the approaches: whether they require a synthetic graph, whether word position is reflected in the model, whether the model can deal with arbitrary length text, and whether the model is capable of inductive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_24",
            "content": "BoW-Based Text Categorization",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "133-ARR_v2_25",
            "content": "Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document. Given paired training examples (x, y) \u2208 D, each consisting of a bag-of-words x \u2208 R n vocab and a class label y \u2208 Y, the goal is to learn a generalizable function \u0177 = f (BoW) \u03b8 (x) with parameters \u03b8 such that arg max( \u0177) preferably equals the true label y for input x.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_26",
            "content": "As BoW-based model, we consider a one hidden layer WideMLP (i. e., two layers in total). We experiment with pure BoW, TF-IDF weighted, and averaged GloVe input representations. We also use a two hidden layers WideMLP-2. We list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_27",
            "content": "Graph-Based Text Categorization",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "133-ARR_v2_28",
            "content": "Graph-based text categorization approaches first set up a synthetic graph on the basis of the text corpus D in the form of an adjacency matrix \u00c2 := make-graph(D). For instance, in TextGCN the graph is set up in two parts: word-word connections are modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document. Then, a parameterized function f (graph) \u03b8 (X, \u00c2) is learned that uses the graph as input, where X are the node features. The graph is composed of word and document nodes, each receiving its own embedding (by setting X = I). In inductive learning, however, there is no embedding of the test documents. Note that the graph-based approaches from the current literature such as TextGCN also disregard word order, similar to the BoW-based models described above. A detailed discussion of the connection between TextGCN and MLP is provided in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_29",
            "content": "We consider top performing graph-based models from the literature, namely TextGCN along with its successors HeteGCN, TensorGCN, HyperGAT, DADGNN, as well as simplified GCN (SGC) (Wu et al., 2019). We do not run our own experiments for the graph-based models but rely on the original work and extensive studies by Ding et al. (2020) and Ragesh et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_30",
            "content": "Sequence-Based Text Categorization",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "133-ARR_v2_31",
            "content": "We consider RNNs, LSTMs, and Transformers as sequence-based models. These models are aware of the order of the words in the input text in the sense that they are able to exploit word order information. Thus, the key difference to the BoW-based and graph-based families is that the word order is reflected by sequence-based model. The model sig-",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_32",
            "content": "nature is \u0177 = f (sequence) \u03b8 ( x 1 , x 2 , . . . , x k ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_33",
            "content": "where k is the (maximum) sequence length. Word position is modeled by a dedicated positional encoding. For instance, in BERT each position is associated with an embedding vector that is added to the word embedding at input level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_34",
            "content": "For the sequence-based models, we run our own experiments with BERT and DistilBERT, while reporting the scores of a pretrained LSTM from Ding et al. (2020) for comparison.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_35",
            "content": "Experimental Apparatus",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "133-ARR_v2_36",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "133-ARR_v2_37",
            "content": "We use the same datasets and train-test split as in TextGCN (Yao et al., 2019). Those datasets are 20ng, R8, R52, ohsumed, and MR. Twenty Newsgroups (20ng) 1 (bydate version) contains long posts categorized into 20 newsgroups. The mean sequence length is 551 words with a standard deviation (SD) of 2,047. R8 and R52 are subsets of the Reuters 21578 news dataset with 8 and 52 classes, respectively. The mean sequence length and SD is 119 \u00b1 128 words for R8, and 126 \u00b1 133 words for R52. Ohsumed 2 is a corpus of medical abstracts from the MEDLINE database that are categorized into diseases (one per abstract). The mean sequence length is 285 \u00b1 123 words. Movie Reviews (MR) 3 (Pang and Lee, 2005), split by Tang et al. (2015), is a binary sentiment analysis dataset on sentence level (mean sequence length and SD: 25 \u00b1 11). Table 2 shows the dataset characteristics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_38",
            "content": "Inductive and Transductive Setups",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "133-ARR_v2_39",
            "content": "We distinguish between a transductive and an inductive setup for text categorization. In the transductive setup, as used in TextGCN, the test documents are visible and actually used for the preprocessing step. In the inductive setting, the test documents remain unseen until test time (i. e., they are not available for preprocessing). We report the scores of the graph-based models for both setups from the literature, where available. BoW-based and sequence-based models are inherently inductive. Ragesh et al. (2021) have evaluated a variant of TextGCN that is capable of inductive learning, which we include in our results, too.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_40",
            "content": "Procedure and Hyperparameter Settings",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "133-ARR_v2_41",
            "content": "We have extracted accuracy scores from the literature according to our systematic selection from Section 2. Below, we provide a detailed description of the procedure for the models that we have run ourselves. We borrow the tokenization strategy from BERT (Devlin et al., 2019) along with its uncased vocabulary. The tokenizer relies primarily on WordPiece (Wu et al., 2016) for a high coverage while maintaining a small vocabulary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_42",
            "content": "Training our BoW-Models. Our WideMLP has one hidden layer with 1,024 rectified linear units (one input-to-hidden and one hidden-to-output layer). We apply dropout after each hidden layer, notably also after the initial embedding layer. Only for GloVe+WideMLP, neither dropout nor ReLU is applied to the frozen pretrained embeddings but only on subsequent layers. The variant WideMLP-2 has two ReLU-activated hidden layers (three layers in total) with 1, 024 hidden units each. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018;Nakkiran et al., 2020). In pre-experiments, we realized that MLPs are not very sensitive to hyperparameter choices. Therefore, we optimize crossentropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10 \u22123 , a linearly decaying learning rate schedule and train for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity, along with a dropout ratio of 0.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_43",
            "content": "Fine-tuning our BERT models. For BERT and DistilBERT, we fine-tune for 10 epochs with a linearly decaying learning rate of 5 \u2022 10 \u22125 and an effective batch size of 128 via gradient accumulation of 8 x 16 batches. We truncate all inputs to 512 tokens. To isolate the influence of word order on BERT's performance, we conduct two further ablations. First, we set all position embeddings to zero and disable their gradient (BERT w/o pos ids). By doing this, we force BERT to operate on a bag-ofwords without any notion of word order or position. Second, we shuffle each sequence to augment the training data. We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuf. augm.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_44",
            "content": "Measures",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "133-ARR_v2_45",
            "content": "We report accuracy as evaluation metric, which is equivalent to Micro-F1 in single-label classification (see Appendix C). We repeat all experiments five times with different random initialization of the parameters and report the mean and standard deviation of these five runs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_46",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "133-ARR_v2_47",
            "content": "Effectiveness",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "133-ARR_v2_48",
            "content": "Table 3 shows the accuracy scores for the text categorization models on the five datasets. All graphbased models in the transductive setting show similar accuracy scores (maximum difference is 2 points). As expected, the scores decrease in the inductive setting up to a point where they are matched or even outperformed by our WideMLP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_49",
            "content": "In the inductive setting, the WideMLP models perform best among the BoW models, in particular, TFIDF+WideMLP and WideMLP on an unweighted BoW. The best-performing graph-based model is HyperGAT, yet DADGNN has a slight advantage on R8, R52, and MR. For the sequencebased models, BERT attains the highest scores, closely followed by DistilBERT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_50",
            "content": "The strong performance of WideMLP rivals all graph-based techniques reported in the literature, in particular, the recently published graph-inducing methods. MLP only falls behind HyperGAT, which relies on topic models to set up the graph. Another observation is that 1 hidden layer (but wide) is sufficient for the tasks, as the scores for MLP variants with 2 hidden layers are lower. We further observe that both pure BoW and TF-IDF weighted BoW lead to better results than approaches that exploit pretrained word embeddings such as GloVe-MLP, fastText, and SWEM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_51",
            "content": "With its immense pretraining, BERT yields the overall highest scores, closely followed by Distil-BERT. DistilBERT outperforms HyperGAT by 7 points on the MR dataset while being on par on the others. BERT outperforms the strongest graphbased competitor, HyperGAT, by 8 points on MR, 1.5 points on ohsumed, 1 point on R52 and R8, and 0.5 points on 20ng.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_52",
            "content": "Our results further confirm that position embeddings are important for BERT with a notable decrease when those are omitted. Augmenting the data with shuffled sequences has led to neither a consistent decrease nor increase in performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_53",
            "content": "Efficiency",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "133-ARR_v2_54",
            "content": "Parameter Count of the Models Table 4 lists the parameter counts of the models. Even though the MLP is fully-connected on top of a bag-ofwords with the dimensionality of the vocabulary size, it has only half of the parameters as Distil-BERT and a quarter of the parameters of BERT. Using TF-IDF does not change the number of model parameters. Due to the high vocabulary size, GloVe-based models have a high number of parameters, but the majority of those is frozen, i. e., does not get gradient updates during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_55",
            "content": "Runtime Performance of the Models We provide the total running times in Table 5 as observed while conducting the experiments on a single NVIDIA A100-SXM4-40GB card. All WideMLP variants are an order of magnitude faster than Dis-tilBERT when considering the average runtime per epoch. DistilBERT is twice as fast as the original BERT. The transformers are only faster than BoW models on the MR dataset. This is because the sequences in the MR dataset are much shorter and less O(L 2 ) attention weights have to be computed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_56",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "133-ARR_v2_57",
            "content": "Key Insights Our experiments show that our MLP models using BoW outperform the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting. Furthermore, the MLP models are comparable to Hyper-GAT. Only transformer-based BERT and Distil-BERT models outperform our MLP and set a new state-of-the-art. This result is important for two reasons: First, the strong performance of a pure BoW-MLP questions the added value of synthetic graphs in models like TextGCN to the text categorization task. Only HyperGAT, which uses the expensive Latent Dirichlet Allocation for computing the graph, slightly outperforms our BoW-WideMLP in two out of five datasets. Thus, we argue that using strong baseline models for text classification is important to assess the true scientific advancement (Dacrema et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_58",
            "content": "Second, in contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity. Furthermore, we experiment with more hidden layers (see WideMLP-2), but do not observe any improvement when the single hidden layer is sufficiently wide. A possible explanation is that already a single hidden layer is sufficient to approximate any compact function to an arbitrary degree of accuracy depending on the width of the hidden layer (Cybenko, 1989).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_59",
            "content": "Finally, a new state-of-the-art is set by the transformer model BERT, which is not very surpris- ing. However, as our efficiency analysis shows, the MLPs require only a fraction of the parameters and are faster in their combined training and inference time except for the MR dataset. The attention mechanism of (standard) Transformers is quadratic in the sequence length, which leads to slower processing of long sequences. With larger batches, the speed of the MLP could be increased even further.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_60",
            "content": "Graph-based models come with high training costs, as not only the graph has to be first computed, but also a GNN has to be trained. For standard GNN methods, the whole graph has to fit into the GPU memory and mini-batching is nontrivial, but possible with dedicated sampling techniques for GNNs (Fey et al., 2021). Furthermore, the original TextGCN is inherently transductive, i. e., it has to be retrained whenever new documents appear. Strictly transductive models are effectively useless in practice (Lu et al., 2019) except for applications, in which a partially labeled corpus needs to be fully annotated. However, recent extensions such as HeteGCN, Hyper-GAT, and DADGNN already relax this constraint and enable inductive learning. Nevertheless, worddocument graphs require O(N 2 ) space, where N is the number of documents plus the vocabulary size, which is a hurdle for large-scale applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_61",
            "content": "There are also tasks where the natural structure of the graph data provides more information than the mere text, e. g., citations networks or connections in social graphs. In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Velickovic et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018). GNNs also find application in various NLP tasks, other than classification (Wu et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_62",
            "content": "An interesting factor is the ability of the models to capture word order. BoW models disregard word order entirely and yield good results, but still fall behind order-aware Transformer models. In an extensive study, Conneau et al. (2018) have shown that memorizing the word content (which words appear at all) is most indicative of downstream task performance. Sinha et al. (2021) have experimented with pretraining BERT by disabling word order during pretraining and show that it makes surprisingly little difference for fine-tuning. In their study, word order is preserved during fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_63",
            "content": "In our experiments, we have conducted complementary experiments: we have used a BERT model that is pretrained with word order, but we have deactivated the position encoding during fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_64",
            "content": "Our results show that there is a notable drop in performance but the model does not fail completely.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_65",
            "content": "Other NLP tasks such as question answering (Rajpurkar et al., 2016) or natural language inference (Wang et al., 2019a) can also be regarded as text classification on a technical level. Here, the positional information of the sequence is more important than for pure topical text categorization. One can expect that BoW-based models perform worse than sequence-based models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_66",
            "content": "Generalizability We expect that similar observations would be made on other text classification datasets because we have already covered a range of different characteristics: long and short texts, topical categorization (20ng, Reuters, and Ohsumed) and sentiment prediction (MR) in the domains of forum postings, news, movie reviews, and medical abstracts. Our results are in line with those from other fields, who have reported a resurgence of MLPs. For example, in business prediction, an MLP baseline outperforms various other Deep Learning models (Venugopal et al., 2021;Yedida et al., 2021). In computer vision, Tolstikhin et al. ( 2021) and Melas-Kyriazi (2021) proposed attention-free MLP models that are on par with the Vision Transformer . In natural language processing, Liu et al. (2021a) show similar results, while acknowledging that a small attention module is necessary for some tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_67",
            "content": "We acknowledge that the experimental datasets are limited to English. While word order is important in the English language, it is notable that methods that discard word order still work well for text categorization. Another possible bias is the comparability of the results. However, we carefully checked all relevant parameters such as the train/test split, the number of classes in the datasets, if datasets have been pre-processed in such a way that, e. g., makes a task easier like reducing the number of classes, the training procedure, and the reported evaluation metrics. Regarding our efficency analysis, we made sure to report numbers for the parameter count and a measure for the speed other than FLOPs, as recommended by . Since runtime is heavily dependant on training parameters such as batch size, we complement this with asymptotic complexity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_68",
            "content": "Practical Impact and Future Work Our study has an immediate impact on practitioners who seek to employ robust text categorization models in research projects and in industrial operational environments. Furthermore, we advocate to use an MLP baseline in future text categorization research, for which we provide concrete guidelines in Appendix A. As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods (Peng et al., 2018;Xiao et al., 2019). Another interesting yet challenging setting would be few-shot classification (Brown et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_69",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "133-ARR_v2_70",
            "content": "We argue that a wide multi-layer perceptron enhanced with today's best practices should be considered as a strong baseline for text classification tasks. In fact, the experiments show that our WideMLP is oftentimes on-par or even better than recently proposed models that synthesize a graph structure from the text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_71",
            "content": "The source code is available online: https://github.com/lgalke/ text-clf-baselines",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_72",
            "content": "The focus of this work is text classification. Potential risks that apply to text classification in general also apply to this work. Nonetheless, we present alternatives to commonly used pretrained language models, which suffer from various sources of bias due to the large and poorly manageable data used for pretraining (Bender et al., 2021). In contrast, the presented alternatives render full control over the training data and, thus, contribute to circumvent the biases otherwise introduced during pretraining.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_73",
            "content": "On the basis of our results, we provide recommendations for designing a WideMLP baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_74",
            "content": "We recommend using modern subword tokenizers such as BERT-like WordPiece or SentencePiece that yield a high coverage while needing a relatively small vocabulary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_75",
            "content": "In contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer. Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_76",
            "content": "Depth vs. Width In text classification, width seems more important than depth. We recommend to use a single, wide hidden layer, i. e., one input-tohidden and one hidden-to-output layer, e. g., with 1,024 hidden units and ReLU activation. While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018;Nakkiran et al., 2020). We further motivate the choice of using wide layers with results from multi-label text classification (Galke et al., 2017), which has shown that a (wide) MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression. Follow-up work (Mai et al., 2018) then found that also CNN and LSTM do not substantially improve over the wide MLP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_77",
            "content": "Having a fully-connected layer on-top of a bagof-words leads to a high number of learnable parameters. Still, the wide first input-to-hidden layer can be implemented efficiently by using an embedding layer followed by aggregation, which avoids large matrix multiplications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_78",
            "content": "In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might be beneficial for other, more challenging datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_79",
            "content": "We seek to find an optimization strategy that does not require dataset-specific hyperparameter tuning. This comprises optimizing cross-entropy with Adam (Kingma and Ba, 2015) and default learning rate 10 \u22123 , a linearly decaying learning rate schedule and training for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_80",
            "content": "For regularization during this prolonged training, we suggest to use a high dropout ratio of 0.5. Regarding initialization, we rely on framework defaults, i. e., N (0, 1) for the initial embedding layer and random uniform U(\u2212 d input , d output ) for subsequent layers' weight and bias parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_81",
            "content": "TextGCN uses the PMI matrix to set up edge weights for word-word connections. A single layer Text-GCN is a BoW-MLP, except for the document embedding. The one-hop neighbors are words which are aggregated after a nonlinear transform.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_82",
            "content": "The basic GCN equation H = \u03c3( \u00c2XW ) reveals that the order of transformation and neighborhood aggregation is irrelevant. The document embedding implies that TextGCN is a semisupervised technique. Truly new documents, as in inductive learning scenarios, would need a special treatment such as using an all zero embedding vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_83",
            "content": "A two-layer MLP can be characterized by the equation \u0177 = W (2) \u03c3(W (1) x + b (1) ) + b (2) . On bag-of-words inputs, the first layer W (1) x + b (1) can be replaced by an equivalent embedding layer with weighting (e. g., TF-IDF or length normalization) being applied during aggregation of the embedding vectors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_84",
            "content": "The first layer of TextGCN is equivalent to aggregating embedding vectors. A standard GCN layer with shared weights has the form (assuming self-loops have been inserted)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_85",
            "content": "h i = j\u2208N (i) a ij W (1) x j + b (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_86",
            "content": "Now in TextGCN node features are given by the identity, such that x j is one-hot. Then we can rewrite the first layer of Text-GCN as an aggregation of embeddings E. We gain",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_87",
            "content": "h i = j\u2208N (i)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_88",
            "content": "a ij E j as W x + b may again be replaced by an embedding matrix if applied to one hot vectors x. Now E contains two types of embedding vectors: word embeddings and document embeddings corresponding to word nodes and document nodes. We see that the first layer of TextGCN is essentially an aggregation of word embeddings plus the document embedding. Only with a second layer, TextGCN considers the embedding of other documents whose words are connected to the present documents' words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_89",
            "content": "In multiclass classification, we have a single true label for each instance and the predictions are constrained to a single prediction per instance. As a consequence, the measures accuracy and Micro-F1 coincide to the same formula. Micro-F1 aggregates true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) globally. It can be expressed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_90",
            "content": "Micro-F 1 = 2 c TP c 2 c TP c + c FP c + c FN c ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_91",
            "content": "where c iterates over all classes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_92",
            "content": "While the accuracy can be expressed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_93",
            "content": "Acc = c TP c + c TN c c TP c + c TN c + c FP c + c FN c",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_94",
            "content": "In multiclass classification, every true positive is also a true negative for all other classes. When summing those up over the entire dataset, we obtain",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "133-ARR_v2_95",
            "content": "UNKNOWN, None, , 2021. A survey on data augmentation for text classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. A survey on data augmentation for text classification",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_96",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Emily Bender",
                    "Timnit Gebru",
                    "Angelina Mcmillan-Major",
                    "Shmargaret Shmitchell"
                ],
                "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "133-ARR_v2_97",
            "content": "David Blei, Andrew Ng, Michael Jordan, Latent dirichlet allocation, 2001-12-03, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, MIT Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "David Blei",
                    "Andrew Ng",
                    "Michael Jordan"
                ],
                "title": "Latent dirichlet allocation",
                "pub_date": "2001-12-03",
                "pub_title": "Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic",
                "pub": "MIT Press"
            }
        },
        {
            "ix": "133-ARR_v2_98",
            "content": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, Enriching word vectors with subword information, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Piotr Bojanowski",
                    "Edouard Grave",
                    "Armand Joulin",
                    "Tomas Mikolov"
                ],
                "title": "Enriching word vectors with subword information",
                "pub_date": "2017",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_99",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Language models are fewshot learners, 2020-12-06, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tom Brown",
                    "Benjamin Mann",
                    "Nick Ryder",
                    "Melanie Subbiah"
                ],
                "title": "Language models are fewshot learners",
                "pub_date": "2020-12-06",
                "pub_title": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_100",
            "content": "Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00efc Barrault, Marco Baroni, What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Alexis Conneau",
                    "German Kruszewski",
                    "Guillaume Lample",
                    "Lo\u00efc Barrault",
                    "Marco Baroni"
                ],
                "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "133-ARR_v2_101",
            "content": "George Cybenko, Approximation by superpositions of a sigmoidal function, 1989, Math. Control. Signals Syst, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "George Cybenko"
                ],
                "title": "Approximation by superpositions of a sigmoidal function",
                "pub_date": "1989",
                "pub_title": "Math. Control. Signals Syst",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_102",
            "content": "Paolo Maurizio Ferrari Dacrema, Dietmar Cremonesi,  Jannach, Are we really making much progress? A worrying analysis of recent neural recommendation approaches, 2019-09-16, Proceedings of the 13th ACM Conference on Recommender Systems, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Paolo Maurizio Ferrari Dacrema",
                    "Dietmar Cremonesi",
                    " Jannach"
                ],
                "title": "Are we really making much progress? A worrying analysis of recent neural recommendation approaches",
                "pub_date": "2019-09-16",
                "pub_title": "Proceedings of the 13th ACM Conference on Recommender Systems",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_103",
            "content": "UNKNOWN, None, 2021, The efficiency misnomer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "The efficiency misnomer",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_104",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "133-ARR_v2_105",
            "content": "Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, Huan Liu, Be more with less: Hypergraph attention networks for inductive text classification, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Kaize Ding",
                    "Jianling Wang",
                    "Jundong Li",
                    "Dingcheng Li",
                    "Huan Liu"
                ],
                "title": "Be more with less: Hypergraph attention networks for inductive text classification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_106",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Alexey Dosovitskiy",
                    "Lucas Beyer",
                    "Alexander Kolesnikov",
                    "Dirk Weissenborn",
                    "Xiaohua Zhai",
                    "Thomas Unterthiner",
                    "Mostafa Dehghani",
                    "Matthias Minderer",
                    "Georg Heigold",
                    "Sylvain Gelly",
                    "Jakob Uszkoreit",
                    "Neil Houlsby"
                ],
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
                "pub_date": "2021-05-03",
                "pub_title": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_107",
            "content": "Matthias Fey, Jan Lenssen, Frank Weichert, Jure Leskovec, GNNAutoScale: Scalable and expressive graph neural networks via historical embeddings, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Matthias Fey",
                    "Jan Lenssen",
                    "Frank Weichert",
                    "Jure Leskovec"
                ],
                "title": "GNNAutoScale: Scalable and expressive graph neural networks via historical embeddings",
                "pub_date": "2021-07-24",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "133-ARR_v2_108",
            "content": "UNKNOWN, None, 2021, A practical survey on faster and lighter transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "A practical survey on faster and lighter transformers",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_109",
            "content": "Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunsch, Ansgar Scherp, Using titles vs. fulltext as source for automated semantic document annotation, 2017-12-04, Proceedings of the Knowledge Capture Conference, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Lukas Galke",
                    "Florian Mai",
                    "Alan Schelten",
                    "Dennis Brunsch",
                    "Ansgar Scherp"
                ],
                "title": "Using titles vs. fulltext as source for automated semantic document annotation",
                "pub_date": "2017-12-04",
                "pub_title": "Proceedings of the Knowledge Capture Conference",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_110",
            "content": "Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao, Deep learning-based text classification: A comprehensive review, 2021, ACM Comput. Surv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Shervin Minaee",
                    "Nal Kalchbrenner",
                    "Erik Cambria",
                    "Narjes Nikzad",
                    "Meysam Chenaghlu",
                    "Jianfeng Gao"
                ],
                "title": "Deep learning-based text classification: A comprehensive review",
                "pub_date": "2021",
                "pub_title": "ACM Comput. Surv",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_111",
            "content": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Deep double descent: Where bigger models and more data hurt, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Preetum Nakkiran",
                    "Gal Kaplun",
                    "Yamini Bansal",
                    "Tristan Yang"
                ],
                "title": "Deep double descent: Where bigger models and more data hurt",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_112",
            "content": "UNKNOWN, None, 2018, Towards understanding the role of over-parametrization in generalization of neural networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Towards understanding the role of over-parametrization in generalization of neural networks",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_113",
            "content": "Yukio Ohsawa, Nels Benson, Masahiko Yachida, Keygraph: Automatic indexing by cooccurrence graph based on building construction metaphor, 1998-04-22, Proceedings of the IEEE Forum on Research and Technology Advances in Digital Libraries, IEEE ADL '98, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Yukio Ohsawa",
                    "Nels Benson",
                    "Masahiko Yachida"
                ],
                "title": "Keygraph: Automatic indexing by cooccurrence graph based on building construction metaphor",
                "pub_date": "1998-04-22",
                "pub_title": "Proceedings of the IEEE Forum on Research and Technology Advances in Digital Libraries, IEEE ADL '98",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "133-ARR_v2_114",
            "content": "Bo Pang, Lillian Lee, Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales, 2005, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Bo Pang",
                    "Lillian Lee"
                ],
                "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
                "pub_date": "2005",
                "pub_title": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_115",
            "content": "Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Large-scale hierarchical text classification with recursively regularized deep graph-cnn, 2018-04-23, Proceedings of the 2018 World Wide Web Conference on World Wide Web, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Hao Peng",
                    "Jianxin Li",
                    "Yu He",
                    "Yaopeng Liu",
                    "Mengjiao Bao",
                    "Lihong Wang"
                ],
                "title": "Large-scale hierarchical text classification with recursively regularized deep graph-cnn",
                "pub_date": "2018-04-23",
                "pub_title": "Proceedings of the 2018 World Wide Web Conference on World Wide Web",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_116",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "GloVe: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "133-ARR_v2_117",
            "content": "Rahul Ragesh, Sundararajan Sellamanickam, Arun Iyer, Ramakrishna Bairi, Vijay Lingam, HeteGCN: Heterogeneous graph convolutional networks for text classification, 2021-03-08, WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Rahul Ragesh",
                    "Sundararajan Sellamanickam",
                    "Arun Iyer",
                    "Ramakrishna Bairi",
                    "Vijay Lingam"
                ],
                "title": "HeteGCN: Heterogeneous graph convolutional networks for text classification",
                "pub_date": "2021-03-08",
                "pub_title": "WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_118",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100,000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "133-ARR_v2_119",
            "content": "Stuart Rose, Dave Engel, Nick Cramer, Wendy Cowley, Automatic keyword extraction from individual documents, 2010, Text Mining, John Wiley & Sons.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Stuart Rose",
                    "Dave Engel",
                    "Nick Cramer",
                    "Wendy Cowley"
                ],
                "title": "Automatic keyword extraction from individual documents",
                "pub_date": "2010",
                "pub_title": "Text Mining",
                "pub": "John Wiley & Sons"
            }
        },
        {
            "ix": "133-ARR_v2_120",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_121",
            "content": "UNKNOWN, None, 2018, Pitfalls of graph neural network evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Pitfalls of graph neural network evaluation",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_122",
            "content": "Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin, Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Dinghan Shen",
                    "Guoyin Wang",
                    "Wenlin Wang",
                    "Martin Renqiang Min",
                    "Qinliang Su",
                    "Yizhe Zhang",
                    "Chunyuan Li",
                    "Ricardo Henao",
                    "Lawrence Carin"
                ],
                "title": "Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "133-ARR_v2_123",
            "content": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Koustuv Sinha",
                    "Robin Jia",
                    "Dieuwke Hupkes",
                    "Joelle Pineau",
                    "Adina Williams",
                    "Douwe Kiela"
                ],
                "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_124",
            "content": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, MobileBERT: a compact task-agnostic BERT for resource-limited devices, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Zhiqing Sun",
                    "Hongkun Yu",
                    "Xiaodan Song",
                    "Renjie Liu",
                    "Yiming Yang",
                    "Denny Zhou"
                ],
                "title": "MobileBERT: a compact task-agnostic BERT for resource-limited devices",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_125",
            "content": "Jian Tang, Meng Qu, Qiaozhu Mei, PTE: predictive text embedding through large-scale heterogeneous text networks, 2015-08-10, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Jian Tang",
                    "Meng Qu",
                    "Qiaozhu Mei"
                ],
                "title": "PTE: predictive text embedding through large-scale heterogeneous text networks",
                "pub_date": "2015-08-10",
                "pub_title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_126",
            "content": "UNKNOWN, None, 2020, Efficient transformers: A survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Efficient transformers: A survey",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_127",
            "content": "UNKNOWN, None, 2021, MLP-Mixer: An all-MLP architecture for vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "MLP-Mixer: An all-MLP architecture for vision",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_128",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_129",
            "content": "Illia Kaiser,  Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_130",
            "content": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Petar Velickovic",
                    "Guillem Cucurull",
                    "Arantxa Casanova",
                    "Adriana Romero",
                    "Pietro Li\u00f2",
                    "Yoshua Bengio"
                ],
                "title": "Graph attention networks",
                "pub_date": "2018-04-30",
                "pub_title": "6th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_131",
            "content": "Ishwar Venugopal, Jessica T\u00f6llich, Michael Fairbank, Ansgar Scherp, A comparison of deeplearning methods for analysing and predicting business processes, 2021-07-18, International Joint Conference on Neural Networks, IJCNN 2021, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Ishwar Venugopal",
                    "Jessica T\u00f6llich",
                    "Michael Fairbank",
                    "Ansgar Scherp"
                ],
                "title": "A comparison of deeplearning methods for analysing and predicting business processes",
                "pub_date": "2021-07-18",
                "pub_title": "International Joint Conference on Neural Networks, IJCNN 2021",
                "pub": "IEEE"
            }
        },
        {
            "ix": "133-ARR_v2_132",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_133",
            "content": "Ruishuang Wang, Zhao Li, Jian Cao, Tong Chen, Lei Wang, Convolutional recurrent neural networks for text classification, 2019, International Joint Conference on Neural Networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Ruishuang Wang",
                    "Zhao Li",
                    "Jian Cao",
                    "Tong Chen",
                    "Lei Wang"
                ],
                "title": "Convolutional recurrent neural networks for text classification",
                "pub_date": "2019",
                "pub_title": "International Joint Conference on Neural Networks",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_134",
            "content": "UNKNOWN, None, 2019, , IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": "IEEE"
            }
        },
        {
            "ix": "133-ARR_v2_135",
            "content": "Yequan Wang, Aixin Sun, Jialong Han, Ying Liu, Xiaoyan Zhu, Sentiment analysis by capsules, 2018-04-23, Proceedings of the 2018 World Wide Web Conference on World Wide Web, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Yequan Wang",
                    "Aixin Sun",
                    "Jialong Han",
                    "Ying Liu",
                    "Xiaoyan Zhu"
                ],
                "title": "Sentiment analysis by capsules",
                "pub_date": "2018-04-23",
                "pub_title": "Proceedings of the 2018 World Wide Web Conference on World Wide Web",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_136",
            "content": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, Simplifying graph convolutional networks, 2019-06, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Felix Wu",
                    "Amauri Souza",
                    "Tianyi Zhang",
                    "Christopher Fifty",
                    "Tao Yu",
                    "Kilian Weinberger"
                ],
                "title": "Simplifying graph convolutional networks",
                "pub_date": "2019-06",
                "pub_title": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019",
                "pub": "PMLR"
            }
        },
        {
            "ix": "133-ARR_v2_137",
            "content": "UNKNOWN, None, 2021, Graph neural networks for natural language processing: A survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Graph neural networks for natural language processing: A survey",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_138",
            "content": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Google's neural machine translation system: Bridging the gap between human and machine translation, 2016, ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Yonghui Wu",
                    "Mike Schuster",
                    "Zhifeng Chen",
                    "Quoc Le",
                    "Mohammad Norouzi"
                ],
                "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
                "pub_date": "2016",
                "pub_title": "ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_139",
            "content": "Huiru Xiao, Xin Liu, Yangqiu Song, Efficient path prediction for semi-supervised and weakly supervised hierarchical text classification, 2019-05-13, The World Wide Web Conference, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Huiru Xiao",
                    "Xin Liu",
                    "Yangqiu Song"
                ],
                "title": "Efficient path prediction for semi-supervised and weakly supervised hierarchical text classification",
                "pub_date": "2019-05-13",
                "pub_title": "The World Wide Web Conference",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_140",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 2019-01-27, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Liang Yao",
                    "Chengsheng Mao",
                    "Yuan Luo"
                ],
                "title": "The Thirty-First Innovative Applications of Artificial Intelligence Conference",
                "pub_date": "2019-01-27",
                "pub_title": "The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "133-ARR_v2_141",
            "content": "UNKNOWN, None, 2021, When SIMPLE is better than complex: A case study on deep learning for predicting Bugzilla issue close time, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "When SIMPLE is better than complex: A case study on deep learning for predicting Bugzilla issue close time",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_142",
            "content": "Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, Yuxin Zhou, Bayesian performance comparison of text classifiers, 2016-07-17, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Dell Zhang",
                    "Jun Wang",
                    "Emine Yilmaz",
                    "Xiaoling Wang",
                    "Yuxin Zhou"
                ],
                "title": "Bayesian performance comparison of text classifiers",
                "pub_date": "2016-07-17",
                "pub_title": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016",
                "pub": "ACM"
            }
        },
        {
            "ix": "133-ARR_v2_143",
            "content": "Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou, Weakly-supervised text classification based on keyword graph, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Lu Zhang",
                    "Jiandong Ding",
                    "Yi Xu",
                    "Yingyao Liu",
                    "Shuigeng Zhou"
                ],
                "title": "Weakly-supervised text classification based on keyword graph",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_144",
            "content": "Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, Bo Xu, Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Peng Zhou",
                    "Zhenyu Qi",
                    "Suncong Zheng",
                    "Jiaming Xu",
                    "Hongyun Bao",
                    "Bo Xu"
                ],
                "title": "Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling",
                "pub_date": "2016",
                "pub_title": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "pub": null
            }
        },
        {
            "ix": "133-ARR_v2_145",
            "content": "Xujuan Zhou, Raj Gururajan, Yuefeng Li, Revathi Venkataraman, Xiaohui Tao, Ghazal Bargshady, Prabal Datta Barua, and Srinivas Kondalsamy-Chennakesavan. 2020. A survey on text classification and its applications, , Web Intell, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Xujuan Zhou",
                    "Raj Gururajan",
                    "Yuefeng Li",
                    "Revathi Venkataraman",
                    "Xiaohui Tao",
                    "Ghazal Bargshady"
                ],
                "title": "Prabal Datta Barua, and Srinivas Kondalsamy-Chennakesavan. 2020. A survey on text classification and its applications",
                "pub_date": null,
                "pub_title": "Web Intell",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "133-ARR_v2_0@0",
            "content": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_0",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@0",
            "content": "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@1",
            "content": "We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and Hete-GCN in an inductive text classification setting and is comparable with HyperGAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 129,
            "end": 343,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@2",
            "content": "Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 345,
            "end": 475,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@3",
            "content": "These results question the importance of synthetic graphs used in modern text classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 477,
            "end": 566,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@4",
            "content": "In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an O(N 2 ) graph, where N is the vocabulary plus corpus size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 568,
            "end": 771,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_2@5",
            "content": "Finally, since Transformers need to compute O(L 2 ) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_2",
            "start": 773,
            "end": 955,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_4@0",
            "content": "Text categorization is the task of assigning topical categories to text units such as documents, social media postings, or news articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_4",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_4@1",
            "content": "Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_4",
            "start": 138,
            "end": 345,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_5@0",
            "content": "There are approaches based on a Bag of Words (BoW) that perform text categorization purely on the basis of a multiset of tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_5",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_5@1",
            "content": "Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embed-dings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_5",
            "start": 129,
            "end": 520,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_5@2",
            "content": "These models count the occurrence of all tokens in the input sequence, while disregarding word position and order, and then rely on word embeddings and fully connected feedforward layer(s).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_5",
            "start": 522,
            "end": 710,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_5@3",
            "content": "We call these BoW-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_5",
            "start": 712,
            "end": 742,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_6@0",
            "content": "Among the most popular recent methods for text categorization are graph-based models such as TextGCN (Yao et al., 2019) that first induce a synthetic word-document co-occurence graph over the corpus and subsequently apply a graph neural network (GNN) to perform the classification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_6",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_6@1",
            "content": "Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_6",
            "start": 287,
            "end": 475,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_7@0",
            "content": "Finally, there is the well-known Transformer (Vaswani et al., 2017) universe with models such as BERT (Devlin et al., 2019) and its sizereduced variants such as DistilBERT (Sanh et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_7",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_7@1",
            "content": "Here, the input is a (fixed-length) sequence of tokens, which is then fed into multiple layers of self-attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_7",
            "start": 193,
            "end": 305,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_7@2",
            "content": "Lightweight versions such as Distil-BERT and others (Tay et al., 2020;Fournier et al., 2021) use less parameters but operate on the same type of input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_7",
            "start": 307,
            "end": 457,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_7@3",
            "content": "Together with recurrent models such as LSTMs, we call these sequence-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_7",
            "start": 459,
            "end": 540,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@0",
            "content": "In this paper, we hypothesize that text categorization can be very well conducted by simple but effective BoW-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@1",
            "content": "We investigate this research question in three steps: First, we conduct an in-depth analysis of the literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 124,
            "end": 234,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@2",
            "content": "We review the key research in the field of text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 236,
            "end": 298,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@3",
            "content": "From this analysis, we derive the different families of methods, the established benchmark datasets, and identify the top performing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 300,
            "end": 440,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@4",
            "content": "We decide for which models we report numbers from the literature and which models we run on our own.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 442,
            "end": 541,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@5",
            "content": "Overall, we compare 16 different methods from the families of BoW-based models (8 methods), sequence-based models (3 methods), and graphbased models (5 methods).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 543,
            "end": 703,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_8@6",
            "content": "We run our own experiments for 7 of these methods on 5 text categorization datasets, while we report the results from the literature for the remaining methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_8",
            "start": 705,
            "end": 863,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@0",
            "content": "The result is surprising: Our own BoW-based MLP, called the WideMLP, with only one wide hidden layer, outperforms many of the recent graphbased models for inductive text categorization (Yao et al., 2019;Ragesh et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@1",
            "content": "Moreover, we did not find any reported scores for BERT-based methods from the sequence-based family.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 225,
            "end": 324,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@2",
            "content": "Thus, we fine-tuned our own BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 326,
            "end": 415,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@3",
            "content": "These models set a new state of the art.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 417,
            "end": 456,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@4",
            "content": "On a metalevel, our study shows that MLPs have largely been ignored as competitor methods in experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 458,
            "end": 562,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@5",
            "content": "It seems as if MLPs have been forgotten as baseline in the literature, which instead is focusing mostly on other advanced Deep Learning architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 564,
            "end": 713,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@6",
            "content": "Considering strong baselines is, however, an important means to argue about true scientific advancement (Shen et al., 2018;Dacrema et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 715,
            "end": 859,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_9@7",
            "content": "Simple models are also often preferred in industry due to lower operational and maintenance costs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_9",
            "start": 861,
            "end": 958,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_10@0",
            "content": "Below, we introduce our methodology and results from the literature study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_10",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_10@1",
            "content": "Subsequently, we introduce the families of models in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_10",
            "start": 75,
            "end": 137,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_10@2",
            "content": "Thereafter, we describe the experimental procedure in Section 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_10",
            "start": 139,
            "end": 202,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_10@3",
            "content": "We present the results of our experiments in Section 5 and discuss our findings in Section 6, before we conclude.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_10",
            "start": 204,
            "end": 316,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_11@0",
            "content": "Literature on Text Categorization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_11",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@0",
            "content": "Methodology In a first step, we have analyzed recent surveys on text categorization and comparison studies (Minaee et al., 2021;Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019;Galke et al., 2017;. These cover the range from shallow to deep classification models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@1",
            "content": "Second, we have screened for literature in key NLP and AI venues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 302,
            "end": 366,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@2",
            "content": "Finally, we have complemented our search by checking results and papers on paperswithcode.com.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 368,
            "end": 461,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@3",
            "content": "On the basis of this input, we have determined three families of methods and benchmark datasets (see Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 463,
            "end": 572,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@4",
            "content": "We focus our analysis on identifying models per family showing strong performance and select the methods to include in our study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 574,
            "end": 702,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@5",
            "content": "For all models, we have verified that the same train-test split is used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 704,
            "end": 775,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_12@6",
            "content": "We check whether modified versions of the datasets have been used (e. g., fewer classes), to avoid bias and wrongfully giving advantages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_12",
            "start": 777,
            "end": 913,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@0",
            "content": "BoW-based Models Classical machine learning models that operate on a BoW-based input are extensively discussed in two surveys (Kowsari et al., 2019;Kadhim, 2019) and other comparison studies (Galke et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@1",
            "content": "Iyyer et al. (2015 proposed DAN, which combine word embeddings and deep feedforward networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 213,
            "end": 305,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@2",
            "content": "It is an MLP with 1-6 hidden layers, non-linear activation, dropout, and Ada-Grad as optimization method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 307,
            "end": 411,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@3",
            "content": "The results suggest to use pretrained embeddings such as GloVe (Pennington et al., 2014) over a randomly initialized neural bag of-words (Kalchbrenner et al., 2014) as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 413,
            "end": 586,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@4",
            "content": "In fastText (Bojanowski et al., 2017;Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 588,
            "end": 719,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@5",
            "content": "Furthermore, Shen et al. (2018) explore embedding pooling variants and find that SWEM can rival approaches based on recurrent (RNN) and convolutional neural networks (CNN).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 721,
            "end": 892,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_13@6",
            "content": "We consider fastText, SWEM, and a DAN-like deeper MLP in our comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_13",
            "start": 894,
            "end": 965,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_14@0",
            "content": "Note that those approaches that rely on logistic regression on top of pretrained word embeddings, e. g., fastText, share a similar architecture as an MLP with one hidden layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_14",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_14@1",
            "content": "However, the standard training protocol involves pretraining the word embedding on large amounts of unlabeled text and then freezing the word embeddings while training the logistic regression (Mikolov et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_14",
            "start": 177,
            "end": 391,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_15@0",
            "content": "Graph-based Models Using graphs induced from text for the task of text categorization has a long history in the community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_15",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_15@1",
            "content": "An early work is the term co-occurrence graph of the KeyGraph algorithm (Ohsawa et al., 1998).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_15",
            "start": 123,
            "end": 216,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_15@2",
            "content": "The graph is split into segments, representing the key concepts in the document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_15",
            "start": 218,
            "end": 297,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_15@3",
            "content": "Co-occurence graphs have also been used for automatic keyword extraction such as in RAKE (Rose et al., 2010) and can be also used for classification (Zhang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_15",
            "start": 299,
            "end": 468,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@0",
            "content": "Modern approaches exploit this idea in combination with graph neural networks (GNN) (Hamilton, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@1",
            "content": "Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al., 2020) as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), andDADGNN (Liu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 102,
            "end": 359,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@2",
            "content": "We briefly discuss these models: In TextGCN, the authors set up a graph based on word-word connections given by window-based pointwise mutual information and word-document TF-IDF scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 361,
            "end": 546,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@3",
            "content": "They use a one-hot encoding as node features and apply a two-layer graph convolutional network (Kipf and Welling, 2017) on the graph to carry out the node classification task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 548,
            "end": 722,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@4",
            "content": "HeteGCN combines ideas from Predictive Text Embedding (Tang et al., 2015) and TextGCN and split the adjacency matrix into its word-document and word-word sub-matrices and fuse the different layers' representations when required.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 724,
            "end": 951,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@5",
            "content": "TensorGCN uses multiple ways of converting text data into graph data including a semantic graph created with an LSTM, a syntactic graph created by dependency parsing, and a sequential graph based on word co-occurrence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 953,
            "end": 1170,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@6",
            "content": "HyperGAT extended the idea of text-induced graphs for text classification to hypergraphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 1172,
            "end": 1260,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@7",
            "content": "The model uses graph attention and two kinds of hyperedges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 1262,
            "end": 1320,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@8",
            "content": "Sequential hyperedges represent the relation between sentences and their words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 1322,
            "end": 1400,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@9",
            "content": "Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2001).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 1402,
            "end": 1497,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_16@10",
            "content": "Finally, DADGNN is a graph-based approach that uses attention diffusion and decoupling techniques to tackle oversmoothing of the GNN and to be able to stack more layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_16",
            "start": 1499,
            "end": 1667,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@0",
            "content": "In TextGCN's original transductive formulation, the entire graph including the test set needs to be known for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@1",
            "content": "This may be prohibitive in practical applications as each batch of new documents would require retraining the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 120,
            "end": 235,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@2",
            "content": "When these methods are adapted for inductive learning, where the test set is unseen, they achieve notably lower scores (Ragesh et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 237,
            "end": 377,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@3",
            "content": "GNNs for text classification use corpus statistics, e. g., pointwise mutual information (PMI), to connect related words in a graph (Yao et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 379,
            "end": 528,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@4",
            "content": "When these were omitted, the GNNs would collapse to bag-of-words MLPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 530,
            "end": 599,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@5",
            "content": "Thus, GNNs have access to more information than BoW-MLPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 601,
            "end": 657,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_17@6",
            "content": "GloVe (Pennington et al., 2014) also captures PMI corpus statistics, which is why we include an MLP on GloVe input representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_17",
            "start": 659,
            "end": 789,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@0",
            "content": "Sequence models: RNN and CNN Recurrent neural networks (RNN) are a natural choice for any NLP task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@1",
            "content": "However, it turned out to be challenging to find numbers reported on text categorization in the literature that can be used as references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 100,
            "end": 237,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@2",
            "content": "The bidirectional LSTM with two-dimensional max pooling BLSTM-2DCNN has been applied on a stripped-down to 4 classes version of the 20ng dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 239,
            "end": 383,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@3",
            "content": "Thus, the high score of 96.5 reported for 4ng cannot be compared with papers applied on the full 20ng dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 385,
            "end": 494,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@4",
            "content": "Also Text-RCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 496,
            "end": 628,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@5",
            "content": "The results of Text-RCNN are identical with BLSTM-2DCNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 630,
            "end": 685,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@6",
            "content": "For the MR dataset, BLSTM-2DCNN provides no information on the specific split of the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 687,
            "end": 779,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@7",
            "content": "RNN-Capsule is a sentiment analysis method reaching an accuracy of 83.8 on the MR dataset, but with a different train-test split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 781,
            "end": 909,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@8",
            "content": "Lyu and Liu (2020) combine a 2D-CNN with bidirectional RNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 911,
            "end": 969,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@9",
            "content": "Another work applying a combination of a convolutional layer and an LSTM layer is by Wang et al. (2019b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 971,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@10",
            "content": "The authors experiment with five English and two Chinese datasets, which are not in the set of representative datasets we identified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 1077,
            "end": 1209,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_18@11",
            "content": "The authors report that their approach outperforms existing models like fastText on two of the five English datasets and both Chinese datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_18",
            "start": 1211,
            "end": 1353,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_19@0",
            "content": "Sequence models: Transformers Surprisingly, only few works consider Transformer models for text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_19",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_19@1",
            "content": "A recent work shows that BERT outperforms classic TF-IDF BoW approaches on English, Chinese, and Portuguese text classification datasets (Gonz\u00e1lez-Carvajal and Garrido-Merch\u00e1n, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_19",
            "start": 112,
            "end": 294,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_19@2",
            "content": "We have not found any results of transformer-based models reported on those text categorization datasets that are commonly used in the graph-based approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_19",
            "start": 296,
            "end": 453,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@0",
            "content": "Therefore, we fine-tune BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019) on those datasets ourselves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@1",
            "content": "BERT is a large pretrained language model on the basis of Transformers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 115,
            "end": 185,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@2",
            "content": "Dis-tilBERT (Sanh et al., 2019) is a distilled version of BERT with 40% reduced parameters while retaining 97% of BERT's language understanding capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 187,
            "end": 343,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@3",
            "content": "TinyBERT (Jiao et al., 2020) and Mo-bileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 345,
            "end": 463,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@4",
            "content": "We chose Dis-tilBERT because it can be fine-tuned independently from the BERT teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 465,
            "end": 550,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_20@5",
            "content": "Its inference times are 60% faster than BERT, which makes it more likely to be reusable by labs with limited resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_20",
            "start": 552,
            "end": 670,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_21@0",
            "content": "Summary From our literature survey, we see that all recent methods are based on graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_21",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_21@1",
            "content": "BoW-based methods are hardly found in experiments, while, likewise surprisingly, Transformer-based sequence models are extremely scarce in the literature on topical text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_21",
            "start": 88,
            "end": 272,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_21@2",
            "content": "The recent surveys on text categorization include both classical and Deep Learning models, but none considered a simple MLP except for the inclusion of DAN (Iyyer et al., 2015) in Li et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_21",
            "start": 274,
            "end": 470,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_22@0",
            "content": "Models for Text Categorization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_22",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_23@0",
            "content": "We formally introduce the three families of models for text categorization, namely the BoW-based, graph-based, and sequence-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_23",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_23@1",
            "content": "Table 1 summarizes the key properties of the approaches: whether they require a synthetic graph, whether word position is reflected in the model, whether the model can deal with arbitrary length text, and whether the model is capable of inductive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_23",
            "start": 138,
            "end": 393,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_24@0",
            "content": "BoW-Based Text Categorization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_24",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_25@0",
            "content": "Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_25",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_25@1",
            "content": "Given paired training examples (x, y) \u2208 D, each consisting of a bag-of-words x \u2208 R n vocab and a class label y \u2208 Y, the goal is to learn a generalizable function \u0177 = f (BoW) \u03b8 (x) with parameters \u03b8 such that arg max( \u0177) preferably equals the true label y for input x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_25",
            "start": 155,
            "end": 421,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_26@0",
            "content": "As BoW-based model, we consider a one hidden layer WideMLP (i. e., two layers in total).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_26",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_26@1",
            "content": "We experiment with pure BoW, TF-IDF weighted, and averaged GloVe input representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_26",
            "start": 89,
            "end": 175,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_26@2",
            "content": "We also use a two hidden layers WideMLP-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_26",
            "start": 177,
            "end": 218,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_26@3",
            "content": "We list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_26",
            "start": 220,
            "end": 325,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_27@0",
            "content": "Graph-Based Text Categorization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_27",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@0",
            "content": "Graph-based text categorization approaches first set up a synthetic graph on the basis of the text corpus D in the form of an adjacency matrix \u00c2 := make-graph(D).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@1",
            "content": "For instance, in TextGCN the graph is set up in two parts: word-word connections are modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 163,
            "end": 357,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@2",
            "content": "Then, a parameterized function f (graph) \u03b8 (X, \u00c2) is learned that uses the graph as input, where X are the node features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 359,
            "end": 479,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@3",
            "content": "The graph is composed of word and document nodes, each receiving its own embedding (by setting X = I).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 481,
            "end": 582,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@4",
            "content": "In inductive learning, however, there is no embedding of the test documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 584,
            "end": 659,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@5",
            "content": "Note that the graph-based approaches from the current literature such as TextGCN also disregard word order, similar to the BoW-based models described above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 661,
            "end": 816,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_28@6",
            "content": "A detailed discussion of the connection between TextGCN and MLP is provided in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_28",
            "start": 818,
            "end": 907,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_29@0",
            "content": "We consider top performing graph-based models from the literature, namely TextGCN along with its successors HeteGCN, TensorGCN, HyperGAT, DADGNN, as well as simplified GCN (SGC) (Wu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_29",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_29@1",
            "content": "We do not run our own experiments for the graph-based models but rely on the original work and extensive studies by Ding et al. (2020) and Ragesh et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_29",
            "start": 197,
            "end": 356,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_30@0",
            "content": "Sequence-Based Text Categorization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_30",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_31@0",
            "content": "We consider RNNs, LSTMs, and Transformers as sequence-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_31",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_31@1",
            "content": "These models are aware of the order of the words in the input text in the sense that they are able to exploit word order information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_31",
            "start": 68,
            "end": 200,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_31@2",
            "content": "Thus, the key difference to the BoW-based and graph-based families is that the word order is reflected by sequence-based model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_31",
            "start": 202,
            "end": 328,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_31@3",
            "content": "The model sig-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_31",
            "start": 330,
            "end": 343,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_32@0",
            "content": "nature is \u0177 = f (sequence) \u03b8 ( x 1 , x 2 , . . . , x k ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_32",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_33@0",
            "content": "where k is the (maximum) sequence length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_33",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_33@1",
            "content": "Word position is modeled by a dedicated positional encoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_33",
            "start": 42,
            "end": 101,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_33@2",
            "content": "For instance, in BERT each position is associated with an embedding vector that is added to the word embedding at input level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_33",
            "start": 103,
            "end": 228,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_34@0",
            "content": "For the sequence-based models, we run our own experiments with BERT and DistilBERT, while reporting the scores of a pretrained LSTM from Ding et al. (2020) for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_34",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_35@0",
            "content": "Experimental Apparatus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_35",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_36@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_36",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@0",
            "content": "We use the same datasets and train-test split as in TextGCN (Yao et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@1",
            "content": "Those datasets are 20ng, R8, R52, ohsumed, and MR. Twenty Newsgroups (20ng) 1 (bydate version) contains long posts categorized into 20 newsgroups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 80,
            "end": 225,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@2",
            "content": "The mean sequence length is 551 words with a standard deviation (SD) of 2,047.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 227,
            "end": 304,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@3",
            "content": "R8 and R52 are subsets of the Reuters 21578 news dataset with 8 and 52 classes, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 306,
            "end": 398,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@4",
            "content": "The mean sequence length and SD is 119 \u00b1 128 words for R8, and 126 \u00b1 133 words for R52.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 400,
            "end": 486,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@5",
            "content": "Ohsumed 2 is a corpus of medical abstracts from the MEDLINE database that are categorized into diseases (one per abstract).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 488,
            "end": 610,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@6",
            "content": "The mean sequence length is 285 \u00b1 123 words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 612,
            "end": 655,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@7",
            "content": "Movie Reviews (MR) 3 (Pang and Lee, 2005), split by Tang et al. (2015), is a binary sentiment analysis dataset on sentence level (mean sequence length and SD: 25 \u00b1 11).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 657,
            "end": 824,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_37@8",
            "content": "Table 2 shows the dataset characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_37",
            "start": 826,
            "end": 867,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_38@0",
            "content": "Inductive and Transductive Setups",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_38",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@0",
            "content": "We distinguish between a transductive and an inductive setup for text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@1",
            "content": "In the transductive setup, as used in TextGCN, the test documents are visible and actually used for the preprocessing step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 86,
            "end": 208,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@2",
            "content": "In the inductive setting, the test documents remain unseen until test time (i. e., they are not available for preprocessing).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 210,
            "end": 334,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@3",
            "content": "We report the scores of the graph-based models for both setups from the literature, where available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 336,
            "end": 435,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@4",
            "content": "BoW-based and sequence-based models are inherently inductive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 437,
            "end": 497,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_39@5",
            "content": "Ragesh et al. (2021) have evaluated a variant of TextGCN that is capable of inductive learning, which we include in our results, too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_39",
            "start": 499,
            "end": 631,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_40@0",
            "content": "Procedure and Hyperparameter Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_40",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_41@0",
            "content": "We have extracted accuracy scores from the literature according to our systematic selection from Section 2. Below, we provide a detailed description of the procedure for the models that we have run ourselves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_41",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_41@1",
            "content": "We borrow the tokenization strategy from BERT (Devlin et al., 2019) along with its uncased vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_41",
            "start": 209,
            "end": 310,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_41@2",
            "content": "The tokenizer relies primarily on WordPiece (Wu et al., 2016) for a high coverage while maintaining a small vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_41",
            "start": 312,
            "end": 430,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@0",
            "content": "Training our BoW-Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@1",
            "content": "Our WideMLP has one hidden layer with 1,024 rectified linear units (one input-to-hidden and one hidden-to-output layer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 25,
            "end": 144,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@2",
            "content": "We apply dropout after each hidden layer, notably also after the initial embedding layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 146,
            "end": 234,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@3",
            "content": "Only for GloVe+WideMLP, neither dropout nor ReLU is applied to the frozen pretrained embeddings but only on subsequent layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 236,
            "end": 361,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@4",
            "content": "The variant WideMLP-2 has two ReLU-activated hidden layers (three layers in total) with 1, 024 hidden units each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 363,
            "end": 475,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@5",
            "content": "While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018;Nakkiran et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 477,
            "end": 706,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@6",
            "content": "In pre-experiments, we realized that MLPs are not very sensitive to hyperparameter choices.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 708,
            "end": 798,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_42@7",
            "content": "Therefore, we optimize crossentropy with Adam (Kingma and Ba, 2015) and its default learning rate of 10 \u22123 , a linearly decaying learning rate schedule and train for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity, along with a dropout ratio of 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_42",
            "start": 800,
            "end": 1131,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@0",
            "content": "Fine-tuning our BERT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@1",
            "content": "For BERT and DistilBERT, we fine-tune for 10 epochs with a linearly decaying learning rate of 5 \u2022 10 \u22125 and an effective batch size of 128 via gradient accumulation of 8 x 16 batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 29,
            "end": 211,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@2",
            "content": "We truncate all inputs to 512 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 213,
            "end": 249,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@3",
            "content": "To isolate the influence of word order on BERT's performance, we conduct two further ablations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 251,
            "end": 345,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@4",
            "content": "First, we set all position embeddings to zero and disable their gradient (BERT w/o pos ids).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 347,
            "end": 438,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@5",
            "content": "By doing this, we force BERT to operate on a bag-ofwords without any notion of word order or position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 440,
            "end": 541,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@6",
            "content": "Second, we shuffle each sequence to augment the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 543,
            "end": 604,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_43@7",
            "content": "We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuf. augm.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_43",
            "start": 606,
            "end": 724,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_44@0",
            "content": "Measures",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_44",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_45@0",
            "content": "We report accuracy as evaluation metric, which is equivalent to Micro-F1 in single-label classification (see Appendix C).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_45",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_45@1",
            "content": "We repeat all experiments five times with different random initialization of the parameters and report the mean and standard deviation of these five runs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_45",
            "start": 122,
            "end": 275,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_46@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_46",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_47@0",
            "content": "Effectiveness",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_47",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_48@0",
            "content": "Table 3 shows the accuracy scores for the text categorization models on the five datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_48",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_48@1",
            "content": "All graphbased models in the transductive setting show similar accuracy scores (maximum difference is 2 points).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_48",
            "start": 91,
            "end": 202,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_48@2",
            "content": "As expected, the scores decrease in the inductive setting up to a point where they are matched or even outperformed by our WideMLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_48",
            "start": 204,
            "end": 334,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_49@0",
            "content": "In the inductive setting, the WideMLP models perform best among the BoW models, in particular, TFIDF+WideMLP and WideMLP on an unweighted BoW.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_49",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_49@1",
            "content": "The best-performing graph-based model is HyperGAT, yet DADGNN has a slight advantage on R8, R52, and MR. For the sequencebased models, BERT attains the highest scores, closely followed by DistilBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_49",
            "start": 143,
            "end": 341,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_50@0",
            "content": "The strong performance of WideMLP rivals all graph-based techniques reported in the literature, in particular, the recently published graph-inducing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_50",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_50@1",
            "content": "MLP only falls behind HyperGAT, which relies on topic models to set up the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_50",
            "start": 158,
            "end": 238,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_50@2",
            "content": "Another observation is that 1 hidden layer (but wide) is sufficient for the tasks, as the scores for MLP variants with 2 hidden layers are lower.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_50",
            "start": 240,
            "end": 384,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_50@3",
            "content": "We further observe that both pure BoW and TF-IDF weighted BoW lead to better results than approaches that exploit pretrained word embeddings such as GloVe-MLP, fastText, and SWEM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_50",
            "start": 386,
            "end": 564,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_51@0",
            "content": "With its immense pretraining, BERT yields the overall highest scores, closely followed by Distil-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_51",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_51@1",
            "content": "DistilBERT outperforms HyperGAT by 7 points on the MR dataset while being on par on the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_51",
            "start": 103,
            "end": 197,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_51@2",
            "content": "BERT outperforms the strongest graphbased competitor, HyperGAT, by 8 points on MR, 1.5 points on ohsumed, 1 point on R52 and R8, and 0.5 points on 20ng.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_51",
            "start": 199,
            "end": 350,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_52@0",
            "content": "Our results further confirm that position embeddings are important for BERT with a notable decrease when those are omitted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_52",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_52@1",
            "content": "Augmenting the data with shuffled sequences has led to neither a consistent decrease nor increase in performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_52",
            "start": 124,
            "end": 236,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_53@0",
            "content": "Efficiency",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_53",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_54@0",
            "content": "Parameter Count of the Models Table 4 lists the parameter counts of the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_54",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_54@1",
            "content": "Even though the MLP is fully-connected on top of a bag-ofwords with the dimensionality of the vocabulary size, it has only half of the parameters as Distil-BERT and a quarter of the parameters of BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_54",
            "start": 80,
            "end": 280,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_54@2",
            "content": "Using TF-IDF does not change the number of model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_54",
            "start": 282,
            "end": 341,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_54@3",
            "content": "Due to the high vocabulary size, GloVe-based models have a high number of parameters, but the majority of those is frozen, i. e., does not get gradient updates during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_54",
            "start": 343,
            "end": 518,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_55@0",
            "content": "Runtime Performance of the Models We provide the total running times in Table 5 as observed while conducting the experiments on a single NVIDIA A100-SXM4-40GB card.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_55",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_55@1",
            "content": "All WideMLP variants are an order of magnitude faster than Dis-tilBERT when considering the average runtime per epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_55",
            "start": 165,
            "end": 282,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_55@2",
            "content": "DistilBERT is twice as fast as the original BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_55",
            "start": 284,
            "end": 332,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_55@3",
            "content": "The transformers are only faster than BoW models on the MR dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_55",
            "start": 334,
            "end": 400,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_55@4",
            "content": "This is because the sequences in the MR dataset are much shorter and less O(L 2 ) attention weights have to be computed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_55",
            "start": 402,
            "end": 521,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_56@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_56",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@0",
            "content": "Key Insights Our experiments show that our MLP models using BoW outperform the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@1",
            "content": "Furthermore, the MLP models are comparable to Hyper-GAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 170,
            "end": 225,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@2",
            "content": "Only transformer-based BERT and Distil-BERT models outperform our MLP and set a new state-of-the-art.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 227,
            "end": 327,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@3",
            "content": "This result is important for two reasons: First, the strong performance of a pure BoW-MLP questions the added value of synthetic graphs in models like TextGCN to the text categorization task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 329,
            "end": 519,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@4",
            "content": "Only HyperGAT, which uses the expensive Latent Dirichlet Allocation for computing the graph, slightly outperforms our BoW-WideMLP in two out of five datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 521,
            "end": 678,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_57@5",
            "content": "Thus, we argue that using strong baseline models for text classification is important to assess the true scientific advancement (Dacrema et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_57",
            "start": 680,
            "end": 830,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_58@0",
            "content": "Second, in contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_58",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_58@1",
            "content": "Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_58",
            "start": 203,
            "end": 314,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_58@2",
            "content": "Furthermore, we experiment with more hidden layers (see WideMLP-2), but do not observe any improvement when the single hidden layer is sufficiently wide.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_58",
            "start": 316,
            "end": 468,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_58@3",
            "content": "A possible explanation is that already a single hidden layer is sufficient to approximate any compact function to an arbitrary degree of accuracy depending on the width of the hidden layer (Cybenko, 1989).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_58",
            "start": 470,
            "end": 674,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_59@0",
            "content": "Finally, a new state-of-the-art is set by the transformer model BERT, which is not very surpris- ing. However, as our efficiency analysis shows, the MLPs require only a fraction of the parameters and are faster in their combined training and inference time except for the MR dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_59",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_59@1",
            "content": "The attention mechanism of (standard) Transformers is quadratic in the sequence length, which leads to slower processing of long sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_59",
            "start": 284,
            "end": 422,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_59@2",
            "content": "With larger batches, the speed of the MLP could be increased even further.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_59",
            "start": 424,
            "end": 497,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@0",
            "content": "Graph-based models come with high training costs, as not only the graph has to be first computed, but also a GNN has to be trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@1",
            "content": "For standard GNN methods, the whole graph has to fit into the GPU memory and mini-batching is nontrivial, but possible with dedicated sampling techniques for GNNs (Fey et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 132,
            "end": 313,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@2",
            "content": "Furthermore, the original TextGCN is inherently transductive, i. e., it has to be retrained whenever new documents appear.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 315,
            "end": 436,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@3",
            "content": "Strictly transductive models are effectively useless in practice (Lu et al., 2019) except for applications, in which a partially labeled corpus needs to be fully annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 438,
            "end": 609,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@4",
            "content": "However, recent extensions such as HeteGCN, Hyper-GAT, and DADGNN already relax this constraint and enable inductive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 611,
            "end": 736,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_60@5",
            "content": "Nevertheless, worddocument graphs require O(N 2 ) space, where N is the number of documents plus the vocabulary size, which is a hurdle for large-scale applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_60",
            "start": 738,
            "end": 902,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_61@0",
            "content": "There are also tasks where the natural structure of the graph data provides more information than the mere text, e. g., citations networks or connections in social graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_61",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_61@1",
            "content": "In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Velickovic et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_61",
            "start": 172,
            "end": 407,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_61@2",
            "content": "GNNs also find application in various NLP tasks, other than classification (Wu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_61",
            "start": 409,
            "end": 501,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_62@0",
            "content": "An interesting factor is the ability of the models to capture word order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_62",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_62@1",
            "content": "BoW models disregard word order entirely and yield good results, but still fall behind order-aware Transformer models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_62",
            "start": 74,
            "end": 191,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_62@2",
            "content": "In an extensive study, Conneau et al. (2018) have shown that memorizing the word content (which words appear at all) is most indicative of downstream task performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_62",
            "start": 193,
            "end": 359,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_62@3",
            "content": "Sinha et al. (2021) have experimented with pretraining BERT by disabling word order during pretraining and show that it makes surprisingly little difference for fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_62",
            "start": 361,
            "end": 533,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_62@4",
            "content": "In their study, word order is preserved during fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_62",
            "start": 535,
            "end": 593,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_63@0",
            "content": "In our experiments, we have conducted complementary experiments: we have used a BERT model that is pretrained with word order, but we have deactivated the position encoding during fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_63",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_64@0",
            "content": "Our results show that there is a notable drop in performance but the model does not fail completely.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_64",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_65@0",
            "content": "Other NLP tasks such as question answering (Rajpurkar et al., 2016) or natural language inference (Wang et al., 2019a) can also be regarded as text classification on a technical level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_65",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_65@1",
            "content": "Here, the positional information of the sequence is more important than for pure topical text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_65",
            "start": 185,
            "end": 293,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_65@2",
            "content": "One can expect that BoW-based models perform worse than sequence-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_65",
            "start": 295,
            "end": 372,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_66@0",
            "content": "Generalizability We expect that similar observations would be made on other text classification datasets because we have already covered a range of different characteristics: long and short texts, topical categorization (20ng, Reuters, and Ohsumed) and sentiment prediction (MR) in the domains of forum postings, news, movie reviews, and medical abstracts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_66",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_66@1",
            "content": "Our results are in line with those from other fields, who have reported a resurgence of MLPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_66",
            "start": 357,
            "end": 449,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_66@2",
            "content": "For example, in business prediction, an MLP baseline outperforms various other Deep Learning models (Venugopal et al., 2021;Yedida et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_66",
            "start": 451,
            "end": 595,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_66@3",
            "content": "In computer vision, Tolstikhin et al. ( 2021) and Melas-Kyriazi (2021) proposed attention-free MLP models that are on par with the Vision Transformer .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_66",
            "start": 597,
            "end": 747,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_66@4",
            "content": "In natural language processing, Liu et al. (2021a) show similar results, while acknowledging that a small attention module is necessary for some tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_66",
            "start": 749,
            "end": 899,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@0",
            "content": "We acknowledge that the experimental datasets are limited to English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@1",
            "content": "While word order is important in the English language, it is notable that methods that discard word order still work well for text categorization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 70,
            "end": 215,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@2",
            "content": "Another possible bias is the comparability of the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 217,
            "end": 274,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@3",
            "content": "However, we carefully checked all relevant parameters such as the train/test split, the number of classes in the datasets, if datasets have been pre-processed in such a way that, e. g., makes a task easier like reducing the number of classes, the training procedure, and the reported evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 276,
            "end": 578,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@4",
            "content": "Regarding our efficency analysis, we made sure to report numbers for the parameter count and a measure for the speed other than FLOPs, as recommended by .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 580,
            "end": 733,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_67@5",
            "content": "Since runtime is heavily dependant on training parameters such as batch size, we complement this with asymptotic complexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_67",
            "start": 735,
            "end": 858,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_68@0",
            "content": "Practical Impact and Future Work Our study has an immediate impact on practitioners who seek to employ robust text categorization models in research projects and in industrial operational environments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_68",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_68@1",
            "content": "Furthermore, we advocate to use an MLP baseline in future text categorization research, for which we provide concrete guidelines in Appendix A. As future work, it would be interesting to analyze multi-label classification tasks and to compare with hierarchical text categorization methods (Peng et al., 2018;Xiao et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_68",
            "start": 202,
            "end": 528,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_68@2",
            "content": "Another interesting yet challenging setting would be few-shot classification (Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_68",
            "start": 530,
            "end": 627,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_69@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_69",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_70@0",
            "content": "We argue that a wide multi-layer perceptron enhanced with today's best practices should be considered as a strong baseline for text classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_70",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_70@1",
            "content": "In fact, the experiments show that our WideMLP is oftentimes on-par or even better than recently proposed models that synthesize a graph structure from the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_70",
            "start": 154,
            "end": 314,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_71@0",
            "content": "The source code is available online: https://github.com/lgalke/ text-clf-baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_71",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_72@0",
            "content": "The focus of this work is text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_72",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_72@1",
            "content": "Potential risks that apply to text classification in general also apply to this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_72",
            "start": 47,
            "end": 131,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_72@2",
            "content": "Nonetheless, we present alternatives to commonly used pretrained language models, which suffer from various sources of bias due to the large and poorly manageable data used for pretraining (Bender et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_72",
            "start": 133,
            "end": 343,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_72@3",
            "content": "In contrast, the presented alternatives render full control over the training data and, thus, contribute to circumvent the biases otherwise introduced during pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_72",
            "start": 345,
            "end": 514,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_73@0",
            "content": "On the basis of our results, we provide recommendations for designing a WideMLP baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_73",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_74@0",
            "content": "We recommend using modern subword tokenizers such as BERT-like WordPiece or SentencePiece that yield a high coverage while needing a relatively small vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_74",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_75@0",
            "content": "In contrast to conventional wisdom (Iyyer et al., 2015), we find that pretrained embeddings, e. g., GloVe, can have a detrimental effect when compared to using an MLP with one wide hidden layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_75",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_75@1",
            "content": "Such an MLP circumvents the bottleneck of the small dimensionality of word embeddings and has a higher capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_75",
            "start": 195,
            "end": 306,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_76@0",
            "content": "Depth vs. Width In text classification, width seems more important than depth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_76",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_76@1",
            "content": "We recommend to use a single, wide hidden layer, i. e., one input-tohidden and one hidden-to-output layer, e. g., with 1,024 hidden units and ReLU activation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_76",
            "start": 79,
            "end": 236,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_76@2",
            "content": "While this might be overparameterized for single-label text classification tasks with few classes, we rely on recent findings that overparameterization leads to better generalization (Neyshabur et al., 2018;Nakkiran et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_76",
            "start": 238,
            "end": 467,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_76@3",
            "content": "We further motivate the choice of using wide layers with results from multi-label text classification (Galke et al., 2017), which has shown that a (wide) MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_76",
            "start": 469,
            "end": 728,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_76@4",
            "content": "Follow-up work (Mai et al., 2018) then found that also CNN and LSTM do not substantially improve over the wide MLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_76",
            "start": 730,
            "end": 844,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_77@0",
            "content": "Having a fully-connected layer on-top of a bagof-words leads to a high number of learnable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_77",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_77@1",
            "content": "Still, the wide first input-to-hidden layer can be implemented efficiently by using an embedding layer followed by aggregation, which avoids large matrix multiplications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_77",
            "start": 103,
            "end": 272,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_78@0",
            "content": "In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might be beneficial for other, more challenging datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_78",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_79@0",
            "content": "We seek to find an optimization strategy that does not require dataset-specific hyperparameter tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_79",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_79@1",
            "content": "This comprises optimizing cross-entropy with Adam (Kingma and Ba, 2015) and default learning rate 10 \u22123 , a linearly decaying learning rate schedule and training for a high amount of steps (Nakkiran et al., 2020) (we use 100 epochs) with small batch sizes (we use 16) for sufficient stochasticity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_79",
            "start": 103,
            "end": 399,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_80@0",
            "content": "For regularization during this prolonged training, we suggest to use a high dropout ratio of 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_80",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_80@1",
            "content": "Regarding initialization, we rely on framework defaults, i. e., N (0, 1) for the initial embedding layer and random uniform U(\u2212 d input , d output ) for subsequent layers' weight and bias parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_80",
            "start": 98,
            "end": 296,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_81@0",
            "content": "TextGCN uses the PMI matrix to set up edge weights for word-word connections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_81",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_81@1",
            "content": "A single layer Text-GCN is a BoW-MLP, except for the document embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_81",
            "start": 78,
            "end": 149,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_81@2",
            "content": "The one-hop neighbors are words which are aggregated after a nonlinear transform.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_81",
            "start": 151,
            "end": 231,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_82@0",
            "content": "The basic GCN equation H = \u03c3( \u00c2XW ) reveals that the order of transformation and neighborhood aggregation is irrelevant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_82",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_82@1",
            "content": "The document embedding implies that TextGCN is a semisupervised technique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_82",
            "start": 121,
            "end": 194,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_82@2",
            "content": "Truly new documents, as in inductive learning scenarios, would need a special treatment such as using an all zero embedding vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_82",
            "start": 196,
            "end": 326,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_83@0",
            "content": "A two-layer MLP can be characterized by the equation \u0177 = W (2) \u03c3(W (1) x + b (1) ) + b (2) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_83",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_83@1",
            "content": "On bag-of-words inputs, the first layer W (1) x + b (1) can be replaced by an equivalent embedding layer with weighting (e. g., TF-IDF or length normalization) being applied during aggregation of the embedding vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_83",
            "start": 93,
            "end": 310,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_84@0",
            "content": "The first layer of TextGCN is equivalent to aggregating embedding vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_84",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_84@1",
            "content": "A standard GCN layer with shared weights has the form (assuming self-loops have been inserted)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_84",
            "start": 75,
            "end": 168,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_85@0",
            "content": "h i = j\u2208N (i) a ij W (1) x j + b (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_85",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_86@0",
            "content": "Now in TextGCN node features are given by the identity, such that x j is one-hot. Then we can rewrite the first layer of Text-GCN as an aggregation of embeddings E. We gain",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_86",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_87@0",
            "content": "h i = j\u2208N (i)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_87",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_88@0",
            "content": "a ij E j as W x + b may again be replaced by an embedding matrix if applied to one hot vectors x. Now E contains two types of embedding vectors: word embeddings and document embeddings corresponding to word nodes and document nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_88",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_88@1",
            "content": "We see that the first layer of TextGCN is essentially an aggregation of word embeddings plus the document embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_88",
            "start": 233,
            "end": 348,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_88@2",
            "content": "Only with a second layer, TextGCN considers the embedding of other documents whose words are connected to the present documents' words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_88",
            "start": 350,
            "end": 484,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_89@0",
            "content": "In multiclass classification, we have a single true label for each instance and the predictions are constrained to a single prediction per instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_89",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_89@1",
            "content": "As a consequence, the measures accuracy and Micro-F1 coincide to the same formula.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_89",
            "start": 149,
            "end": 230,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_89@2",
            "content": "Micro-F1 aggregates true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) globally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_89",
            "start": 232,
            "end": 349,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_89@3",
            "content": "It can be expressed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_89",
            "start": 351,
            "end": 373,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_90@0",
            "content": "Micro-F 1 = 2 c TP c 2 c TP c + c FP c + c FN c ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_90",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_91@0",
            "content": "where c iterates over all classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_91",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_92@0",
            "content": "While the accuracy can be expressed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_92",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_93@0",
            "content": "Acc = c TP c + c TN c c TP c + c TN c + c FP c + c FN c",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_93",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_94@0",
            "content": "In multiclass classification, every true positive is also a true negative for all other classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_94",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_94@1",
            "content": "When summing those up over the entire dataset, we obtain",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_94",
            "start": 97,
            "end": 152,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_95@0",
            "content": "UNKNOWN, None, , 2021. A survey on data augmentation for text classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_95",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_96@0",
            "content": "Emily Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_96",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_97@0",
            "content": "David Blei, Andrew Ng, Michael Jordan, Latent dirichlet allocation, 2001-12-03, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, MIT Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_97",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_98@0",
            "content": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, Enriching word vectors with subword information, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_98",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_99@0",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Language models are fewshot learners, 2020-12-06, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_99",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_100@0",
            "content": "Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00efc Barrault, Marco Baroni, What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_100",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_101@0",
            "content": "George Cybenko, Approximation by superpositions of a sigmoidal function, 1989, Math. Control. Signals Syst, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_101",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_102@0",
            "content": "Paolo Maurizio Ferrari Dacrema, Dietmar Cremonesi,  Jannach, Are we really making much progress? A worrying analysis of recent neural recommendation approaches, 2019-09-16, Proceedings of the 13th ACM Conference on Recommender Systems, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_102",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_103@0",
            "content": "UNKNOWN, None, 2021, The efficiency misnomer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_103",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_104@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_104",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_105@0",
            "content": "Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, Huan Liu, Be more with less: Hypergraph attention networks for inductive text classification, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_105",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_106@0",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_106",
            "start": 0,
            "end": 377,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_107@0",
            "content": "Matthias Fey, Jan Lenssen, Frank Weichert, Jure Leskovec, GNNAutoScale: Scalable and expressive graph neural networks via historical embeddings, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_107",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_108@0",
            "content": "UNKNOWN, None, 2021, A practical survey on faster and lighter transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_108",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_109@0",
            "content": "Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunsch, Ansgar Scherp, Using titles vs. fulltext as source for automated semantic document annotation, 2017-12-04, Proceedings of the Knowledge Capture Conference, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_109",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_110@0",
            "content": "Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao, Deep learning-based text classification: A comprehensive review, 2021, ACM Comput. Surv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_110",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_111@0",
            "content": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Deep double descent: Where bigger models and more data hurt, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_111",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2018, Towards understanding the role of over-parametrization in generalization of neural networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_112",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_113@0",
            "content": "Yukio Ohsawa, Nels Benson, Masahiko Yachida, Keygraph: Automatic indexing by cooccurrence graph based on building construction metaphor, 1998-04-22, Proceedings of the IEEE Forum on Research and Technology Advances in Digital Libraries, IEEE ADL '98, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_113",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_114@0",
            "content": "Bo Pang, Lillian Lee, Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales, 2005, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_114",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_115@0",
            "content": "Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Large-scale hierarchical text classification with recursively regularized deep graph-cnn, 2018-04-23, Proceedings of the 2018 World Wide Web Conference on World Wide Web, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_115",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_116@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_116",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_117@0",
            "content": "Rahul Ragesh, Sundararajan Sellamanickam, Arun Iyer, Ramakrishna Bairi, Vijay Lingam, HeteGCN: Heterogeneous graph convolutional networks for text classification, 2021-03-08, WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_117",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_118@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_118",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_119@0",
            "content": "Stuart Rose, Dave Engel, Nick Cramer, Wendy Cowley, Automatic keyword extraction from individual documents, 2010, Text Mining, John Wiley & Sons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_119",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_120@0",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_120",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2018, Pitfalls of graph neural network evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_121",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_122@0",
            "content": "Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin, Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_122",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_123@0",
            "content": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_123",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_124@0",
            "content": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, MobileBERT: a compact task-agnostic BERT for resource-limited devices, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_124",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_125@0",
            "content": "Jian Tang, Meng Qu, Qiaozhu Mei, PTE: predictive text embedding through large-scale heterogeneous text networks, 2015-08-10, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_125",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_126@0",
            "content": "UNKNOWN, None, 2020, Efficient transformers: A survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_126",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2021, MLP-Mixer: An all-MLP architecture for vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_127",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_128@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_128",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_129@0",
            "content": "Illia Kaiser,  Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_129",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_130@0",
            "content": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018-04-30, 6th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_130",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_131@0",
            "content": "Ishwar Venugopal, Jessica T\u00f6llich, Michael Fairbank, Ansgar Scherp, A comparison of deeplearning methods for analysing and predicting business processes, 2021-07-18, International Joint Conference on Neural Networks, IJCNN 2021, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_131",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_132@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_132",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_133@0",
            "content": "Ruishuang Wang, Zhao Li, Jian Cao, Tong Chen, Lei Wang, Convolutional recurrent neural networks for text classification, 2019, International Joint Conference on Neural Networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_133",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_134@0",
            "content": "UNKNOWN, None, 2019, , IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_134",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_135@0",
            "content": "Yequan Wang, Aixin Sun, Jialong Han, Ying Liu, Xiaoyan Zhu, Sentiment analysis by capsules, 2018-04-23, Proceedings of the 2018 World Wide Web Conference on World Wide Web, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_135",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_136@0",
            "content": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, Simplifying graph convolutional networks, 2019-06, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_136",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_137@0",
            "content": "UNKNOWN, None, 2021, Graph neural networks for natural language processing: A survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_137",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_138@0",
            "content": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Google's neural machine translation system: Bridging the gap between human and machine translation, 2016, ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_138",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_139@0",
            "content": "Huiru Xiao, Xin Liu, Yangqiu Song, Efficient path prediction for semi-supervised and weakly supervised hierarchical text classification, 2019-05-13, The World Wide Web Conference, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_139",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_140@0",
            "content": "Liang Yao, Chengsheng Mao, Yuan Luo, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 2019-01-27, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_140",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_141@0",
            "content": "UNKNOWN, None, 2021, When SIMPLE is better than complex: A case study on deep learning for predicting Bugzilla issue close time, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_141",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_142@0",
            "content": "Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, Yuxin Zhou, Bayesian performance comparison of text classifiers, 2016-07-17, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_142",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_143@0",
            "content": "Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou, Weakly-supervised text classification based on keyword graph, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_143",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_144@0",
            "content": "Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, Bo Xu, Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_144",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "133-ARR_v2_145@0",
            "content": "Xujuan Zhou, Raj Gururajan, Yuefeng Li, Revathi Venkataraman, Xiaohui Tao, Ghazal Bargshady, Prabal Datta Barua, and Srinivas Kondalsamy-Chennakesavan. 2020. A survey on text classification and its applications, , Web Intell, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "133-ARR_v2_145",
            "start": 0,
            "end": 226,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_1",
            "tgt_ix": "133-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_1",
            "tgt_ix": "133-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_4",
            "tgt_ix": "133-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_5",
            "tgt_ix": "133-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_6",
            "tgt_ix": "133-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_7",
            "tgt_ix": "133-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_10",
            "tgt_ix": "133-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_14",
            "tgt_ix": "133-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_15",
            "tgt_ix": "133-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_19",
            "tgt_ix": "133-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_21",
            "tgt_ix": "133-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_23",
            "tgt_ix": "133-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_25",
            "tgt_ix": "133-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_24",
            "tgt_ix": "133-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_24",
            "tgt_ix": "133-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_24",
            "tgt_ix": "133-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_26",
            "tgt_ix": "133-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_27",
            "tgt_ix": "133-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_27",
            "tgt_ix": "133-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_27",
            "tgt_ix": "133-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_29",
            "tgt_ix": "133-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_31",
            "tgt_ix": "133-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_32",
            "tgt_ix": "133-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_33",
            "tgt_ix": "133-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_34",
            "tgt_ix": "133-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_36",
            "tgt_ix": "133-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_36",
            "tgt_ix": "133-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_38",
            "tgt_ix": "133-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_38",
            "tgt_ix": "133-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_41",
            "tgt_ix": "133-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_40",
            "tgt_ix": "133-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_40",
            "tgt_ix": "133-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_40",
            "tgt_ix": "133-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_40",
            "tgt_ix": "133-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_44",
            "tgt_ix": "133-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_44",
            "tgt_ix": "133-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_45",
            "tgt_ix": "133-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_46",
            "tgt_ix": "133-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_46",
            "tgt_ix": "133-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_48",
            "tgt_ix": "133-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_49",
            "tgt_ix": "133-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_50",
            "tgt_ix": "133-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_51",
            "tgt_ix": "133-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_46",
            "tgt_ix": "133-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_52",
            "tgt_ix": "133-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_54",
            "tgt_ix": "133-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_53",
            "tgt_ix": "133-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_53",
            "tgt_ix": "133-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_53",
            "tgt_ix": "133-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_58",
            "tgt_ix": "133-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_61",
            "tgt_ix": "133-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_63",
            "tgt_ix": "133-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_64",
            "tgt_ix": "133-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_65",
            "tgt_ix": "133-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_59",
            "tgt_ix": "133-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_68",
            "tgt_ix": "133-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_70",
            "tgt_ix": "133-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_71",
            "tgt_ix": "133-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_72",
            "tgt_ix": "133-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_73",
            "tgt_ix": "133-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_75",
            "tgt_ix": "133-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_77",
            "tgt_ix": "133-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_74",
            "tgt_ix": "133-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_79",
            "tgt_ix": "133-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_78",
            "tgt_ix": "133-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_81",
            "tgt_ix": "133-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_82",
            "tgt_ix": "133-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_83",
            "tgt_ix": "133-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_84",
            "tgt_ix": "133-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_85",
            "tgt_ix": "133-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_86",
            "tgt_ix": "133-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_87",
            "tgt_ix": "133-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_80",
            "tgt_ix": "133-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_89",
            "tgt_ix": "133-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_90",
            "tgt_ix": "133-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_91",
            "tgt_ix": "133-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_92",
            "tgt_ix": "133-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_93",
            "tgt_ix": "133-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_88",
            "tgt_ix": "133-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "133-ARR_v2_0",
            "tgt_ix": "133-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_1",
            "tgt_ix": "133-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_2",
            "tgt_ix": "133-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_3",
            "tgt_ix": "133-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_4",
            "tgt_ix": "133-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_4",
            "tgt_ix": "133-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_5",
            "tgt_ix": "133-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_5",
            "tgt_ix": "133-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_5",
            "tgt_ix": "133-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_5",
            "tgt_ix": "133-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_6",
            "tgt_ix": "133-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_6",
            "tgt_ix": "133-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_7",
            "tgt_ix": "133-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_7",
            "tgt_ix": "133-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_7",
            "tgt_ix": "133-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_7",
            "tgt_ix": "133-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_8",
            "tgt_ix": "133-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_9",
            "tgt_ix": "133-ARR_v2_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_10",
            "tgt_ix": "133-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_10",
            "tgt_ix": "133-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_10",
            "tgt_ix": "133-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_10",
            "tgt_ix": "133-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_11",
            "tgt_ix": "133-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_12",
            "tgt_ix": "133-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_13",
            "tgt_ix": "133-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_14",
            "tgt_ix": "133-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_14",
            "tgt_ix": "133-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_15",
            "tgt_ix": "133-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_15",
            "tgt_ix": "133-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_15",
            "tgt_ix": "133-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_15",
            "tgt_ix": "133-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_16",
            "tgt_ix": "133-ARR_v2_16@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_17",
            "tgt_ix": "133-ARR_v2_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_18",
            "tgt_ix": "133-ARR_v2_18@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_19",
            "tgt_ix": "133-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_19",
            "tgt_ix": "133-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_19",
            "tgt_ix": "133-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_20",
            "tgt_ix": "133-ARR_v2_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_21",
            "tgt_ix": "133-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_21",
            "tgt_ix": "133-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_21",
            "tgt_ix": "133-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_22",
            "tgt_ix": "133-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_23",
            "tgt_ix": "133-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_23",
            "tgt_ix": "133-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_24",
            "tgt_ix": "133-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_25",
            "tgt_ix": "133-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_25",
            "tgt_ix": "133-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_26",
            "tgt_ix": "133-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_26",
            "tgt_ix": "133-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_26",
            "tgt_ix": "133-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_26",
            "tgt_ix": "133-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_27",
            "tgt_ix": "133-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_28",
            "tgt_ix": "133-ARR_v2_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_29",
            "tgt_ix": "133-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_29",
            "tgt_ix": "133-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_30",
            "tgt_ix": "133-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_31",
            "tgt_ix": "133-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_31",
            "tgt_ix": "133-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_31",
            "tgt_ix": "133-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_31",
            "tgt_ix": "133-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_32",
            "tgt_ix": "133-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_33",
            "tgt_ix": "133-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_33",
            "tgt_ix": "133-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_33",
            "tgt_ix": "133-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_34",
            "tgt_ix": "133-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_35",
            "tgt_ix": "133-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_36",
            "tgt_ix": "133-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_37",
            "tgt_ix": "133-ARR_v2_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_38",
            "tgt_ix": "133-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_39",
            "tgt_ix": "133-ARR_v2_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_40",
            "tgt_ix": "133-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_41",
            "tgt_ix": "133-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_41",
            "tgt_ix": "133-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_41",
            "tgt_ix": "133-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_42",
            "tgt_ix": "133-ARR_v2_42@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_43",
            "tgt_ix": "133-ARR_v2_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_44",
            "tgt_ix": "133-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_45",
            "tgt_ix": "133-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_45",
            "tgt_ix": "133-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_46",
            "tgt_ix": "133-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_47",
            "tgt_ix": "133-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_48",
            "tgt_ix": "133-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_48",
            "tgt_ix": "133-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_48",
            "tgt_ix": "133-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_49",
            "tgt_ix": "133-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_49",
            "tgt_ix": "133-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_50",
            "tgt_ix": "133-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_50",
            "tgt_ix": "133-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_50",
            "tgt_ix": "133-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_50",
            "tgt_ix": "133-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_51",
            "tgt_ix": "133-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_51",
            "tgt_ix": "133-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_51",
            "tgt_ix": "133-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_52",
            "tgt_ix": "133-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_52",
            "tgt_ix": "133-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_53",
            "tgt_ix": "133-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_54",
            "tgt_ix": "133-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_54",
            "tgt_ix": "133-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_54",
            "tgt_ix": "133-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_54",
            "tgt_ix": "133-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_55",
            "tgt_ix": "133-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_56",
            "tgt_ix": "133-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_57",
            "tgt_ix": "133-ARR_v2_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_58",
            "tgt_ix": "133-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_58",
            "tgt_ix": "133-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_58",
            "tgt_ix": "133-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_58",
            "tgt_ix": "133-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_59",
            "tgt_ix": "133-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_59",
            "tgt_ix": "133-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_59",
            "tgt_ix": "133-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_60",
            "tgt_ix": "133-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_61",
            "tgt_ix": "133-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_61",
            "tgt_ix": "133-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_61",
            "tgt_ix": "133-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_62",
            "tgt_ix": "133-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_63",
            "tgt_ix": "133-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_64",
            "tgt_ix": "133-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_65",
            "tgt_ix": "133-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_65",
            "tgt_ix": "133-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_65",
            "tgt_ix": "133-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_66",
            "tgt_ix": "133-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_67",
            "tgt_ix": "133-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_68",
            "tgt_ix": "133-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_68",
            "tgt_ix": "133-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_68",
            "tgt_ix": "133-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_69",
            "tgt_ix": "133-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_70",
            "tgt_ix": "133-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_70",
            "tgt_ix": "133-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_71",
            "tgt_ix": "133-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_72",
            "tgt_ix": "133-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_72",
            "tgt_ix": "133-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_72",
            "tgt_ix": "133-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_72",
            "tgt_ix": "133-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_73",
            "tgt_ix": "133-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_74",
            "tgt_ix": "133-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_75",
            "tgt_ix": "133-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_75",
            "tgt_ix": "133-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_76",
            "tgt_ix": "133-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_77",
            "tgt_ix": "133-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_77",
            "tgt_ix": "133-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_78",
            "tgt_ix": "133-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_79",
            "tgt_ix": "133-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_79",
            "tgt_ix": "133-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_80",
            "tgt_ix": "133-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_80",
            "tgt_ix": "133-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_81",
            "tgt_ix": "133-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_81",
            "tgt_ix": "133-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_81",
            "tgt_ix": "133-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_82",
            "tgt_ix": "133-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_82",
            "tgt_ix": "133-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_82",
            "tgt_ix": "133-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_83",
            "tgt_ix": "133-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_83",
            "tgt_ix": "133-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_84",
            "tgt_ix": "133-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_84",
            "tgt_ix": "133-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_85",
            "tgt_ix": "133-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_86",
            "tgt_ix": "133-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_87",
            "tgt_ix": "133-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_88",
            "tgt_ix": "133-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_88",
            "tgt_ix": "133-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_88",
            "tgt_ix": "133-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_89",
            "tgt_ix": "133-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_89",
            "tgt_ix": "133-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_89",
            "tgt_ix": "133-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_89",
            "tgt_ix": "133-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_90",
            "tgt_ix": "133-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_91",
            "tgt_ix": "133-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_92",
            "tgt_ix": "133-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_93",
            "tgt_ix": "133-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_94",
            "tgt_ix": "133-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_94",
            "tgt_ix": "133-ARR_v2_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_95",
            "tgt_ix": "133-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_96",
            "tgt_ix": "133-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_97",
            "tgt_ix": "133-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_98",
            "tgt_ix": "133-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_99",
            "tgt_ix": "133-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_100",
            "tgt_ix": "133-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_101",
            "tgt_ix": "133-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_102",
            "tgt_ix": "133-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_103",
            "tgt_ix": "133-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_104",
            "tgt_ix": "133-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_105",
            "tgt_ix": "133-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_106",
            "tgt_ix": "133-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_107",
            "tgt_ix": "133-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_108",
            "tgt_ix": "133-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_109",
            "tgt_ix": "133-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_110",
            "tgt_ix": "133-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_111",
            "tgt_ix": "133-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_112",
            "tgt_ix": "133-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_113",
            "tgt_ix": "133-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_114",
            "tgt_ix": "133-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_115",
            "tgt_ix": "133-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_116",
            "tgt_ix": "133-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_117",
            "tgt_ix": "133-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_118",
            "tgt_ix": "133-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_119",
            "tgt_ix": "133-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_120",
            "tgt_ix": "133-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_121",
            "tgt_ix": "133-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_122",
            "tgt_ix": "133-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_123",
            "tgt_ix": "133-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_124",
            "tgt_ix": "133-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_125",
            "tgt_ix": "133-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_126",
            "tgt_ix": "133-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_127",
            "tgt_ix": "133-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_128",
            "tgt_ix": "133-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_129",
            "tgt_ix": "133-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_130",
            "tgt_ix": "133-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_131",
            "tgt_ix": "133-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_132",
            "tgt_ix": "133-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_133",
            "tgt_ix": "133-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_134",
            "tgt_ix": "133-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_135",
            "tgt_ix": "133-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_136",
            "tgt_ix": "133-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_137",
            "tgt_ix": "133-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_138",
            "tgt_ix": "133-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_139",
            "tgt_ix": "133-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_140",
            "tgt_ix": "133-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_141",
            "tgt_ix": "133-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_142",
            "tgt_ix": "133-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_143",
            "tgt_ix": "133-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_144",
            "tgt_ix": "133-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "133-ARR_v2_145",
            "tgt_ix": "133-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1054,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "133-ARR",
        "version": 2
    }
}