{
    "nodes": [
        {
            "ix": "59-ARR_v1_0",
            "content": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_2",
            "content": "Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as biomedical domain are vastly under-explored. To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge. We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "59-ARR_v1_4",
            "content": "Pre-trained language models (PLMs; ) have orchestrated incredible progress on myriads of few-or zero-shot language understanding tasks, by pre-training model parameters in a task-agnostic way and transferring knowledge to specific downstream tasks via finetuning (Brown et al., 2020;Petroni et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_5",
            "content": "To better understand the underlying knowledge transfer mechanism behind these achievements, 1 Code and data are attached in the submission. many knowledge probing approaches and benchmark datasets have been proposed (Petroni et al., 2019;Jiang et al., 2020a;Zhong et al., 2021). This is typically done by formulating knowledge triples as cloze-style queries with the objects being masked (see Table 1) and using the PLM to fill the single (Petroni et al., 2019) or multiple (Ghazvininejad et al., 2019) [Mask] token(s) without further fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_6",
            "content": "In parallel, it has been shown that specialised PLMs (e.g., BioBERT; Lee et al. 2020, Blue-BERT;Peng et al. 2019 and PubMedBERT;Gu et al. 2020) substantially improve the performance in several biomedical tasks (Gu et al., 2020). The biomedical domain is an interesting testbed for investigating knowledge probing for its unique challenges (including vocabulary size, multi-token entities), and the practical benefit of potentially disposing the expensive knowledge base construction process. However, research on knowledge probing in this domain is largely under-explored.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_7",
            "content": "To facilitate research in this direction, we present a well-curated biomedical knowledge probing benchmark, MedLAMA, that consists of 19 thoroughly selected relations. Each relation contains 1k queries (19k queries in total with at most 10 answers each), which are extracted from the large UMLS (Bodenreider, 2004) biomedical knowledge graph and verified by domain experts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_8",
            "content": "We use automatic metrics to identify the hard examples based on the hardness of exposing answers from their query tokens. See Table 1 for a sample of easy and hard examples from MedLAMA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_9",
            "content": "A considerable challenge in probing in biomedical domain is handling multi-token encoding of the answers (e.g. in MedLAMA only 2.6% of the answers are single-token, while in the English set of mLAMA; Kassner et al. 2021, 98% are singletoken), where all existing approaches (i.e., mask predict; Petroni et al. 2019, retrieval-based;Dufter et al. 2021, and generation-based;Gao et al. 2020) struggle to be effective. 2 For example, the mask predict approach (Jiang et al., 2020a) which performs well in probing multilingual knowledge achieves less than 1% accuracy on MedLAMA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_10",
            "content": "To address the aforementioned challenge, we propose a new method, Contrastive-Probe, that first adjusts the representation space of the underlying PLMs by using a retrieval-based contrastive learning objective (like 'rewiring' the switchboard to the target appliances Liu et al. 2021c) then retrieves answers based on their representation similarities to the queries. Notably, our Contrastive-Probe does not require using the MLM heads during probing, which avoids the vocabulary bias across different models. Additionally, retrievalbased probe is effective for addressing the multitoken challenge, as it avoids the need to generate multiple tokens from the MLM vocabulary. We show that Contrastive-Probe facilitates absolute improvements of up-to \u223c6% and \u223c25% on the acc@1 and acc@10 probing performance compared with the existing approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_11",
            "content": "We further highlight that the elicited knowledge by Contrastive-Probe is not gained from the additional random sentences, but from the original pretrained parameters, which echos the previous finding of Liu et al. (2021b); Glava\u0161 and Vuli\u0107 (2021). Additionally, we demonstrate that different stateof-the-art PLMs and transformer layers are suited for different types of relational knowledge, and different relations requires different depth of tuning, suggesting that both the layers and tuning depth should be considered when infusing knowledge over different relations. Furthermore, expert evaluation of PLM responses on a subset of MedLAMA highlights that expert-crafted resources such as UMLS still do not include the full spectrum of factual knowledge, indicating that the factual information encoded in PLMs is richer than what is reflected by the automatic evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_12",
            "content": "The findings of our work, along with the proposed MedLAMA and Contrastive-Probe, highlight both the unique challenges of the biomedical domain and the unexploited potential of PLMs. We hope our research to shed light on what domainspecialised PLMs capture and how it could be better resurfaced, with minimum cost, for probing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_13",
            "content": "MedLAMA",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "59-ARR_v1_14",
            "content": "To facilitate research of knowledge probing in the biomedical domain, we create the MedLAMA benchmark based on the largest biomedical knowledge graph UMLS (Bodenreider, 2004). UMLS 3 is a comprehensive metathesaurus containing 3.6 million entities and more than 35.2 million knowledge triples over 818 relation types which are integrated from various ontologies, including SNOMED CT, MeSH and the NCBI taxonomy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_15",
            "content": "Creating a LAMA-style (Petroni et al., 2019) probing benchmark from such a knowledge graph poses its own challenges: (1) UMLS is a collection of knowledge graphs with more than 150 ontologies constructed by different organisations with very different schemata and emphasis; (2) a significant amount of entity names (from certain vocabularies) are unnatural language (e.g., t(8;21)(q22;q22) denoting an observed karyotypic abnormality) which can hardly be understood by the existing PLMs, with tokenisation tailored for natural language; (3) some queries (constructed from knowledge triples) can have up to hundreds of answers (i.e., 1-to-N relations), complicating the interpretation of probing performance; and (4) some queries may expose answers in themselves (e.g., answer within queries), making it challenging to interpret relative accuracy scores. Selection of Relationship Types. In order to obtain high-quality knowledge queries, we conducted multiple rounds of manual filtering on the relation level to exclude uninformative relations or relations that are only important in the ontological context but do not contain interesting semantics as a natural language (e.g, taxonomy and measurement relations). We also excluded relations with insufficient triples/entities. Then, we manually checked the knowledge triples for each relation to filter out those that contain unnatural language entities and ensure that their queries are semantically meaningful. Additionally, in the cases of 1-to-N relations where there are multiple gold answers for the same query, we constrained all the queries to contain at most 10 gold answers. These steps resulted in 19 relations with each containing 1k randomly sampled knowledge queries. See Appendix for the detailed relation names and their corresponding prompts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_16",
            "content": "Easy vs. Hard Queries. Recent works (Poerner et al., 2020;Shwartz et al., 2020) have discovered that PLMs are overly reliant on the surface form of entities to guess the correct answer of a knowledge query. The PLMs \"cheat\" by detecting lexical overlaps between the query and answer surface forms instead of exercising their abilities of predicting factual knowledge. For instance, PLMs can easily deal with the triple <Dengue virus live antigen CYD serotype 1, may-prevent, Dengue> since the answer is part of the query. To mitigate such bias, we also create a hard query set for each relation by selecting a subset of their corresponding 1k queries using token and matching metrics (i.e., exact matching and ROUGE-L (Lin and Och, 2004)). For more details see the Appendix. We refer to the final filtered and original queries as the hard sets and full sets, respectively. Figure 1 (left) shows the count of hard vs. full sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_17",
            "content": "The Multi-token Issue. One of the key challenges for probing MedLAMA is the multi-token decoding of its entity names. In MedLAMA there are only 2.6% of the entity names that are singletoken 4 while in the English set of mLAMA and LAMA (Petroni et al., 2019) the percentage of single-token answers are 98% and 100%, respectively. Figure 1 (right) shows the percentage of answers by different token numbers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_18",
            "content": "Type Answer space MLM Fill-mask (Petroni et al., 2019) MP PLM Vocab X-FACTR (Jiang et al., 2020a) MP PLM Vocab Generative PLMs 3 Existing Multi-token Knowledge Probing Approaches",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_19",
            "content": "While the pioneer works in PLM knowledge probing mainly focused on the single-token entities, many recent works have started exploring the solutions for the multi-token scenario Jiang et al., 2020a;De Cao et al., 2021). These knowledge probing approaches can be categorised, based on answer search space and reliance on MLM head, into three categories: mask predict, generation-based, and retrieval-based.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_20",
            "content": "Table 2 summarises their key differences. Mask Predict. Mask predict (Petroni et al., 2019;Jiang et al., 2020a) is one of the most commonly used approaches to probe knowledge for masked PLMs (e.g. BERT). The mask predict approach uses the MLM head to fill a single mask token for a cloze-style query, and the output token is subjected to the PLM vocabulary (Petroni et al., 2019). Since many real-world entity names are encoded with multiple tokens, the mask predict approach has also been extended to predict multitoken answers using the conditional masked language model (Jiang et al., 2020a;Ghazvininejad et al., 2019). Figure 2(a) shows the prediction process. Specifically, given a query, the probing task is formulated as: 1) filling masks in parallel independently (Independent); 2) filling masks from left to right autoregressively (Order); 3) filling tokens sorted by the maximum confidence greedily (Confidence). After all mask tokens are replaced with the initial predictions, the predictions can be further refined by iteratively modifying one token at a time until convergence or until the maximum number of iterations is reached (Jiang et al., 2020a). For example, Order+Order represents that the answers are initially predicted by Order and then refined by Order. In this paper we examined two of these approaches, i.e. Independent and Order+Order, based on our initial exploration. Generation-based. Recently, many generation based PLMs have been presented for text generation tasks, such as BART and T5 (Raffel et al., 2020). These generative PLMs are trained with a de-noising objective to restore its original form autoregressively Raffel et al., 2020). Such an autoregressive generation process is analogous to the Order probing approach, thus the generative PLMs can be directly used to generate answers for each query. Retrieval-based. Mask predict and Generationbased approaches need to use the PLM vocabulary as their search spaces for answer tokens, which may generate answers that are not in the answer set. In particular, when probing the masked PLMs using their MLM heads, the predicted result might not be a good indicator for measuring the amount of knowledge captured by these PLMs. This is mainly because the MLM head will be eventually dropped during the downstream task fine-tuning while the MLM head normally accounts for more than 20% of the total PLM parameters. Alternatively, the retrieval-based probing 4 Contrastive-Probe: Cloze-style Task as a Self-retrieving Game",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_21",
            "content": "To better transform the PLM encoders for the cloze-style probing task, we propose Contrastive-Probe which pre-trains on a small number of sentences sampled from the PLM's original pre-training corpora with a contrastive selfsupervising objective, inspired by the Mirror-BERT (Liu et al., 2021b). Our contrastive pretraining does not require the MLM head or any additional external knowledge, and can be completed in less than one minute on 2 \u00d7 2080Ti GPUs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_22",
            "content": "Self-supervised Contrastive Rewiring. We randomly sample a small set of sentences (e.g. 10k, see \u00a75.2 for stability analysis of Contrastive-Probe on several randomly sampled sets), and replace their tail tokens (e.g. the last 50% excluding the full stop) with a [Mask] token. Then these transformed sentences are taken as the queries of the cloze-style self-retrieving game. In the following we show an example of transforming a sentence into a cloze-style query:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_23",
            "content": "Sentence: Social-distancing largely reduces coronavirus infections.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_24",
            "content": "Query: Social-distancing largely [Mask].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_25",
            "content": "where \"reduces coronavirus infections\" is marked as a positive answer of this query. Given a batch, the cloze-style self-retrieving game is to ask the PLMs to retrieve the positive answer from all the queries and answers in the same batch. Our Contrastive-Probe tackles this by optimising an InfoNCE objective (Oord et al., 2018),",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_26",
            "content": "L = \u2212 N i=1 log exp(cos( f (x i ), f (x p ))/\u03c4) x j \u2208N i exp(cos( f (x i ), f (x j ))/\u03c4) ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_27",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "59-ARR_v1_28",
            "content": "In this section we conduct extensive experiments to verify whether Contrastive-Probe is effective for probing biomedical PLMs. First, we experiment with Contrastive-Probe and existing probing approaches on MedLAMA benchmark ( \u00a75.1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_29",
            "content": "Then, we conduct in-depth analysis of the stability and applicability of Contrastive-Probe in probing biomedical PLMs ( \u00a75.2). Finally, we report an evaluation of a biomedical expert on the probing predictions and highlight our findings ( \u00a75.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_30",
            "content": "Contrastive-Probe Rewiring.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_31",
            "content": "We train our Contrastive-Probe based on 10k sentences which are randomly sampled from the PubMed texts 5 using a mask ratio of 0.5. The best hyperparameters and their tuning options are provided in Appendix. Probing Baselines. For the mask predict approach, we use the original implementation of X-FACTR (Jiang et al., 2020a), and set the beam size and the number of masks to 5. Both mask predict and retrieval-based approaches are tested under both the general domain and biomedical domain BERT models, i.e. Bert-based-uncased (Devlin et al., 2019), BlueBERT , BioBERT that are pre-trained on large biomedical corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_32",
            "content": "Benchmarking on MedLAMA",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "59-ARR_v1_33",
            "content": "Comparing Various Probing Approaches. Table 3 shows the overall results of various probing baselines on MedLAMA. It can be seen that the performances of all the existing probing approaches (i.e. generative PLMs, X-FACTR and mask predict) are very low (<1% for acc@1 and <4% for acc@10) regardless of the underlying PLM, which are not effective indicators for measuring knowledge captured. In contrast, our Contrastive-Probe obtains absolute improvements by up-to \u223c 6% and \u223c 25% on acc@1 and acc10 respectively comparing with the three existing approaches, which validates its effectiveness on measuring the knowledge probing performance. In particular, PubMedBERT model obtains the best probing performance (7.32% in accuracy) for these biomedical queries, validating its effectiveness of capturing biomedical knowledge comparing with other PLMs (i.e. BERT, BlueBERT and BioBERT).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_34",
            "content": "Benchmarking with Contrastive-Probe. To further examine the effectiveness of PLMs in captur-",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_35",
            "content": "Figure 3: Performance over answer lengths.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_36",
            "content": "ing biomedical knowledge, we benchmarked several state-of-the-art biomedical PLMs (including pure pre-trained and knowledge-enhanced models) on MedLAMA through our Contrastive-Probe. Table 4 shows the probing results over the full and hard sets (detailed macro and micro accuracies are provided in Appendix). In general, we can observe that these biomedical PLMs always perform better than general-domain PLMs (i.e., BERT). Also, we observe the decay of performance of all these models on the more challenging hard set queries. While PubMedBERT performs the best under all metrics, CoderBERT (which is the knowledge infused PubMedBERT) achieves better performance on micro acc@1, highlighting the benefits of knowledge infusion pre-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_37",
            "content": "Comparison per Answer Length. Since different PLMs use different tokenizers, we use char length of the query answers to split MedLAMA into different bins and test the probing performance over various answer lengths. Figure 3 shows the result. We can see that the performance of retrievalbased probing in Contrastive-Probe increases as the answer length increase while the performance of mask predict dropped significantly. This result validates that our Contrastive-Probe (retrievalbased) are more reliable at predicting longer answers than the mask predict approach since the lat- 4 shows the acc@1 performance over top 9 relations and the micro average performance of all the 19 relations. We can see that the standard deviations are small and the performance over different sets of samples shows the similar trend. This further highlights that the probing success of Contrastive-Probe is not due the selected pre-training sentences. Intuitively, the contrastive self-retrieving game ( \u00a74) is equivalent to the formulation of the cloze-style filling task, hence tuning the underlying PLMs makes them better suited for knowledge elicitation needed during probing (like 'rewiring' the switchboards). Additionally, from Figure 4 we can also observe that different relations exhibit very different trends during pre-training steps of Contrastive-Probe and peak under different steps, suggesting that we need to treat different types of relational knowledge with different tuning depths when infusing knowledge. We leave further exploration of this to future work. Probing by Relations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_38",
            "content": "Expert Evaluation on Predictions",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "59-ARR_v1_39",
            "content": "To assess whether the actual probing performance could be possibly higher than what is reflected by the commonly used automatic evaluation, we conducted a human evaluation on the prediction result. Specifically, we sample 15 queries and predict their top-10 answers using Contrastive-Probe based on PubMedBERT and ask the assessor 10 to rate the predictions on a scale of [1,5].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_40",
            "content": "Figure 8 shows the confusion matrices. 11 We observe the followings: (1) There are 3 UMLS answers that are annotated with score level 1-4 (precisely, level 3), which indicates UMLS answers might not always be the perfect answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_41",
            "content": "(2) There are 20 annotated perfect answers (score 5) in the top 10 predictions that are not marked as the gold answers in the UMLS, which suggests the UMLS does not include all the expected gold knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_42",
            "content": "(3) In general, PubMedBERT achieves an 8.67% (13/150) acc@10 under gold answers, but under the expert annotation the acc@10 is 22%(33/150), which means the probing performance is higher than what evaluated using the automatically extracted answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_43",
            "content": "Related Work and Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "59-ARR_v1_44",
            "content": "Knowledge Probing Benchmarks for PLMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_45",
            "content": "LAMA (Petroni et al., 2019), which starts this line of work, is a collection of single-token knowledge triples extracted from sources including Wikidata and ConceptNet (Speer et al., 2017). To mitigate the problem of information leakage from the head entity, Poerner et al. (2019) propose LAMA-UHN, which is a hard subset of LAMA that has less token overlaps in head and tail entities. X-FACTR (Jiang et al., 2020a) and mLAMA extend knowledge probing to the multilingual scenario and introduce multi-token answers. They each propose decoding methods that generate multi-token answers, which we have shown to work poorly on MedLAMA. BioLAMA (Sung et al., 2021) is a concurrent work that also releases a benchmark for biomedical knowledge probing. We provide a comparison between LAMA, BioLAMA and MedLAMA in terms of (# relations, # queries, avg # answers per query, avg # characters per answer) in the Appendix. 12 Probing via Prompt Engineering. Knowledge probing is sensitive to what prompt is used (Jiang et al., 2020b). To bootstrap the probing performance, Jiang et al. (2020b) mine more prompts and ensemble them during inference. Later works parameterised the prompts and made them trainable (Shin et al., 2020b;Fichtel et al., 2021;Qin and Eisner, 2021). We have opted out promptengineering methods that require training data in this work, as tuning the prompts are essentially tuning an additional (parameterised) model on top of PLMs. As pointed out by Fichtel et al. (2021), prompt tuning requires large amounts of training data from the task. Since task training data is used, the additional model parameters are exposed to the target data distribution and can solve the set set by overfitting to such biases . In our work, by adaptively finetuning the model with a small set of raw sentences, we elicit the knowl- 12 Our comparison indicated that MedLAMA and BioLAMA have no overlaps in queries, allowing both resources to complement each other. edge out from PLMs but do not expose the data biases from the benchmark (MedLAMA). Biomedical Knowledge Probing. Nadkarni et al. (2021) train PLMs as KB completion models and test on the same task to understand how much knowledge is in biomedical PLMs. BioLAMA focuses on the continuous prompt learning method OptiPrompt (Zhong et al., 2021), which also requires ground-truth training data from the task. Overall, compared to BioLAMA, we have provided a more comprehensive set of probing experiments and analysis, including proposing a novel probing technique and providing human evaluations of model predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_46",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "59-ARR_v1_47",
            "content": "In this work, we created a carefully curated biomedical probing benchmark, MedLAMA, from the UMLS knowledge graph. We illustrated that state-of-the-art probing techniques and biomedical pre-trained languages models (PLMs) struggle to cope with the challenging nature (e.g. multitoken answers) of this specialised domain, reaching only an underwhelming 3% of acc@10. To reduce the gap, we further proposed a novel contrastive recipe which rewires the underlying PLMs without using any probing-specific data and illustrated that with a lightweight pre-training their accuracies could be pushed to 28%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_48",
            "content": "Our experiments also revealed that different layers of transformers encode different types of information, reflected by their individual success at handling certain types of prompts. Additionally, using a human expert, we showed that the existing evaluation criteria could overpenalise the models as many valid responses that PLMs produce are not in the ground truth UMLS knowledge graph. This further highlights the importance of having a human in the loop to better understand the potentials and limitations of PLMs in encoding domain specific factual knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_49",
            "content": "Our findings indicate that the real lower bound on the amount of factual knowledge encoded by PLMs is higher than we estimated, since such bound can be continuously improved by optimising both the encoding space (e.g. using our selfsupervised contrastive learning technique) and the input space (e.g. using the prompt optimising techniques (Shin et al., 2020a;Qin and Eisner, 2021)). We leave further exploration of integrating the two possibilities to future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_50",
            "content": "Table 5 shows the detailed relation names and their manual prompts of our MedLAMA.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_51",
            "content": "In this paper, we use two automatic metrics to distinguish hard and easy queries. In particular, we first filter out easy queries by an exact matching metric (i.e. the exactly matching all the words of answer from queries). Since our MedLAMA contains multiple answers for queries, we use a threshold on the average exact matching score, i.e. avg-match>0.1, to filter out easy examples, where avg-match is calculated by: avg-match = Count(matched answers) Count(total answers) .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_52",
            "content": "This metric can remove all the queries that match the whole string of answers. However, some common sub-strings between queries and answers also prone to reveal answers, particularly benefiting those retrieval-based probing approaches. E.g. <Magnesium Chloride, may-prevent, Magnesium Deficiency>. Therefore, we further calculate the ROUGE-L score (Lin and Och, 2004) for all the queries by regarding <query, answers> pairs as the <hypothesis, reference> pairs, and further filter out the ROUGE-L>0.1 queries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_53",
            "content": "During the writing of this work, we notice that Sung et al. (2021) also released a biomedical knowledge probing benchmark, called BioLAMA, which is a work concurrent to ours. In Table 6, we compare our MedLAMA with LAMA (Petroni et al., 2019) and BioLAMA (Sung et al., 2021) in terms of their statistics. We found that there is only 1 overlapped relation (i.e. may treat) between BioLAMA and our MedLAMA, and no same query can be found. Moreover, Sung et al. (2021) only use two existing probing approach on their proposed BioLAMA, while in this paper we further proposed a new probing approach Contrastive-Probe. We also evaluate our Contrastive-Probe in BioLAMA, and the result is shown in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_54",
            "content": "To further investigate the impact of the mask ratio to the probing performance, we also test our Contrastive-Probe based on PubMedBERT over different mask ratios ({0.1, 0.2, 0.3, 0.4, 0.5}) under the 10 random sentence sets, the result of which is shown in Figure 9. We can see that over different mask ratios the Contrastive-Probe always reaches their best performance under certain pre-training steps. And the performance curves of mask ratios are different over the full and hard sets, but they all achieves a generally good performance when the mask ratio is 0.5, which validates that different mask ratios favour different types queries. acc@1 acc@5 acc@1 acc@5 acc@1 acc@5 Table 8: Hyperparameters along with their search grid. * marks the values used to obtain the reported results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_55",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Iz Beltagy",
                    "Kyle Lo",
                    "Arman Cohan"
                ],
                "title": "SciB-ERT: A pretrained language model for scientific text",
                "pub_date": "2019",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_56",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Olivier Bodenreider"
                ],
                "title": "The unified medical language system (umls): integrating biomedical terminology",
                "pub_date": "2004",
                "pub_title": "Nucleic acids research",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_57",
            "content": "Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry,  Askell, Language models are few-shot learners, 2020, EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Benjamin Tom B Brown",
                    "Nick Mann",
                    "Melanie Ryder",
                    "Jared Subbiah",
                    "Prafulla Kaplan",
                    "Arvind Dhariwal",
                    "Pranav Neelakantan",
                    "Girish Shyam",
                    "Amanda Sastry",
                    " Askell"
                ],
                "title": "Language models are few-shot learners",
                "pub_date": "2020",
                "pub_title": "EACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_58",
            "content": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu, Knowledgeable or educated guess? revisiting language models as knowledge bases, 2021, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Boxi Cao",
                    "Hongyu Lin",
                    "Xianpei Han",
                    "Le Sun",
                    "Lingyong Yan",
                    "Meng Liao",
                    "Tong Xue",
                    "Jin Xu"
                ],
                "title": "Knowledgeable or educated guess? revisiting language models as knowledge bases",
                "pub_date": "2021",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_59",
            "content": "UNKNOWN, None, 2021, Autoregressive entity retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Autoregressive entity retrieval",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_60",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_61",
            "content": "Philipp Dufter, Nora Kassner, Hinrich Sch\u00fctze, Static embeddings as efficient knowledge bases, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Philipp Dufter",
                    "Nora Kassner",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Static embeddings as efficient knowledge bases",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_62",
            "content": "Leandra Fichtel, Jan-Christoph Kalo, Wolf-Tilo Balke, Prompt tuning or fine-tuninginvestigating relational knowledge in pre-trained language models, 2021, AKBC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Leandra Fichtel",
                    "Jan-Christoph Kalo",
                    "Wolf-Tilo Balke"
                ],
                "title": "Prompt tuning or fine-tuninginvestigating relational knowledge in pre-trained language models",
                "pub_date": "2021",
                "pub_title": "AKBC",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_63",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_64",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Omer Levy",
                    "Yinhan Liu",
                    "Luke Zettlemoyer"
                ],
                "title": "Mask-predict: Parallel decoding of conditional masked language models",
                "pub_date": "2019",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_65",
            "content": "Goran Glava\u0161, Ivan Vuli\u0107, Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation, 2021, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Goran Glava\u0161",
                    "Ivan Vuli\u0107"
                ],
                "title": "Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation",
                "pub_date": "2021",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_66",
            "content": "Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing, , ACM Transactions on Computing for Healthcare, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Yu Gu",
                    "Robert Tinn",
                    "Hao Cheng",
                    "Michael Lucas",
                    "Naoto Usuyama",
                    "Xiaodong Liu",
                    "Tristan Naumann"
                ],
                "title": "Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing",
                "pub_date": null,
                "pub_title": "ACM Transactions on Computing for Healthcare",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_67",
            "content": "UNKNOWN, None, 2019, Clinicalbert: Modeling clinical notes and predicting hospital readmission, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_68",
            "content": "Zhengbao Jiang, Antonios Anastasopoulos, Haibo Ding, and Graham Neubig. 2020a. X-factr: Multilingual factual knowledge retrieval from pretrained language models, , EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Zhengbao Jiang",
                    "Antonios Anastasopoulos"
                ],
                "title": "Haibo Ding, and Graham Neubig. 2020a. X-factr: Multilingual factual knowledge retrieval from pretrained language models",
                "pub_date": null,
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_69",
            "content": "UNKNOWN, None, 2020, How can we know what language models know?, TACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "How can we know what language models know?",
                "pub": "TACL"
            }
        },
        {
            "ix": "59-ARR_v1_70",
            "content": "Nora Kassner, Philipp Dufter, Hinrich Sch\u00fctze, Multilingual lama: Investigating knowledge in multilingual pretrained language models, 2021, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Nora Kassner",
                    "Philipp Dufter",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Multilingual lama: Investigating knowledge in multilingual pretrained language models",
                "pub_date": "2021",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_71",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, BioBERT: a pretrained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jinhyuk Lee",
                    "Wonjin Yoon",
                    "Sungdong Kim",
                    "Donghyeon Kim",
                    "Sunkyu Kim",
                    "Chan Ho So",
                    "Jaewoo Kang"
                ],
                "title": "BioBERT: a pretrained biomedical language representation model for biomedical text mining",
                "pub_date": "2020",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_72",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_73",
            "content": "Chin-Yew Lin, Franz Josef Och, Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics, 2004, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Chin-Yew Lin",
                    "Franz Josef Och"
                ],
                "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
                "pub_date": "2004",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_74",
            "content": "UNKNOWN, None, 2021, Selfalignment pre-training for biomedical entity representations, NAACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Selfalignment pre-training for biomedical entity representations",
                "pub": "NAACL"
            }
        },
        {
            "ix": "59-ARR_v1_75",
            "content": "Fangyu Liu, Ivan Vuli\u0107, Anna Korhonen, Nigel Collier, Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Fangyu Liu",
                    "Ivan Vuli\u0107",
                    "Anna Korhonen",
                    "Nigel Collier"
                ],
                "title": "Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_76",
            "content": "Qianchu Liu, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vuli\u0107, Mirrorwic: On eliciting word-in-context representations from pretrained language models, 2021, CoNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Qianchu Liu",
                    "Fangyu Liu",
                    "Nigel Collier",
                    "Anna Korhonen",
                    "Ivan Vuli\u0107"
                ],
                "title": "Mirrorwic: On eliciting word-in-context representations from pretrained language models",
                "pub_date": "2021",
                "pub_title": "CoNLL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_77",
            "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta: A robustly optimized bert pretraining approach, 2020, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Yinhan Liu",
                    "Myle Ott",
                    "Naman Goyal",
                    "Jingfei Du",
                    "Mandar Joshi",
                    "Danqi Chen",
                    "Omer Levy",
                    "Mike Lewis",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "pub_date": "2020",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_78",
            "content": "George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen, Alexander Wong, Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "George Michalopoulos",
                    "Yuanxin Wang",
                    "Hussam Kaka",
                    "Helen Chen",
                    "Alexander Wong"
                ],
                "title": "Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_79",
            "content": "UNKNOWN, None, 2021, Scientific language models for biomedical knowledge base completion: An empirical study, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Scientific language models for biomedical knowledge base completion: An empirical study",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_80",
            "content": "UNKNOWN, None, 2018, Representation learning with contrastive predictive coding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Representation learning with contrastive predictive coding",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_81",
            "content": "Yifan Peng, Shankai Yan, Zhiyong Lu, Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets, 2019, BioNLP Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Yifan Peng",
                    "Shankai Yan",
                    "Zhiyong Lu"
                ],
                "title": "Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets",
                "pub_date": "2019",
                "pub_title": "BioNLP Workshop",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_82",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_83",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases, 2019, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Language models as knowledge bases",
                "pub_date": "2019",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_84",
            "content": "UNKNOWN, None, , Erol Bahadroglu, Alec Peltekian, and Gr\u00e9goire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Erol Bahadroglu, Alec Peltekian, and Gr\u00e9goire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_85",
            "content": "UNKNOWN, None, 2019, E-bert: Efficient-yet-effective entity embeddings for bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "E-bert: Efficient-yet-effective entity embeddings for bert",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_86",
            "content": "Nina Poerner, Ulli Waltinger, Hinrich Sch\u00fctze, E-bert: Efficient-yet-effective entity embeddings for bert, 2020, EMNLP: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Nina Poerner",
                    "Ulli Waltinger",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "E-bert: Efficient-yet-effective entity embeddings for bert",
                "pub_date": "2020",
                "pub_title": "EMNLP: Findings",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_87",
            "content": "Guanghui Qin, Jason Eisner, Learning how to ask: Querying lms with mixtures of soft prompts, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Guanghui Qin",
                    "Jason Eisner"
                ],
                "title": "Learning how to ask: Querying lms with mixtures of soft prompts",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_88",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_89",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Taylor Shin",
                    "Yasaman Razeghi",
                    "I Robert L Logan",
                    "Eric Wallace",
                    "Sameer Singh"
                ],
                "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_90",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, Eliciting knowledge from language models using automatically generated prompts, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Taylor Shin",
                    "Yasaman Razeghi",
                    "I Robert L Logan",
                    "Eric Wallace",
                    "Sameer Singh"
                ],
                "title": "Eliciting knowledge from language models using automatically generated prompts",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_91",
            "content": "Vered Shwartz, Rachel Rudinger, Oyvind Tafjord, you are grounded!\": Latent name artifacts in pre-trained language models, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Vered Shwartz",
                    "Rachel Rudinger",
                    "Oyvind Tafjord"
                ],
                "title": "you are grounded!\": Latent name artifacts in pre-trained language models",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_92",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_93",
            "content": "Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang, Can language models be biomedical knowledge bases, 2021, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Mujeen Sung",
                    "Jinhyuk Lee",
                    "Sean Yi",
                    "Minji Jeon",
                    "Sungdong Kim",
                    "Jaewoo Kang"
                ],
                "title": "Can language models be biomedical knowledge bases",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_94",
            "content": "UNKNOWN, None, 2020, Coder: Knowledge infused cross-lingual medical term embedding for term normalization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Coder: Knowledge infused cross-lingual medical term embedding for term normalization",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_95",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is [mask]: Learning vs. learning to recall, 2021, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Zexuan Zhong",
                    "Dan Friedman",
                    "Danqi Chen"
                ],
                "title": "Factual probing is [mask]: Learning vs. learning to recall",
                "pub_date": "2021",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_96",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_97",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_98",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_99",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_100",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_101",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_102",
            "content": "UNKNOWN, None, , Knowledge-enhanced, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Knowledge-enhanced",
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_103",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_104",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_105",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "59-ARR_v1_106",
            "content": "UNKNOWN, None, , Table 10: Benchmarking biomedical PLMs on MedLAMA via our Contrastive-Probe, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Table 10: Benchmarking biomedical PLMs on MedLAMA via our Contrastive-Probe",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "59-ARR_v1_0@0",
            "content": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_0",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@0",
            "content": "Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@1",
            "content": "Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as biomedical domain are vastly under-explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 127,
            "end": 278,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@2",
            "content": "To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 280,
            "end": 452,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@3",
            "content": "We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 454,
            "end": 573,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@4",
            "content": "While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 575,
            "end": 767,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@5",
            "content": "To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 769,
            "end": 933,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@6",
            "content": "While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 935,
            "end": 1026,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@7",
            "content": "Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 1028,
            "end": 1213,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@8",
            "content": "We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 1215,
            "end": 1334,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_2@9",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_2",
            "start": 1336,
            "end": 1336,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_4@0",
            "content": "Pre-trained language models (PLMs; ) have orchestrated incredible progress on myriads of few-or zero-shot language understanding tasks, by pre-training model parameters in a task-agnostic way and transferring knowledge to specific downstream tasks via finetuning (Brown et al., 2020;Petroni et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_4",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_5@0",
            "content": "To better understand the underlying knowledge transfer mechanism behind these achievements, 1 Code and data are attached in the submission.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_5",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_5@1",
            "content": "many knowledge probing approaches and benchmark datasets have been proposed (Petroni et al., 2019;Jiang et al., 2020a;Zhong et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_5",
            "start": 140,
            "end": 277,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_5@2",
            "content": "This is typically done by formulating knowledge triples as cloze-style queries with the objects being masked (see Table 1) and using the PLM to fill the single (Petroni et al., 2019) or multiple (Ghazvininejad et al., 2019) [Mask] token(s) without further fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_5",
            "start": 279,
            "end": 546,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_6@0",
            "content": "In parallel, it has been shown that specialised PLMs (e.g., BioBERT; Lee et al. 2020, Blue-BERT;Peng et al. 2019 and PubMedBERT;Gu et al. 2020) substantially improve the performance in several biomedical tasks (Gu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_6",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_6@1",
            "content": "The biomedical domain is an interesting testbed for investigating knowledge probing for its unique challenges (including vocabulary size, multi-token entities), and the practical benefit of potentially disposing the expensive knowledge base construction process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_6",
            "start": 229,
            "end": 490,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_6@2",
            "content": "However, research on knowledge probing in this domain is largely under-explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_6",
            "start": 492,
            "end": 571,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_7@0",
            "content": "To facilitate research in this direction, we present a well-curated biomedical knowledge probing benchmark, MedLAMA, that consists of 19 thoroughly selected relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_7",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_7@1",
            "content": "Each relation contains 1k queries (19k queries in total with at most 10 answers each), which are extracted from the large UMLS (Bodenreider, 2004) biomedical knowledge graph and verified by domain experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_7",
            "start": 168,
            "end": 372,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_8@0",
            "content": "We use automatic metrics to identify the hard examples based on the hardness of exposing answers from their query tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_8",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_8@1",
            "content": "See Table 1 for a sample of easy and hard examples from MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_8",
            "start": 122,
            "end": 185,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_9@0",
            "content": "A considerable challenge in probing in biomedical domain is handling multi-token encoding of the answers (e.g. in MedLAMA only 2.6% of the answers are single-token, while in the English set of mLAMA; Kassner et al. 2021, 98% are singletoken), where all existing approaches (i.e., mask predict; Petroni et al. 2019, retrieval-based;Dufter et al. 2021, and generation-based;Gao et al. 2020) struggle to be effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_9",
            "start": 0,
            "end": 413,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_9@1",
            "content": "2 For example, the mask predict approach (Jiang et al., 2020a) which performs well in probing multilingual knowledge achieves less than 1% accuracy on MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_9",
            "start": 415,
            "end": 573,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_10@0",
            "content": "To address the aforementioned challenge, we propose a new method, Contrastive-Probe, that first adjusts the representation space of the underlying PLMs by using a retrieval-based contrastive learning objective (like 'rewiring' the switchboard to the target appliances Liu et al. 2021c) then retrieves answers based on their representation similarities to the queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_10",
            "start": 0,
            "end": 366,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_10@1",
            "content": "Notably, our Contrastive-Probe does not require using the MLM heads during probing, which avoids the vocabulary bias across different models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_10",
            "start": 368,
            "end": 508,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_10@2",
            "content": "Additionally, retrievalbased probe is effective for addressing the multitoken challenge, as it avoids the need to generate multiple tokens from the MLM vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_10",
            "start": 510,
            "end": 672,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_10@3",
            "content": "We show that Contrastive-Probe facilitates absolute improvements of up-to \u223c6% and \u223c25% on the acc@1 and acc@10 probing performance compared with the existing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_10",
            "start": 674,
            "end": 842,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_11@0",
            "content": "We further highlight that the elicited knowledge by Contrastive-Probe is not gained from the additional random sentences, but from the original pretrained parameters, which echos the previous finding of Liu et al. (2021b); Glava\u0161 and Vuli\u0107 (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_11",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_11@1",
            "content": "Additionally, we demonstrate that different stateof-the-art PLMs and transformer layers are suited for different types of relational knowledge, and different relations requires different depth of tuning, suggesting that both the layers and tuning depth should be considered when infusing knowledge over different relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_11",
            "start": 248,
            "end": 570,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_11@2",
            "content": "Furthermore, expert evaluation of PLM responses on a subset of MedLAMA highlights that expert-crafted resources such as UMLS still do not include the full spectrum of factual knowledge, indicating that the factual information encoded in PLMs is richer than what is reflected by the automatic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_11",
            "start": 572,
            "end": 874,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_12@0",
            "content": "The findings of our work, along with the proposed MedLAMA and Contrastive-Probe, highlight both the unique challenges of the biomedical domain and the unexploited potential of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_12",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_12@1",
            "content": "We hope our research to shed light on what domainspecialised PLMs capture and how it could be better resurfaced, with minimum cost, for probing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_12",
            "start": 182,
            "end": 325,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_13@0",
            "content": "MedLAMA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_13",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_14@0",
            "content": "To facilitate research of knowledge probing in the biomedical domain, we create the MedLAMA benchmark based on the largest biomedical knowledge graph UMLS (Bodenreider, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_14",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_14@1",
            "content": "UMLS 3 is a comprehensive metathesaurus containing 3.6 million entities and more than 35.2 million knowledge triples over 818 relation types which are integrated from various ontologies, including SNOMED CT, MeSH and the NCBI taxonomy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_14",
            "start": 176,
            "end": 410,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@0",
            "content": "Creating a LAMA-style (Petroni et al., 2019) probing benchmark from such a knowledge graph poses its own challenges: (1) UMLS is a collection of knowledge graphs with more than 150 ontologies constructed by different organisations with very different schemata and emphasis; (2) a significant amount of entity names (from certain vocabularies) are unnatural language (e.g., t(8;21)(q22;q22) denoting an observed karyotypic abnormality) which can hardly be understood by the existing PLMs, with tokenisation tailored for natural language; (3) some queries (constructed from knowledge triples) can have up to hundreds of answers (i.e., 1-to-N relations), complicating the interpretation of probing performance; and (4) some queries may expose answers in themselves (e.g., answer within queries), making it challenging to interpret relative accuracy scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 0,
            "end": 852,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@1",
            "content": "Selection of Relationship Types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 854,
            "end": 885,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@2",
            "content": "In order to obtain high-quality knowledge queries, we conducted multiple rounds of manual filtering on the relation level to exclude uninformative relations or relations that are only important in the ontological context but do not contain interesting semantics as a natural language (e.g, taxonomy and measurement relations).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 887,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@3",
            "content": "We also excluded relations with insufficient triples/entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 1214,
            "end": 1275,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@4",
            "content": "Then, we manually checked the knowledge triples for each relation to filter out those that contain unnatural language entities and ensure that their queries are semantically meaningful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 1277,
            "end": 1461,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@5",
            "content": "Additionally, in the cases of 1-to-N relations where there are multiple gold answers for the same query, we constrained all the queries to contain at most 10 gold answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 1463,
            "end": 1633,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@6",
            "content": "These steps resulted in 19 relations with each containing 1k randomly sampled knowledge queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 1635,
            "end": 1730,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_15@7",
            "content": "See Appendix for the detailed relation names and their corresponding prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_15",
            "start": 1732,
            "end": 1808,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@0",
            "content": "Easy vs. Hard Queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@1",
            "content": "Recent works (Poerner et al., 2020;Shwartz et al., 2020) have discovered that PLMs are overly reliant on the surface form of entities to guess the correct answer of a knowledge query.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 23,
            "end": 205,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@2",
            "content": "The PLMs \"cheat\" by detecting lexical overlaps between the query and answer surface forms instead of exercising their abilities of predicting factual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 207,
            "end": 366,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@3",
            "content": "For instance, PLMs can easily deal with the triple <Dengue virus live antigen CYD serotype 1, may-prevent, Dengue> since the answer is part of the query.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 368,
            "end": 520,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@4",
            "content": "To mitigate such bias, we also create a hard query set for each relation by selecting a subset of their corresponding 1k queries using token and matching metrics (i.e., exact matching and ROUGE-L (Lin and Och, 2004)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 522,
            "end": 738,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@5",
            "content": "For more details see the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 740,
            "end": 773,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@6",
            "content": "We refer to the final filtered and original queries as the hard sets and full sets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 775,
            "end": 871,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_16@7",
            "content": "Figure 1 (left) shows the count of hard vs. full sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_16",
            "start": 873,
            "end": 926,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_17@0",
            "content": "The Multi-token Issue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_17",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_17@1",
            "content": "One of the key challenges for probing MedLAMA is the multi-token decoding of its entity names.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_17",
            "start": 23,
            "end": 116,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_17@2",
            "content": "In MedLAMA there are only 2.6% of the entity names that are singletoken 4 while in the English set of mLAMA and LAMA (Petroni et al., 2019) the percentage of single-token answers are 98% and 100%, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_17",
            "start": 118,
            "end": 327,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_17@3",
            "content": "Figure 1 (right) shows the percentage of answers by different token numbers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_17",
            "start": 329,
            "end": 404,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_18@0",
            "content": "Type Answer space MLM Fill-mask (Petroni et al., 2019) MP PLM Vocab X-FACTR (Jiang et al., 2020a) MP PLM Vocab Generative PLMs 3 Existing Multi-token Knowledge Probing Approaches",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_18",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_19@0",
            "content": "While the pioneer works in PLM knowledge probing mainly focused on the single-token entities, many recent works have started exploring the solutions for the multi-token scenario Jiang et al., 2020a;De Cao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_19",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_19@1",
            "content": "These knowledge probing approaches can be categorised, based on answer search space and reliance on MLM head, into three categories: mask predict, generation-based, and retrieval-based.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_19",
            "start": 220,
            "end": 404,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@0",
            "content": "Table 2 summarises their key differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@1",
            "content": "Mask Predict.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 42,
            "end": 54,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@2",
            "content": "Mask predict (Petroni et al., 2019;Jiang et al., 2020a) is one of the most commonly used approaches to probe knowledge for masked PLMs (e.g. BERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 56,
            "end": 202,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@3",
            "content": "The mask predict approach uses the MLM head to fill a single mask token for a cloze-style query, and the output token is subjected to the PLM vocabulary (Petroni et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 204,
            "end": 379,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@4",
            "content": "Since many real-world entity names are encoded with multiple tokens, the mask predict approach has also been extended to predict multitoken answers using the conditional masked language model (Jiang et al., 2020a;Ghazvininejad et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 381,
            "end": 621,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@5",
            "content": "Figure 2(a) shows the prediction process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 623,
            "end": 663,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@6",
            "content": "Specifically, given a query, the probing task is formulated as: 1) filling masks in parallel independently (Independent); 2) filling masks from left to right autoregressively (Order); 3) filling tokens sorted by the maximum confidence greedily (Confidence).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 665,
            "end": 921,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@7",
            "content": "After all mask tokens are replaced with the initial predictions, the predictions can be further refined by iteratively modifying one token at a time until convergence or until the maximum number of iterations is reached (Jiang et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 923,
            "end": 1164,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@8",
            "content": "For example, Order+Order represents that the answers are initially predicted by Order and then refined by Order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1166,
            "end": 1277,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@9",
            "content": "In this paper we examined two of these approaches, i.e. Independent and Order+Order, based on our initial exploration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1279,
            "end": 1396,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@10",
            "content": "Generation-based.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1398,
            "end": 1414,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@11",
            "content": "Recently, many generation based PLMs have been presented for text generation tasks, such as BART and T5 (Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1416,
            "end": 1541,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@12",
            "content": "These generative PLMs are trained with a de-noising objective to restore its original form autoregressively Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1543,
            "end": 1671,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@13",
            "content": "Such an autoregressive generation process is analogous to the Order probing approach, thus the generative PLMs can be directly used to generate answers for each query.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1673,
            "end": 1839,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@14",
            "content": "Retrieval-based.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1841,
            "end": 1856,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@15",
            "content": "Mask predict and Generationbased approaches need to use the PLM vocabulary as their search spaces for answer tokens, which may generate answers that are not in the answer set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 1858,
            "end": 2032,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@16",
            "content": "In particular, when probing the masked PLMs using their MLM heads, the predicted result might not be a good indicator for measuring the amount of knowledge captured by these PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 2034,
            "end": 2212,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@17",
            "content": "This is mainly because the MLM head will be eventually dropped during the downstream task fine-tuning while the MLM head normally accounts for more than 20% of the total PLM parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 2214,
            "end": 2398,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_20@18",
            "content": "Alternatively, the retrieval-based probing 4 Contrastive-Probe: Cloze-style Task as a Self-retrieving Game",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_20",
            "start": 2400,
            "end": 2505,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_21@0",
            "content": "To better transform the PLM encoders for the cloze-style probing task, we propose Contrastive-Probe which pre-trains on a small number of sentences sampled from the PLM's original pre-training corpora with a contrastive selfsupervising objective, inspired by the Mirror-BERT (Liu et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_21",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_21@1",
            "content": "Our contrastive pretraining does not require the MLM head or any additional external knowledge, and can be completed in less than one minute on 2 \u00d7 2080Ti GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_21",
            "start": 296,
            "end": 455,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_22@0",
            "content": "Self-supervised Contrastive Rewiring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_22",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_22@1",
            "content": "We randomly sample a small set of sentences (e.g. 10k, see \u00a75.2 for stability analysis of Contrastive-Probe on several randomly sampled sets), and replace their tail tokens (e.g. the last 50% excluding the full stop) with a [Mask] token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_22",
            "start": 38,
            "end": 274,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_22@2",
            "content": "Then these transformed sentences are taken as the queries of the cloze-style self-retrieving game.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_22",
            "start": 276,
            "end": 373,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_22@3",
            "content": "In the following we show an example of transforming a sentence into a cloze-style query:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_22",
            "start": 375,
            "end": 462,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_23@0",
            "content": "Sentence: Social-distancing largely reduces coronavirus infections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_23",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_24@0",
            "content": "Query: Social-distancing largely [Mask].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_24",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_25@0",
            "content": "where \"reduces coronavirus infections\" is marked as a positive answer of this query.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_25",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_25@1",
            "content": "Given a batch, the cloze-style self-retrieving game is to ask the PLMs to retrieve the positive answer from all the queries and answers in the same batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_25",
            "start": 85,
            "end": 238,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_25@2",
            "content": "Our Contrastive-Probe tackles this by optimising an InfoNCE objective (Oord et al., 2018),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_25",
            "start": 240,
            "end": 329,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_26@0",
            "content": "L = \u2212 N i=1 log exp(cos( f (x i ), f (x p ))/\u03c4) x j \u2208N i exp(cos( f (x i ), f (x j ))/\u03c4) ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_26",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_27@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_27",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_28@0",
            "content": "In this section we conduct extensive experiments to verify whether Contrastive-Probe is effective for probing biomedical PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_28",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_28@1",
            "content": "First, we experiment with Contrastive-Probe and existing probing approaches on MedLAMA benchmark ( \u00a75.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_28",
            "start": 127,
            "end": 231,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_29@0",
            "content": "Then, we conduct in-depth analysis of the stability and applicability of Contrastive-Probe in probing biomedical PLMs ( \u00a75.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_29",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_29@1",
            "content": "Finally, we report an evaluation of a biomedical expert on the probing predictions and highlight our findings ( \u00a75.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_29",
            "start": 127,
            "end": 244,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_30@0",
            "content": "Contrastive-Probe Rewiring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_30",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_31@0",
            "content": "We train our Contrastive-Probe based on 10k sentences which are randomly sampled from the PubMed texts 5 using a mask ratio of 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_31",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_31@1",
            "content": "The best hyperparameters and their tuning options are provided in Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_31",
            "start": 132,
            "end": 206,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_31@2",
            "content": "Probing Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_31",
            "start": 208,
            "end": 225,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_31@3",
            "content": "For the mask predict approach, we use the original implementation of X-FACTR (Jiang et al., 2020a), and set the beam size and the number of masks to 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_31",
            "start": 227,
            "end": 377,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_31@4",
            "content": "Both mask predict and retrieval-based approaches are tested under both the general domain and biomedical domain BERT models, i.e. Bert-based-uncased (Devlin et al., 2019), BlueBERT , BioBERT that are pre-trained on large biomedical corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_31",
            "start": 379,
            "end": 618,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_32@0",
            "content": "Benchmarking on MedLAMA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_32",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_33@0",
            "content": "Comparing Various Probing Approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_33",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_33@1",
            "content": "Table 3 shows the overall results of various probing baselines on MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_33",
            "start": 38,
            "end": 111,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_33@2",
            "content": "It can be seen that the performances of all the existing probing approaches (i.e. generative PLMs, X-FACTR and mask predict) are very low (<1% for acc@1 and <4% for acc@10) regardless of the underlying PLM, which are not effective indicators for measuring knowledge captured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_33",
            "start": 113,
            "end": 387,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_33@3",
            "content": "In contrast, our Contrastive-Probe obtains absolute improvements by up-to \u223c 6% and \u223c 25% on acc@1 and acc10 respectively comparing with the three existing approaches, which validates its effectiveness on measuring the knowledge probing performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_33",
            "start": 389,
            "end": 636,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_33@4",
            "content": "In particular, PubMedBERT model obtains the best probing performance (7.32% in accuracy) for these biomedical queries, validating its effectiveness of capturing biomedical knowledge comparing with other PLMs (i.e. BERT, BlueBERT and BioBERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_33",
            "start": 638,
            "end": 879,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_34@0",
            "content": "Benchmarking with Contrastive-Probe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_34",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_34@1",
            "content": "To further examine the effectiveness of PLMs in captur-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_34",
            "start": 37,
            "end": 91,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_35@0",
            "content": "Figure 3: Performance over answer lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_35",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_36@0",
            "content": "ing biomedical knowledge, we benchmarked several state-of-the-art biomedical PLMs (including pure pre-trained and knowledge-enhanced models) on MedLAMA through our Contrastive-Probe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_36",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_36@1",
            "content": "Table 4 shows the probing results over the full and hard sets (detailed macro and micro accuracies are provided in Appendix).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_36",
            "start": 183,
            "end": 307,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_36@2",
            "content": "In general, we can observe that these biomedical PLMs always perform better than general-domain PLMs (i.e., BERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_36",
            "start": 309,
            "end": 422,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_36@3",
            "content": "Also, we observe the decay of performance of all these models on the more challenging hard set queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_36",
            "start": 424,
            "end": 526,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_36@4",
            "content": "While PubMedBERT performs the best under all metrics, CoderBERT (which is the knowledge infused PubMedBERT) achieves better performance on micro acc@1, highlighting the benefits of knowledge infusion pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_36",
            "start": 528,
            "end": 740,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@0",
            "content": "Comparison per Answer Length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@1",
            "content": "Since different PLMs use different tokenizers, we use char length of the query answers to split MedLAMA into different bins and test the probing performance over various answer lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 30,
            "end": 214,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@2",
            "content": "Figure 3 shows the result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 216,
            "end": 241,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@3",
            "content": "We can see that the performance of retrievalbased probing in Contrastive-Probe increases as the answer length increase while the performance of mask predict dropped significantly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 243,
            "end": 421,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@4",
            "content": "This result validates that our Contrastive-Probe (retrievalbased) are more reliable at predicting longer answers than the mask predict approach since the lat- 4 shows the acc@1 performance over top 9 relations and the micro average performance of all the 19 relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 423,
            "end": 690,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@5",
            "content": "We can see that the standard deviations are small and the performance over different sets of samples shows the similar trend.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 692,
            "end": 816,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@6",
            "content": "This further highlights that the probing success of Contrastive-Probe is not due the selected pre-training sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 818,
            "end": 934,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@7",
            "content": "Intuitively, the contrastive self-retrieving game ( \u00a74) is equivalent to the formulation of the cloze-style filling task, hence tuning the underlying PLMs makes them better suited for knowledge elicitation needed during probing (like 'rewiring' the switchboards).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 936,
            "end": 1198,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@8",
            "content": "Additionally, from Figure 4 we can also observe that different relations exhibit very different trends during pre-training steps of Contrastive-Probe and peak under different steps, suggesting that we need to treat different types of relational knowledge with different tuning depths when infusing knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 1200,
            "end": 1507,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@9",
            "content": "We leave further exploration of this to future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 1509,
            "end": 1560,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_37@10",
            "content": "Probing by Relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_37",
            "start": 1562,
            "end": 1582,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_38@0",
            "content": "Expert Evaluation on Predictions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_38",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_39@0",
            "content": "To assess whether the actual probing performance could be possibly higher than what is reflected by the commonly used automatic evaluation, we conducted a human evaluation on the prediction result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_39",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_39@1",
            "content": "Specifically, we sample 15 queries and predict their top-10 answers using Contrastive-Probe based on PubMedBERT and ask the assessor 10 to rate the predictions on a scale of [1,5].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_39",
            "start": 198,
            "end": 377,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_40@0",
            "content": "Figure 8 shows the confusion matrices.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_40",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_40@1",
            "content": "11 We observe the followings: (1) There are 3 UMLS answers that are annotated with score level 1-4 (precisely, level 3), which indicates UMLS answers might not always be the perfect answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_40",
            "start": 39,
            "end": 228,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_41@0",
            "content": "(2) There are 20 annotated perfect answers (score 5) in the top 10 predictions that are not marked as the gold answers in the UMLS, which suggests the UMLS does not include all the expected gold knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_41",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_42@0",
            "content": "(3) In general, PubMedBERT achieves an 8.67% (13/150) acc@10 under gold answers, but under the expert annotation the acc@10 is 22%(33/150), which means the probing performance is higher than what evaluated using the automatically extracted answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_42",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_43@0",
            "content": "Related Work and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_43",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_44@0",
            "content": "Knowledge Probing Benchmarks for PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_44",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@0",
            "content": "LAMA (Petroni et al., 2019), which starts this line of work, is a collection of single-token knowledge triples extracted from sources including Wikidata and ConceptNet (Speer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@1",
            "content": "To mitigate the problem of information leakage from the head entity, Poerner et al. (2019) propose LAMA-UHN, which is a hard subset of LAMA that has less token overlaps in head and tail entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 190,
            "end": 384,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@2",
            "content": "X-FACTR (Jiang et al., 2020a) and mLAMA extend knowledge probing to the multilingual scenario and introduce multi-token answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 386,
            "end": 513,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@3",
            "content": "They each propose decoding methods that generate multi-token answers, which we have shown to work poorly on MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 515,
            "end": 630,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@4",
            "content": "BioLAMA (Sung et al., 2021) is a concurrent work that also releases a benchmark for biomedical knowledge probing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 632,
            "end": 744,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@5",
            "content": "We provide a comparison between LAMA, BioLAMA and MedLAMA in terms of (# relations, # queries, avg # answers per query, avg # characters per answer) in the Appendix. 12 Probing via Prompt Engineering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 746,
            "end": 945,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@6",
            "content": "Knowledge probing is sensitive to what prompt is used (Jiang et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 947,
            "end": 1022,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@7",
            "content": "To bootstrap the probing performance, Jiang et al. (2020b) mine more prompts and ensemble them during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1024,
            "end": 1135,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@8",
            "content": "Later works parameterised the prompts and made them trainable (Shin et al., 2020b;Fichtel et al., 2021;Qin and Eisner, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1137,
            "end": 1261,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@9",
            "content": "We have opted out promptengineering methods that require training data in this work, as tuning the prompts are essentially tuning an additional (parameterised) model on top of PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1263,
            "end": 1443,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@10",
            "content": "As pointed out by Fichtel et al. (2021), prompt tuning requires large amounts of training data from the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1445,
            "end": 1553,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@11",
            "content": "Since task training data is used, the additional model parameters are exposed to the target data distribution and can solve the set set by overfitting to such biases .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1555,
            "end": 1721,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@12",
            "content": "In our work, by adaptively finetuning the model with a small set of raw sentences, we elicit the knowl- 12 Our comparison indicated that MedLAMA and BioLAMA have no overlaps in queries, allowing both resources to complement each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1723,
            "end": 1957,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@13",
            "content": "edge out from PLMs but do not expose the data biases from the benchmark (MedLAMA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 1959,
            "end": 2040,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@14",
            "content": "Biomedical Knowledge Probing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 2042,
            "end": 2070,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@15",
            "content": "Nadkarni et al. (2021) train PLMs as KB completion models and test on the same task to understand how much knowledge is in biomedical PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 2072,
            "end": 2210,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@16",
            "content": "BioLAMA focuses on the continuous prompt learning method OptiPrompt (Zhong et al., 2021), which also requires ground-truth training data from the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 2212,
            "end": 2362,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_45@17",
            "content": "Overall, compared to BioLAMA, we have provided a more comprehensive set of probing experiments and analysis, including proposing a novel probing technique and providing human evaluations of model predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_45",
            "start": 2364,
            "end": 2571,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_46@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_46",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_47@0",
            "content": "In this work, we created a carefully curated biomedical probing benchmark, MedLAMA, from the UMLS knowledge graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_47",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_47@1",
            "content": "We illustrated that state-of-the-art probing techniques and biomedical pre-trained languages models (PLMs) struggle to cope with the challenging nature (e.g. multitoken answers) of this specialised domain, reaching only an underwhelming 3% of acc@10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_47",
            "start": 115,
            "end": 364,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_47@2",
            "content": "To reduce the gap, we further proposed a novel contrastive recipe which rewires the underlying PLMs without using any probing-specific data and illustrated that with a lightweight pre-training their accuracies could be pushed to 28%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_47",
            "start": 366,
            "end": 598,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_48@0",
            "content": "Our experiments also revealed that different layers of transformers encode different types of information, reflected by their individual success at handling certain types of prompts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_48",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_48@1",
            "content": "Additionally, using a human expert, we showed that the existing evaluation criteria could overpenalise the models as many valid responses that PLMs produce are not in the ground truth UMLS knowledge graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_48",
            "start": 183,
            "end": 387,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_48@2",
            "content": "This further highlights the importance of having a human in the loop to better understand the potentials and limitations of PLMs in encoding domain specific factual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_48",
            "start": 389,
            "end": 563,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_49@0",
            "content": "Our findings indicate that the real lower bound on the amount of factual knowledge encoded by PLMs is higher than we estimated, since such bound can be continuously improved by optimising both the encoding space (e.g. using our selfsupervised contrastive learning technique) and the input space (e.g. using the prompt optimising techniques (Shin et al., 2020a;Qin and Eisner, 2021)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_49",
            "start": 0,
            "end": 382,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_49@1",
            "content": "We leave further exploration of integrating the two possibilities to future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_49",
            "start": 384,
            "end": 464,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_50@0",
            "content": "Table 5 shows the detailed relation names and their manual prompts of our MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_50",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_51@0",
            "content": "In this paper, we use two automatic metrics to distinguish hard and easy queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_51",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_51@1",
            "content": "In particular, we first filter out easy queries by an exact matching metric (i.e. the exactly matching all the words of answer from queries).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_51",
            "start": 82,
            "end": 222,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_51@2",
            "content": "Since our MedLAMA contains multiple answers for queries, we use a threshold on the average exact matching score, i.e. avg-match>0.1, to filter out easy examples, where avg-match is calculated by: avg-match = Count(matched answers) Count(total answers) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_51",
            "start": 224,
            "end": 476,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_52@0",
            "content": "This metric can remove all the queries that match the whole string of answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_52",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_52@1",
            "content": "However, some common sub-strings between queries and answers also prone to reveal answers, particularly benefiting those retrieval-based probing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_52",
            "start": 79,
            "end": 234,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_52@2",
            "content": "E.g. <Magnesium Chloride, may-prevent, Magnesium Deficiency>. Therefore, we further calculate the ROUGE-L score (Lin and Och, 2004) for all the queries by regarding <query, answers> pairs as the <hypothesis, reference> pairs, and further filter out the ROUGE-L>0.1 queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_52",
            "start": 236,
            "end": 508,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_53@0",
            "content": "During the writing of this work, we notice that Sung et al. (2021) also released a biomedical knowledge probing benchmark, called BioLAMA, which is a work concurrent to ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_53",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_53@1",
            "content": "In Table 6, we compare our MedLAMA with LAMA (Petroni et al., 2019) and BioLAMA (Sung et al., 2021) in terms of their statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_53",
            "start": 175,
            "end": 303,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_53@2",
            "content": "We found that there is only 1 overlapped relation (i.e. may treat) between BioLAMA and our MedLAMA, and no same query can be found.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_53",
            "start": 305,
            "end": 435,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_53@3",
            "content": "Moreover, Sung et al. (2021) only use two existing probing approach on their proposed BioLAMA, while in this paper we further proposed a new probing approach Contrastive-Probe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_53",
            "start": 437,
            "end": 612,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_53@4",
            "content": "We also evaluate our Contrastive-Probe in BioLAMA, and the result is shown in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_53",
            "start": 614,
            "end": 690,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_54@0",
            "content": "To further investigate the impact of the mask ratio to the probing performance, we also test our Contrastive-Probe based on PubMedBERT over different mask ratios ({0.1, 0.2, 0.3, 0.4, 0.5}) under the 10 random sentence sets, the result of which is shown in Figure 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_54",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_54@1",
            "content": "We can see that over different mask ratios the Contrastive-Probe always reaches their best performance under certain pre-training steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_54",
            "start": 267,
            "end": 402,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_54@2",
            "content": "And the performance curves of mask ratios are different over the full and hard sets, but they all achieves a generally good performance when the mask ratio is 0.5, which validates that different mask ratios favour different types queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_54",
            "start": 404,
            "end": 641,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_54@3",
            "content": "acc@1 acc@5 acc@1 acc@5 acc@1 acc@5 Table 8: Hyperparameters along with their search grid.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_54",
            "start": 643,
            "end": 732,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_54@4",
            "content": "* marks the values used to obtain the reported results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_54",
            "start": 734,
            "end": 788,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_55@0",
            "content": "Iz Beltagy, Kyle Lo, Arman Cohan, SciB-ERT: A pretrained language model for scientific text, 2019, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_55",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_56@0",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_56",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_57@0",
            "content": "Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry,  Askell, Language models are few-shot learners, 2020, EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_57",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_58@0",
            "content": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu, Knowledgeable or educated guess? revisiting language models as knowledge bases, 2021, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_58",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_59@0",
            "content": "UNKNOWN, None, 2021, Autoregressive entity retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_59",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_60@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_60",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_61@0",
            "content": "Philipp Dufter, Nora Kassner, Hinrich Sch\u00fctze, Static embeddings as efficient knowledge bases, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_61",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_62@0",
            "content": "Leandra Fichtel, Jan-Christoph Kalo, Wolf-Tilo Balke, Prompt tuning or fine-tuninginvestigating relational knowledge in pre-trained language models, 2021, AKBC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_62",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_63@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_63",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_64@0",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_64",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_65@0",
            "content": "Goran Glava\u0161, Ivan Vuli\u0107, Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation, 2021, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_65",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_66@0",
            "content": "Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing, , ACM Transactions on Computing for Healthcare, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_66",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_67@0",
            "content": "UNKNOWN, None, 2019, Clinicalbert: Modeling clinical notes and predicting hospital readmission, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_67",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_68@0",
            "content": "Zhengbao Jiang, Antonios Anastasopoulos, Haibo Ding, and Graham Neubig. 2020a. X-factr: Multilingual factual knowledge retrieval from pretrained language models, , EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_68",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_69@0",
            "content": "UNKNOWN, None, 2020, How can we know what language models know?, TACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_69",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_70@0",
            "content": "Nora Kassner, Philipp Dufter, Hinrich Sch\u00fctze, Multilingual lama: Investigating knowledge in multilingual pretrained language models, 2021, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_70",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_71@0",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, BioBERT: a pretrained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_71",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_72@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_72",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_73@0",
            "content": "Chin-Yew Lin, Franz Josef Och, Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics, 2004, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_73",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_74@0",
            "content": "UNKNOWN, None, 2021, Selfalignment pre-training for biomedical entity representations, NAACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_74",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_75@0",
            "content": "Fangyu Liu, Ivan Vuli\u0107, Anna Korhonen, Nigel Collier, Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_75",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_76@0",
            "content": "Qianchu Liu, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vuli\u0107, Mirrorwic: On eliciting word-in-context representations from pretrained language models, 2021, CoNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_76",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_77@0",
            "content": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta: A robustly optimized bert pretraining approach, 2020, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_77",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_78@0",
            "content": "George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen, Alexander Wong, Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_78",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2021, Scientific language models for biomedical knowledge base completion: An empirical study, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_79",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2018, Representation learning with contrastive predictive coding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_80",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_81@0",
            "content": "Yifan Peng, Shankai Yan, Zhiyong Lu, Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets, 2019, BioNLP Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_81",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_82@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_82",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_83@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases, 2019, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_83",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_84@0",
            "content": "UNKNOWN, None, , Erol Bahadroglu, Alec Peltekian, and Gr\u00e9goire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_84",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2019, E-bert: Efficient-yet-effective entity embeddings for bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_85",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_86@0",
            "content": "Nina Poerner, Ulli Waltinger, Hinrich Sch\u00fctze, E-bert: Efficient-yet-effective entity embeddings for bert, 2020, EMNLP: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_86",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_87@0",
            "content": "Guanghui Qin, Jason Eisner, Learning how to ask: Querying lms with mixtures of soft prompts, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_87",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_88@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_88",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_89@0",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_89",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_90@0",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, Eliciting knowledge from language models using automatically generated prompts, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_90",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_91@0",
            "content": "Vered Shwartz, Rachel Rudinger, Oyvind Tafjord, you are grounded!\": Latent name artifacts in pre-trained language models, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_91",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_92@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_92",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_93@0",
            "content": "Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang, Can language models be biomedical knowledge bases, 2021, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_93",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2020, Coder: Knowledge infused cross-lingual medical term embedding for term normalization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_94",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_95@0",
            "content": "Zexuan Zhong, Dan Friedman, Danqi Chen, Factual probing is [mask]: Learning vs. learning to recall, 2021, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_95",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_96@0",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_96",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_97",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_98",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_99@0",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_99",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2019, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_100",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_101",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_102@0",
            "content": "UNKNOWN, None, , Knowledge-enhanced, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_102",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_103",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_104@0",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_104",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_105",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_106@0",
            "content": "UNKNOWN, None, , Table 10: Benchmarking biomedical PLMs on MedLAMA via our Contrastive-Probe, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_106",
            "start": 0,
            "end": 94,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_1",
            "tgt_ix": "59-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_1",
            "tgt_ix": "59-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_4",
            "tgt_ix": "59-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_5",
            "tgt_ix": "59-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_6",
            "tgt_ix": "59-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_7",
            "tgt_ix": "59-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_8",
            "tgt_ix": "59-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_9",
            "tgt_ix": "59-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_10",
            "tgt_ix": "59-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_11",
            "tgt_ix": "59-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_12",
            "tgt_ix": "59-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_14",
            "tgt_ix": "59-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_18",
            "tgt_ix": "59-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_19",
            "tgt_ix": "59-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_21",
            "tgt_ix": "59-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_22",
            "tgt_ix": "59-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_23",
            "tgt_ix": "59-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_24",
            "tgt_ix": "59-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_25",
            "tgt_ix": "59-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_17",
            "tgt_ix": "59-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_26",
            "tgt_ix": "59-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_28",
            "tgt_ix": "59-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_29",
            "tgt_ix": "59-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_30",
            "tgt_ix": "59-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_35",
            "tgt_ix": "59-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_34",
            "tgt_ix": "59-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_39",
            "tgt_ix": "59-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_40",
            "tgt_ix": "59-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_41",
            "tgt_ix": "59-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_42",
            "tgt_ix": "59-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_44",
            "tgt_ix": "59-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_43",
            "tgt_ix": "59-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_43",
            "tgt_ix": "59-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_43",
            "tgt_ix": "59-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_47",
            "tgt_ix": "59-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_48",
            "tgt_ix": "59-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_49",
            "tgt_ix": "59-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_51",
            "tgt_ix": "59-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_50",
            "tgt_ix": "59-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_52",
            "tgt_ix": "59-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_0",
            "tgt_ix": "59-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_1",
            "tgt_ix": "59-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_2",
            "tgt_ix": "59-ARR_v1_2@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_3",
            "tgt_ix": "59-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_4",
            "tgt_ix": "59-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_5",
            "tgt_ix": "59-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_5",
            "tgt_ix": "59-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_5",
            "tgt_ix": "59-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_6",
            "tgt_ix": "59-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_6",
            "tgt_ix": "59-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_6",
            "tgt_ix": "59-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_7",
            "tgt_ix": "59-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_7",
            "tgt_ix": "59-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_8",
            "tgt_ix": "59-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_8",
            "tgt_ix": "59-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_9",
            "tgt_ix": "59-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_9",
            "tgt_ix": "59-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_10",
            "tgt_ix": "59-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_10",
            "tgt_ix": "59-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_10",
            "tgt_ix": "59-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_10",
            "tgt_ix": "59-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_11",
            "tgt_ix": "59-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_11",
            "tgt_ix": "59-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_11",
            "tgt_ix": "59-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_12",
            "tgt_ix": "59-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_12",
            "tgt_ix": "59-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_13",
            "tgt_ix": "59-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_14",
            "tgt_ix": "59-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_14",
            "tgt_ix": "59-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_15",
            "tgt_ix": "59-ARR_v1_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_16",
            "tgt_ix": "59-ARR_v1_16@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_17",
            "tgt_ix": "59-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_17",
            "tgt_ix": "59-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_17",
            "tgt_ix": "59-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_17",
            "tgt_ix": "59-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_18",
            "tgt_ix": "59-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_19",
            "tgt_ix": "59-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_19",
            "tgt_ix": "59-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_20",
            "tgt_ix": "59-ARR_v1_20@18",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_21",
            "tgt_ix": "59-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_21",
            "tgt_ix": "59-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_22",
            "tgt_ix": "59-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_22",
            "tgt_ix": "59-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_22",
            "tgt_ix": "59-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_22",
            "tgt_ix": "59-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_23",
            "tgt_ix": "59-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_24",
            "tgt_ix": "59-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_25",
            "tgt_ix": "59-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_25",
            "tgt_ix": "59-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_25",
            "tgt_ix": "59-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_26",
            "tgt_ix": "59-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_27",
            "tgt_ix": "59-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_28",
            "tgt_ix": "59-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_28",
            "tgt_ix": "59-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_29",
            "tgt_ix": "59-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_29",
            "tgt_ix": "59-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_30",
            "tgt_ix": "59-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_31",
            "tgt_ix": "59-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_32",
            "tgt_ix": "59-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_33",
            "tgt_ix": "59-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_34",
            "tgt_ix": "59-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_34",
            "tgt_ix": "59-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_35",
            "tgt_ix": "59-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_36",
            "tgt_ix": "59-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_37",
            "tgt_ix": "59-ARR_v1_37@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_38",
            "tgt_ix": "59-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_39",
            "tgt_ix": "59-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_39",
            "tgt_ix": "59-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_40",
            "tgt_ix": "59-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_40",
            "tgt_ix": "59-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_41",
            "tgt_ix": "59-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_42",
            "tgt_ix": "59-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_43",
            "tgt_ix": "59-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_44",
            "tgt_ix": "59-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_45",
            "tgt_ix": "59-ARR_v1_45@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_46",
            "tgt_ix": "59-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_47",
            "tgt_ix": "59-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_47",
            "tgt_ix": "59-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_47",
            "tgt_ix": "59-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_48",
            "tgt_ix": "59-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_48",
            "tgt_ix": "59-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_48",
            "tgt_ix": "59-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_49",
            "tgt_ix": "59-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_49",
            "tgt_ix": "59-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_50",
            "tgt_ix": "59-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_51",
            "tgt_ix": "59-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_51",
            "tgt_ix": "59-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_51",
            "tgt_ix": "59-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_52",
            "tgt_ix": "59-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_52",
            "tgt_ix": "59-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_52",
            "tgt_ix": "59-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_53",
            "tgt_ix": "59-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_54",
            "tgt_ix": "59-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_54",
            "tgt_ix": "59-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_54",
            "tgt_ix": "59-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_54",
            "tgt_ix": "59-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_54",
            "tgt_ix": "59-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_55",
            "tgt_ix": "59-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_56",
            "tgt_ix": "59-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_57",
            "tgt_ix": "59-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_58",
            "tgt_ix": "59-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_59",
            "tgt_ix": "59-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_60",
            "tgt_ix": "59-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_61",
            "tgt_ix": "59-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_62",
            "tgt_ix": "59-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_63",
            "tgt_ix": "59-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_64",
            "tgt_ix": "59-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_65",
            "tgt_ix": "59-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_66",
            "tgt_ix": "59-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_67",
            "tgt_ix": "59-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_68",
            "tgt_ix": "59-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_69",
            "tgt_ix": "59-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_70",
            "tgt_ix": "59-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_71",
            "tgt_ix": "59-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_72",
            "tgt_ix": "59-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_73",
            "tgt_ix": "59-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_74",
            "tgt_ix": "59-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_75",
            "tgt_ix": "59-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_76",
            "tgt_ix": "59-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_77",
            "tgt_ix": "59-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_78",
            "tgt_ix": "59-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_79",
            "tgt_ix": "59-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_80",
            "tgt_ix": "59-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_81",
            "tgt_ix": "59-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_82",
            "tgt_ix": "59-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_83",
            "tgt_ix": "59-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_84",
            "tgt_ix": "59-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_85",
            "tgt_ix": "59-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_86",
            "tgt_ix": "59-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_87",
            "tgt_ix": "59-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_88",
            "tgt_ix": "59-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_89",
            "tgt_ix": "59-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_90",
            "tgt_ix": "59-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_91",
            "tgt_ix": "59-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_92",
            "tgt_ix": "59-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_93",
            "tgt_ix": "59-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_94",
            "tgt_ix": "59-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_95",
            "tgt_ix": "59-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_96",
            "tgt_ix": "59-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_97",
            "tgt_ix": "59-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_98",
            "tgt_ix": "59-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_99",
            "tgt_ix": "59-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_100",
            "tgt_ix": "59-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_101",
            "tgt_ix": "59-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_102",
            "tgt_ix": "59-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_103",
            "tgt_ix": "59-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_104",
            "tgt_ix": "59-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_105",
            "tgt_ix": "59-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_106",
            "tgt_ix": "59-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1130,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "59-ARR",
        "version": 1
    }
}