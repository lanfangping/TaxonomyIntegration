{
    "nodes": [
        {
            "ix": "59-ARR_v1_review2_0",
            "content": "59-ARR_v1_review2",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_review2_1",
            "content": "paper_summary. This paper presents a carefully curated biomedical probing benchmark - MedLAMA. The authors transform the relations in the UMLS knowledge graph to form triples for probing PLMs\u2019 biomedical knowledge. The curated benchmark consists of 19 relations with 1000 examples each. The authors find that existing probing methods fail to achieve meaningful performance on MedLAMA. Therefore, they propose to \u201crewire the PLM parameters for retrieval\u201d. They propose Contrastive-Probe that fine-tunes the PLM with little data from the original pretraining corpora. The proposed method improves ACC@1 and ACC@10 on MedLAMA to 7%/27%. The authors further conduct experiments for different PLMs, layers, and different relations. Finally, they perform an examination by an expert on the predicted answers and find that the automatic evaluation might underestimate the performance of the models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_review2_2",
            "content": "summary_of_strengths. - The paper is well-written and easy to follow. The presentation of the ideas is clear.\n-This paper presents a biomedical probing benchmark that might reinforce the research in this direction.\n-The paper proposes a simple method to make PLMs more suitable for retrieval probing, hence improving the probing results. The improved results make the comparison on the benchmark more meaningful.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_review2_3",
            "content": "summary_of_weaknesses. The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "59-ARR_v1_review2_4",
            "content": "comments,_suggestions_and_typos. - You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n-Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?",
            "ntype": "p",
            "meta": null
        }
    ],
    "span_nodes": [
        {
            "ix": "59-ARR_v1_review2_0@0",
            "content": "59-ARR_v1_review2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_0",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@0",
            "content": "paper_summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@1",
            "content": "This paper presents a carefully curated biomedical probing benchmark - MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 15,
            "end": 93,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@2",
            "content": "The authors transform the relations in the UMLS knowledge graph to form triples for probing PLMs\u2019 biomedical knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 95,
            "end": 213,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@3",
            "content": "The curated benchmark consists of 19 relations with 1000 examples each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 215,
            "end": 285,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@4",
            "content": "The authors find that existing probing methods fail to achieve meaningful performance on MedLAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 287,
            "end": 383,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@5",
            "content": "Therefore, they propose to \u201crewire the PLM parameters for retrieval\u201d.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 385,
            "end": 453,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@6",
            "content": "They propose Contrastive-Probe that fine-tunes the PLM with little data from the original pretraining corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 455,
            "end": 564,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@7",
            "content": "The proposed method improves ACC@1 and ACC@10 on MedLAMA to 7%/27%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 566,
            "end": 632,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@8",
            "content": "The authors further conduct experiments for different PLMs, layers, and different relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 634,
            "end": 725,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_1@9",
            "content": "Finally, they perform an examination by an expert on the predicted answers and find that the automatic evaluation might underestimate the performance of the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_1",
            "start": 727,
            "end": 890,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@0",
            "content": "summary_of_strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@1",
            "content": "- The paper is well-written and easy to follow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 22,
            "end": 68,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@2",
            "content": "The presentation of the ideas is clear.\n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 70,
            "end": 109,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@3",
            "content": "-This paper presents a biomedical probing benchmark that might reinforce the research in this direction.\n",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 110,
            "end": 214,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@4",
            "content": "-The paper proposes a simple method to make PLMs more suitable for retrieval probing, hence improving the probing results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 215,
            "end": 336,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_2@5",
            "content": "The improved results make the comparison on the benchmark more meaningful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_2",
            "start": 338,
            "end": 411,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_3@0",
            "content": "summary_of_weaknesses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_3",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_3@1",
            "content": "The technical novelty is rather lacking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_3",
            "start": 23,
            "end": 62,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_3@2",
            "content": "Although I believe this doesn't affect the contribution of this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_3",
            "start": 64,
            "end": 133,
            "label": {}
        },
        {
            "ix": "59-ARR_v1_review2_4@0",
            "content": "comments,_suggestions_and_typos. - You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n-Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "59-ARR_v1_review2_4",
            "start": 0,
            "end": 311,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_3",
            "tgt_ix": "59-ARR_v1_review2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "59-ARR_v1_review2_0",
            "tgt_ix": "59-ARR_v1_review2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_1",
            "tgt_ix": "59-ARR_v1_review2_1@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_2",
            "tgt_ix": "59-ARR_v1_review2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_3",
            "tgt_ix": "59-ARR_v1_review2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_3",
            "tgt_ix": "59-ARR_v1_review2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_3",
            "tgt_ix": "59-ARR_v1_review2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "59-ARR_v1_review2_4",
            "tgt_ix": "59-ARR_v1_review2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "59-ARR_v1_review2",
    "meta": {
        "ix_counter": 25,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy"
    }
}