{
    "nodes": [
        {
            "ix": "69-ARR_v2_0",
            "content": "Measuring Fairness of Text Classifiers via Prediction Sensitivity",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_2",
            "content": "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation -ACCU-MULATED PREDICTION SENSITIVITY, which measures fairness in machine learning models based on the model's prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans' perception of fairness. We conduct experiments on two text classification datasets -JIGSAW TOXICITY, and BIAS IN BIOS, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "69-ARR_v2_4",
            "content": "Ongoing research is increasingly emphasizing the development of methods which detect and mitigate unfair social bias present in machine learningbased language processing models. These methods come under the umbrella of algorithmic fairness which has been quantitatively expressed with numerous definitions (Mehrabi et al., 2019;Jacobs and Wallach, 2019). These fairness definitions are * * Work done while working at Amazon broadly categorized into two types, i.e, individual fairness and group fairness. Individual fairness (e.g., counter-factual fairness (Kusner et al., 2017)) is aimed at evaluating whether a model gives similar predictions for individuals with similar personal attributes (e.g., age or race). On the other hand, group fairness (e.g., statistical parity (Dwork et al., 2012)) evaluates fairness across cohorts with same protected attributes instead of individuals (Mehrabi et al., 2019). Although these two broad categories of fairness define valid notions of fairness, human understanding of fairness is also used to measure fairness in machine learning models . Existing studies often consider only one or two these verticals of measuring fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_5",
            "content": "In our work, we propose a formulation based on models sensitivity to input features -the accumulated prediction sensitivity, to measure fairness of model predictions. We establish its theoretical relationship with statistical parity (group fairness) and individual fairness (Dwork et al., 2012) metrics. We then demonstrate the correlation between the proposed metric and human perception of fairness using empirical experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_6",
            "content": "Researchers have proposed metrics to quantify fairness based on a model's sensitivity to input features. Specifically, ; Ngong et al. (2020) propose a prediction sensitivity metric that attempts to quantify the extent to which a single prediction depends on a protected attribute. The protected attribute encodes the membership status of an individual in a protected group. Prediction sensitivity can be seen as a form of feature attribution, but specialized to the protected attribute. In our work, we extend their concept of prediction sensitivity to propose accumulated prediction sensitivity. Akin to the metric proposed by Ngong et al., 2020), our metric also relies on model output's sensitivity to changes in input features. Our metric generalizes their notion of sensitivity, where the model sensitivity to various input features can be weighted non-uniformly. We show that the formulation follows certain properties for the chosen definitions of group and individual fairness and also present several methodologies to select weights assigned to sensitivity of model's output to input features. For each selection, we present the correlation between the accumulated prediction sensitivity and human assessment of the model-output fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_7",
            "content": "We define our metric in Section 3 and present bounds on it (under settings when a classifier follows the selected group fairness or individual fairness constraints) in Sections 4 and 5, respectively. Next, given that the human perception of fairness is not theoretically defined, we present an empirical study on two text classification tasks in Section 6. We request a group of annotators to annotate whether they think that model output is biased against a specific gender and observe that the proposed metric correlates positively with more biased outcomes. We then observe correlations between our metric and the stated human understanding of fairness. We find that not only the proposed accumulated prediction sensitivity metric correlates positively with human perception of bias, but also beats an existing baseline based on counterfactual fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "69-ARR_v2_9",
            "content": "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019;Mehrabi et al., 2019;Sheng et al., 2019). Dwork et al. (2012) and Kusner et al. (2017) focus on individual fairness and propose novel classification approaches to ensure that a classification decision is fair towards an individual. Another set of works focus on group fairness. Corbett-Davies et al. (2017) present fair classification to ensure population from different race groups receive similar treatment. Hardt et al. (2016) focus on shifting the cost of incorrect classification from disadvantaged groups. Zhao and Chang (2020) Multiple recent works also focus on developing new dataset and associated metrics to capture various types of biases. For example, and Nangia et al. (2020) propose dataset and metrics to measure social biases and stereotypes in language model generations, Bolukbasi et al. (2016); Caliskan et al. (2017); Manzini et al. (2019) define metrics to access gender and race biases in word vector representations, and define metrics to quantify and mitigate biases in visual recognition task. Ethayarajh (2020) propose Bernstein bounds to represent uncertainty about the bias. Majority of these bias metrics are automatically computed, for example, using a regard classifier (Sheng et al., 2019), sentiment classifier , toxicity classifier (Dixon et al., 2018) or true positive rate difference between privileged and underprivileged groups (De-Arteaga et al., 2019). A few works additionally validate the alignment of these automatically computed bias metrics with human understanding of biases by collecting annotations of biases on a subset of test data from crowdworkers (Sheng et al., 2019;. Blodgett et al. (2021Blodgett et al. ( , 2020 discuss the limitations of several of these bias datasets and measurements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_10",
            "content": "However, the majority of existing bias metrics are specific to the model type and the application domain used, they may not be tested for correlation with human judgement of biases, and their relationship to existing definitions of fairness has not been explored. Additionally, metrics such as true positive or error difference between groups requires ground truth labels, thereby making their computation in real-time systems difficult. Speicher et al. (2018) have attempted to present unified approach to measuring group and individual fairness via inequality indices, however we note that such metrics are non-trivial to extend to unstructured data such as text. For example, gender information in a text may be subtle (e.g. mention of softball) and it is unclear whether presence of this word should be considered to impact the genderness of the text. Accumulated prediction sensitivity metric, presented in this paper, attempts to address all the above limitations of existing bias metrics. We acknowledge that the proposed metric is yet to be associated with other notions of fairness (e.g. preference based notion of fairness (Zafar et al., 2017)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_11",
            "content": "Accumulated Prediction Sensitivity",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "69-ARR_v2_12",
            "content": "Below, we define accumulated prediction sensitivity, a metric that captures the sensitivity of a model to protected attributes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_13",
            "content": "Definition 1 (Accumulated Prediction sensitivity).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_14",
            "content": "Let x \u2208 X be a feature vector drawn from the input space X. Let w, v be stochastic vectors whose entries are non-negative values that sum to one. Given x, let f be a K-class classifier, such that f (x) = [f 1 (x), .., f k (x), .., f K (x)] denotes the K-dimensional probability output generated by the classifier. We define accumulated prediction sensitivity P as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_15",
            "content": "P = w T J v; where J (k, i) = \u2202f k (x) \u2202x i .(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_16",
            "content": "J is a matrix 1 such that the (k, i) th entry is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_17",
            "content": "\u2202f k (x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_18",
            "content": "\u2202x i , where x i is the i th entry in x. The product w T J sums the absolute derivatives | \u2202f k (x) \u2202x i | across f k , k = 1, .., K and returns a vector of summed derivatives with respect to each x i \u2208 x. The product of v with w T J further averages the derivatives across all the features x i \u2208 x to yield the scalar P .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_19",
            "content": "The value \u2202f k (x) \u2202x i captures the expected change in model output for the k th class given a perturbation in x i . If x i is a protected feature, arguably a smaller value of \u2202f k (x) \u2202x i implies a fairer model; as then the model's outcome does not change sharply with changes in x i . To capture the sensitivity of the model with respect to the protected features, one also needs to choose v judiciously. For example, given the explicit set of protected features in x, one can select v such that only entries corresponding to those features are assigned a non-zero value, while the rest are set to zero. Given this heuristic, we expect the value P to be smaller for fairer models. In the following sections, we connect the accumulated prediction sensitivity to two known notions of fairness and human perception of fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_20",
            "content": "Relation to Group Fairness: Statistical Parity",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "69-ARR_v2_21",
            "content": "Given a set of protected features (e.g. gender), a model satisfies statistical parity if model outcome is independent of the protected features (we note that identifying protected features may not always be feasible in the real world). We represent the feature vector x = [x p , x l ], where x p is the set of protected features and x l is the remainder. Accordingly, we choose v to be a vector such that the entries that sum | \u2202f k (xp) \u2202x i |\u2200x p \u2208 x p in J are nonzero; and zero otherwise. This choice is intuitive as then we sum the gradients in J that correspond to protected features and measure model's sensitivity to them. The predictor f (x) will satisfy statistical parity if f (x p , x l ) = f (x p , x l )\u2200x p = x p . Given this, we state the following theorem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_22",
            "content": "Theorem 1. Given a vector v with non-zero entries corresponding to x p and zero entries for x l , if the predictor f (x) satisfies statistical parity with respect to x p , accumulated prediction sensitivity will be zero.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_23",
            "content": "Proof: If f (x) satisfies statistical parity with respect to x p , the values \u2202f k (x) \u2202xp \u2200x p \u2208 x p will be all zeros. This is due to the fact that the function f k (x) can not be defined based on entries x p \u2208 x p for it to be independent of them. Therefore, for every multiplication in the product J v, either the entry",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_24",
            "content": "\u2202f k (x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_25",
            "content": "\u2202xp will be 0 or the entry in v corresponding to x l will be 0. Hence, P will be 0.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_26",
            "content": "Appendix A presents empirical results in computing P on a synthetic dataset. We construct a dataset where a feature (hair length) correlates with a protected attribute (gender). We show that if the modeler unintentionally uses the correlated feature while attempting to build a classifier with statistical parity, our metric can be used for evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_27",
            "content": "5 Relation to Individual Fairness Dwork et al. (2012) state the notion of individual based fairness as: \"We interpret the goal of mapping similar people similarly to mean that the distributions assigned to similar people are similar\". They propose adding a Lipschitz property constraint during the classifier optimization. Given a loss function L defined to optimize the parameters \u03b8 of the classifier f (x), a distance function d(x, x ) that computes distance between data-points x, x , another distance function D(f (x)), f (x )) that computes distance between classifier predictions on x, x and a constant L, Dwork et al. (2012) propose the following constrained optimization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_28",
            "content": "min \u03b8 L; such that D(f (x)), f (x )) < Ld(x, x ); \u2200x, x \u2208 X.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_29",
            "content": "(2)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_30",
            "content": "It is natural to choose an Lp norm (Bourbaki, 1987) for d and D. For a classifier f that is trained with the above constrained optimization and the choice of distance metrics D, d is an Lp norm, we state the following.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_31",
            "content": "Theorem 2. If the predictor f (x) is trained with the constrained optimization stated in Eq. (2), the accumulated prediction sensitivity will be upper bounded by L.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_32",
            "content": "Proof: We restate the constraint in Eq. ( 2) as (Note that the inequality sign does not change as distance metrics D, d are required to be positive for",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_33",
            "content": "x = x ) \u2200x = x , L > D(f (x), f (x )) d(x, x ) . (3",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_34",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_35",
            "content": "Given the inequality holds for any pair of x, x , it must also hold for an x of the following choice.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_36",
            "content": "x = x + [0, 0, \u2206x i , 0, 0], where \u2206x i is a scalar perturbation in the i th entry in x. For a chosen Lp norm, Eq (3) becomes",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_37",
            "content": "L > [ K k=1 |f k (x) \u2212 f k (x )| p ] 1 p |\u2206x i | > [|f k (x) \u2212 f k (x )| p ] 1 p |\u2206x i | . (4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_38",
            "content": ") Since each entry |f k (x) \u2212 f k (x )| p , k = 1, .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_39",
            "content": ".K is expected to be non-zero and zeroing out all such entries (but one) will yield a lower value than the summation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_40",
            "content": "K k=1 |f k (x) \u2212 f k (x )| p .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_41",
            "content": "We can rewrite Eq. ( 4) as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_42",
            "content": "|f k (x) \u2212 f k (x + [0, 0, \u2206x i , 0, 0])| |\u2206x i | .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_43",
            "content": "We can further chose \u2206x i such that it is small perturbation, leading to the following.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_44",
            "content": "\u2206x i \u21920 |f k (x) \u2212 f k (x + [0, 0, \u2206x i , 0, 0])| |\u2206x i | = \u2202f k (x) \u2202x i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_45",
            "content": "Therefore, each entry in J is upper bounded by L. As vectors v, w are stochastic and they compute weighted averages of bounded entries in J , P (defined in Eq. ( 1)) must be less than or equal to L.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_46",
            "content": "We also note that as L becomes larger, the constraint in the Eq. (2) becomes looser. Therefore, a higher value of L during optimization is expected to loosen the fairness constraint as well as the bound on fairness sensitivity. This aligns with our intuition of lower values of P for fairer models. We compute value of L on a synthetically generated classification data, optimized with the individual fairness constraint in equation 2. The results are presented in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_47",
            "content": "Correlations with Human Perception of Fairness",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "69-ARR_v2_48",
            "content": "While the conditional statistical parity and individual fairness establish theoretical constraints on the model behaviour (e.g. independence from protected features and similarity in prediction outcomes for similar data-points), humans may carry a different notion of fairness for model outcomes on individual data-points. This notion may be based on their understanding of cultural norms, which in turn effect their decisions in identifying which model outputs could be considered biased. In this section, we present experiments that correlate accumulated prediction sensitivity with human perception of fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_49",
            "content": "Human Perception of Fairness",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "69-ARR_v2_50",
            "content": "Given a data-point x and model prediction f (x), we assign one of the K classes to the data-point. In order to evaluate the human perception of fairness on the data-point, we request a group of annotators to evaluate the model prediction (taken as the argmax of the model output) and assess whether they believe the output is biased. For instance, given the social/cultural norms, a profession classifier assigning a data-point \"she worked in a hospital\" to nurse instead of doctor can be perceived as biased. To correlate the accumulated prediction sensitivity P with the human understanding of fairness, we conduct experiments on two text classification datasets. We describe the datasets below, followed by our choices for w and v.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_51",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "69-ARR_v2_52",
            "content": "We experiment with our proposed metric on two classification tasks, i.e, occupation classification on Bias in Bios dataset (De-Arteaga et al., 2019) 2 and toxicity classification with Jigsaw Toxicity dataset 3 . We focus on these two datasets as they have been investigated in several previous studies and have been reported to carry significant presence of bias. BIAS IN BIOS data (De-Arteaga et al., 2019) is purposed to train occupation classifier which predicts occupation given the biography of an individual. For this data, the task classifier is an occupation classification model which is composed of a standard LSTM-based encoder combined with the output layer of 28 nodes, i.e, number of occupation classes. 2 The data is available at https://github.com/microsoft/biosbias JIGSAW TOXICITY dataset is commonly used to train toxic classifier which is tasked to predict if an input sentence is toxic or not. This dataset has input sentences as the comments from Wikipedia's talk page edits labeled with the degree of toxicity. In this dataset, the task classifier is a binary classifier trained to predict whether a comment is toxic or not. We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier. The task classifier trained with Jigsaw Toxicity dataset achieved an AUC of 0.957. Table 4 in appendix summarizes the train/test/valid split for the 2 datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_53",
            "content": "Selecting the vectors w",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "69-ARR_v2_54",
            "content": "The vector w sums up the absolute partial derivatives of f k (x) with respect to a given feature x i , \u2200k = 1, .., K. In our setup, we consider input features to be the word embeddings and the matrix J is computed over the same. Given a Ddimensional word embedding, K classes and N words in x, J will be a matrix of size (K) \u00d7 (DN ). In all our experiments, we choose w to be a uniform vector with entries 1/K. Such a choice assigns equal weight to the partial derivatives computed over each class. One may chose to put a higher weight on derivatives computed over a specific class, if there is a reason to believe that the accumulated prediction sensitivity should be informed more with respect to that class. For instance, for a classifier that stratifies medical images into various diseases (Agrawal et al., 2019), disparity in model performance with respect to malicious diseases can be considered more costly. Therefore, derivatives for classes that represent more malicious disease can be weighted higher.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_55",
            "content": "Selecting the vectors v",
            "ntype": "title",
            "meta": {
                "section": "6.4"
            }
        },
        {
            "ix": "69-ARR_v2_56",
            "content": "Through the vector v, we aim to select words in x that carry gendered information. We use two formulations for the the vector v as discussed below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_57",
            "content": "Using a list of gendered words",
            "ntype": "title",
            "meta": {
                "section": "6.4.1"
            }
        },
        {
            "ix": "69-ARR_v2_58",
            "content": "In this setup, we use the set of gendered words from (Bolukbasi et al., 2016) and assign entries in v corresponding to those words as 1/(N g \u00d7 D), where N g is the count of gendered words in the data-point.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_59",
            "content": "Using a Protected Status Model (PSM)",
            "ntype": "title",
            "meta": {
                "section": "6.4.2"
            }
        },
        {
            "ix": "69-ARR_v2_60",
            "content": "While prior work has used word matching to a pre-defined corpus of tokens describing various demographic cohorts (Bolukbasi et al., 2016), these corpus do not contain words that stereotypically are associated with a particular cohort but may not be explicitly tied to that cohort. For example, the word \"volleyball\" is associated with females in the analysis presented by (Dinan et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_61",
            "content": "To capture this nuance, we propose using another classifier (that acts on the same dataset as used to train the original classifier, for which we aim to compute P ) and using it to identify tokens containing information about the protected attribute (e.g. gender). We discuss the model training below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_62",
            "content": "Protected Status Model: To extend accumulated prediction sensitivity to settings with no explicit protected attribute, we train a protected status model g. Given the data-point x, goal of the PSM model g(x) is to predict the protected attributes. Given a trained g(x), we then compute another matrix J g , where the (j, i) th entry is | \u2202gm(x)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_63",
            "content": "x i | (g m is the probability outcomes corresponding to the m th protected attribute class; e.g. male in a gender classifier). We then define an entry v i \u2208 v as j J g (m, i) (the vector v is normalized to be stochastic). Intuitively, the sum j J g (m, i) captures the model output sensitivity with respect to the input features x i and is expected to higher if x i carries more gendered information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_64",
            "content": "In our experiments, we train separate PSM models for gender sensitivity computation on Bias-inbios and Jigsaw data-sets, as each data-point in these data-sets is additionally labeled with a binary gender class (male/female) 4 . Gender PSMs predicts the associated gender given the datapoint x. Training PSM on the same datasets used to train the task classifier f helps capture the gender stereotypes present in the respective datasets. For instance, in a given dataset, if the word \"volleyball\" appears more often in the data-points that correspond to the female gender, the gender classifier's sensitivity to this word is expected to be high as the classifier may pay higher emphasis to this word for gender classification. We use the same model architecture as the task classifiers for PSM. PSM for gender classification achieve an accuracy of 98.79% (Male Acc:98.84% Female Acc:98.17%) and 95.39% (Male Acc:95.92% Female Acc:96.22%) for Bias in bios and Jigsaw Toxicity datasets, respectively. These accuracies are computed over the same train/test split as the task classifier. We use the bootstrap method to compute statistical significance (Koehn, 2004) at p-value<0.05.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_65",
            "content": "Using Word Embedding Vectors",
            "ntype": "title",
            "meta": {
                "section": "6.4.3"
            }
        },
        {
            "ix": "69-ARR_v2_66",
            "content": "In addition to using the list of gendered words and PSM, we also test with a setting where we multiply the word embedding vectors to the proposed formulations of v. We stack the word embedding vectors for each word x i \u2208 x to obtain a vector of embeddings e i . We perform an element-wise multiplication of the embedding vectors e i with the vector with entries 1/(N g \u00d7D) for gendered words or j J g (j, i) obtained using PSM. This choice is motivated based upon the findings in (Han et al., 2020). They leverage the magnitude of embedding vectors in determining saliency of the input words for the classification task at hand. Their proposed methodology computes saliency maps over the features x i \u2208 x by multiplying embedding vectors with partial derivatives of the class probabilities with respect to embedding vectors themselves.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_67",
            "content": "Fairness Metrics",
            "ntype": "title",
            "meta": {
                "section": "6.5"
            }
        },
        {
            "ix": "69-ARR_v2_68",
            "content": "We experiment with six fairness metrics. Out of the six, one metric is a baseline based on counterfactual fairness and the rest are variants of the accumulated prediction sensitivity P . Counter-factual Fairness (CF) : We use the counter-factual fairness definition mentioned in Garg et al. (2019) and compute the metric as the difference in model predictions between the original sample f (x) and its corresponding counter-factual gendered sample f ( x). We take the L1 norm of the vector f (x) \u2212 f ( x). For example, we take the difference in predictions between the sample \"She practices dentistry\" and \"He practices dentistry\", which is the corresponding counter-factual sample. We use the definitional gender token substitutions from Bolukbasi et al. (2016) to create counter-factual samples. P1: Uniformly weighted prediction sensitivity : In this setting, the values of w and v are set to uniform values 1 K and 1 DN , respectively. This is a weak baseline as the choice of v does not provide any information regarding the gender-ness of the input words. P2: Weighted Prediction Sensitivity based on PSM : In this setting, w is chosen to be a uniform vector, while v is chosen based on the PSM model. P3: Weighted Prediction sensitivity + Embedding weights : In this setting, v is chosen based on the PSM model (akin to the metric in P2) which is further multiplied element-wise with the word embedding vectors. P4: Hard gender weights based Prediction sensitivity : In this metric, we use the list of gendered words described in section 6.4.1 to determine v. The value of entries in v is set to 1 DNg . P5: Hard gender weights based prediction sensitivity + Embeddings: This setting is same as above, except entries in v are further multiplied element-wise with the word embedding vectors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_69",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "6.6"
            }
        },
        {
            "ix": "69-ARR_v2_70",
            "content": "To evaluate whether the proposed prediction sensitivity correlates with human perception of fairness, we collect annotations from crowd workers using the Amazon Mechanical Turk platform. Crowd workers are asked to annotate if a model prediction appears to be a biased prediction or not. For Bias in Bios dataset, each sample presented to the annotators has the biography and occupation predicted by the model. We collect annotations on a random sample of the test set. For each biography and a predicted occupation, we ask annotators to label if the prediction is indicative of bias or if it is unbiased. Bias refers to a situation where an occupation is incorrectly predicted based on the gender associated with the biography. For instance, if the input biography is \"she studied at Harvard Medical School and practices dentistry.\" and is predicted as nurse, then we call this prediction biased since the biography fits better for a doctor. In case of unbiased predictions, the prediction is not expected to be influenced by the gender content in the biography. Table 3 presents a sample of examples provided to the annotators for the Bias in bios dataset. Each page in the annotation task consisted of ten biography-profession pairs. We collect annotations for each biography-profession pair from at least three annotators and pick the label with majority vote. Similarly for Jigsaw Toxicity dataset, each sample presented to the annotators contains the text and associated toxicity predicted by the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_71",
            "content": "We restrict the set of annotators to be master annotators and the location of annotators to be Unites States. Based on the initial pilot studies conducted in the Amazon Mechanical Turk platform, we setup a payment rate to ensure a fair compensation of at least 15$/hour for all annotators that work at an average pace. We annotated 900 test data-points from each dataset. We note that these test data-points were misclassified by the classifiers f trained for each dataset. While such a sampling may not conform to the true distribution of biased/unbiased model outcomes on the overall test set, we expect to get more biased samples amongst the misclassified samples. The distribution between biased and unbiased outputs was about 55:45 for Bias in Bios and 50:50 for Jigsaw Toxicity. For the Bias in Bios and Jigsaw Toxicity datsets, we obtained a Fliess' kappa of 0.43 and 0.47, respectively, amongst the three annotators. This is considered a moderate level of agreement, which we believe is expected for an relatively ambiguous task to identify model outcomes influenced by gender. We compute mutual information and bi-serial correlations as the primary measures of association between the human annotations and the accumulated model sensitivity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_72",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "69-ARR_v2_73",
            "content": "Table 1 lists the bi-serial correlations and mutual information between manual annotations and the different fairness metrics. First, we observe that correlations of the baseline with human judgement are mediocre (0.326 and 0.214) compared to the human judgement. We attribute this to the fact that the metric attempts to quantify a fairly subjective assessment of bias that may have different interpretation (as also pointed out by the moderate level of annotation agreement across annotators). However, the proposed variants of P have stronger correlations compared to the counter-factual baseline (except the method P1). As expected, we see the smallest correlation for P1, since this metric does not account for gender-ness in v. However, metrics that determine v based on PSM prediction sensitivity and gendered words get higher correlations over P1 and the CF baseline. Variant of P with v informed using the embedding vectors further lead to improved correlations. We also observe weaker statistical significance in the case of Jigsaw Toxicity due to a weaker PSM. We attribute this to the noise present in gender annotations for Jigsaw Toxicity dataset. Hence, the performance of PSM in predicting the protected status is crucial for accurately measuring fairness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_74",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "7.1"
            }
        },
        {
            "ix": "69-ARR_v2_75",
            "content": "In order to further analyse the effect of PSM, we look into heat-maps capturing w T J and v separately. As a reminder, the first quantity captures the weighted average of partial derivatives of class probabilites with respect to the input features, while the second quantity computes the weights assigned to sum up the aforementioned averages. Table 2 shows while v mostly captures gendered words such as \"she\", \"her\" and \"woman\", it also captures words such as \"social\", \"architecture\" and \"cheated\" to carry more gendered information compared to other words. While these words conventionally are not gendered, for the datasets at hand, they seem to provide information whether the input data-point belongs to male/female gender. We also note that w T J weighs on occupation specific tokens such as \"physician\", \"executive\", etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_76",
            "content": "This finding supports our motivations to compute v based on PSM and capturing feature attributions assigned to tokens that are implicitly related to a specific gender (instead of the definitional gender tokens only). Hence, by incorporating PSM in computing P , we can capture bias present in nontrivial gendered tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_77",
            "content": "Considerations for Accumulated Prediction Sensitivity Metric",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "69-ARR_v2_78",
            "content": "While the results showcase the promise of our metric, we draw the attention of the reader to the following considerations: (1) We observed that the metric quality depends on choice of the hyperparameters w and v. In this regard, our metric is not different from other metrics that also depend on a hyper-parameter choice. For example, any classifier based metric has a threshold parameter and counterfactual fairness metrics rely on hyperparameters such as the selected gendered words. (2) Our metric only works for models for which gradients can be computed. Most modern deep learning based models carry this property. (3) Lastly, we note that it is hard to interpret the absolute value of the proposed metric. The metric value should be used for relative comparison of two models which share input feature space and label space.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_79",
            "content": "In addition, we note two considerations for relying on a PSM classifier. First, training it requires access to gender labels. Second, the PSM model itself could be biased. Given that gender labels may not always be available for the dataset used to train model at hand, we study the impact of transferring a PSM model trained on a different dataset on computing our metric. We also evaluate the effect of bias in PSM model on the overall metric value and present results in the Appendix D. We make observations such as the quality of the metric degrades as PSM becomes more biased. Based on these observations, we recommend that if modeler is not able to obtain high performance PSM models, they fall back to using sources such as gendered words for computing the vector v.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_80",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "9"
            }
        },
        {
            "ix": "69-ARR_v2_81",
            "content": "Evaluating fairness is a challenging task as it requires selecting a notion of fairness (e.g. group or individual fairness) and then identifying metrics that can capture these notions of fairness while evaluating a classifier. Additionally, certain notions of fairness may not be well defined and can change based upon social norms (e.g. \"volleyball\" being closely associated with females); that may seep into the dataset at hand. In this work, we define an accumulated prediction sensitivity metric that relies on the partial derivatives of model's class probabilities with respect to input features. We establish properties of this metric with respect to the three verticals of fairness metrics: group, individual and human-perception based. We provide bounds on the metric's value when a predictor is expected to carry statistical parity or is trained with individual fairness. We also evaluate this metric with fairness as perceived through human evaluation of model outputs. We test variants of the proposed metric against an existing baseline derived from counter-factual fairness and observe better mutual information and correlation. Specifically, a variant of the metric that relies on a Protected Status Model (that identifies tokens that carry gender information but may not conventionally be considered gendered) yields the best correlation with the human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_82",
            "content": "In the future, one can associate the proposed formulation with other categories of group and individual fairness (Mehrabi et al., 2019). We also aim to test the metric on other datasets with other protected attributes (e.g. race, nationality). Finally, we can compare the metric across these datasets to compare trends across protected groups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_83",
            "content": "Broader Impact and Ethics Statement",
            "ntype": "title",
            "meta": {
                "section": "10"
            }
        },
        {
            "ix": "69-ARR_v2_84",
            "content": "This work can be used to evaluate bias in models, and thus used to evaluate models serving human consumers. As with all metrics, the metric does not capture all notions of bias, and thus should not be the only consideration for serving models. While this is a valid risk, this is one that is not specific to prediction sensitivity. Good use of this metric requires users to be cognizant of these strengths and weaknesses. We also note that the metric requires defining protected attributes (e.g. gender) and our work carries the limitation that the selected datasets contain binary gender annotations. Defining protected attributes may not always be possible and when possible, the protected attribute classes may not be comprehensive.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_85",
            "content": "Let us consider a classification task on whether to hire a person given the following features: x 1 is the person's educational experience in years, x 2 is their hair length and x 3 is their gender. We synthetically generate data for individuals in this dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_86",
            "content": "x 1 is drawn uniformly randomly between 0 and 10. x 3 is (again) considered to be binary gender (set 0 for male and 1 for female drawn from a bernoulli distribution) and x 2 is drawn from a Gaussian distribution conditioned on x 3 . x 2 \u223c N (2, 10) (Gaussian distribution with a mean 2 and variance 10) if x 3 = 0 and x 2 \u223c N (10, 10) if x 3 = 1. We sample 10,000 data-points from the above distribution to generate a dataset. Let us consider two cases with two different classifiers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_87",
            "content": "Case 1: Classifier depends on x 1 , x 2 In this case, the modeler only deems x 3 to be the protected feature. Let us assume that they build a classifier as shown in equation 5. Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not. f = \u03c3((x 1 \u2212 5) + (x 2 \u2212 6))",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_88",
            "content": "(5)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_89",
            "content": "Given only x 3 is considered as the protected feature by the modeler, they will set the vector v to [0, 0, 1] T . Let us assume that the modeler sets P as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_90",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_91",
            "content": "We recommend the modeler computes \u2202x 2 \u2202x 3 and \u2202x 1 \u2202x 3 and if they are non-zero, use the chain rule in equation 7 to compute P .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_92",
            "content": "\u2202f k ([x 1 , x 2 ]) \u2202x 3 = \u2202f k ([x 1 , x 2 ]) \u2202x 2 \u2202x 2 \u2202x 3 (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_93",
            "content": "For the dataset generated above, we compute the partials \u2202x 2 \u2202x 3 and \u2202x 1 \u2202x 3 . Additionally, since x 3 is a discrete variable, we approximate partial derivatives using all available right-difference quotients and left-difference quotients, as shown in equation 9. In order to compute \u2202x The mean above is computed over all n = m. Similarly,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_94",
            "content": "\u2202x 1 \u2202x 3 x 3 =x m 3 = Mean x m 1 \u2212 x n 1 x m 3 \u2212 x n 3 (9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_95",
            "content": "Given the dataset we generated, we compute values for \u2202x 1 \u2202x 3 x 3 =x m 3 and \u2202x 2 \u2202x 3 x 3 =x m 3 for an arbitrarily chosen m. We obtain values of 7.98 and 0.01, respectively. Note that we expect the second value to be 0, but due to noise in gradient approximation obtain a non-zero value. We re-write equation 6 as shown below and plug in the values of the partials. We obtain a non-zero value of P in this case.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_96",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (10) = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 2 \u2202x 2 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 2 \u2202x 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_97",
            "content": "Case 2: Classifier only depends only on x 1 In this case, the modeler deems both x 2 , x 3 to be protected features and builds a classifier as depicted below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_98",
            "content": "f = \u03c3(x 1 \u2212 5)(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_99",
            "content": "Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not. Additionally, given x 2 and x 3 are protected features, P is set to",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_100",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 1 2 1 2 \uf8f9 \uf8fb (13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_101",
            "content": "Given that the classifier does not explicitly rely on x 2 and x 3 , we can rewrite equation 14 as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_102",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 1 \u2202x 1 \u2202x 2 \u2202f 1 \u2202x 1 \u2202x 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 1 \u2202x 1 \u2202x 2 \u2202f 2 \u2202x 1 \u2202x 1 \u2202x 3 \uf8ee \uf8f0 0 1 2 1 2 \uf8f9 \uf8fb (14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_103",
            "content": "We obtain the partial derivatives \u2202x 1 \u2202x 2 x 2 =x m 2 and \u2202x 1 \u2202x 3 x 3 =x m",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_104",
            "content": ". For an arbitrary chosen x m 1 , we obtain values of 0.01 and -0.01. While we expect both these values to be zero given our data construction, they are non-zero due to the gradient approximation. Barring the noise in gradient computation, P is 0 in this case.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "69-ARR_v2_105",
            "content": "Taruna Agrawal, Rahul Gupta, Shrikanth Narayanan, On evaluating CNN representations for low resource medical image classification, 2019-05-12, IEEE International Conference on Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Taruna Agrawal",
                    "Rahul Gupta",
                    "Shrikanth Narayanan"
                ],
                "title": "On evaluating CNN representations for low resource medical image classification",
                "pub_date": "2019-05-12",
                "pub_title": "IEEE International Conference on Acoustics, Speech and Signal Processing",
                "pub": "IEEE"
            }
        },
        {
            "ix": "69-ARR_v2_106",
            "content": "Su Lin, Solon Blodgett, Hal Barocas, Iii Daum\u00e9, Hanna Wallach, Language (technology) is power: A critical survey of \"bias\" in NLP, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    " Su Lin",
                    "Solon Blodgett",
                    "Hal Barocas",
                    "Iii Daum\u00e9",
                    "Hanna Wallach"
                ],
                "title": "Language (technology) is power: A critical survey of \"bias\" in NLP",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_107",
            "content": "Su Lin, Gilsinia Blodgett, Alexandra Lopez, Robert Olteanu, Hanna Sim,  Wallach, Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    " Su Lin",
                    "Gilsinia Blodgett",
                    "Alexandra Lopez",
                    "Robert Olteanu",
                    "Hanna Sim",
                    " Wallach"
                ],
                "title": "Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_108",
            "content": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai, Man is to computer programmer as woman is to homemaker?, 2016, debiasing word embeddings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Tolga Bolukbasi",
                    "Kai-Wei Chang",
                    "James Zou",
                    "Venkatesh Saligrama",
                    "Adam Kalai"
                ],
                "title": "Man is to computer programmer as woman is to homemaker?",
                "pub_date": "2016",
                "pub_title": "debiasing word embeddings",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_109",
            "content": "UNKNOWN, None, 1987, Topological vector spaces, elements of mathematics, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "1987",
                "pub_title": "Topological vector spaces, elements of mathematics",
                "pub": "Springer"
            }
        },
        {
            "ix": "69-ARR_v2_110",
            "content": "Aylin Caliskan, Joanna Bryson, Arvind Narayanan, Semantics derived automatically from language corpora contain human-like biases, 2017, Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Aylin Caliskan",
                    "Joanna Bryson",
                    "Arvind Narayanan"
                ],
                "title": "Semantics derived automatically from language corpora contain human-like biases",
                "pub_date": "2017",
                "pub_title": "Science",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_111",
            "content": "Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq, Algorithmic decision making and the cost of fairness, 2017, Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Sam Corbett-Davies",
                    "Emma Pierson",
                    "Avi Feller",
                    "Sharad Goel",
                    "Aziz Huq"
                ],
                "title": "Algorithmic decision making and the cost of fairness",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_112",
            "content": "Maria De-Arteaga, Alexey Romanov, H Wallach, J Chayes, C Borgs, A Chouldechova, K Sahin Cem Geyik, A Kenthapadi,  Kalai, Bias in bios: A case study of semantic representation bias in a high-stakes setting, 2019, Proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Maria De-Arteaga",
                    "Alexey Romanov",
                    "H Wallach",
                    "J Chayes",
                    "C Borgs",
                    "A Chouldechova",
                    "K Sahin Cem Geyik",
                    "A Kenthapadi",
                    " Kalai"
                ],
                "title": "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Conference on Fairness, Accountability, and Transparency",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_113",
            "content": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Bold: Dataset and metrics for measuring biases in open-ended language generation, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jwala Dhamala",
                    "Tony Sun",
                    "Varun Kumar",
                    "Satyapriya Krishna",
                    "Yada Pruksachatkun",
                    "Kai-Wei Chang",
                    "Rahul Gupta"
                ],
                "title": "Bold: Dataset and metrics for measuring biases in open-ended language generation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_114",
            "content": "Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, Adina Williams, Multidimensional gender bias classification, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Emily Dinan",
                    "Angela Fan",
                    "Ledell Wu",
                    "Jason Weston",
                    "Douwe Kiela",
                    "Adina Williams"
                ],
                "title": "Multidimensional gender bias classification",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_115",
            "content": "Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, Lucy Vasserman, Measuring and mitigating unintended bias in text classification, 2018-02-02, Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Lucas Dixon",
                    "John Li",
                    "Jeffrey Sorensen",
                    "Nithum Thain",
                    "Lucy Vasserman"
                ],
                "title": "Measuring and mitigating unintended bias in text classification",
                "pub_date": "2018-02-02",
                "pub_title": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",
                "pub": "ACM"
            }
        },
        {
            "ix": "69-ARR_v2_116",
            "content": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel, Fairness through awareness, 2012-01-08, Innovations in Theoretical Computer Science, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Cynthia Dwork",
                    "Moritz Hardt",
                    "Toniann Pitassi",
                    "Omer Reingold",
                    "Richard Zemel"
                ],
                "title": "Fairness through awareness",
                "pub_date": "2012-01-08",
                "pub_title": "Innovations in Theoretical Computer Science",
                "pub": "ACM"
            }
        },
        {
            "ix": "69-ARR_v2_117",
            "content": "Kawin Ethayarajh, Is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Kawin Ethayarajh"
                ],
                "title": "Is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "69-ARR_v2_118",
            "content": "Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed Chi, Alex Beutel, Counterfactual fairness in text classification through robustness, 2019-01-27, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Sahaj Garg",
                    "Vincent Perot",
                    "Nicole Limtiaco",
                    "Ankur Taly",
                    "Ed Chi",
                    "Alex Beutel"
                ],
                "title": "Counterfactual fairness in text classification through robustness",
                "pub_date": "2019-01-27",
                "pub_title": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019",
                "pub": "ACM"
            }
        },
        {
            "ix": "69-ARR_v2_119",
            "content": "Xiaochuang Han, Byron Wallace, Yulia Tsvetkov, Explaining black box predictions and unveiling data artifacts through influence functions, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Xiaochuang Han",
                    "Byron Wallace",
                    "Yulia Tsvetkov"
                ],
                "title": "Explaining black box predictions and unveiling data artifacts through influence functions",
                "pub_date": "2020-07-05",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "69-ARR_v2_120",
            "content": "Moritz Hardt, Eric Price, Nati Srebro, Equality of opportunity in supervised learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Moritz Hardt",
                    "Eric Price",
                    "Nati Srebro"
                ],
                "title": "Equality of opportunity in supervised learning",
                "pub_date": "2016-12-05",
                "pub_title": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_121",
            "content": "UNKNOWN, None, 2019, Measurement and fairness, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Measurement and fairness",
                "pub": "CoRR"
            }
        },
        {
            "ix": "69-ARR_v2_122",
            "content": "J Michael, Seth Kearns, Aaron Neel, Zhiwei Steven Roth,  Wu, An empirical study of rich subgroup fairness for machine learning, 2019-01-29, Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 2019, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "J Michael",
                    "Seth Kearns",
                    "Aaron Neel",
                    "Zhiwei Steven Roth",
                    " Wu"
                ],
                "title": "An empirical study of rich subgroup fairness for machine learning",
                "pub_date": "2019-01-29",
                "pub_title": "Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 2019",
                "pub": "ACM"
            }
        },
        {
            "ix": "69-ARR_v2_123",
            "content": "Philipp Koehn, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL, 2004-07-26, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Philipp Koehn"
                ],
                "title": "A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL",
                "pub_date": "2004-07-26",
                "pub_title": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
                "pub": "ACL"
            }
        },
        {
            "ix": "69-ARR_v2_124",
            "content": "Matt Kusner, Joshua Loftus, Chris Russell, Ricardo Silva, Counterfactual fairness, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Matt Kusner",
                    "Joshua Loftus",
                    "Chris Russell",
                    "Ricardo Silva"
                ],
                "title": "Counterfactual fairness",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_125",
            "content": "Thomas Manzini, Yao Lim, Alan Chong, Yulia Black,  Tsvetkov, Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Thomas Manzini",
                    "Yao Lim",
                    "Alan Chong",
                    "Yulia Black",
                    " Tsvetkov"
                ],
                "title": "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "69-ARR_v2_126",
            "content": "UNKNOWN, None, 2009, Towards a measure of individual fairness for deep learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Towards a measure of individual fairness for deep learning",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_127",
            "content": "UNKNOWN, None, 1908, A survey on bias and fairness in machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "1908",
                "pub_title": "A survey on bias and fairness in machine learning",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_128",
            "content": "Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel Bowman, Crows-pairs: A challenge dataset for measuring social biases in masked language models, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Nikita Nangia",
                    "Clara Vania",
                    "Rasika Bhalerao",
                    "Samuel Bowman"
                ],
                "title": "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_129",
            "content": "UNKNOWN, None, 2020, Towards auditability for fairness in deep learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Towards auditability for fairness in deep learning",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_130",
            "content": "Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, Kai-Wei Chang, Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classification, 2021-08-01, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yada Pruksachatkun",
                    "Satyapriya Krishna",
                    "Jwala Dhamala",
                    "Rahul Gupta",
                    "Kai-Wei Chang"
                ],
                "title": "Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classification",
                "pub_date": "2021-08-01",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_131",
            "content": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng, The woman worked as a babysitter: On biases in language generation, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Emily Sheng",
                    "Kai-Wei Chang",
                    "Premkumar Natarajan",
                    "Nanyun Peng"
                ],
                "title": "The woman worked as a babysitter: On biases in language generation",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "69-ARR_v2_132",
            "content": "Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna Gummadi, Adish Singla, Adrian Weller, Muhammad Bilal Zafar, A unified approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices, 2018-08-19, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Till Speicher",
                    "Hoda Heidari",
                    "Nina Grgic-Hlaca",
                    "Krishna Gummadi",
                    "Adish Singla",
                    "Adrian Weller",
                    "Muhammad Bilal Zafar"
                ],
                "title": "A unified approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices",
                "pub_date": "2018-08-19",
                "pub_title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "pub": "ACM"
            }
        },
        {
            "ix": "69-ARR_v2_133",
            "content": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai Elsherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, William Wang, Mitigating gender bias in natural language processing: Literature review, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Tony Sun",
                    "Andrew Gaut",
                    "Shirlyn Tang",
                    "Yuxin Huang",
                    "Mai Elsherief",
                    "Jieyu Zhao",
                    "Diba Mirza",
                    "Elizabeth Belding",
                    "Kai-Wei Chang",
                    "William Wang"
                ],
                "title": "Mitigating gender bias in natural language processing: Literature review",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "69-ARR_v2_134",
            "content": "Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, Vicente Ordonez, Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Tianlu Wang",
                    "Jieyu Zhao",
                    "Mark Yatskar",
                    "Kai-Wei Chang",
                    "Vicente Ordonez"
                ],
                "title": "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
                "pub_date": "2019-10-27",
                "pub_title": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019",
                "pub": "IEEE"
            }
        },
        {
            "ix": "69-ARR_v2_135",
            "content": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, Krishna Gummadi, Adrian Weller, From parity to preference-based notions of fairness in classification, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Muhammad Bilal Zafar",
                    "Isabel Valera",
                    "Manuel Gomez-Rodriguez",
                    "Krishna Gummadi",
                    "Adrian Weller"
                ],
                "title": "From parity to preference-based notions of fairness in classification",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "69-ARR_v2_136",
            "content": "Jieyu Zhao, Kai-Wei Chang, LOGAN: Local group bias detection by clustering, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Jieyu Zhao",
                    "Kai-Wei Chang"
                ],
                "title": "LOGAN: Local group bias detection by clustering",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "69-ARR_v2_0@0",
            "content": "Measuring Fairness of Text Classifiers via Prediction Sensitivity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@0",
            "content": "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@1",
            "content": "Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 136,
            "end": 311,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@2",
            "content": "In this work, we propose a new formulation -ACCU-MULATED PREDICTION SENSITIVITY, which measures fairness in machine learning models based on the model's prediction sensitivity to perturbations in input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 313,
            "end": 523,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@3",
            "content": "The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 525,
            "end": 728,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@4",
            "content": "We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 730,
            "end": 867,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@5",
            "content": "It also correlates well with humans' perception of fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 869,
            "end": 928,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@6",
            "content": "We conduct experiments on two text classification datasets -JIGSAW TOXICITY, and BIAS IN BIOS, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 930,
            "end": 1138,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_2@7",
            "content": "We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_2",
            "start": 1140,
            "end": 1334,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@0",
            "content": "Ongoing research is increasingly emphasizing the development of methods which detect and mitigate unfair social bias present in machine learningbased language processing models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@1",
            "content": "These methods come under the umbrella of algorithmic fairness which has been quantitatively expressed with numerous definitions (Mehrabi et al., 2019;Jacobs and Wallach, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 178,
            "end": 353,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@2",
            "content": "These fairness definitions are * * Work done while working at Amazon broadly categorized into two types, i.e, individual fairness and group fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 355,
            "end": 503,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@3",
            "content": "Individual fairness (e.g., counter-factual fairness (Kusner et al., 2017)) is aimed at evaluating whether a model gives similar predictions for individuals with similar personal attributes (e.g., age or race).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 505,
            "end": 713,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@4",
            "content": "On the other hand, group fairness (e.g., statistical parity (Dwork et al., 2012)) evaluates fairness across cohorts with same protected attributes instead of individuals (Mehrabi et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 715,
            "end": 907,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@5",
            "content": "Although these two broad categories of fairness define valid notions of fairness, human understanding of fairness is also used to measure fairness in machine learning models .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 909,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_4@6",
            "content": "Existing studies often consider only one or two these verticals of measuring fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_4",
            "start": 1085,
            "end": 1170,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_5@0",
            "content": "In our work, we propose a formulation based on models sensitivity to input features -the accumulated prediction sensitivity, to measure fairness of model predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_5",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_5@1",
            "content": "We establish its theoretical relationship with statistical parity (group fairness) and individual fairness (Dwork et al., 2012) metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_5",
            "start": 167,
            "end": 302,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_5@2",
            "content": "We then demonstrate the correlation between the proposed metric and human perception of fairness using empirical experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_5",
            "start": 304,
            "end": 428,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@0",
            "content": "Researchers have proposed metrics to quantify fairness based on a model's sensitivity to input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@1",
            "content": "Specifically, ; Ngong et al. (2020) propose a prediction sensitivity metric that attempts to quantify the extent to which a single prediction depends on a protected attribute.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 105,
            "end": 279,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@2",
            "content": "The protected attribute encodes the membership status of an individual in a protected group.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 281,
            "end": 372,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@3",
            "content": "Prediction sensitivity can be seen as a form of feature attribution, but specialized to the protected attribute.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 374,
            "end": 485,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@4",
            "content": "In our work, we extend their concept of prediction sensitivity to propose accumulated prediction sensitivity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 487,
            "end": 595,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@5",
            "content": "Akin to the metric proposed by Ngong et al., 2020), our metric also relies on model output's sensitivity to changes in input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 597,
            "end": 730,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@6",
            "content": "Our metric generalizes their notion of sensitivity, where the model sensitivity to various input features can be weighted non-uniformly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 732,
            "end": 867,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@7",
            "content": "We show that the formulation follows certain properties for the chosen definitions of group and individual fairness and also present several methodologies to select weights assigned to sensitivity of model's output to input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 869,
            "end": 1101,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_6@8",
            "content": "For each selection, we present the correlation between the accumulated prediction sensitivity and human assessment of the model-output fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_6",
            "start": 1103,
            "end": 1246,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_7@0",
            "content": "We define our metric in Section 3 and present bounds on it (under settings when a classifier follows the selected group fairness or individual fairness constraints) in Sections 4 and 5, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_7",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_7@1",
            "content": "Next, given that the human perception of fairness is not theoretically defined, we present an empirical study on two text classification tasks in Section 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_7",
            "start": 200,
            "end": 355,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_7@2",
            "content": "We request a group of annotators to annotate whether they think that model output is biased against a specific gender and observe that the proposed metric correlates positively with more biased outcomes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_7",
            "start": 357,
            "end": 559,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_7@3",
            "content": "We then observe correlations between our metric and the stated human understanding of fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_7",
            "start": 561,
            "end": 655,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_7@4",
            "content": "We find that not only the proposed accumulated prediction sensitivity metric correlates positively with human perception of bias, but also beats an existing baseline based on counterfactual fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_7",
            "start": 657,
            "end": 855,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@0",
            "content": "Multiple efforts have looked into defining, measuring, and mitigating biases in NLP models (Sun et al., 2019;Mehrabi et al., 2019;Sheng et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@1",
            "content": "Dwork et al. (2012) and Kusner et al. (2017) focus on individual fairness and propose novel classification approaches to ensure that a classification decision is fair towards an individual.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 151,
            "end": 339,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@2",
            "content": "Another set of works focus on group fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 341,
            "end": 385,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@3",
            "content": "Corbett-Davies et al. (2017) present fair classification to ensure population from different race groups receive similar treatment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 387,
            "end": 517,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@4",
            "content": "Hardt et al. (2016) focus on shifting the cost of incorrect classification from disadvantaged groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 519,
            "end": 619,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@5",
            "content": "Zhao and Chang (2020) Multiple recent works also focus on developing new dataset and associated metrics to capture various types of biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 621,
            "end": 759,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@6",
            "content": "For example, and Nangia et al. (2020) propose dataset and metrics to measure social biases and stereotypes in language model generations, Bolukbasi et al. (2016); Caliskan et al. (2017); Manzini et al. (2019) define metrics to access gender and race biases in word vector representations, and define metrics to quantify and mitigate biases in visual recognition task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 761,
            "end": 1127,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@7",
            "content": "Ethayarajh (2020) propose Bernstein bounds to represent uncertainty about the bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 1129,
            "end": 1211,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@8",
            "content": "Majority of these bias metrics are automatically computed, for example, using a regard classifier (Sheng et al., 2019), sentiment classifier , toxicity classifier (Dixon et al., 2018) or true positive rate difference between privileged and underprivileged groups (De-Arteaga et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 1213,
            "end": 1501,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_9@9",
            "content": "A few works additionally validate the alignment of these automatically computed bias metrics with human understanding of biases by collecting annotations of biases on a subset of test data from crowdworkers (Sheng et al., 2019;. Blodgett et al. (2021Blodgett et al. ( , 2020 discuss the limitations of several of these bias datasets and measurements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_9",
            "start": 1503,
            "end": 1852,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@0",
            "content": "However, the majority of existing bias metrics are specific to the model type and the application domain used, they may not be tested for correlation with human judgement of biases, and their relationship to existing definitions of fairness has not been explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@1",
            "content": "Additionally, metrics such as true positive or error difference between groups requires ground truth labels, thereby making their computation in real-time systems difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 264,
            "end": 436,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@2",
            "content": "Speicher et al. (2018) have attempted to present unified approach to measuring group and individual fairness via inequality indices, however we note that such metrics are non-trivial to extend to unstructured data such as text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 438,
            "end": 664,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@3",
            "content": "For example, gender information in a text may be subtle (e.g. mention of softball) and it is unclear whether presence of this word should be considered to impact the genderness of the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 666,
            "end": 854,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@4",
            "content": "Accumulated prediction sensitivity metric, presented in this paper, attempts to address all the above limitations of existing bias metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 856,
            "end": 994,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_10@5",
            "content": "We acknowledge that the proposed metric is yet to be associated with other notions of fairness (e.g. preference based notion of fairness (Zafar et al., 2017)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_10",
            "start": 996,
            "end": 1154,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_11@0",
            "content": "Accumulated Prediction Sensitivity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_11",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_12@0",
            "content": "Below, we define accumulated prediction sensitivity, a metric that captures the sensitivity of a model to protected attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_12",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_13@0",
            "content": "Definition 1 (Accumulated Prediction sensitivity).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_13",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_14@0",
            "content": "Let x \u2208 X be a feature vector drawn from the input space X. Let w, v be stochastic vectors whose entries are non-negative values that sum to one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_14",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_14@1",
            "content": "Given x, let f be a K-class classifier, such that f (x) = [f 1 (x), .., f k (x), .., f K (x)] denotes the K-dimensional probability output generated by the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_14",
            "start": 146,
            "end": 312,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_14@2",
            "content": "We define accumulated prediction sensitivity P as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_14",
            "start": 314,
            "end": 363,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_15@0",
            "content": "P = w T J v; where J (k, i) = \u2202f k (x) \u2202x i .(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_15",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_16@0",
            "content": "J is a matrix 1 such that the (k, i) th entry is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_16",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_17@0",
            "content": "\u2202f k (x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_17",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_18@0",
            "content": "\u2202x i , where x i is the i th entry in x. The product w T J sums the absolute derivatives | \u2202f k (x) \u2202x i | across f k , k = 1, .., K and returns a vector of summed derivatives with respect to each x i \u2208 x. The product of v with w T J further averages the derivatives across all the features x i \u2208 x to yield the scalar P .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_18",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@0",
            "content": "The value \u2202f k (x) \u2202x i captures the expected change in model output for the k th class given a perturbation in x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@1",
            "content": "If x i is a protected feature, arguably a smaller value of \u2202f k (x) \u2202x i implies a fairer model; as then the model's outcome does not change sharply with changes in x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 118,
            "end": 287,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@2",
            "content": "To capture the sensitivity of the model with respect to the protected features, one also needs to choose v judiciously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 289,
            "end": 407,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@3",
            "content": "For example, given the explicit set of protected features in x, one can select v such that only entries corresponding to those features are assigned a non-zero value, while the rest are set to zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 409,
            "end": 606,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@4",
            "content": "Given this heuristic, we expect the value P to be smaller for fairer models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 608,
            "end": 683,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_19@5",
            "content": "In the following sections, we connect the accumulated prediction sensitivity to two known notions of fairness and human perception of fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_19",
            "start": 685,
            "end": 827,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_20@0",
            "content": "Relation to Group Fairness: Statistical Parity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_20",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@0",
            "content": "Given a set of protected features (e.g. gender), a model satisfies statistical parity if model outcome is independent of the protected features (we note that identifying protected features may not always be feasible in the real world).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@1",
            "content": "We represent the feature vector x = [x p , x l ], where x p is the set of protected features and x l is the remainder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 236,
            "end": 353,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@2",
            "content": "Accordingly, we choose v to be a vector such that the entries that sum | \u2202f k (xp) \u2202x i |\u2200x p \u2208 x p in J are nonzero; and zero otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 355,
            "end": 491,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@3",
            "content": "This choice is intuitive as then we sum the gradients in J that correspond to protected features and measure model's sensitivity to them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 493,
            "end": 629,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@4",
            "content": "The predictor f (x) will satisfy statistical parity if f (x p , x l ) = f (x p , x l )\u2200x p = x p .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 631,
            "end": 728,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_21@5",
            "content": "Given this, we state the following theorem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_21",
            "start": 730,
            "end": 772,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_22@0",
            "content": "Theorem 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_22",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_22@1",
            "content": "Given a vector v with non-zero entries corresponding to x p and zero entries for x l , if the predictor f (x) satisfies statistical parity with respect to x p , accumulated prediction sensitivity will be zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_22",
            "start": 11,
            "end": 219,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_23@0",
            "content": "Proof: If f (x) satisfies statistical parity with respect to x p , the values \u2202f k (x) \u2202xp \u2200x p \u2208 x p will be all zeros.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_23",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_23@1",
            "content": "This is due to the fact that the function f k (x) can not be defined based on entries x p \u2208 x p for it to be independent of them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_23",
            "start": 121,
            "end": 249,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_23@2",
            "content": "Therefore, for every multiplication in the product J v, either the entry",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_23",
            "start": 251,
            "end": 322,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_24@0",
            "content": "\u2202f k (x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_24",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_25@0",
            "content": "\u2202xp will be 0 or the entry in v corresponding to x l will be 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_25",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_25@1",
            "content": "Hence, P will be 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_25",
            "start": 64,
            "end": 82,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_26@0",
            "content": "Appendix A presents empirical results in computing P on a synthetic dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_26",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_26@1",
            "content": "We construct a dataset where a feature (hair length) correlates with a protected attribute (gender).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_26",
            "start": 77,
            "end": 176,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_26@2",
            "content": "We show that if the modeler unintentionally uses the correlated feature while attempting to build a classifier with statistical parity, our metric can be used for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_26",
            "start": 178,
            "end": 351,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_27@0",
            "content": "5 Relation to Individual Fairness Dwork et al. (2012) state the notion of individual based fairness as: \"We interpret the goal of mapping similar people similarly to mean that the distributions assigned to similar people are similar\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_27",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_27@1",
            "content": "They propose adding a Lipschitz property constraint during the classifier optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_27",
            "start": 235,
            "end": 321,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_27@2",
            "content": "Given a loss function L defined to optimize the parameters \u03b8 of the classifier f (x), a distance function d(x, x ) that computes distance between data-points x, x , another distance function D(f (x)), f (x )) that computes distance between classifier predictions on x, x and a constant L, Dwork et al. (2012) propose the following constrained optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_27",
            "start": 323,
            "end": 678,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_28@0",
            "content": "min \u03b8 L; such that D(f (x)), f (x )) < Ld(x, x ); \u2200x, x \u2208 X.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_28",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_29@0",
            "content": "(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_29",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_30@0",
            "content": "It is natural to choose an Lp norm (Bourbaki, 1987) for d and D. For a classifier f that is trained with the above constrained optimization and the choice of distance metrics D, d is an Lp norm, we state the following.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_30",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_31@0",
            "content": "Theorem 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_31",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_31@1",
            "content": "If the predictor f (x) is trained with the constrained optimization stated in Eq. (2), the accumulated prediction sensitivity will be upper bounded by L.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_31",
            "start": 11,
            "end": 163,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_32@0",
            "content": "Proof: We restate the constraint in Eq. ( 2) as (Note that the inequality sign does not change as distance metrics D, d are required to be positive for",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_32",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_33@0",
            "content": "x = x ) \u2200x = x , L > D(f (x), f (x )) d(x, x ) . (3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_33",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_34@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_34",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_35@0",
            "content": "Given the inequality holds for any pair of x, x , it must also hold for an x of the following choice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_35",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_36@0",
            "content": "x = x + [0, 0, \u2206x i , 0, 0], where \u2206x i is a scalar perturbation in the i th entry in x. For a chosen Lp norm, Eq (3) becomes",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_36",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_37@0",
            "content": "L > [ K k=1 |f k (x) \u2212 f k (x )| p ] 1 p |\u2206x i | > [|f k (x) \u2212 f k (x )| p ] 1 p |\u2206x i | . (4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_37",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_38@0",
            "content": ") Since each entry |f k (x) \u2212 f k (x )| p , k = 1, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_38",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_39@0",
            "content": ".K is expected to be non-zero and zeroing out all such entries (but one) will yield a lower value than the summation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_39",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_40@0",
            "content": "K k=1 |f k (x) \u2212 f k (x )| p .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_40",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_41@0",
            "content": "We can rewrite Eq. ( 4) as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_41",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_42@0",
            "content": "|f k (x) \u2212 f k (x + [0, 0, \u2206x i , 0, 0])| |\u2206x i | .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_42",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_43@0",
            "content": "We can further chose \u2206x i such that it is small perturbation, leading to the following.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_43",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_44@0",
            "content": "\u2206x i \u21920 |f k (x) \u2212 f k (x + [0, 0, \u2206x i , 0, 0])| |\u2206x i | = \u2202f k (x) \u2202x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_44",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_45@0",
            "content": "Therefore, each entry in J is upper bounded by L. As vectors v, w are stochastic and they compute weighted averages of bounded entries in J , P (defined in Eq. ( 1)) must be less than or equal to L.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_45",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_46@0",
            "content": "We also note that as L becomes larger, the constraint in the Eq. (2) becomes looser.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_46",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_46@1",
            "content": "Therefore, a higher value of L during optimization is expected to loosen the fairness constraint as well as the bound on fairness sensitivity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_46",
            "start": 85,
            "end": 226,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_46@2",
            "content": "This aligns with our intuition of lower values of P for fairer models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_46",
            "start": 228,
            "end": 297,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_46@3",
            "content": "We compute value of L on a synthetically generated classification data, optimized with the individual fairness constraint in equation 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_46",
            "start": 299,
            "end": 434,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_46@4",
            "content": "The results are presented in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_46",
            "start": 436,
            "end": 475,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_47@0",
            "content": "Correlations with Human Perception of Fairness",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_47",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_48@0",
            "content": "While the conditional statistical parity and individual fairness establish theoretical constraints on the model behaviour (e.g. independence from protected features and similarity in prediction outcomes for similar data-points), humans may carry a different notion of fairness for model outcomes on individual data-points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_48",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_48@1",
            "content": "This notion may be based on their understanding of cultural norms, which in turn effect their decisions in identifying which model outputs could be considered biased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_48",
            "start": 323,
            "end": 488,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_48@2",
            "content": "In this section, we present experiments that correlate accumulated prediction sensitivity with human perception of fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_48",
            "start": 490,
            "end": 613,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_49@0",
            "content": "Human Perception of Fairness",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_49",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_50@0",
            "content": "Given a data-point x and model prediction f (x), we assign one of the K classes to the data-point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_50",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_50@1",
            "content": "In order to evaluate the human perception of fairness on the data-point, we request a group of annotators to evaluate the model prediction (taken as the argmax of the model output) and assess whether they believe the output is biased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_50",
            "start": 99,
            "end": 332,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_50@2",
            "content": "For instance, given the social/cultural norms, a profession classifier assigning a data-point \"she worked in a hospital\" to nurse instead of doctor can be perceived as biased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_50",
            "start": 334,
            "end": 508,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_50@3",
            "content": "To correlate the accumulated prediction sensitivity P with the human understanding of fairness, we conduct experiments on two text classification datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_50",
            "start": 510,
            "end": 664,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_50@4",
            "content": "We describe the datasets below, followed by our choices for w and v.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_50",
            "start": 666,
            "end": 733,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_51@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_51",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@0",
            "content": "We experiment with our proposed metric on two classification tasks, i.e, occupation classification on Bias in Bios dataset (De-Arteaga et al., 2019) 2 and toxicity classification with Jigsaw Toxicity dataset 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@1",
            "content": "We focus on these two datasets as they have been investigated in several previous studies and have been reported to carry significant presence of bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 212,
            "end": 362,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@2",
            "content": "BIAS IN BIOS data (De-Arteaga et al., 2019) is purposed to train occupation classifier which predicts occupation given the biography of an individual.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 364,
            "end": 513,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@3",
            "content": "For this data, the task classifier is an occupation classification model which is composed of a standard LSTM-based encoder combined with the output layer of 28 nodes, i.e, number of occupation classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 515,
            "end": 716,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@4",
            "content": "2 The data is available at https://github.com/microsoft/biosbias JIGSAW TOXICITY dataset is commonly used to train toxic classifier which is tasked to predict if an input sentence is toxic or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 718,
            "end": 913,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@5",
            "content": "This dataset has input sentences as the comments from Wikipedia's talk page edits labeled with the degree of toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 915,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@6",
            "content": "In this dataset, the task classifier is a binary classifier trained to predict whether a comment is toxic or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 1034,
            "end": 1146,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@7",
            "content": "We labeled the samples with >0.5 toxicity score as toxic and others as non-toxic to train the task classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 1148,
            "end": 1257,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@8",
            "content": "The task classifier trained with Jigsaw Toxicity dataset achieved an AUC of 0.957.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 1259,
            "end": 1340,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_52@9",
            "content": "Table 4 in appendix summarizes the train/test/valid split for the 2 datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_52",
            "start": 1342,
            "end": 1418,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_53@0",
            "content": "Selecting the vectors w",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_53",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@0",
            "content": "The vector w sums up the absolute partial derivatives of f k (x) with respect to a given feature x i , \u2200k = 1, .., K. In our setup, we consider input features to be the word embeddings and the matrix J is computed over the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@1",
            "content": "Given a Ddimensional word embedding, K classes and N words in x, J will be a matrix of size (K) \u00d7 (DN ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 229,
            "end": 332,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@2",
            "content": "In all our experiments, we choose w to be a uniform vector with entries 1/K. Such a choice assigns equal weight to the partial derivatives computed over each class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 334,
            "end": 497,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@3",
            "content": "One may chose to put a higher weight on derivatives computed over a specific class, if there is a reason to believe that the accumulated prediction sensitivity should be informed more with respect to that class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 499,
            "end": 709,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@4",
            "content": "For instance, for a classifier that stratifies medical images into various diseases (Agrawal et al., 2019), disparity in model performance with respect to malicious diseases can be considered more costly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 711,
            "end": 914,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_54@5",
            "content": "Therefore, derivatives for classes that represent more malicious disease can be weighted higher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_54",
            "start": 916,
            "end": 1011,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_55@0",
            "content": "Selecting the vectors v",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_55",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_56@0",
            "content": "Through the vector v, we aim to select words in x that carry gendered information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_56",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_56@1",
            "content": "We use two formulations for the the vector v as discussed below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_56",
            "start": 83,
            "end": 146,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_57@0",
            "content": "Using a list of gendered words",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_57",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_58@0",
            "content": "In this setup, we use the set of gendered words from (Bolukbasi et al., 2016) and assign entries in v corresponding to those words as 1/(N g \u00d7 D), where N g is the count of gendered words in the data-point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_58",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_59@0",
            "content": "Using a Protected Status Model (PSM)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_59",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_60@0",
            "content": "While prior work has used word matching to a pre-defined corpus of tokens describing various demographic cohorts (Bolukbasi et al., 2016), these corpus do not contain words that stereotypically are associated with a particular cohort but may not be explicitly tied to that cohort.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_60",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_60@1",
            "content": "For example, the word \"volleyball\" is associated with females in the analysis presented by (Dinan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_60",
            "start": 281,
            "end": 392,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_61@0",
            "content": "To capture this nuance, we propose using another classifier (that acts on the same dataset as used to train the original classifier, for which we aim to compute P ) and using it to identify tokens containing information about the protected attribute (e.g. gender).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_61",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_61@1",
            "content": "We discuss the model training below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_61",
            "start": 265,
            "end": 300,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_62@0",
            "content": "Protected Status Model: To extend accumulated prediction sensitivity to settings with no explicit protected attribute, we train a protected status model g. Given the data-point x, goal of the PSM model g(x) is to predict the protected attributes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_62",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_62@1",
            "content": "Given a trained g(x), we then compute another matrix J g , where the (j, i) th entry is | \u2202gm(x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_62",
            "start": 247,
            "end": 342,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_63@0",
            "content": "x i | (g m is the probability outcomes corresponding to the m th protected attribute class; e.g. male in a gender classifier).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_63",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_63@1",
            "content": "We then define an entry v i \u2208 v as j J g (m, i) (the vector v is normalized to be stochastic).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_63",
            "start": 127,
            "end": 220,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_63@2",
            "content": "Intuitively, the sum j J g (m, i) captures the model output sensitivity with respect to the input features x i and is expected to higher if x i carries more gendered information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_63",
            "start": 222,
            "end": 399,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@0",
            "content": "In our experiments, we train separate PSM models for gender sensitivity computation on Bias-inbios and Jigsaw data-sets, as each data-point in these data-sets is additionally labeled with a binary gender class (male/female) 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@1",
            "content": "Gender PSMs predicts the associated gender given the datapoint x. Training PSM on the same datasets used to train the task classifier f helps capture the gender stereotypes present in the respective datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 228,
            "end": 435,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@2",
            "content": "For instance, in a given dataset, if the word \"volleyball\" appears more often in the data-points that correspond to the female gender, the gender classifier's sensitivity to this word is expected to be high as the classifier may pay higher emphasis to this word for gender classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 437,
            "end": 724,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@3",
            "content": "We use the same model architecture as the task classifiers for PSM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 726,
            "end": 792,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@4",
            "content": "PSM for gender classification achieve an accuracy of 98.79% (Male Acc:98.84% Female Acc:98.17%) and 95.39% (Male Acc:95.92% Female Acc:96.22%) for Bias in bios and Jigsaw Toxicity datasets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 794,
            "end": 996,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@5",
            "content": "These accuracies are computed over the same train/test split as the task classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 998,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_64@6",
            "content": "We use the bootstrap method to compute statistical significance (Koehn, 2004) at p-value<0.05.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_64",
            "start": 1083,
            "end": 1176,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_65@0",
            "content": "Using Word Embedding Vectors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_65",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_66@0",
            "content": "In addition to using the list of gendered words and PSM, we also test with a setting where we multiply the word embedding vectors to the proposed formulations of v. We stack the word embedding vectors for each word x i \u2208 x to obtain a vector of embeddings e i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_66",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_66@1",
            "content": "We perform an element-wise multiplication of the embedding vectors e i with the vector with entries 1/(N g \u00d7D) for gendered words or j J g (j, i) obtained using PSM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_66",
            "start": 262,
            "end": 426,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_66@2",
            "content": "This choice is motivated based upon the findings in (Han et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_66",
            "start": 428,
            "end": 498,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_66@3",
            "content": "They leverage the magnitude of embedding vectors in determining saliency of the input words for the classification task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_66",
            "start": 500,
            "end": 627,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_66@4",
            "content": "Their proposed methodology computes saliency maps over the features x i \u2208 x by multiplying embedding vectors with partial derivatives of the class probabilities with respect to embedding vectors themselves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_66",
            "start": 629,
            "end": 834,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_67@0",
            "content": "Fairness Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_67",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@0",
            "content": "We experiment with six fairness metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@1",
            "content": "Out of the six, one metric is a baseline based on counterfactual fairness and the rest are variants of the accumulated prediction sensitivity P .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 41,
            "end": 185,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@2",
            "content": "Counter-factual Fairness (CF) : We use the counter-factual fairness definition mentioned in Garg et al. (2019) and compute the metric as the difference in model predictions between the original sample f (x) and its corresponding counter-factual gendered sample f ( x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 187,
            "end": 454,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@3",
            "content": "We take the L1 norm of the vector f (x) \u2212 f ( x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 456,
            "end": 504,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@4",
            "content": "For example, we take the difference in predictions between the sample \"She practices dentistry\" and \"He practices dentistry\", which is the corresponding counter-factual sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 506,
            "end": 681,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@5",
            "content": "We use the definitional gender token substitutions from Bolukbasi et al. (2016) to create counter-factual samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 683,
            "end": 796,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@6",
            "content": "P1: Uniformly weighted prediction sensitivity : In this setting, the values of w and v are set to uniform values 1 K and 1 DN , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 798,
            "end": 938,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@7",
            "content": "This is a weak baseline as the choice of v does not provide any information regarding the gender-ness of the input words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 940,
            "end": 1060,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@8",
            "content": "P2: Weighted Prediction Sensitivity based on PSM : In this setting, w is chosen to be a uniform vector, while v is chosen based on the PSM model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 1062,
            "end": 1206,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@9",
            "content": "P3: Weighted Prediction sensitivity + Embedding weights : In this setting, v is chosen based on the PSM model (akin to the metric in P2) which is further multiplied element-wise with the word embedding vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 1208,
            "end": 1417,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@10",
            "content": "P4: Hard gender weights based Prediction sensitivity : In this metric, we use the list of gendered words described in section 6.4.1 to determine v. The value of entries in v is set to 1 DNg .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 1419,
            "end": 1609,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_68@11",
            "content": "P5: Hard gender weights based prediction sensitivity + Embeddings: This setting is same as above, except entries in v are further multiplied element-wise with the word embedding vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_68",
            "start": 1611,
            "end": 1796,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_69@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_69",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@0",
            "content": "To evaluate whether the proposed prediction sensitivity correlates with human perception of fairness, we collect annotations from crowd workers using the Amazon Mechanical Turk platform.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@1",
            "content": "Crowd workers are asked to annotate if a model prediction appears to be a biased prediction or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 187,
            "end": 285,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@2",
            "content": "For Bias in Bios dataset, each sample presented to the annotators has the biography and occupation predicted by the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 287,
            "end": 408,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@3",
            "content": "We collect annotations on a random sample of the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 410,
            "end": 467,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@4",
            "content": "For each biography and a predicted occupation, we ask annotators to label if the prediction is indicative of bias or if it is unbiased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 469,
            "end": 603,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@5",
            "content": "Bias refers to a situation where an occupation is incorrectly predicted based on the gender associated with the biography.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 605,
            "end": 726,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@6",
            "content": "For instance, if the input biography is \"she studied at Harvard Medical School and practices dentistry.\" and is predicted as nurse, then we call this prediction biased since the biography fits better for a doctor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 728,
            "end": 940,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@7",
            "content": "In case of unbiased predictions, the prediction is not expected to be influenced by the gender content in the biography.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 942,
            "end": 1061,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@8",
            "content": "Table 3 presents a sample of examples provided to the annotators for the Bias in bios dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 1063,
            "end": 1156,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@9",
            "content": "Each page in the annotation task consisted of ten biography-profession pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 1158,
            "end": 1234,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@10",
            "content": "We collect annotations for each biography-profession pair from at least three annotators and pick the label with majority vote.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 1236,
            "end": 1362,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_70@11",
            "content": "Similarly for Jigsaw Toxicity dataset, each sample presented to the annotators contains the text and associated toxicity predicted by the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_70",
            "start": 1364,
            "end": 1507,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@0",
            "content": "We restrict the set of annotators to be master annotators and the location of annotators to be Unites States.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@1",
            "content": "Based on the initial pilot studies conducted in the Amazon Mechanical Turk platform, we setup a payment rate to ensure a fair compensation of at least 15$/hour for all annotators that work at an average pace.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 110,
            "end": 317,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@2",
            "content": "We annotated 900 test data-points from each dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 319,
            "end": 370,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@3",
            "content": "We note that these test data-points were misclassified by the classifiers f trained for each dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 372,
            "end": 472,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@4",
            "content": "While such a sampling may not conform to the true distribution of biased/unbiased model outcomes on the overall test set, we expect to get more biased samples amongst the misclassified samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 474,
            "end": 666,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@5",
            "content": "The distribution between biased and unbiased outputs was about 55:45 for Bias in Bios and 50:50 for Jigsaw Toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 668,
            "end": 783,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@6",
            "content": "For the Bias in Bios and Jigsaw Toxicity datsets, we obtained a Fliess' kappa of 0.43 and 0.47, respectively, amongst the three annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 785,
            "end": 923,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@7",
            "content": "This is considered a moderate level of agreement, which we believe is expected for an relatively ambiguous task to identify model outcomes influenced by gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 925,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_71@8",
            "content": "We compute mutual information and bi-serial correlations as the primary measures of association between the human annotations and the accumulated model sensitivity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_71",
            "start": 1086,
            "end": 1249,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_72@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_72",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@0",
            "content": "Table 1 lists the bi-serial correlations and mutual information between manual annotations and the different fairness metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@1",
            "content": "First, we observe that correlations of the baseline with human judgement are mediocre (0.326 and 0.214) compared to the human judgement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 127,
            "end": 262,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@2",
            "content": "We attribute this to the fact that the metric attempts to quantify a fairly subjective assessment of bias that may have different interpretation (as also pointed out by the moderate level of annotation agreement across annotators).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 264,
            "end": 494,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@3",
            "content": "However, the proposed variants of P have stronger correlations compared to the counter-factual baseline (except the method P1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 496,
            "end": 622,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@4",
            "content": "As expected, we see the smallest correlation for P1, since this metric does not account for gender-ness in v. However, metrics that determine v based on PSM prediction sensitivity and gendered words get higher correlations over P1 and the CF baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 624,
            "end": 874,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@5",
            "content": "Variant of P with v informed using the embedding vectors further lead to improved correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 876,
            "end": 970,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@6",
            "content": "We also observe weaker statistical significance in the case of Jigsaw Toxicity due to a weaker PSM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 972,
            "end": 1070,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@7",
            "content": "We attribute this to the noise present in gender annotations for Jigsaw Toxicity dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 1072,
            "end": 1160,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_73@8",
            "content": "Hence, the performance of PSM in predicting the protected status is crucial for accurately measuring fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_73",
            "start": 1162,
            "end": 1271,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_74@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_74",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_75@0",
            "content": "In order to further analyse the effect of PSM, we look into heat-maps capturing w T J and v separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_75",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_75@1",
            "content": "As a reminder, the first quantity captures the weighted average of partial derivatives of class probabilites with respect to the input features, while the second quantity computes the weights assigned to sum up the aforementioned averages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_75",
            "start": 104,
            "end": 342,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_75@2",
            "content": "Table 2 shows while v mostly captures gendered words such as \"she\", \"her\" and \"woman\", it also captures words such as \"social\", \"architecture\" and \"cheated\" to carry more gendered information compared to other words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_75",
            "start": 344,
            "end": 559,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_75@3",
            "content": "While these words conventionally are not gendered, for the datasets at hand, they seem to provide information whether the input data-point belongs to male/female gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_75",
            "start": 561,
            "end": 729,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_75@4",
            "content": "We also note that w T J weighs on occupation specific tokens such as \"physician\", \"executive\", etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_75",
            "start": 731,
            "end": 829,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_76@0",
            "content": "This finding supports our motivations to compute v based on PSM and capturing feature attributions assigned to tokens that are implicitly related to a specific gender (instead of the definitional gender tokens only).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_76",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_76@1",
            "content": "Hence, by incorporating PSM in computing P , we can capture bias present in nontrivial gendered tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_76",
            "start": 217,
            "end": 319,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_77@0",
            "content": "Considerations for Accumulated Prediction Sensitivity Metric",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_77",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@0",
            "content": "While the results showcase the promise of our metric, we draw the attention of the reader to the following considerations: (1) We observed that the metric quality depends on choice of the hyperparameters w and v. In this regard, our metric is not different from other metrics that also depend on a hyper-parameter choice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@1",
            "content": "For example, any classifier based metric has a threshold parameter and counterfactual fairness metrics rely on hyperparameters such as the selected gendered words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 322,
            "end": 484,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@2",
            "content": "(2) Our metric only works for models for which gradients can be computed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 486,
            "end": 558,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@3",
            "content": "Most modern deep learning based models carry this property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 560,
            "end": 618,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@4",
            "content": "(3) Lastly, we note that it is hard to interpret the absolute value of the proposed metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 620,
            "end": 710,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_78@5",
            "content": "The metric value should be used for relative comparison of two models which share input feature space and label space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_78",
            "start": 712,
            "end": 829,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@0",
            "content": "In addition, we note two considerations for relying on a PSM classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@1",
            "content": "First, training it requires access to gender labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 73,
            "end": 124,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@2",
            "content": "Second, the PSM model itself could be biased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 126,
            "end": 170,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@3",
            "content": "Given that gender labels may not always be available for the dataset used to train model at hand, we study the impact of transferring a PSM model trained on a different dataset on computing our metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 172,
            "end": 372,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@4",
            "content": "We also evaluate the effect of bias in PSM model on the overall metric value and present results in the Appendix D. We make observations such as the quality of the metric degrades as PSM becomes more biased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 374,
            "end": 580,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_79@5",
            "content": "Based on these observations, we recommend that if modeler is not able to obtain high performance PSM models, they fall back to using sources such as gendered words for computing the vector v.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_79",
            "start": 582,
            "end": 772,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_80@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_80",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@0",
            "content": "Evaluating fairness is a challenging task as it requires selecting a notion of fairness (e.g. group or individual fairness) and then identifying metrics that can capture these notions of fairness while evaluating a classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@1",
            "content": "Additionally, certain notions of fairness may not be well defined and can change based upon social norms (e.g. \"volleyball\" being closely associated with females); that may seep into the dataset at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 227,
            "end": 429,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@2",
            "content": "In this work, we define an accumulated prediction sensitivity metric that relies on the partial derivatives of model's class probabilities with respect to input features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 431,
            "end": 600,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@3",
            "content": "We establish properties of this metric with respect to the three verticals of fairness metrics: group, individual and human-perception based.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 602,
            "end": 742,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@4",
            "content": "We provide bounds on the metric's value when a predictor is expected to carry statistical parity or is trained with individual fairness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 744,
            "end": 879,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@5",
            "content": "We also evaluate this metric with fairness as perceived through human evaluation of model outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 881,
            "end": 978,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@6",
            "content": "We test variants of the proposed metric against an existing baseline derived from counter-factual fairness and observe better mutual information and correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 980,
            "end": 1140,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_81@7",
            "content": "Specifically, a variant of the metric that relies on a Protected Status Model (that identifies tokens that carry gender information but may not conventionally be considered gendered) yields the best correlation with the human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_81",
            "start": 1142,
            "end": 1378,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_82@0",
            "content": "In the future, one can associate the proposed formulation with other categories of group and individual fairness (Mehrabi et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_82",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_82@1",
            "content": "We also aim to test the metric on other datasets with other protected attributes (e.g. race, nationality).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_82",
            "start": 137,
            "end": 242,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_82@2",
            "content": "Finally, we can compare the metric across these datasets to compare trends across protected groups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_82",
            "start": 244,
            "end": 342,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_83@0",
            "content": "Broader Impact and Ethics Statement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_83",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@0",
            "content": "This work can be used to evaluate bias in models, and thus used to evaluate models serving human consumers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@1",
            "content": "As with all metrics, the metric does not capture all notions of bias, and thus should not be the only consideration for serving models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 108,
            "end": 242,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@2",
            "content": "While this is a valid risk, this is one that is not specific to prediction sensitivity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 244,
            "end": 330,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@3",
            "content": "Good use of this metric requires users to be cognizant of these strengths and weaknesses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 332,
            "end": 420,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@4",
            "content": "We also note that the metric requires defining protected attributes (e.g. gender) and our work carries the limitation that the selected datasets contain binary gender annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 422,
            "end": 600,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_84@5",
            "content": "Defining protected attributes may not always be possible and when possible, the protected attribute classes may not be comprehensive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_84",
            "start": 602,
            "end": 734,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_85@0",
            "content": "Let us consider a classification task on whether to hire a person given the following features: x 1 is the person's educational experience in years, x 2 is their hair length and x 3 is their gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_85",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_85@1",
            "content": "We synthetically generate data for individuals in this dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_85",
            "start": 199,
            "end": 261,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_86@0",
            "content": "x 1 is drawn uniformly randomly between 0 and 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_86",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_86@1",
            "content": "x 3 is (again) considered to be binary gender (set 0 for male and 1 for female drawn from a bernoulli distribution) and x 2 is drawn from a Gaussian distribution conditioned on x 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_86",
            "start": 50,
            "end": 231,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_86@2",
            "content": "x 2 \u223c N (2, 10) (Gaussian distribution with a mean 2 and variance 10) if x 3 = 0 and x 2 \u223c N (10, 10) if x 3 = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_86",
            "start": 233,
            "end": 345,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_86@3",
            "content": "We sample 10,000 data-points from the above distribution to generate a dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_86",
            "start": 347,
            "end": 425,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_86@4",
            "content": "Let us consider two cases with two different classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_86",
            "start": 427,
            "end": 483,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_87@0",
            "content": "Case 1: Classifier depends on x 1 , x 2 In this case, the modeler only deems x 3 to be the protected feature. Let us assume that they build a classifier as shown in equation 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_87",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_87@1",
            "content": "Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_87",
            "start": 177,
            "end": 255,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_87@2",
            "content": "f = \u03c3((x 1 \u2212 5) + (x 2 \u2212 6))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_87",
            "start": 257,
            "end": 284,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_88@0",
            "content": "(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_88",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_89@0",
            "content": "Given only x 3 is considered as the protected feature by the modeler, they will set the vector v to [0, 0, 1] T . Let us assume that the modeler sets P as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_89",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_90@0",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_90",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_91@0",
            "content": "We recommend the modeler computes \u2202x 2 \u2202x 3 and \u2202x 1 \u2202x 3 and if they are non-zero, use the chain rule in equation 7 to compute P .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_91",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_92@0",
            "content": "\u2202f k ([x 1 , x 2 ]) \u2202x 3 = \u2202f k ([x 1 , x 2 ]) \u2202x 2 \u2202x 2 \u2202x 3 (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_92",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_93@0",
            "content": "For the dataset generated above, we compute the partials \u2202x 2 \u2202x 3 and \u2202x 1 \u2202x 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_93",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_93@1",
            "content": "Additionally, since x 3 is a discrete variable, we approximate partial derivatives using all available right-difference quotients and left-difference quotients, as shown in equation 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_93",
            "start": 83,
            "end": 266,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_93@2",
            "content": "In order to compute \u2202x The mean above is computed over all n = m. Similarly,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_93",
            "start": 268,
            "end": 343,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_94@0",
            "content": "\u2202x 1 \u2202x 3 x 3 =x m 3 = Mean x m 1 \u2212 x n 1 x m 3 \u2212 x n 3 (9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_94",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_95@0",
            "content": "Given the dataset we generated, we compute values for \u2202x 1 \u2202x 3 x 3 =x m 3 and \u2202x 2 \u2202x 3 x 3 =x m 3 for an arbitrarily chosen m. We obtain values of 7.98 and 0.01, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_95",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_95@1",
            "content": "Note that we expect the second value to be 0, but due to noise in gradient approximation obtain a non-zero value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_95",
            "start": 178,
            "end": 290,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_95@2",
            "content": "We re-write equation 6 as shown below and plug in the values of the partials.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_95",
            "start": 292,
            "end": 368,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_95@3",
            "content": "We obtain a non-zero value of P in this case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_95",
            "start": 370,
            "end": 414,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_96@0",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (10) = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 2 \u2202x 2 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 2 \u2202x 2 \u2202x 3 \uf8ee \uf8f0 0 0 1 \uf8f9 \uf8fb (11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_96",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_97@0",
            "content": "Case 2: Classifier only depends only on x 1 In this case, the modeler deems both x 2 , x 3 to be protected features and builds a classifier as depicted below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_97",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_98@0",
            "content": "f = \u03c3(x 1 \u2212 5)(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_98",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_99@0",
            "content": "Lets assume that the modeler assigns a hire decision if f > 0.5, otherwise not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_99",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_99@1",
            "content": "Additionally, given x 2 and x 3 are protected features, P is set to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_99",
            "start": 80,
            "end": 146,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_100@0",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 2 \u2202f 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 2 \u2202f 2 \u2202x 3 \uf8ee \uf8f0 0 1 2 1 2 \uf8f9 \uf8fb (13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_100",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_101@0",
            "content": "Given that the classifier does not explicitly rely on x 2 and x 3 , we can rewrite equation 14 as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_101",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_102@0",
            "content": "P = 1 2 1 2 \u2202f 1 \u2202x 1 \u2202f 1 \u2202x 1 \u2202x 1 \u2202x 2 \u2202f 1 \u2202x 1 \u2202x 1 \u2202x 3 \u2202f 2 \u2202x 1 \u2202f 2 \u2202x 1 \u2202x 1 \u2202x 2 \u2202f 2 \u2202x 1 \u2202x 1 \u2202x 3 \uf8ee \uf8f0 0 1 2 1 2 \uf8f9 \uf8fb (14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_102",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_103@0",
            "content": "We obtain the partial derivatives \u2202x 1 \u2202x 2 x 2 =x m 2 and \u2202x 1 \u2202x 3 x 3 =x m",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_103",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_104@0",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_104",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_104@1",
            "content": "For an arbitrary chosen x m 1 , we obtain values of 0.01 and -0.01.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_104",
            "start": 2,
            "end": 68,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_104@2",
            "content": "While we expect both these values to be zero given our data construction, they are non-zero due to the gradient approximation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_104",
            "start": 70,
            "end": 195,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_104@3",
            "content": "Barring the noise in gradient computation, P is 0 in this case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_104",
            "start": 197,
            "end": 259,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_105@0",
            "content": "Taruna Agrawal, Rahul Gupta, Shrikanth Narayanan, On evaluating CNN representations for low resource medical image classification, 2019-05-12, IEEE International Conference on Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_105",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_106@0",
            "content": "Su Lin, Solon Blodgett, Hal Barocas, Iii Daum\u00e9, Hanna Wallach, Language (technology) is power: A critical survey of \"bias\" in NLP, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_106",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_107@0",
            "content": "Su Lin, Gilsinia Blodgett, Alexandra Lopez, Robert Olteanu, Hanna Sim,  Wallach, Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_107",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_108@0",
            "content": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai, Man is to computer programmer as woman is to homemaker?, 2016, debiasing word embeddings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_108",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_109@0",
            "content": "UNKNOWN, None, 1987, Topological vector spaces, elements of mathematics, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_109",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_110@0",
            "content": "Aylin Caliskan, Joanna Bryson, Arvind Narayanan, Semantics derived automatically from language corpora contain human-like biases, 2017, Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_110",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_111@0",
            "content": "Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq, Algorithmic decision making and the cost of fairness, 2017, Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_111",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_112@0",
            "content": "Maria De-Arteaga, Alexey Romanov, H Wallach, J Chayes, C Borgs, A Chouldechova, K Sahin Cem Geyik, A Kenthapadi,  Kalai, Bias in bios: A case study of semantic representation bias in a high-stakes setting, 2019, Proceedings of the Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_112",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_113@0",
            "content": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Bold: Dataset and metrics for measuring biases in open-ended language generation, 2021, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_113",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_114@0",
            "content": "Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, Adina Williams, Multidimensional gender bias classification, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_114",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_115@0",
            "content": "Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, Lucy Vasserman, Measuring and mitigating unintended bias in text classification, 2018-02-02, Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_115",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_116@0",
            "content": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel, Fairness through awareness, 2012-01-08, Innovations in Theoretical Computer Science, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_116",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_117@0",
            "content": "Kawin Ethayarajh, Is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_117",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_118@0",
            "content": "Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed Chi, Alex Beutel, Counterfactual fairness in text classification through robustness, 2019-01-27, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_118",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_119@0",
            "content": "Xiaochuang Han, Byron Wallace, Yulia Tsvetkov, Explaining black box predictions and unveiling data artifacts through influence functions, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_119",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_120@0",
            "content": "Moritz Hardt, Eric Price, Nati Srebro, Equality of opportunity in supervised learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_120",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2019, Measurement and fairness, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_121",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_122@0",
            "content": "J Michael, Seth Kearns, Aaron Neel, Zhiwei Steven Roth,  Wu, An empirical study of rich subgroup fairness for machine learning, 2019-01-29, Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 2019, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_122",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_123@0",
            "content": "Philipp Koehn, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL, 2004-07-26, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_123",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_124@0",
            "content": "Matt Kusner, Joshua Loftus, Chris Russell, Ricardo Silva, Counterfactual fairness, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_124",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_125@0",
            "content": "Thomas Manzini, Yao Lim, Alan Chong, Yulia Black,  Tsvetkov, Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_125",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_126@0",
            "content": "UNKNOWN, None, 2009, Towards a measure of individual fairness for deep learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_126",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_127@0",
            "content": "UNKNOWN, None, 1908, A survey on bias and fairness in machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_127",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_128@0",
            "content": "Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel Bowman, Crows-pairs: A challenge dataset for measuring social biases in masked language models, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_128",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_129@0",
            "content": "UNKNOWN, None, 2020, Towards auditability for fairness in deep learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_129",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_130@0",
            "content": "Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, Kai-Wei Chang, Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classification, 2021-08-01, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_130",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_131@0",
            "content": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng, The woman worked as a babysitter: On biases in language generation, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_131",
            "start": 0,
            "end": 345,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_132@0",
            "content": "Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna Gummadi, Adish Singla, Adrian Weller, Muhammad Bilal Zafar, A unified approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices, 2018-08-19, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_132",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_133@0",
            "content": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai Elsherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, William Wang, Mitigating gender bias in natural language processing: Literature review, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_133",
            "start": 0,
            "end": 347,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_134@0",
            "content": "Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, Vicente Ordonez, Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_134",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_135@0",
            "content": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, Krishna Gummadi, Adrian Weller, From parity to preference-based notions of fairness in classification, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_135",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "69-ARR_v2_136@0",
            "content": "Jieyu Zhao, Kai-Wei Chang, LOGAN: Local group bias detection by clustering, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "69-ARR_v2_136",
            "start": 0,
            "end": 227,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_1",
            "tgt_ix": "69-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_1",
            "tgt_ix": "69-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_5",
            "tgt_ix": "69-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_8",
            "tgt_ix": "69-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_8",
            "tgt_ix": "69-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_8",
            "tgt_ix": "69-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_12",
            "tgt_ix": "69-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_13",
            "tgt_ix": "69-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_14",
            "tgt_ix": "69-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_15",
            "tgt_ix": "69-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_16",
            "tgt_ix": "69-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_17",
            "tgt_ix": "69-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_18",
            "tgt_ix": "69-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_22",
            "tgt_ix": "69-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_23",
            "tgt_ix": "69-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_24",
            "tgt_ix": "69-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_25",
            "tgt_ix": "69-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_26",
            "tgt_ix": "69-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_27",
            "tgt_ix": "69-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_28",
            "tgt_ix": "69-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_29",
            "tgt_ix": "69-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_30",
            "tgt_ix": "69-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_31",
            "tgt_ix": "69-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_32",
            "tgt_ix": "69-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_33",
            "tgt_ix": "69-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_34",
            "tgt_ix": "69-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_35",
            "tgt_ix": "69-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_36",
            "tgt_ix": "69-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_37",
            "tgt_ix": "69-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_38",
            "tgt_ix": "69-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_39",
            "tgt_ix": "69-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_40",
            "tgt_ix": "69-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_41",
            "tgt_ix": "69-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_42",
            "tgt_ix": "69-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_44",
            "tgt_ix": "69-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_45",
            "tgt_ix": "69-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_43",
            "tgt_ix": "69-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_48",
            "tgt_ix": "69-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_49",
            "tgt_ix": "69-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_49",
            "tgt_ix": "69-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_51",
            "tgt_ix": "69-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_51",
            "tgt_ix": "69-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_53",
            "tgt_ix": "69-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_53",
            "tgt_ix": "69-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_55",
            "tgt_ix": "69-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_55",
            "tgt_ix": "69-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_56",
            "tgt_ix": "69-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_57",
            "tgt_ix": "69-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_57",
            "tgt_ix": "69-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_58",
            "tgt_ix": "69-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_60",
            "tgt_ix": "69-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_61",
            "tgt_ix": "69-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_62",
            "tgt_ix": "69-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_63",
            "tgt_ix": "69-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_65",
            "tgt_ix": "69-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_65",
            "tgt_ix": "69-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_67",
            "tgt_ix": "69-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_67",
            "tgt_ix": "69-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_69",
            "tgt_ix": "69-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_69",
            "tgt_ix": "69-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_69",
            "tgt_ix": "69-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_72",
            "tgt_ix": "69-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_72",
            "tgt_ix": "69-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_72",
            "tgt_ix": "69-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_74",
            "tgt_ix": "69-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_74",
            "tgt_ix": "69-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_74",
            "tgt_ix": "69-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_76",
            "tgt_ix": "69-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_77",
            "tgt_ix": "69-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_77",
            "tgt_ix": "69-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_77",
            "tgt_ix": "69-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_80",
            "tgt_ix": "69-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_80",
            "tgt_ix": "69-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_80",
            "tgt_ix": "69-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_82",
            "tgt_ix": "69-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_85",
            "tgt_ix": "69-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_87",
            "tgt_ix": "69-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_88",
            "tgt_ix": "69-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_89",
            "tgt_ix": "69-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_90",
            "tgt_ix": "69-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_91",
            "tgt_ix": "69-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_92",
            "tgt_ix": "69-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_93",
            "tgt_ix": "69-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_94",
            "tgt_ix": "69-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_95",
            "tgt_ix": "69-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_96",
            "tgt_ix": "69-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_97",
            "tgt_ix": "69-ARR_v2_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_98",
            "tgt_ix": "69-ARR_v2_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_99",
            "tgt_ix": "69-ARR_v2_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_100",
            "tgt_ix": "69-ARR_v2_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_101",
            "tgt_ix": "69-ARR_v2_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_102",
            "tgt_ix": "69-ARR_v2_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_103",
            "tgt_ix": "69-ARR_v2_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "69-ARR_v2_0",
            "tgt_ix": "69-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_1",
            "tgt_ix": "69-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_2",
            "tgt_ix": "69-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_3",
            "tgt_ix": "69-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_4",
            "tgt_ix": "69-ARR_v2_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_5",
            "tgt_ix": "69-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_5",
            "tgt_ix": "69-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_5",
            "tgt_ix": "69-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_6",
            "tgt_ix": "69-ARR_v2_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_7",
            "tgt_ix": "69-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_8",
            "tgt_ix": "69-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_9",
            "tgt_ix": "69-ARR_v2_9@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_10",
            "tgt_ix": "69-ARR_v2_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_11",
            "tgt_ix": "69-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_12",
            "tgt_ix": "69-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_13",
            "tgt_ix": "69-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_14",
            "tgt_ix": "69-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_14",
            "tgt_ix": "69-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_14",
            "tgt_ix": "69-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_15",
            "tgt_ix": "69-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_16",
            "tgt_ix": "69-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_17",
            "tgt_ix": "69-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_18",
            "tgt_ix": "69-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_19",
            "tgt_ix": "69-ARR_v2_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_20",
            "tgt_ix": "69-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_21",
            "tgt_ix": "69-ARR_v2_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_22",
            "tgt_ix": "69-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_22",
            "tgt_ix": "69-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_23",
            "tgt_ix": "69-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_23",
            "tgt_ix": "69-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_23",
            "tgt_ix": "69-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_24",
            "tgt_ix": "69-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_25",
            "tgt_ix": "69-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_25",
            "tgt_ix": "69-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_26",
            "tgt_ix": "69-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_26",
            "tgt_ix": "69-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_26",
            "tgt_ix": "69-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_27",
            "tgt_ix": "69-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_27",
            "tgt_ix": "69-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_27",
            "tgt_ix": "69-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_28",
            "tgt_ix": "69-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_29",
            "tgt_ix": "69-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_30",
            "tgt_ix": "69-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_31",
            "tgt_ix": "69-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_31",
            "tgt_ix": "69-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_32",
            "tgt_ix": "69-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_33",
            "tgt_ix": "69-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_34",
            "tgt_ix": "69-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_35",
            "tgt_ix": "69-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_36",
            "tgt_ix": "69-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_37",
            "tgt_ix": "69-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_38",
            "tgt_ix": "69-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_39",
            "tgt_ix": "69-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_40",
            "tgt_ix": "69-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_41",
            "tgt_ix": "69-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_42",
            "tgt_ix": "69-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_43",
            "tgt_ix": "69-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_44",
            "tgt_ix": "69-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_45",
            "tgt_ix": "69-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_46",
            "tgt_ix": "69-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_47",
            "tgt_ix": "69-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_48",
            "tgt_ix": "69-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_48",
            "tgt_ix": "69-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_48",
            "tgt_ix": "69-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_49",
            "tgt_ix": "69-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_50",
            "tgt_ix": "69-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_51",
            "tgt_ix": "69-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_52",
            "tgt_ix": "69-ARR_v2_52@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_53",
            "tgt_ix": "69-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_54",
            "tgt_ix": "69-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_55",
            "tgt_ix": "69-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_56",
            "tgt_ix": "69-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_56",
            "tgt_ix": "69-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_57",
            "tgt_ix": "69-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_58",
            "tgt_ix": "69-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_59",
            "tgt_ix": "69-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_60",
            "tgt_ix": "69-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_60",
            "tgt_ix": "69-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_61",
            "tgt_ix": "69-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_61",
            "tgt_ix": "69-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_62",
            "tgt_ix": "69-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_62",
            "tgt_ix": "69-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_63",
            "tgt_ix": "69-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_63",
            "tgt_ix": "69-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_63",
            "tgt_ix": "69-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_64",
            "tgt_ix": "69-ARR_v2_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_65",
            "tgt_ix": "69-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_66",
            "tgt_ix": "69-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_67",
            "tgt_ix": "69-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_68",
            "tgt_ix": "69-ARR_v2_68@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_69",
            "tgt_ix": "69-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_70",
            "tgt_ix": "69-ARR_v2_70@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_71",
            "tgt_ix": "69-ARR_v2_71@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_72",
            "tgt_ix": "69-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_73",
            "tgt_ix": "69-ARR_v2_73@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_74",
            "tgt_ix": "69-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_75",
            "tgt_ix": "69-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_76",
            "tgt_ix": "69-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_76",
            "tgt_ix": "69-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_77",
            "tgt_ix": "69-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_78",
            "tgt_ix": "69-ARR_v2_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_79",
            "tgt_ix": "69-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_80",
            "tgt_ix": "69-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_81",
            "tgt_ix": "69-ARR_v2_81@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_82",
            "tgt_ix": "69-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_82",
            "tgt_ix": "69-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_82",
            "tgt_ix": "69-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_83",
            "tgt_ix": "69-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_84",
            "tgt_ix": "69-ARR_v2_84@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_85",
            "tgt_ix": "69-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_85",
            "tgt_ix": "69-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_86",
            "tgt_ix": "69-ARR_v2_86@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_87",
            "tgt_ix": "69-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_87",
            "tgt_ix": "69-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_87",
            "tgt_ix": "69-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_88",
            "tgt_ix": "69-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_89",
            "tgt_ix": "69-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_90",
            "tgt_ix": "69-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_91",
            "tgt_ix": "69-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_92",
            "tgt_ix": "69-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_93",
            "tgt_ix": "69-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_93",
            "tgt_ix": "69-ARR_v2_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_93",
            "tgt_ix": "69-ARR_v2_93@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_94",
            "tgt_ix": "69-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_95",
            "tgt_ix": "69-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_95",
            "tgt_ix": "69-ARR_v2_95@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_95",
            "tgt_ix": "69-ARR_v2_95@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_95",
            "tgt_ix": "69-ARR_v2_95@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_96",
            "tgt_ix": "69-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_97",
            "tgt_ix": "69-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_98",
            "tgt_ix": "69-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_99",
            "tgt_ix": "69-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_99",
            "tgt_ix": "69-ARR_v2_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_100",
            "tgt_ix": "69-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_101",
            "tgt_ix": "69-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_102",
            "tgt_ix": "69-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_103",
            "tgt_ix": "69-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_104",
            "tgt_ix": "69-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_104",
            "tgt_ix": "69-ARR_v2_104@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_104",
            "tgt_ix": "69-ARR_v2_104@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_104",
            "tgt_ix": "69-ARR_v2_104@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_105",
            "tgt_ix": "69-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_106",
            "tgt_ix": "69-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_107",
            "tgt_ix": "69-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_108",
            "tgt_ix": "69-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_109",
            "tgt_ix": "69-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_110",
            "tgt_ix": "69-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_111",
            "tgt_ix": "69-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_112",
            "tgt_ix": "69-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_113",
            "tgt_ix": "69-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_114",
            "tgt_ix": "69-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_115",
            "tgt_ix": "69-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_116",
            "tgt_ix": "69-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_117",
            "tgt_ix": "69-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_118",
            "tgt_ix": "69-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_119",
            "tgt_ix": "69-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_120",
            "tgt_ix": "69-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_121",
            "tgt_ix": "69-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_122",
            "tgt_ix": "69-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_123",
            "tgt_ix": "69-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_124",
            "tgt_ix": "69-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_125",
            "tgt_ix": "69-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_126",
            "tgt_ix": "69-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_127",
            "tgt_ix": "69-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_128",
            "tgt_ix": "69-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_129",
            "tgt_ix": "69-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_130",
            "tgt_ix": "69-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_131",
            "tgt_ix": "69-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_132",
            "tgt_ix": "69-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_133",
            "tgt_ix": "69-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_134",
            "tgt_ix": "69-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_135",
            "tgt_ix": "69-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "69-ARR_v2_136",
            "tgt_ix": "69-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 940,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "69-ARR",
        "version": 2
    }
}