{
    "nodes": [
        {
            "ix": "203-ARR_v2_0",
            "content": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_2",
            "content": "While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages. In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality textto-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "203-ARR_v2_4",
            "content": "The advance of deep learning (Vaswani et al., 2017;Goodfellow et al., 2014) has enabled great improvements in the field of Text-to-Speech (TTS). (Towards-)end-to-end models, such as Tacotron 2 (Wang et al., 2017;Shen et al., 2018), Trans-formerTTS (Li et al., 2019b), FastSpeech 2 (Ren et al., 2019, FastPitch (\u0141a\u0144cucki, 2021) and many more famous instances (e.g. Ar\u0131k et al. (2017) and Prenger et al. (2019)) allow for speech synthesis with unprecedented quality and controllability. The models mentioned here rely on vocoders, such as WaveNet (van den Oord et al., 2016), MelGAN (Kumar et al., 2019), Parallel Wave-GAN or HiFi-GAN to turn the parametric representations that they produce into waveforms. Recently proposed models even include some with the ability to go directly to the waveform from a grapheme or phoneme input sequence, such as EATS (Donahue et al., 2020) or VITS (Kim et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_5",
            "content": "While these methods all perform remarkably well if given enough data, cross-lingual use of data remains a key challenge in TTS. Most modern methods are limited to languages and domains that are rich in resources, which over 6,000 languages are not. Attempts at reducing the required resources in a target language by making use of transfer learning from multilingual data have been made by Azizah et al. (2020); ; Chen et al. (2019). The mismatch of input spaces however requires complex architectural changes, which limits their ability to be used in conjunction with other modern TTS architectures. Attempts at fixing the issue of having to transfer knowledge from a source to a target by just jointly training on a mixed set of more and less resource rich languages have been made by He et al. (2021); de Korte et al. (2020); Yang and He (2020), which requires complex training procedures. In this work, we will also attempt to transfer knowledge from a set of high resource languages to a low resource language. We fix previous shortcomings by 1) using a linguistically motivated representation of the inputs to such a system (articulatory and phonological features of phonemes) that enables cross-lingual knowledge sharing and 2) applying the model agnostic meta learning (MAML) framework (Finn et al., 2017) to the field of low-resource TTS for the first time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_6",
            "content": "Using articulatory features as inputs for neural TTS has been attempted recently by Staib et al. (2020) and Wells et al. (2021), following the classical approach of Jakobson et al. (1961). Both achieved good results when applying this idea to the codeswitching problem, since unseen phonemes in the input space no longer map to nonsensical positions, as it would be the case for the standard embedding-lookup. It has to be noted however, that this only works across languages with similar types of phonemes. Also Gutkin (2017) have applied phonological features to low-resource TTS with fair success. They did however rely on supplementary features, such as dependency parsers and morphological analyzers. Furthermore all of their data and models are proprietary and can therefore not be used to compare results to. In this work, we extend the use of articulatory inputs with the MAML framework to enable very simple yet well working low-resource TTS that can be applied to almost all modern TTS architectures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_7",
            "content": "We encounter severe instabilities when using MAML on TTS, which make the standard formulation of MAML infeasible to use. Thus we also propose a modification to MAML, which reduces the procedure's complexity. This allows us to create a set of parameters of a model that can be used to fine-tune to a well working single-language singlespeaker TTS model with as little as 30 minutes of paired training data available and even enables zeroshot adaptation to unseen languages. We evaluate the success of our approach with both automatic measures and human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_8",
            "content": "Our contributions are as follows: 1) We show that it is beneficial to train a TTS model on articulatory features rather than on phoneme-identities, even in the standard single-language high-resource case; 2) We introduce a training procedure that is closely related to MAML which allows training a set of parameters for a TTS model that can be fine-tuned in a low resource scenario; 3) We provide insights on how much data and training time are required to fine-tune a model across different languages and speakers simultaneously using said meta-parameters; 4) We show that the metaparameters can generalize to unseen phonemes and rapidly improve their ability to properly pronounce them when fine-tuning. 1 2 Background and Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_9",
            "content": "Input Representations",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "203-ARR_v2_10",
            "content": "Character Embeddings The simplest approach to representing text as input to a TTS is using indexes of graphemes to look up embeddings. This is however prone to mistakes. Taylor and Richmond (2020) bring up the example of coathanger. If the TTS is not aware of the morpheme boundary between the coat and the hang, it will be inclined to produce something like [k2T@In\u00c3@] rather than the correct [koUthaeN@]. Such a representation of the input will be highly language dependent, since special pronunciation rules rarely hold for more than a single language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_11",
            "content": "The textual input can be augmented by adding information, such as morpheme boundaries, intonation phrase boundaries derived from e.g. syntactic parsing as is done in many TTS frontends (Schr\u00f6der and Trouvain, 2003;Clark et al., 2007;Ebden and Sproat, 2015), or even the semantic identity of the word a character belongs to, using e.g. BERT embeddings (Hayashi et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_12",
            "content": "Phoneme Embeddings Rather than looking up embeddings for graphemes, it is often beneficial to use embeddings of phonemes. Phonemizers (Bisani and Ney, 2008;Taylor, 2005;Rao et al., 2015) produce a sequence of phonetic units, which correlate with the segments in the audio much more than raw text. One such standard of phonetic representation which we make use of is the International Phonetic Alphabet (IPA). Using this set of phonetic units alleviates the problems of TTS fine-tuning and transfer-learning to low-resource domains, because the phonetic units should be mostly language independent. Deri and Knight (2016) provide a data driven approach for the grapheme to phoneme conversion task, which performs well on over 500 languages and can be adapted fairly easily to any new low-resource language. There remains however one major challenge: The use of different phoneme sets for each language, leading to completely unseen units in inference or fine-tuning data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_13",
            "content": "Latent Representations Li et al. (2019a) claim that multilinguality in speech recognition and TTS can be achieved by changing the input to a latent representation that is trained across languages. While their results seem very promising, their technique needs training data in all languages it should be applied to, which rules out zero-shot settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_14",
            "content": "Articulatory Features We fix the shortcoming of not being able to handle unseen phonemes by specifying phonemes in terms of articulatory features such as position (e.g. frontness of the tongue) and category (e.g. voicedness). We show that systems trained on this input can produce a phoneme given nothing but an articulatory description and thus generalize to unseen phonemes. This makes the transfer of knowledge across languages much simpler. A similar approach for the purpose of handling codeswitching has been done in Staib et al. (2020). Our work builds on top of theirs by extending the idea to transfer learning an entire TTS in a new language with minimal data, making use of meta learning on top of articulatory features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_15",
            "content": "Model Agnostic Meta Learning (MAML)",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "203-ARR_v2_16",
            "content": "The goal of MAML (Finn et al., 2017) is to find a set of parameters, that work well as initialization point for multiple tasks, including unseen ones. The procedure consists of an outer loop and an inner loop. The outer loop starts with a set of parameters, which we will call the Meta Model. The inner loop trains task specific copies of the Meta Model for a low amount of steps. Once the inner loop is complete, the loss for each of the models is calculated, summed, and backpropagated to the original Meta Model by unrolling the inner loop. This includes the very costly calculation of second order derivatives. The Meta Model is then updated and the inner loop starts again.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_17",
            "content": "This procedure moves the initialization point closer to the optimal configuration for each of the trained tasks, which generalizes to even unseen tasks. Multiple variants of MAML have been suggested that try to fix the high computational cost of the second order derivatives. The simplest one is called first-order MAML and simply applies the gradient of the task specific model at the end of the inner loop directly to the Meta Model. Other variants are described in Antoniou et al. (2019); Rajeswaran et al. (2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_18",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "203-ARR_v2_19",
            "content": "System Description",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "203-ARR_v2_20",
            "content": "For the implementation of our method, we use the open source IMS Toucan speech synthesis toolkit, first introduced in (Lux et al., 2021), which is in turn based on the ESPnet end-to-end speech processing toolkit (Watanabe et al., 2018;Hayashi et al., 2020Hayashi et al., , 2021. Neekhara et al. (2021) show, that it is beneficial to fine-tune a single-speaker model to a new speaker rather than to train a multispeaker model. Inspired by this, we decided to also use a model that is not conditioned on speakers or on languages rather than a conditioned multispeaker multi-lingual model and fine-tune it on the data from a new speaker in a new language. In preliminary experimentation we got similar results to them within one language, but found their method to not work across languages. In comparison to the fine-tuning of a simple single speaker model, we found training and fine-tuning a model conditioned on language embeddings and speaker embeddings much more sensitive to the choice of hyperparameters. Figure 1 shows an overview of our system, underlining how it is not specific to a certain archi- tecture, but could instead be used in conjunction with almost all modern TTS methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_21",
            "content": "Tacotron 2 For our implementation of Tacotron 2 (Shen et al., 2018), we make use of the forward attention with transition agent introduced in Zhang et al. ( 2018), which uses a CTC-like forward variable (Graves et al., 2006) to promote the quick learning of monotonic alignment between text and speech. To further help with this, we make use of the guided attention loss introduced in Tachibana et al. (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_22",
            "content": "FastSpeech 2 To train the parallel FastSpeech 2 model , annotations of durations for each phoneme are needed. These also have to be generated for the low-resource fine-tuning data. To that end, we generate alignments using the encoder-decoder attention map of a Tacotron 2 model. Following ; Shih et al. (2021); , we apply the Viterbi algorithm to find the most probable monotonic path through the attention map, which significantly improves the quality of the alignments. This is especially important, because we train our FastSpeech 2 model with pitch and energy labels that are averaged over the duration of each individual phoneme to allow for great controllability during inference, as is introduced by \u0141a\u0144cucki (2021). Incorrect alignments would lead to followup errors such as an unnaturally flat prosody.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_23",
            "content": "Furthermore, we make use of the conformer block (Gulati et al., 2020) as the encoder and decoder, rather than the standard transformer (Vaswani et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_24",
            "content": "Articulatory Vectors",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "203-ARR_v2_25",
            "content": "PanPhon The PanPhon resource (Mortensen et al., 2016) can be used to get linguistic specifications of phonemes. It comes with an open-source tool 2 which we use to convert phonemes into numeric vectors. Each vector encodes one feature per dimension and takes the value of either -1, 0 or 1, putting the features on a scale wherever meaningful. This featureset also includes phonological features which go beyond simple phonetics, such as whether a phoneme is syllabic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_26",
            "content": "Papercup Additionally we make use of the purely articulatory description system of phonemes introduced in Staib et al. ( 2020), which we will call Papercup features in the following. For the encoding we use one-hot vectors, similar to their implementation. Some of the features, like openness or frontness, should be on a scale rather than one-hot encoded. However since the articulatory vector is fed into a fully connected layer, we leave the reconstruction of this dependency between features for the network to learn.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_27",
            "content": "Language Agnostic Meta Learning",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "203-ARR_v2_28",
            "content": "We find that the standard implementation of MAML does not work well for the TTS task. The inner loop needs hundreds of updates in order to make a significant change to the performance of the task specific model. This is probably due to the TTS task being a one-to-many mapping task, where the loss function of measuring the distance to a spectrogram is not an accurate objective for the TTS. For every text, there are infinitely many spectrograms, which could be considered gold data. Those spectrograms could differ in e.g. the speaker who reads the text and how they read the text. Since there are no conditioning signals, the TTS has to update its parameters towards a certain speaker's characteristics in general. However because in our case each task is a different language and a different speaker, the training becomes highly unstable. So ideally we would either need to run MAML's inner loop until convergence, which is generally infeasible, or stabilize the procedure by not allowing the model to adapt further to one task than to the others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_29",
            "content": "To fix this issue, we calculate the Meta Model's loss on one batch per language. We then sum up the losses, backpropagate and update the Meta Model directly using Adam (Kingma and Ba, 2015). This stabilizes the learning procedure, but still allows the model to update its parameters towards a more universal configuration. Since we have to make this simplification to MAML in order to deal with the different languages as tasks, we call this procedure language agnostic meta learning (LAML). Ultimately, the model should not care about the language it is fine-tuned in, since it should be close to a universal representation of an acoustic model. To give an exact notion of our modifications: We simplified equation 1 to equation 2, where opt is a gradient descent update, B i is a batch sampled from task i, L is an objective function, \u0398 is the set of parameters from the Meta Model and \u03b8 i is the set of parameters specific to task i. To the best of our knowledge, we are the first to successfully apply MAML to TTS with languages being the tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_30",
            "content": "for t steps do:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_31",
            "content": "\u0398 t = opt \u0398 t\u22121 , \u2207 i L (\u03b8 i,d , B i )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_32",
            "content": "where \u03b8 i,d=0 = \u0398 t\u22121 and for d steps do:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_33",
            "content": "\u03b8 i,d = opt (\u03b8 i,d\u22121 , \u2207L (\u03b8 i,d\u22121 , B i ))(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_34",
            "content": "for t steps do:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_35",
            "content": "\u0398 t = opt \u0398 t\u22121 , \u2207 i L (\u0398 t\u22121 , B i )(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_36",
            "content": "4 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_37",
            "content": "In this section we will go over the experiments we conducted. First we will evaluate the articulatory features on their own in a single language setting using automatic measures. Then we will evaluate the combination of LAML and articulatory features in a cross-lingual setting using both automatic measures and human evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_38",
            "content": "In our experiments we make use of the following datasets: The English Nancy Krebs dataset (16h) from the Blizzard challenge 2011 (Wilhelms-Tricarico et al., 2011;King and Karaiskos, 2011); The German dataset of the speaker Karlsson (29h) from the HUI-Audio-Corpus-German (Puchtler et al., 2021); The Greek (4h), Spanish (24h), Finnish (11h), Russian (21h), Hungarian (10h), Dutch (14h) and French (19h) subsets of the CSS10 dataset (Park and Mulc, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_39",
            "content": "Mono-Lingual Experiments",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "203-ARR_v2_40",
            "content": "Embedding Function Design",
            "ntype": "title",
            "meta": {
                "section": "4.1.1"
            }
        },
        {
            "ix": "203-ARR_v2_41",
            "content": "To explore our first hypothesis, we investigate the capabilities of the articulatory phoneme representations to be used in a single-speaker and singlelanguage TTS system. To compare different ways of embedding the features, we train only the embedding function. As gold data we use the embeddings from a well trained lookup-table based Tacotron 2 model. In table 1 we show the average distances of all articulatory vectors as projected by the embedding function to their identity based embedding counterpart. The distance d between two embedding vectors A and B is defined in equation 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_42",
            "content": "d = i |A i \u2212 B i | \u2212 i A i \u2022 B i i A 2 i \u2022 i B 2 i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_43",
            "content": "(3) This distance function is also used as the objective function. The embedding functions are each trained for 3000 epochs using Adam (Kingma and Ba, 2015) with a batchsize of 32. The first column shows the results of the articulatory features being fed into a linear layer that projects them into a 512 dimensional space. The second column shows the results of the articulatory features being fed into a linear layer that projects them into a 100 dimensional space, applies the tanh activation function and then further projects them into a 512 dimensional space. As can be seen from the results, it is beneficial to both concatenate the PanPhon features with the Papercup features despite their overlap and to add a nonlinearity into the embedding function to match the embeddingspace of a well trained",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_44",
            "content": "Convergence Time",
            "ntype": "title",
            "meta": {
                "section": "4.1.2"
            }
        },
        {
            "ix": "203-ARR_v2_45",
            "content": "To investigate the impact that the articulatory features have on their own, we train a Tacotron 2 with and without them on the Nancy dataset and compare their training time and final quality. While the model trained on embedding tables shows a clear diagonal alignment of text and spectrogram frames on an unseen test sentence after 2,000 steps, the one trained on articulatory features does so already at 500 steps. This is visualized in figure 2. The decoder of the Tacotron 2 model can only start to learn to decode after the alignment of inputs to outputs is learned. So learning the alignment earlier gives the articulatory model a clear benefit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_46",
            "content": "After training for 80,000 steps however, our own subjective assessment finds no difference in quality between the two. The earlier convergence of the alignment however shows a possible advantage of using the articulatory features on low-resource tasks, as quicker training progress means that training can be stopped earlier, before overfitting on little data becomes too problematic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_47",
            "content": "Cross-Lingual Experiments",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "203-ARR_v2_48",
            "content": "In order to investigate the effectiveness of our proposed LAML procedure, we train a Tacotron 2 model and a FastSpeech 2 model on the full Karlsson dataset as a strong baseline. We also train another Tacotron 2 model and another FastSpeech 2 model on speech in 8 languages with one speaker per language (Nancy dataset and CSS10 dataset) and fine-tune those models on a randomly chosen 30 minute subset from the Karlsson dataset. To our surprise, we did not only match, but even outperform the model trained on 29 hours with the model fine-tuned on just 30 minutes in multiple metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_49",
            "content": "As a second baseline we tried to train another meta-checkpoint using the embedding lookup-table approach to also further investigate the effectiveness of the articulatory features. We did however not manage to get such a model to converge to a usable state. This already shows the superiority of the articulatory feature representations for such a multilingual use-case.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_50",
            "content": "Furthermore we tried to fine-tune the well trained English single speaker models from the first experiment on the 30 minutes of German to have another baseline that can be used to measure the impact of the LAML procedure. This setup however also did not yield any usable results. During the fine-tuning process, the model was capable of speaking German with a strong English accent, yet it did not properly learn to speak in the voice of the target speaker. By the time the model learned to speak in the new speaker's voice, it had overfitted the 30 minutes of training data and collapsed, producing no more intelligible speech. We conclude that the method proposed in this paper not only improves on the ability to use cross-lingual data easily, but actually enables it in the first place. Both the articulatory features, as well as the LAML pretraining seem necessary to achieve cross-lingual fine-tuning on low-resource data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_51",
            "content": "The texts we use for the following experiments are disjunct from any training data used. Human speech as gold standard is not used, since we are interested in the difference in performance between the systems, not their absolute performance. The close to state-of-the-art performance of the baselines is considered as given, considering their ideal training conditions and use of proven methods. Furthermore, we chose to use German as our benchmark language over an actual low-resource language, since it is much easier to acquire reliable ratings on intelligibility and naturalness for German, than it would be for an actual low-resource language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_52",
            "content": "Intelligibility",
            "ntype": "title",
            "meta": {
                "section": "4.2.1"
            }
        },
        {
            "ix": "203-ARR_v2_53",
            "content": "To compare intelligibility between our baseline models and our low-resource models, we use the word error rate (WER) of an automatic speech recognition system (ASR) as a proxy. We synthesize 100 sentences of German radio news texts taken from the DIRNDL corpus (Eckart et al., 2012) with each of our baselines and corresponding low-resource systems. Table 2 shows WERs that the German IMS-Speech ASR (Denisov and Vu, 2019) achieves on the synthesized data. For both Tacotron 2 and the FastSpeech 2 based system, the WER of the low-resource model is slightly lower than that of the baseline, thus the low-resource models performed slightly better.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_54",
            "content": "Looking into the cases where the low-resource WER Baseline Low-Resource Tacotron 2 13.1% 12.7% FastSpeech 2 9.9% 9.7%",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_55",
            "content": "Table 2: WER of the synthesis systems on 100 radio news texts measured using the IMS-Speech ASR.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_56",
            "content": "system outperformed the baseline, we find codeswitched segments, where the texts contain names of Russian cities. Since the pretraining data of the low-resource model includes Russian speech, it seems to have not forgotten entirely about what it has seen in the pretraining phase, which in our interpretation confirms the effectiveness of the LAML against the catastrophic-forgetting problem (French, 1999) of regular pretraining.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_57",
            "content": "Naturalness",
            "ntype": "title",
            "meta": {
                "section": "4.2.2"
            }
        },
        {
            "ix": "203-ARR_v2_58",
            "content": "In order to assess the naturalness of the fine-tuned models, we conduct a preference study with 34 native speakers of German. Each participant is shown 12 phonetically balanced samples produced by the Tacotron 2 and FastSpeech 2 models. For every sentence, there is one sample produced by the baseline and one by the low-resource model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_59",
            "content": "The participants are then asked to indicate their subjective overall preference between the two samples. The results for Tacotron 2 are shown in figure 3 (a). The low-resource system was the preferred system in more than half of the cases, with an equal rating taking up more than another third, showing a clear preference for the low-resource model over the baseline. The results for FastSpeech 2, as seen in figure 3 (b), are a lot more balanced. While the baseline is preferred more often than the lowresource variant, it is not the case in the majority of the ratings. In 56% of the cases, the model finetuned on 30 minutes of data was perceived to be as good or better than the model trained on 29 hours.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_60",
            "content": "Computational Resources All models were trained on a single NVIDIA A6000 GPU. Training the Tacotron Baseline took 2 days. Training time of the FastSpeech Baseline was 1 day. Training time of the meta-checkpoint was 4 days, finetuning to a new model from the meta-checkpoint however only takes 2 hours. The HiFi-GAN vocoder used to generate all samples took 4 days to train and was not fine-tuned on the unseen data. We did not perform hyperparameter searches and used the suggested default settings for all methods, which worked sufficiently well, but could surely be improved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_61",
            "content": "Further Analysis and Future Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "203-ARR_v2_62",
            "content": "What is the ideal amount of training steps for fine-tuning? To investigate the amount of update steps needed to fully adapt to the new speaker with the added difficulty of learning a new language, we show the cosine similarity of a speaker embedding of the fine-tuned model to that of the ground truth throughout the fine-tuning process in figure 4. The speaker embedding is built according to the ECAPA-TDNN architecture (Desplanques et al., 2020) and provided open source by SpeechBrain (Ravanelli et al., 2021). It is trained on VoxCeleb 1 and 2 (Nagrani et al., 2017(Nagrani et al., , 2019Chung et al., 2018) which to the best of our knowledge does not overlap with any of the other training and evaluation data we used. We tried to decrease adaptation time further by incorporating said speaker embedding similarity as an additional objective function, similar to Nachmani et al. (2018), we did however see only marginal improvements in the amount of steps needed at the expense of greatly increased training time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_63",
            "content": "Can this setup handle zero-shot phonemes?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_64",
            "content": "We show the model's zero shot capabilities in figure 5. We removed Dutch and Finnish from the training data of the meta-checkpoint and trained another version of it, to be able to see how it handles all of the now completely unseen phonemes specific to German. While their correct position in plot (a) can be considered given, since it shows the articulatory featurespace, their meaningful positions in plot (b) and (c) show that the meta-checkpoint does not just collapse the vector of the unseen phoneme to the one it is most similar to, but actually generalizes. While their pronunciation when produced does not match the correct pronunciation perfectly, it can be understood in the context of a longer sequence. This is congruent with the results of Staib et al. (2020). During the adaptation phase, the pronunciation of the unseen phonemes rapidly matches the correct pronunciation after less than 100 steps. Does this setup learn the difference between language and speaker? When analyzing the finetuned meta-checkpoint, we observed that it seems to link the language of the input to the voice of the speaker. For example when synthesizing an unseen Hungarian text using Tacotron 2, the voice of the synthesis resembles that of the Hungarian female speaker, even though the model has been fine-tuned on the male German speaker and there are no additional conditioning signals. We hypothesize that the LAML procedure induces certain subsets of parameters in the model to be speaker dependent and the encoder of the model priming those parameters purely based on the phoneme sequence. This leads us to believe, that the fine-tuning of all parameters in the model may neither be necessary, nor even the best way of adapting to new data. This also fits the observations of the speaker embedding over time, since the Tacotron model adapts to the new speaker very rapidly. Further investigations into the interactions between parameter groups could allow cutting down the amount of parameters that need to be trained significantly, further reducing the need for training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_65",
            "content": "How can we bring down FastSpeech 2's data need further? A similar observation regarding language and speaker can be made with FastSpeech 2, however as could be seen from the experiment on naturalness and the training time, the FastSpeech 2 model can benefit more from additional data and training time. This may come down to its nearly twice as high parameter count. So a more effective fine-tuning strategy, that considers some parameters as constants, could benefit the fine-tuning capabilities of the FastSpeech 2 model greatly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_66",
            "content": "Does this work across language families? One limitation to our findings is that we investigated only the transfer of languages that share similar phoneme inventories. It is possible that fine-tuning to a language that uses e.g. the lexical tone rather than pitch accents or word accents would require pretraining in more closely related high-resource languages, such as Chinese. However, as Vu and Schultz (2013) find in their analysis of multilingual ASR, the fast adaptation of an acoustic model trained on multiple languages to unseen languages works well, even across different language families. We thus believe that the technique and analysis presented in this paper also holds across language families and types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_67",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "203-ARR_v2_68",
            "content": "In this paper, we show an approach for training a model in a language for which only 30 minutes of data are available by making use of articulatory features and language agnostic meta learning. The main takeaways from our work are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_69",
            "content": "Articulatory Features for TTS Using articulatory features as the input representation to a TTS system enables the use of multilingual data without the need for increased architectural complexity, such as language specific projection spaces. It is furthermore beneficial to use even in singlelanguage scenarios, since the knowledge sharing between phonemes makes the TTS system converge much earlier to an usable state during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_70",
            "content": "MAML on TTS Applying MAML to TTS does not work well. If we however remove the inner loop, we are able to pretrain a low-resource capable checkpoint for TTS. This modification not only makes it work, it also simplifies the formulation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_71",
            "content": "Zero-shot capabilities The use of articulatory features enables zero-shot inference on unseen phonemes. This is further enhanced by the LAML training procedure. The implications of this are particularly interesting for codeswitching, as Staib et al. (2020); Wells et al. (2021) have pointed out previously. Using these two techniques in conjunction could be used to reduce the problem of codeswitching to a problem of token-wise language identification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "203-ARR_v2_72",
            "content": "Antreas Antoniou, Harri Edwards, Amos Storkey, How to train your MAML, 2019, Seventh International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Antreas Antoniou",
                    "Harri Edwards",
                    "Amos Storkey"
                ],
                "title": "How to train your MAML",
                "pub_date": "2019",
                "pub_title": "Seventh International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_73",
            "content": "O Sercan, Mike Ar\u0131k, Adam Chrzanowski, Gregory Coates, Andrew Diamos, Yongguo Gibiansky, Xian Kang, John Li, Andrew Miller, Jonathan Ng, Shubho Raiman, Mohammad Sengupta,  Shoeybi, Deep Voice: Real-time Neural Text-to-Speech, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "O Sercan",
                    "Mike Ar\u0131k",
                    "Adam Chrzanowski",
                    "Gregory Coates",
                    "Andrew Diamos",
                    "Yongguo Gibiansky",
                    "Xian Kang",
                    "John Li",
                    "Andrew Miller",
                    "Jonathan Ng",
                    "Shubho Raiman",
                    "Mohammad Sengupta",
                    " Shoeybi"
                ],
                "title": "Deep Voice: Real-time Neural Text-to-Speech",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "203-ARR_v2_74",
            "content": "Kurniawati Azizah, Mirna Adriani, Wisnu Jatmiko, Hierarchical Transfer Learning for Multilingual, Multi-Speaker, and Style Transfer DNN-Based TTS on Low-Resource Languages, 2020, IEEE Access, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Kurniawati Azizah",
                    "Mirna Adriani",
                    "Wisnu Jatmiko"
                ],
                "title": "Hierarchical Transfer Learning for Multilingual, Multi-Speaker, and Style Transfer DNN-Based TTS on Low-Resource Languages",
                "pub_date": "2020",
                "pub_title": "IEEE Access",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_75",
            "content": "UNKNOWN, None, 2021, One TTS alignment to rule them all, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "One TTS alignment to rule them all",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_76",
            "content": "Maximilian Bisani, Hermann Ney, Jointsequence models for grapheme-to-phoneme conversion, 2008, Speech communication, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Maximilian Bisani",
                    "Hermann Ney"
                ],
                "title": "Jointsequence models for grapheme-to-phoneme conversion",
                "pub_date": "2008",
                "pub_title": "Speech communication",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_77",
            "content": "Yuan-Jui Chen, Tao Tu, Cheng-Chieh Yeh, Hung-Yi Lee, End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning, 2019, Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yuan-Jui Chen",
                    "Tao Tu",
                    "Cheng-Chieh Yeh",
                    "Hung-Yi Lee"
                ],
                "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
                "pub_date": "2019",
                "pub_title": "Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_78",
            "content": "UNKNOWN, None, 2018, Voxceleb2: Deep speaker recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Voxceleb2: Deep speaker recognition",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_79",
            "content": "A Robert, Korin Clark, Simon Richmond,  King, Multisyn: Open-domain unit selection for the Festival speech synthesis system, 2007, Speech Communication, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "A Robert",
                    "Korin Clark",
                    "Simon Richmond",
                    " King"
                ],
                "title": "Multisyn: Open-domain unit selection for the Festival speech synthesis system",
                "pub_date": "2007",
                "pub_title": "Speech Communication",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_80",
            "content": "Marcel De Korte, Esther Klabbers, Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling, 2020, Proc. Interspeech 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Marcel De Korte",
                    "Esther Klabbers"
                ],
                "title": "Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling",
                "pub_date": "2020",
                "pub_title": "Proc. Interspeech 2020",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_81",
            "content": "UNKNOWN, None, 2019, IMS-speech: A speech to text tool. Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "IMS-speech: A speech to text tool. Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_82",
            "content": "Aliya Deri, Kevin Knight, Grapheme-tophoneme models for (almost) any language, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Aliya Deri",
                    "Kevin Knight"
                ],
                "title": "Grapheme-tophoneme models for (almost) any language",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "203-ARR_v2_83",
            "content": "Brecht Desplanques, Jenthe Thienpondt, Kris Demuynck, ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification, 2020, Interspeech 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Brecht Desplanques",
                    "Jenthe Thienpondt",
                    "Kris Demuynck"
                ],
                "title": "ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
                "pub_date": "2020",
                "pub_title": "Interspeech 2020",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_84",
            "content": "Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, Karen Simonyan, End-toend Adversarial Text-to-Speech, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jeff Donahue",
                    "Sander Dieleman",
                    "Mikolaj Binkowski",
                    "Erich Elsen",
                    "Karen Simonyan"
                ],
                "title": "End-toend Adversarial Text-to-Speech",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_85",
            "content": "Peter Ebden, Richard Sproat, The Kestrel TTS text normalization system, 2015, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Peter Ebden",
                    "Richard Sproat"
                ],
                "title": "The Kestrel TTS text normalization system",
                "pub_date": "2015",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_86",
            "content": "Kerstin Eckart, Arndt Riester, Katrin Schweitzer, A discourse information radio news database for linguistic analysis, 2012, Linked Data in Linguistics, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Kerstin Eckart",
                    "Arndt Riester",
                    "Katrin Schweitzer"
                ],
                "title": "A discourse information radio news database for linguistic analysis",
                "pub_date": "2012",
                "pub_title": "Linked Data in Linguistics",
                "pub": "Springer"
            }
        },
        {
            "ix": "203-ARR_v2_87",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Chelsea Finn",
                    "Pieter Abbeel",
                    "Sergey Levine"
                ],
                "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
                "pub_date": "2017",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "203-ARR_v2_88",
            "content": "M Robert,  French, Catastrophic forgetting in connectionist networks, 1999, Trends in cognitive sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "M Robert",
                    " French"
                ],
                "title": "Catastrophic forgetting in connectionist networks",
                "pub_date": "1999",
                "pub_title": "Trends in cognitive sciences",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_89",
            "content": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial nets, 2014, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Ian Goodfellow",
                    "Jean Pouget-Abadie",
                    "Mehdi Mirza",
                    "Bing Xu",
                    "David Warde-Farley",
                    "Sherjil Ozair",
                    "Aaron Courville",
                    "Yoshua Bengio"
                ],
                "title": "Generative adversarial nets",
                "pub_date": "2014",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_90",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the 23rd international conference on Machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Alex Graves",
                    "Santiago Fern\u00e1ndez",
                    "Faustino Gomez",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 23rd international conference on Machine learning",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_91",
            "content": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Conformer: Convolution-augmented Transformer for Speech Recognition, 2020, Proc. Interspeech 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Anmol Gulati",
                    "James Qin",
                    "Chung-Cheng Chiu",
                    "Niki Parmar",
                    "Yu Zhang",
                    "Jiahui Yu",
                    "Wei Han",
                    "Shibo Wang",
                    "Zhengdong Zhang",
                    "Yonghui Wu"
                ],
                "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
                "pub_date": "2020",
                "pub_title": "Proc. Interspeech 2020",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_92",
            "content": "Alexander Gutkin, Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages, 2017, Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Alexander Gutkin"
                ],
                "title": "Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages",
                "pub_date": "2017",
                "pub_title": "Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_93",
            "content": "Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, Kazuya Takeda, Shubham Toshniwal, Karen Livescu, Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis, 2019, INTER-SPEECH, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Tomoki Hayashi",
                    "Shinji Watanabe",
                    "Tomoki Toda",
                    "Kazuya Takeda",
                    "Shubham Toshniwal",
                    "Karen Livescu"
                ],
                "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
                "pub_date": "2019",
                "pub_title": "INTER-SPEECH",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_94",
            "content": "Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, Takenori Yoshimura, Shinji Watanabe, Tomoki Toda, Kazuya Takeda, Yu Zhang, Xu Tan, ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit, 2020, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Tomoki Hayashi",
                    "Ryuichi Yamamoto",
                    "Katsuki Inoue",
                    "Takenori Yoshimura",
                    "Shinji Watanabe",
                    "Tomoki Toda",
                    "Kazuya Takeda",
                    "Yu Zhang",
                    "Xu Tan"
                ],
                "title": "ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit",
                "pub_date": "2020",
                "pub_title": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_95",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_96",
            "content": "UNKNOWN, None, 2021, ESPnet2-TTS: Extending the Edge of TTS Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "ESPnet2-TTS: Extending the Edge of TTS Research",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_97",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_98",
            "content": "UNKNOWN, None, 1961, Preliminaries to Speech Analysis: The Distinctive Features and Their Correlates, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "1961",
                "pub_title": "Preliminaries to Speech Analysis: The Distinctive Features and Their Correlates",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_99",
            "content": "Jaehyeon Kim, Sungwon Kim, Jungil Kong, Sungroh Yoon, Glow-tts: A generative flow for text-to-speech via monotonic alignment search, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Jaehyeon Kim",
                    "Sungwon Kim",
                    "Jungil Kong",
                    "Sungroh Yoon"
                ],
                "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_100",
            "content": "Jaehyeon Kim, Jungil Kong, Juhee Son, Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Jaehyeon Kim",
                    "Jungil Kong",
                    "Juhee Son"
                ],
                "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech",
                "pub_date": "2021",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "203-ARR_v2_101",
            "content": "Simon King, Vasilis Karaiskos, The Blizzard Challenge, 2011, Proc. Blizzard Challenge Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Simon King",
                    "Vasilis Karaiskos"
                ],
                "title": "The Blizzard Challenge",
                "pub_date": "2011",
                "pub_title": "Proc. Blizzard Challenge Workshop",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_102",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A Method for Stochastic Optimization, 2015, ICLR (Poster), .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A Method for Stochastic Optimization",
                "pub_date": "2015",
                "pub_title": "ICLR (Poster)",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_103",
            "content": "Jungil Kong, Jaehyeon Kim, Jaekyoung Bae, HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Jungil Kong",
                    "Jaehyeon Kim",
                    "Jaekyoung Bae"
                ],
                "title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_104",
            "content": "Kundan Kumar, Rithesh Kumar, Lucas Thibault De Boissiere, Wei Gestin, Jose Teoh, Alexandre Sotelo, Yoshua De Br\u00e9bisson, Aaron C Bengio,  Courville, MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Kundan Kumar",
                    "Rithesh Kumar",
                    "Lucas Thibault De Boissiere",
                    "Wei Gestin",
                    "Jose Teoh",
                    "Alexandre Sotelo",
                    "Yoshua De Br\u00e9bisson",
                    "Aaron C Bengio",
                    " Courville"
                ],
                "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_105",
            "content": "Adrian \u0141a\u0144cucki, FastPitch: Parallel text-tospeech with pitch prediction, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Adrian \u0141a\u0144cucki"
                ],
                "title": "FastPitch: Parallel text-tospeech with pitch prediction",
                "pub_date": "2021",
                "pub_title": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_106",
            "content": "Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan, Bytes are all you need: Endto-end multilingual speech recognition and synthesis with bytes, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Bo Li",
                    "Yu Zhang",
                    "Tara Sainath",
                    "Yonghui Wu",
                    "William Chan"
                ],
                "title": "Bytes are all you need: Endto-end multilingual speech recognition and synthesis with bytes",
                "pub_date": "2019",
                "pub_title": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_107",
            "content": "Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, Neural speech synthesis with transformer network, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Naihan Li",
                    "Shujie Liu",
                    "Yanqing Liu",
                    "Sheng Zhao",
                    "Ming Liu"
                ],
                "title": "Neural speech synthesis with transformer network",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_108",
            "content": "Florian Lux, Julia Koch, Antje Schweitzer, Ngoc Vu, The IMS Toucan system for the Blizzard Challenge 2021, 2021, Proc. Blizzard Challenge Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Florian Lux",
                    "Julia Koch",
                    "Antje Schweitzer",
                    "Ngoc Vu"
                ],
                "title": "The IMS Toucan system for the Blizzard Challenge 2021",
                "pub_date": "2021",
                "pub_title": "Proc. Blizzard Challenge Workshop",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_109",
            "content": "David Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, Lori Levin, PanPhon: A Resource for Mapping IPA Segments to Articulatory Feature Vectors, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "David Mortensen",
                    "Patrick Littell",
                    "Akash Bharadwaj",
                    "Kartik Goyal",
                    "Chris Dyer",
                    "Lori Levin"
                ],
                "title": "PanPhon: A Resource for Mapping IPA Segments to Articulatory Feature Vectors",
                "pub_date": "2016",
                "pub_title": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "pub": "ACL"
            }
        },
        {
            "ix": "203-ARR_v2_110",
            "content": "Eliya Nachmani, Adam Polyak, Yaniv Taigman, Lior Wolf, Fitting new speakers based on a short untranscribed sample, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Eliya Nachmani",
                    "Adam Polyak",
                    "Yaniv Taigman",
                    "Lior Wolf"
                ],
                "title": "Fitting new speakers based on a short untranscribed sample",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "203-ARR_v2_111",
            "content": "UNKNOWN, None, 2017, Voxceleb: a large-scale speaker identification dataset, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Voxceleb: a large-scale speaker identification dataset",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_112",
            "content": "Arsha Nagrani, Joon Chung, Weidi Xie, Andrew Zisserman, Voxceleb: Large-scale speaker verification in the wild, 2019, Computer Science and Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Arsha Nagrani",
                    "Joon Chung",
                    "Weidi Xie",
                    "Andrew Zisserman"
                ],
                "title": "Voxceleb: Large-scale speaker verification in the wild",
                "pub_date": "2019",
                "pub_title": "Computer Science and Language",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_113",
            "content": "UNKNOWN, None, 2021, Adapting TTS models For New Speakers using Transfer Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Adapting TTS models For New Speakers using Transfer Learning",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_114",
            "content": "Kyubyong Park, Thomas Mulc, CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages, 2019, Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Kyubyong Park",
                    "Thomas Mulc"
                ],
                "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
                "pub_date": "2019",
                "pub_title": "Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_115",
            "content": "Ryan Prenger, Rafael Valle, Bryan Catanzaro, WaveGlow: A flow-based generative network for speech synthesis, 2019, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Ryan Prenger",
                    "Rafael Valle",
                    "Bryan Catanzaro"
                ],
                "title": "WaveGlow: A flow-based generative network for speech synthesis",
                "pub_date": "2019",
                "pub_title": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_116",
            "content": "Pascal Puchtler, Johannes Wirth, Ren\u00e9 Peinl, Hui-audio-corpus-german: A high quality tts dataset, 2021, German Conference on Artificial Intelligence (K\u00fcnstliche Intelligenz), Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Pascal Puchtler",
                    "Johannes Wirth",
                    "Ren\u00e9 Peinl"
                ],
                "title": "Hui-audio-corpus-german: A high quality tts dataset",
                "pub_date": "2021",
                "pub_title": "German Conference on Artificial Intelligence (K\u00fcnstliche Intelligenz)",
                "pub": "Springer"
            }
        },
        {
            "ix": "203-ARR_v2_117",
            "content": "Aravind Rajeswaran, Chelsea Finn, M Sham, Sergey Kakade,  Levine, Meta-learning with implicit gradients, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Aravind Rajeswaran",
                    "Chelsea Finn",
                    "M Sham",
                    "Sergey Kakade",
                    " Levine"
                ],
                "title": "Meta-learning with implicit gradients",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_118",
            "content": "Kanishka Rao, Fuchun Peng, Ha\u015fim Sak, Fran\u00e7oise Beaufays, Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks, 2015, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Kanishka Rao",
                    "Fuchun Peng",
                    "Ha\u015fim Sak",
                    "Fran\u00e7oise Beaufays"
                ],
                "title": "Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks",
                "pub_date": "2015",
                "pub_title": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_119",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_120",
            "content": "Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu, FastSpeech 2: Fast and High-Quality End-to-End Text to Speech, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Yi Ren",
                    "Chenxu Hu",
                    "Xu Tan",
                    "Tao Qin",
                    "Sheng Zhao",
                    "Zhou Zhao",
                    "Tie-Yan Liu"
                ],
                "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_121",
            "content": "Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu, FastSpeech: fast, robust and controllable text to speech, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Yi Ren",
                    "Yangjun Ruan",
                    "Xu Tan",
                    "Tao Qin",
                    "Sheng Zhao",
                    "Zhou Zhao",
                    "Tie-Yan Liu"
                ],
                "title": "FastSpeech: fast, robust and controllable text to speech",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_122",
            "content": "Marc Schr\u00f6der, J\u00fcrgen Trouvain, The German text-to-speech synthesis system MARY: A tool for research, development and teaching, 2003, International Journal of Speech Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Marc Schr\u00f6der",
                    "J\u00fcrgen Trouvain"
                ],
                "title": "The German text-to-speech synthesis system MARY: A tool for research, development and teaching",
                "pub_date": "2003",
                "pub_title": "International Journal of Speech Technology",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_123",
            "content": "UNKNOWN, None, 2018, Natural TTS Synthesis by Conditioning WaveNet on mel Spectrogram Predictions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Natural TTS Synthesis by Conditioning WaveNet on mel Spectrogram Predictions",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_124",
            "content": "J Kevin, Rafael Shih, Rohan Valle, Adrian Badlani, Wei Lancucki, Bryan Ping,  Catanzaro, RAD-TTS: Parallel Flow-Based TTS with Robust Alignment Learning and Diverse Synthesis, 2021, ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "J Kevin",
                    "Rafael Shih",
                    "Rohan Valle",
                    "Adrian Badlani",
                    "Wei Lancucki",
                    "Bryan Ping",
                    " Catanzaro"
                ],
                "title": "RAD-TTS: Parallel Flow-Based TTS with Robust Alignment Learning and Diverse Synthesis",
                "pub_date": "2021",
                "pub_title": "ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_125",
            "content": "UNKNOWN, None, 2020, 2020. Phonological Features for 0-Shot Multilingual Speech Synthesis, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "2020. Phonological Features for 0-Shot Multilingual Speech Synthesis",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_126",
            "content": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara, Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Hideyuki Tachibana",
                    "Katsuya Uenoyama",
                    "Shunsuke Aihara"
                ],
                "title": "Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention",
                "pub_date": "2018",
                "pub_title": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_127",
            "content": "Jason Taylor, Korin Richmond, Enhancing Sequence-to-Sequence Text-to-Speech with Morphology, 2020, INTERSPEECH, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Jason Taylor",
                    "Korin Richmond"
                ],
                "title": "Enhancing Sequence-to-Sequence Text-to-Speech with Morphology",
                "pub_date": "2020",
                "pub_title": "INTERSPEECH",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_128",
            "content": "Paul Taylor, Hidden Markov models for grapheme to phoneme conversion, 2005, Ninth European Conference on Speech Communication and Technology, Citeseer.",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Paul Taylor"
                ],
                "title": "Hidden Markov models for grapheme to phoneme conversion",
                "pub_date": "2005",
                "pub_title": "Ninth European Conference on Speech Communication and Technology",
                "pub": "Citeseer"
            }
        },
        {
            "ix": "203-ARR_v2_129",
            "content": "A\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, WaveNet: A Generative Model for Raw Audio, 2016, 9th ISCA Speech Synthesis Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "A\u00e4ron Van Den Oord",
                    "Sander Dieleman",
                    "Heiga Zen",
                    "Karen Simonyan",
                    "Oriol Vinyals",
                    "Alex Graves",
                    "Nal Kalchbrenner",
                    "Andrew Senior",
                    "Koray Kavukcuoglu"
                ],
                "title": "WaveNet: A Generative Model for Raw Audio",
                "pub_date": "2016",
                "pub_title": "9th ISCA Speech Synthesis Workshop",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_130",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_131",
            "content": "Ngoc Vu, Tanja Schultz, Multilingual multilayer perceptron for rapid language adaptation between and across language families, 2013, Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Ngoc Vu",
                    "Tanja Schultz"
                ],
                "title": "Multilingual multilayer perceptron for rapid language adaptation between and across language families",
                "pub_date": "2013",
                "pub_title": "Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_132",
            "content": "UNKNOWN, None, 2017, Tacotron: Towards End-to-End Speech Synthesis. Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Tacotron: Towards End-to-End Speech Synthesis. Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_133",
            "content": "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson , Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, Tsubasa Ochiai, ESPnet: End-to-end speech processing toolkit, 2018, Proceedings of Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Shinji Watanabe",
                    "Takaaki Hori",
                    "Shigeki Karita",
                    "Tomoki Hayashi",
                    "Jiro Nishitoba",
                    "Yuya Unno",
                    "Nelson ",
                    "Yalta Soplin",
                    "Jahn Heymann",
                    "Matthew Wiesner",
                    "Nanxin Chen",
                    "Adithya Renduchintala",
                    "Tsubasa Ochiai"
                ],
                "title": "ESPnet: End-to-end speech processing toolkit",
                "pub_date": "2018",
                "pub_title": "Proceedings of Interspeech",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_134",
            "content": "Dan Wells, Pilar Oplustil-Gallegos, Simon King, The CSTR entry to the Blizzard Challenge 2021, 2021, Proc. Blizzard Challenge Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Dan Wells",
                    "Pilar Oplustil-Gallegos",
                    "Simon King"
                ],
                "title": "The CSTR entry to the Blizzard Challenge 2021",
                "pub_date": "2021",
                "pub_title": "Proc. Blizzard Challenge Workshop",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_135",
            "content": "Reiner Wilhelms-Tricarico, Brian Mottershead, Rattima Nitisaroj, Michael Baumgartner, John Reichenbach, Gary Marple, The lessac technologies system for blizzard challenge, 2011, Blizzard Challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Reiner Wilhelms-Tricarico",
                    "Brian Mottershead",
                    "Rattima Nitisaroj",
                    "Michael Baumgartner",
                    "John Reichenbach",
                    "Gary Marple"
                ],
                "title": "The lessac technologies system for blizzard challenge",
                "pub_date": "2011",
                "pub_title": "Blizzard Challenge",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_136",
            "content": "UNKNOWN, None, 2020, LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "203-ARR_v2_137",
            "content": "Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim, Parallel waveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram, 2020, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Ryuichi Yamamoto",
                    "Eunwoo Song",
                    "Jae-Min Kim"
                ],
                "title": "Parallel waveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
                "pub_date": "2020",
                "pub_title": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "203-ARR_v2_138",
            "content": "Jingzhou Yang, Lei He, Towards Universal Text-to-Speech, 2020, INTERSPEECH, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Jingzhou Yang",
                    "Lei He"
                ],
                "title": "Towards Universal Text-to-Speech",
                "pub_date": "2020",
                "pub_title": "INTERSPEECH",
                "pub": null
            }
        },
        {
            "ix": "203-ARR_v2_139",
            "content": "Jing-Xuan Zhang, Zhen-Hua Ling, Li-Rong Dai, Forward attention in sequence-to-sequence acoustic modeling for speech synthesis, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Jing-Xuan Zhang",
                    "Zhen-Hua Ling",
                    "Li-Rong Dai"
                ],
                "title": "Forward attention in sequence-to-sequence acoustic modeling for speech synthesis",
                "pub_date": "2018",
                "pub_title": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": "IEEE"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "203-ARR_v2_0@0",
            "content": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_0",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_2@0",
            "content": "While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_2",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_2@1",
            "content": "In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_2",
            "start": 219,
            "end": 399,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_2@2",
            "content": "In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality textto-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_2",
            "start": 401,
            "end": 614,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_4@0",
            "content": "The advance of deep learning (Vaswani et al., 2017;Goodfellow et al., 2014) has enabled great improvements in the field of Text-to-Speech (TTS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_4",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_4@1",
            "content": "(Towards-)end-to-end models, such as Tacotron 2 (Wang et al., 2017;Shen et al., 2018), Trans-formerTTS (Li et al., 2019b), FastSpeech 2 (Ren et al., 2019, FastPitch (\u0141a\u0144cucki, 2021) and many more famous instances (e.g. Ar\u0131k et al. (2017) and Prenger et al. (2019)) allow for speech synthesis with unprecedented quality and controllability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_4",
            "start": 145,
            "end": 483,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_4@2",
            "content": "The models mentioned here rely on vocoders, such as WaveNet (van den Oord et al., 2016), MelGAN (Kumar et al., 2019), Parallel Wave-GAN or HiFi-GAN to turn the parametric representations that they produce into waveforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_4",
            "start": 485,
            "end": 704,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_4@3",
            "content": "Recently proposed models even include some with the ability to go directly to the waveform from a grapheme or phoneme input sequence, such as EATS (Donahue et al., 2020) or VITS (Kim et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_4",
            "start": 706,
            "end": 902,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@0",
            "content": "While these methods all perform remarkably well if given enough data, cross-lingual use of data remains a key challenge in TTS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@1",
            "content": "Most modern methods are limited to languages and domains that are rich in resources, which over 6,000 languages are not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 128,
            "end": 247,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@2",
            "content": "Attempts at reducing the required resources in a target language by making use of transfer learning from multilingual data have been made by Azizah et al. (2020); ; Chen et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 249,
            "end": 432,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@3",
            "content": "The mismatch of input spaces however requires complex architectural changes, which limits their ability to be used in conjunction with other modern TTS architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 434,
            "end": 599,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@4",
            "content": "Attempts at fixing the issue of having to transfer knowledge from a source to a target by just jointly training on a mixed set of more and less resource rich languages have been made by He et al. (2021); de Korte et al. (2020); Yang and He (2020), which requires complex training procedures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 601,
            "end": 891,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@5",
            "content": "In this work, we will also attempt to transfer knowledge from a set of high resource languages to a low resource language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 893,
            "end": 1014,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_5@6",
            "content": "We fix previous shortcomings by 1) using a linguistically motivated representation of the inputs to such a system (articulatory and phonological features of phonemes) that enables cross-lingual knowledge sharing and 2) applying the model agnostic meta learning (MAML) framework (Finn et al., 2017) to the field of low-resource TTS for the first time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_5",
            "start": 1016,
            "end": 1365,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@0",
            "content": "Using articulatory features as inputs for neural TTS has been attempted recently by Staib et al. (2020) and Wells et al. (2021), following the classical approach of Jakobson et al. (1961).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@1",
            "content": "Both achieved good results when applying this idea to the codeswitching problem, since unseen phonemes in the input space no longer map to nonsensical positions, as it would be the case for the standard embedding-lookup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 189,
            "end": 408,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@2",
            "content": "It has to be noted however, that this only works across languages with similar types of phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 410,
            "end": 506,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@3",
            "content": "Also Gutkin (2017) have applied phonological features to low-resource TTS with fair success.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 508,
            "end": 599,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@4",
            "content": "They did however rely on supplementary features, such as dependency parsers and morphological analyzers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 601,
            "end": 704,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@5",
            "content": "Furthermore all of their data and models are proprietary and can therefore not be used to compare results to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 706,
            "end": 814,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_6@6",
            "content": "In this work, we extend the use of articulatory inputs with the MAML framework to enable very simple yet well working low-resource TTS that can be applied to almost all modern TTS architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_6",
            "start": 816,
            "end": 1009,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_7@0",
            "content": "We encounter severe instabilities when using MAML on TTS, which make the standard formulation of MAML infeasible to use.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_7",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_7@1",
            "content": "Thus we also propose a modification to MAML, which reduces the procedure's complexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_7",
            "start": 121,
            "end": 206,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_7@2",
            "content": "This allows us to create a set of parameters of a model that can be used to fine-tune to a well working single-language singlespeaker TTS model with as little as 30 minutes of paired training data available and even enables zeroshot adaptation to unseen languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_7",
            "start": 208,
            "end": 471,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_7@3",
            "content": "We evaluate the success of our approach with both automatic measures and human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_7",
            "start": 473,
            "end": 562,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_8@0",
            "content": "Our contributions are as follows: 1) We show that it is beneficial to train a TTS model on articulatory features rather than on phoneme-identities, even in the standard single-language high-resource case; 2) We introduce a training procedure that is closely related to MAML which allows training a set of parameters for a TTS model that can be fine-tuned in a low resource scenario; 3) We provide insights on how much data and training time are required to fine-tune a model across different languages and speakers simultaneously using said meta-parameters; 4) We show that the metaparameters can generalize to unseen phonemes and rapidly improve their ability to properly pronounce them when fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_8",
            "start": 0,
            "end": 704,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_8@1",
            "content": "1 2 Background and Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_8",
            "start": 706,
            "end": 736,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_9@0",
            "content": "Input Representations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_9",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_10@0",
            "content": "Character Embeddings The simplest approach to representing text as input to a TTS is using indexes of graphemes to look up embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_10",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_10@1",
            "content": "This is however prone to mistakes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_10",
            "start": 135,
            "end": 168,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_10@2",
            "content": "Taylor and Richmond (2020) bring up the example of coathanger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_10",
            "start": 170,
            "end": 231,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_10@3",
            "content": "If the TTS is not aware of the morpheme boundary between the coat and the hang, it will be inclined to produce something like [k2T@In\u00c3@] rather than the correct [koUthaeN@].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_10",
            "start": 233,
            "end": 405,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_10@4",
            "content": "Such a representation of the input will be highly language dependent, since special pronunciation rules rarely hold for more than a single language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_10",
            "start": 407,
            "end": 554,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_11@0",
            "content": "The textual input can be augmented by adding information, such as morpheme boundaries, intonation phrase boundaries derived from e.g. syntactic parsing as is done in many TTS frontends (Schr\u00f6der and Trouvain, 2003;Clark et al., 2007;Ebden and Sproat, 2015), or even the semantic identity of the word a character belongs to, using e.g. BERT embeddings (Hayashi et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_11",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@0",
            "content": "Phoneme Embeddings Rather than looking up embeddings for graphemes, it is often beneficial to use embeddings of phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@1",
            "content": "Phonemizers (Bisani and Ney, 2008;Taylor, 2005;Rao et al., 2015) produce a sequence of phonetic units, which correlate with the segments in the audio much more than raw text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 122,
            "end": 295,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@2",
            "content": "One such standard of phonetic representation which we make use of is the International Phonetic Alphabet (IPA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 297,
            "end": 407,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@3",
            "content": "Using this set of phonetic units alleviates the problems of TTS fine-tuning and transfer-learning to low-resource domains, because the phonetic units should be mostly language independent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 409,
            "end": 596,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@4",
            "content": "Deri and Knight (2016) provide a data driven approach for the grapheme to phoneme conversion task, which performs well on over 500 languages and can be adapted fairly easily to any new low-resource language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 598,
            "end": 804,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_12@5",
            "content": "There remains however one major challenge: The use of different phoneme sets for each language, leading to completely unseen units in inference or fine-tuning data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_12",
            "start": 806,
            "end": 969,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_13@0",
            "content": "Latent Representations Li et al. (2019a) claim that multilinguality in speech recognition and TTS can be achieved by changing the input to a latent representation that is trained across languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_13",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_13@1",
            "content": "While their results seem very promising, their technique needs training data in all languages it should be applied to, which rules out zero-shot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_13",
            "start": 197,
            "end": 350,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_14@0",
            "content": "Articulatory Features We fix the shortcoming of not being able to handle unseen phonemes by specifying phonemes in terms of articulatory features such as position (e.g. frontness of the tongue) and category (e.g. voicedness).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_14",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_14@1",
            "content": "We show that systems trained on this input can produce a phoneme given nothing but an articulatory description and thus generalize to unseen phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_14",
            "start": 226,
            "end": 375,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_14@2",
            "content": "This makes the transfer of knowledge across languages much simpler.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_14",
            "start": 377,
            "end": 443,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_14@3",
            "content": "A similar approach for the purpose of handling codeswitching has been done in Staib et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_14",
            "start": 445,
            "end": 542,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_14@4",
            "content": "Our work builds on top of theirs by extending the idea to transfer learning an entire TTS in a new language with minimal data, making use of meta learning on top of articulatory features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_14",
            "start": 544,
            "end": 730,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_15@0",
            "content": "Model Agnostic Meta Learning (MAML)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_15",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@0",
            "content": "The goal of MAML (Finn et al., 2017) is to find a set of parameters, that work well as initialization point for multiple tasks, including unseen ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@1",
            "content": "The procedure consists of an outer loop and an inner loop.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 151,
            "end": 208,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@2",
            "content": "The outer loop starts with a set of parameters, which we will call the Meta Model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 210,
            "end": 291,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@3",
            "content": "The inner loop trains task specific copies of the Meta Model for a low amount of steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 293,
            "end": 379,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@4",
            "content": "Once the inner loop is complete, the loss for each of the models is calculated, summed, and backpropagated to the original Meta Model by unrolling the inner loop.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 381,
            "end": 542,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@5",
            "content": "This includes the very costly calculation of second order derivatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 544,
            "end": 613,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_16@6",
            "content": "The Meta Model is then updated and the inner loop starts again.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_16",
            "start": 615,
            "end": 677,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_17@0",
            "content": "This procedure moves the initialization point closer to the optimal configuration for each of the trained tasks, which generalizes to even unseen tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_17",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_17@1",
            "content": "Multiple variants of MAML have been suggested that try to fix the high computational cost of the second order derivatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_17",
            "start": 153,
            "end": 274,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_17@2",
            "content": "The simplest one is called first-order MAML and simply applies the gradient of the task specific model at the end of the inner loop directly to the Meta Model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_17",
            "start": 276,
            "end": 434,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_17@3",
            "content": "Other variants are described in Antoniou et al. (2019); Rajeswaran et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_17",
            "start": 436,
            "end": 516,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_18@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_18",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_19@0",
            "content": "System Description",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_19",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@0",
            "content": "For the implementation of our method, we use the open source IMS Toucan speech synthesis toolkit, first introduced in (Lux et al., 2021), which is in turn based on the ESPnet end-to-end speech processing toolkit (Watanabe et al., 2018;Hayashi et al., 2020Hayashi et al., , 2021.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@1",
            "content": "Neekhara et al. (2021) show, that it is beneficial to fine-tune a single-speaker model to a new speaker rather than to train a multispeaker model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 279,
            "end": 424,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@2",
            "content": "Inspired by this, we decided to also use a model that is not conditioned on speakers or on languages rather than a conditioned multispeaker multi-lingual model and fine-tune it on the data from a new speaker in a new language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 426,
            "end": 651,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@3",
            "content": "In preliminary experimentation we got similar results to them within one language, but found their method to not work across languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 653,
            "end": 787,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@4",
            "content": "In comparison to the fine-tuning of a simple single speaker model, we found training and fine-tuning a model conditioned on language embeddings and speaker embeddings much more sensitive to the choice of hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 789,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_20@5",
            "content": "Figure 1 shows an overview of our system, underlining how it is not specific to a certain archi- tecture, but could instead be used in conjunction with almost all modern TTS methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_20",
            "start": 1010,
            "end": 1191,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_21@0",
            "content": "Tacotron 2 For our implementation of Tacotron 2 (Shen et al., 2018), we make use of the forward attention with transition agent introduced in Zhang et al. ( 2018), which uses a CTC-like forward variable (Graves et al., 2006) to promote the quick learning of monotonic alignment between text and speech.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_21",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_21@1",
            "content": "To further help with this, we make use of the guided attention loss introduced in Tachibana et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_21",
            "start": 303,
            "end": 408,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@0",
            "content": "FastSpeech 2 To train the parallel FastSpeech 2 model , annotations of durations for each phoneme are needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@1",
            "content": "These also have to be generated for the low-resource fine-tuning data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 110,
            "end": 179,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@2",
            "content": "To that end, we generate alignments using the encoder-decoder attention map of a Tacotron 2 model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 181,
            "end": 278,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@3",
            "content": "Following ; Shih et al. (2021); , we apply the Viterbi algorithm to find the most probable monotonic path through the attention map, which significantly improves the quality of the alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 280,
            "end": 471,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@4",
            "content": "This is especially important, because we train our FastSpeech 2 model with pitch and energy labels that are averaged over the duration of each individual phoneme to allow for great controllability during inference, as is introduced by \u0141a\u0144cucki (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 473,
            "end": 723,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_22@5",
            "content": "Incorrect alignments would lead to followup errors such as an unnaturally flat prosody.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_22",
            "start": 725,
            "end": 811,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_23@0",
            "content": "Furthermore, we make use of the conformer block (Gulati et al., 2020) as the encoder and decoder, rather than the standard transformer (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_23",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_24@0",
            "content": "Articulatory Vectors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_24",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_25@0",
            "content": "PanPhon The PanPhon resource (Mortensen et al., 2016) can be used to get linguistic specifications of phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_25",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_25@1",
            "content": "It comes with an open-source tool 2 which we use to convert phonemes into numeric vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_25",
            "start": 112,
            "end": 201,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_25@2",
            "content": "Each vector encodes one feature per dimension and takes the value of either -1, 0 or 1, putting the features on a scale wherever meaningful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_25",
            "start": 203,
            "end": 342,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_25@3",
            "content": "This featureset also includes phonological features which go beyond simple phonetics, such as whether a phoneme is syllabic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_25",
            "start": 344,
            "end": 467,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_26@0",
            "content": "Papercup Additionally we make use of the purely articulatory description system of phonemes introduced in Staib et al. ( 2020), which we will call Papercup features in the following.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_26",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_26@1",
            "content": "For the encoding we use one-hot vectors, similar to their implementation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_26",
            "start": 183,
            "end": 255,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_26@2",
            "content": "Some of the features, like openness or frontness, should be on a scale rather than one-hot encoded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_26",
            "start": 257,
            "end": 355,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_26@3",
            "content": "However since the articulatory vector is fed into a fully connected layer, we leave the reconstruction of this dependency between features for the network to learn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_26",
            "start": 357,
            "end": 520,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_27@0",
            "content": "Language Agnostic Meta Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_27",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@0",
            "content": "We find that the standard implementation of MAML does not work well for the TTS task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@1",
            "content": "The inner loop needs hundreds of updates in order to make a significant change to the performance of the task specific model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 86,
            "end": 210,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@2",
            "content": "This is probably due to the TTS task being a one-to-many mapping task, where the loss function of measuring the distance to a spectrogram is not an accurate objective for the TTS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 212,
            "end": 390,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@3",
            "content": "For every text, there are infinitely many spectrograms, which could be considered gold data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 392,
            "end": 483,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@4",
            "content": "Those spectrograms could differ in e.g. the speaker who reads the text and how they read the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 485,
            "end": 582,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@5",
            "content": "Since there are no conditioning signals, the TTS has to update its parameters towards a certain speaker's characteristics in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 584,
            "end": 716,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@6",
            "content": "However because in our case each task is a different language and a different speaker, the training becomes highly unstable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 718,
            "end": 841,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_28@7",
            "content": "So ideally we would either need to run MAML's inner loop until convergence, which is generally infeasible, or stabilize the procedure by not allowing the model to adapt further to one task than to the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_28",
            "start": 843,
            "end": 1050,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@0",
            "content": "To fix this issue, we calculate the Meta Model's loss on one batch per language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@1",
            "content": "We then sum up the losses, backpropagate and update the Meta Model directly using Adam (Kingma and Ba, 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 81,
            "end": 189,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@2",
            "content": "This stabilizes the learning procedure, but still allows the model to update its parameters towards a more universal configuration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 191,
            "end": 321,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@3",
            "content": "Since we have to make this simplification to MAML in order to deal with the different languages as tasks, we call this procedure language agnostic meta learning (LAML).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 323,
            "end": 490,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@4",
            "content": "Ultimately, the model should not care about the language it is fine-tuned in, since it should be close to a universal representation of an acoustic model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 492,
            "end": 645,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_29@5",
            "content": "To give an exact notion of our modifications: We simplified equation 1 to equation 2, where opt is a gradient descent update, B i is a batch sampled from task i, L is an objective function, \u0398 is the set of parameters from the Meta Model and \u03b8 i is the set of parameters specific to task i. To the best of our knowledge, we are the first to successfully apply MAML to TTS with languages being the tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_29",
            "start": 647,
            "end": 1048,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_30@0",
            "content": "for t steps do:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_30",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_31@0",
            "content": "\u0398 t = opt \u0398 t\u22121 , \u2207 i L (\u03b8 i,d , B i )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_31",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_32@0",
            "content": "where \u03b8 i,d=0 = \u0398 t\u22121 and for d steps do:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_32",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_33@0",
            "content": "\u03b8 i,d = opt (\u03b8 i,d\u22121 , \u2207L (\u03b8 i,d\u22121 , B i ))(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_33",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_34@0",
            "content": "for t steps do:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_34",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_35@0",
            "content": "\u0398 t = opt \u0398 t\u22121 , \u2207 i L (\u0398 t\u22121 , B i )(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_35",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_36@0",
            "content": "4 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_36",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_37@0",
            "content": "In this section we will go over the experiments we conducted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_37",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_37@1",
            "content": "First we will evaluate the articulatory features on their own in a single language setting using automatic measures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_37",
            "start": 62,
            "end": 177,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_37@2",
            "content": "Then we will evaluate the combination of LAML and articulatory features in a cross-lingual setting using both automatic measures and human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_37",
            "start": 179,
            "end": 328,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_38@0",
            "content": "In our experiments we make use of the following datasets: The English Nancy Krebs dataset (16h) from the Blizzard challenge 2011 (Wilhelms-Tricarico et al., 2011;King and Karaiskos, 2011); The German dataset of the speaker Karlsson (29h) from the HUI-Audio-Corpus-German (Puchtler et al., 2021); The Greek (4h), Spanish (24h), Finnish (11h), Russian (21h), Hungarian (10h), Dutch (14h) and French (19h) subsets of the CSS10 dataset (Park and Mulc, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_38",
            "start": 0,
            "end": 453,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_39@0",
            "content": "Mono-Lingual Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_39",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_40@0",
            "content": "Embedding Function Design",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_40",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_41@0",
            "content": "To explore our first hypothesis, we investigate the capabilities of the articulatory phoneme representations to be used in a single-speaker and singlelanguage TTS system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_41",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_41@1",
            "content": "To compare different ways of embedding the features, we train only the embedding function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_41",
            "start": 171,
            "end": 260,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_41@2",
            "content": "As gold data we use the embeddings from a well trained lookup-table based Tacotron 2 model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_41",
            "start": 262,
            "end": 352,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_41@3",
            "content": "In table 1 we show the average distances of all articulatory vectors as projected by the embedding function to their identity based embedding counterpart.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_41",
            "start": 354,
            "end": 507,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_41@4",
            "content": "The distance d between two embedding vectors A and B is defined in equation 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_41",
            "start": 509,
            "end": 586,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_42@0",
            "content": "d = i |A i \u2212 B i | \u2212 i A i \u2022 B i i A 2 i \u2022 i B 2 i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_42",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_43@0",
            "content": "(3) This distance function is also used as the objective function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_43",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_43@1",
            "content": "The embedding functions are each trained for 3000 epochs using Adam (Kingma and Ba, 2015) with a batchsize of 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_43",
            "start": 67,
            "end": 179,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_43@2",
            "content": "The first column shows the results of the articulatory features being fed into a linear layer that projects them into a 512 dimensional space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_43",
            "start": 181,
            "end": 322,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_43@3",
            "content": "The second column shows the results of the articulatory features being fed into a linear layer that projects them into a 100 dimensional space, applies the tanh activation function and then further projects them into a 512 dimensional space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_43",
            "start": 324,
            "end": 564,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_43@4",
            "content": "As can be seen from the results, it is beneficial to both concatenate the PanPhon features with the Papercup features despite their overlap and to add a nonlinearity into the embedding function to match the embeddingspace of a well trained",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_43",
            "start": 566,
            "end": 804,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_44@0",
            "content": "Convergence Time",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_44",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_45@0",
            "content": "To investigate the impact that the articulatory features have on their own, we train a Tacotron 2 with and without them on the Nancy dataset and compare their training time and final quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_45",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_45@1",
            "content": "While the model trained on embedding tables shows a clear diagonal alignment of text and spectrogram frames on an unseen test sentence after 2,000 steps, the one trained on articulatory features does so already at 500 steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_45",
            "start": 192,
            "end": 415,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_45@2",
            "content": "This is visualized in figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_45",
            "start": 417,
            "end": 447,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_45@3",
            "content": "The decoder of the Tacotron 2 model can only start to learn to decode after the alignment of inputs to outputs is learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_45",
            "start": 449,
            "end": 570,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_45@4",
            "content": "So learning the alignment earlier gives the articulatory model a clear benefit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_45",
            "start": 572,
            "end": 650,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_46@0",
            "content": "After training for 80,000 steps however, our own subjective assessment finds no difference in quality between the two.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_46",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_46@1",
            "content": "The earlier convergence of the alignment however shows a possible advantage of using the articulatory features on low-resource tasks, as quicker training progress means that training can be stopped earlier, before overfitting on little data becomes too problematic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_46",
            "start": 119,
            "end": 383,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_47@0",
            "content": "Cross-Lingual Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_47",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_48@0",
            "content": "In order to investigate the effectiveness of our proposed LAML procedure, we train a Tacotron 2 model and a FastSpeech 2 model on the full Karlsson dataset as a strong baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_48",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_48@1",
            "content": "We also train another Tacotron 2 model and another FastSpeech 2 model on speech in 8 languages with one speaker per language (Nancy dataset and CSS10 dataset) and fine-tune those models on a randomly chosen 30 minute subset from the Karlsson dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_48",
            "start": 178,
            "end": 427,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_48@2",
            "content": "To our surprise, we did not only match, but even outperform the model trained on 29 hours with the model fine-tuned on just 30 minutes in multiple metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_48",
            "start": 429,
            "end": 583,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_49@0",
            "content": "As a second baseline we tried to train another meta-checkpoint using the embedding lookup-table approach to also further investigate the effectiveness of the articulatory features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_49",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_49@1",
            "content": "We did however not manage to get such a model to converge to a usable state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_49",
            "start": 181,
            "end": 256,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_49@2",
            "content": "This already shows the superiority of the articulatory feature representations for such a multilingual use-case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_49",
            "start": 258,
            "end": 369,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@0",
            "content": "Furthermore we tried to fine-tune the well trained English single speaker models from the first experiment on the 30 minutes of German to have another baseline that can be used to measure the impact of the LAML procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@1",
            "content": "This setup however also did not yield any usable results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 222,
            "end": 278,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@2",
            "content": "During the fine-tuning process, the model was capable of speaking German with a strong English accent, yet it did not properly learn to speak in the voice of the target speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 280,
            "end": 456,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@3",
            "content": "By the time the model learned to speak in the new speaker's voice, it had overfitted the 30 minutes of training data and collapsed, producing no more intelligible speech.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 458,
            "end": 627,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@4",
            "content": "We conclude that the method proposed in this paper not only improves on the ability to use cross-lingual data easily, but actually enables it in the first place.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 629,
            "end": 789,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_50@5",
            "content": "Both the articulatory features, as well as the LAML pretraining seem necessary to achieve cross-lingual fine-tuning on low-resource data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_50",
            "start": 791,
            "end": 927,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_51@0",
            "content": "The texts we use for the following experiments are disjunct from any training data used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_51",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_51@1",
            "content": "Human speech as gold standard is not used, since we are interested in the difference in performance between the systems, not their absolute performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_51",
            "start": 89,
            "end": 240,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_51@2",
            "content": "The close to state-of-the-art performance of the baselines is considered as given, considering their ideal training conditions and use of proven methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_51",
            "start": 242,
            "end": 394,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_51@3",
            "content": "Furthermore, we chose to use German as our benchmark language over an actual low-resource language, since it is much easier to acquire reliable ratings on intelligibility and naturalness for German, than it would be for an actual low-resource language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_51",
            "start": 396,
            "end": 647,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_52@0",
            "content": "Intelligibility",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_52",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_53@0",
            "content": "To compare intelligibility between our baseline models and our low-resource models, we use the word error rate (WER) of an automatic speech recognition system (ASR) as a proxy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_53",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_53@1",
            "content": "We synthesize 100 sentences of German radio news texts taken from the DIRNDL corpus (Eckart et al., 2012) with each of our baselines and corresponding low-resource systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_53",
            "start": 177,
            "end": 348,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_53@2",
            "content": "Table 2 shows WERs that the German IMS-Speech ASR (Denisov and Vu, 2019) achieves on the synthesized data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_53",
            "start": 350,
            "end": 455,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_53@3",
            "content": "For both Tacotron 2 and the FastSpeech 2 based system, the WER of the low-resource model is slightly lower than that of the baseline, thus the low-resource models performed slightly better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_53",
            "start": 457,
            "end": 645,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_54@0",
            "content": "Looking into the cases where the low-resource WER Baseline Low-Resource Tacotron 2 13.1% 12.7% FastSpeech 2 9.9% 9.7%",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_54",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_55@0",
            "content": "Table 2: WER of the synthesis systems on 100 radio news texts measured using the IMS-Speech ASR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_55",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_56@0",
            "content": "system outperformed the baseline, we find codeswitched segments, where the texts contain names of Russian cities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_56",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_56@1",
            "content": "Since the pretraining data of the low-resource model includes Russian speech, it seems to have not forgotten entirely about what it has seen in the pretraining phase, which in our interpretation confirms the effectiveness of the LAML against the catastrophic-forgetting problem (French, 1999) of regular pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_56",
            "start": 114,
            "end": 429,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_57@0",
            "content": "Naturalness",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_57",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_58@0",
            "content": "In order to assess the naturalness of the fine-tuned models, we conduct a preference study with 34 native speakers of German.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_58",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_58@1",
            "content": "Each participant is shown 12 phonetically balanced samples produced by the Tacotron 2 and FastSpeech 2 models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_58",
            "start": 126,
            "end": 235,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_58@2",
            "content": "For every sentence, there is one sample produced by the baseline and one by the low-resource model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_58",
            "start": 237,
            "end": 335,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@0",
            "content": "The participants are then asked to indicate their subjective overall preference between the two samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@1",
            "content": "The results for Tacotron 2 are shown in figure 3 (a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 105,
            "end": 157,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@2",
            "content": "The low-resource system was the preferred system in more than half of the cases, with an equal rating taking up more than another third, showing a clear preference for the low-resource model over the baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 159,
            "end": 367,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@3",
            "content": "The results for FastSpeech 2, as seen in figure 3 (b), are a lot more balanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 369,
            "end": 447,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@4",
            "content": "While the baseline is preferred more often than the lowresource variant, it is not the case in the majority of the ratings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 449,
            "end": 571,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_59@5",
            "content": "In 56% of the cases, the model finetuned on 30 minutes of data was perceived to be as good or better than the model trained on 29 hours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_59",
            "start": 573,
            "end": 708,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@0",
            "content": "Computational Resources All models were trained on a single NVIDIA A6000 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@1",
            "content": "Training the Tacotron Baseline took 2 days.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 78,
            "end": 120,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@2",
            "content": "Training time of the FastSpeech Baseline was 1 day.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 122,
            "end": 172,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@3",
            "content": "Training time of the meta-checkpoint was 4 days, finetuning to a new model from the meta-checkpoint however only takes 2 hours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 174,
            "end": 300,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@4",
            "content": "The HiFi-GAN vocoder used to generate all samples took 4 days to train and was not fine-tuned on the unseen data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 302,
            "end": 414,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_60@5",
            "content": "We did not perform hyperparameter searches and used the suggested default settings for all methods, which worked sufficiently well, but could surely be improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_60",
            "start": 416,
            "end": 576,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_61@0",
            "content": "Further Analysis and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_61",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_62@0",
            "content": "What is the ideal amount of training steps for fine-tuning? To investigate the amount of update steps needed to fully adapt to the new speaker with the added difficulty of learning a new language, we show the cosine similarity of a speaker embedding of the fine-tuned model to that of the ground truth throughout the fine-tuning process in figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_62",
            "start": 0,
            "end": 348,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_62@1",
            "content": "The speaker embedding is built according to the ECAPA-TDNN architecture (Desplanques et al., 2020) and provided open source by SpeechBrain (Ravanelli et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_62",
            "start": 350,
            "end": 513,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_62@2",
            "content": "It is trained on VoxCeleb 1 and 2 (Nagrani et al., 2017(Nagrani et al., , 2019Chung et al., 2018) which to the best of our knowledge does not overlap with any of the other training and evaluation data we used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_62",
            "start": 515,
            "end": 723,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_62@3",
            "content": "We tried to decrease adaptation time further by incorporating said speaker embedding similarity as an additional objective function, similar to Nachmani et al. (2018), we did however see only marginal improvements in the amount of steps needed at the expense of greatly increased training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_62",
            "start": 725,
            "end": 1018,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_63@0",
            "content": "Can this setup handle zero-shot phonemes?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_63",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@0",
            "content": "We show the model's zero shot capabilities in figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@1",
            "content": "We removed Dutch and Finnish from the training data of the meta-checkpoint and trained another version of it, to be able to see how it handles all of the now completely unseen phonemes specific to German.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 56,
            "end": 259,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@2",
            "content": "While their correct position in plot (a) can be considered given, since it shows the articulatory featurespace, their meaningful positions in plot (b) and (c) show that the meta-checkpoint does not just collapse the vector of the unseen phoneme to the one it is most similar to, but actually generalizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 261,
            "end": 564,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@3",
            "content": "While their pronunciation when produced does not match the correct pronunciation perfectly, it can be understood in the context of a longer sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 566,
            "end": 714,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@4",
            "content": "This is congruent with the results of Staib et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 716,
            "end": 773,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@5",
            "content": "During the adaptation phase, the pronunciation of the unseen phonemes rapidly matches the correct pronunciation after less than 100 steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 775,
            "end": 912,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@6",
            "content": "Does this setup learn the difference between language and speaker? When analyzing the finetuned meta-checkpoint, we observed that it seems to link the language of the input to the voice of the speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 914,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@7",
            "content": "For example when synthesizing an unseen Hungarian text using Tacotron 2, the voice of the synthesis resembles that of the Hungarian female speaker, even though the model has been fine-tuned on the male German speaker and there are no additional conditioning signals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 1116,
            "end": 1381,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@8",
            "content": "We hypothesize that the LAML procedure induces certain subsets of parameters in the model to be speaker dependent and the encoder of the model priming those parameters purely based on the phoneme sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 1383,
            "end": 1587,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@9",
            "content": "This leads us to believe, that the fine-tuning of all parameters in the model may neither be necessary, nor even the best way of adapting to new data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 1589,
            "end": 1738,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@10",
            "content": "This also fits the observations of the speaker embedding over time, since the Tacotron model adapts to the new speaker very rapidly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 1740,
            "end": 1871,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_64@11",
            "content": "Further investigations into the interactions between parameter groups could allow cutting down the amount of parameters that need to be trained significantly, further reducing the need for training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_64",
            "start": 1873,
            "end": 2075,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_65@0",
            "content": "How can we bring down FastSpeech 2's data need further?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_65",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_65@1",
            "content": "A similar observation regarding language and speaker can be made with FastSpeech 2, however as could be seen from the experiment on naturalness and the training time, the FastSpeech 2 model can benefit more from additional data and training time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_65",
            "start": 56,
            "end": 301,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_65@2",
            "content": "This may come down to its nearly twice as high parameter count.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_65",
            "start": 303,
            "end": 365,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_65@3",
            "content": "So a more effective fine-tuning strategy, that considers some parameters as constants, could benefit the fine-tuning capabilities of the FastSpeech 2 model greatly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_65",
            "start": 367,
            "end": 530,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_66@0",
            "content": "Does this work across language families?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_66",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_66@1",
            "content": "One limitation to our findings is that we investigated only the transfer of languages that share similar phoneme inventories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_66",
            "start": 41,
            "end": 165,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_66@2",
            "content": "It is possible that fine-tuning to a language that uses e.g. the lexical tone rather than pitch accents or word accents would require pretraining in more closely related high-resource languages, such as Chinese.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_66",
            "start": 167,
            "end": 377,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_66@3",
            "content": "However, as Vu and Schultz (2013) find in their analysis of multilingual ASR, the fast adaptation of an acoustic model trained on multiple languages to unseen languages works well, even across different language families.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_66",
            "start": 379,
            "end": 599,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_66@4",
            "content": "We thus believe that the technique and analysis presented in this paper also holds across language families and types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_66",
            "start": 601,
            "end": 718,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_67@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_67",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_68@0",
            "content": "In this paper, we show an approach for training a model in a language for which only 30 minutes of data are available by making use of articulatory features and language agnostic meta learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_68",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_68@1",
            "content": "The main takeaways from our work are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_68",
            "start": 194,
            "end": 241,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_69@0",
            "content": "Articulatory Features for TTS Using articulatory features as the input representation to a TTS system enables the use of multilingual data without the need for increased architectural complexity, such as language specific projection spaces.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_69",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_69@1",
            "content": "It is furthermore beneficial to use even in singlelanguage scenarios, since the knowledge sharing between phonemes makes the TTS system converge much earlier to an usable state during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_69",
            "start": 241,
            "end": 433,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_70@0",
            "content": "MAML on TTS Applying MAML to TTS does not work well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_70",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_70@1",
            "content": "If we however remove the inner loop, we are able to pretrain a low-resource capable checkpoint for TTS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_70",
            "start": 53,
            "end": 155,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_70@2",
            "content": "This modification not only makes it work, it also simplifies the formulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_70",
            "start": 157,
            "end": 233,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_71@0",
            "content": "Zero-shot capabilities The use of articulatory features enables zero-shot inference on unseen phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_71",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_71@1",
            "content": "This is further enhanced by the LAML training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_71",
            "start": 104,
            "end": 159,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_71@2",
            "content": "The implications of this are particularly interesting for codeswitching, as Staib et al. (2020); Wells et al. (2021) have pointed out previously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_71",
            "start": 161,
            "end": 305,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_71@3",
            "content": "Using these two techniques in conjunction could be used to reduce the problem of codeswitching to a problem of token-wise language identification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_71",
            "start": 307,
            "end": 452,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_72@0",
            "content": "Antreas Antoniou, Harri Edwards, Amos Storkey, How to train your MAML, 2019, Seventh International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_72",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_73@0",
            "content": "O Sercan, Mike Ar\u0131k, Adam Chrzanowski, Gregory Coates, Andrew Diamos, Yongguo Gibiansky, Xian Kang, John Li, Andrew Miller, Jonathan Ng, Shubho Raiman, Mohammad Sengupta,  Shoeybi, Deep Voice: Real-time Neural Text-to-Speech, 2017, Proceedings of the 34th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_73",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_74@0",
            "content": "Kurniawati Azizah, Mirna Adriani, Wisnu Jatmiko, Hierarchical Transfer Learning for Multilingual, Multi-Speaker, and Style Transfer DNN-Based TTS on Low-Resource Languages, 2020, IEEE Access, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_74",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_75@0",
            "content": "UNKNOWN, None, 2021, One TTS alignment to rule them all, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_75",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_76@0",
            "content": "Maximilian Bisani, Hermann Ney, Jointsequence models for grapheme-to-phoneme conversion, 2008, Speech communication, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_76",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_77@0",
            "content": "Yuan-Jui Chen, Tao Tu, Cheng-Chieh Yeh, Hung-Yi Lee, End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning, 2019, Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_77",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_78@0",
            "content": "UNKNOWN, None, 2018, Voxceleb2: Deep speaker recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_78",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_79@0",
            "content": "A Robert, Korin Clark, Simon Richmond,  King, Multisyn: Open-domain unit selection for the Festival speech synthesis system, 2007, Speech Communication, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_79",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_80@0",
            "content": "Marcel De Korte, Esther Klabbers, Efficient Neural Speech Synthesis for Low-Resource Languages Through Multilingual Modeling, 2020, Proc. Interspeech 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_80",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_81@0",
            "content": "UNKNOWN, None, 2019, IMS-speech: A speech to text tool. Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_81",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_82@0",
            "content": "Aliya Deri, Kevin Knight, Grapheme-tophoneme models for (almost) any language, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_82",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_83@0",
            "content": "Brecht Desplanques, Jenthe Thienpondt, Kris Demuynck, ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification, 2020, Interspeech 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_83",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_84@0",
            "content": "Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, Karen Simonyan, End-toend Adversarial Text-to-Speech, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_84",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_85@0",
            "content": "Peter Ebden, Richard Sproat, The Kestrel TTS text normalization system, 2015, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_85",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_86@0",
            "content": "Kerstin Eckart, Arndt Riester, Katrin Schweitzer, A discourse information radio news database for linguistic analysis, 2012, Linked Data in Linguistics, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_86",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_87@0",
            "content": "Chelsea Finn, Pieter Abbeel, Sergey Levine, Model-agnostic meta-learning for fast adaptation of deep networks, 2017, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_87",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_88@0",
            "content": "M Robert,  French, Catastrophic forgetting in connectionist networks, 1999, Trends in cognitive sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_88",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_89@0",
            "content": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial nets, 2014, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_89",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_90@0",
            "content": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber, Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks, 2006, Proceedings of the 23rd international conference on Machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_90",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_91@0",
            "content": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Conformer: Convolution-augmented Transformer for Speech Recognition, 2020, Proc. Interspeech 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_91",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_92@0",
            "content": "Alexander Gutkin, Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages, 2017, Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_92",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_93@0",
            "content": "Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, Kazuya Takeda, Shubham Toshniwal, Karen Livescu, Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis, 2019, INTER-SPEECH, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_93",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_94@0",
            "content": "Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, Takenori Yoshimura, Shinji Watanabe, Tomoki Toda, Kazuya Takeda, Yu Zhang, Xu Tan, ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit, 2020, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_94",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_95@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_95",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_96@0",
            "content": "UNKNOWN, None, 2021, ESPnet2-TTS: Extending the Edge of TTS Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_96",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_97@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_97",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_98@0",
            "content": "UNKNOWN, None, 1961, Preliminaries to Speech Analysis: The Distinctive Features and Their Correlates, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_98",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_99@0",
            "content": "Jaehyeon Kim, Sungwon Kim, Jungil Kong, Sungroh Yoon, Glow-tts: A generative flow for text-to-speech via monotonic alignment search, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_99",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_100@0",
            "content": "Jaehyeon Kim, Jungil Kong, Juhee Son, Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech, 2021, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_100",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_101@0",
            "content": "Simon King, Vasilis Karaiskos, The Blizzard Challenge, 2011, Proc. Blizzard Challenge Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_101",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_102@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A Method for Stochastic Optimization, 2015, ICLR (Poster), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_102",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_103@0",
            "content": "Jungil Kong, Jaehyeon Kim, Jaekyoung Bae, HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_103",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_104@0",
            "content": "Kundan Kumar, Rithesh Kumar, Lucas Thibault De Boissiere, Wei Gestin, Jose Teoh, Alexandre Sotelo, Yoshua De Br\u00e9bisson, Aaron C Bengio,  Courville, MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_104",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_105@0",
            "content": "Adrian \u0141a\u0144cucki, FastPitch: Parallel text-tospeech with pitch prediction, 2021, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_105",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_106@0",
            "content": "Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan, Bytes are all you need: Endto-end multilingual speech recognition and synthesis with bytes, 2019, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_106",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_107@0",
            "content": "Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, Neural speech synthesis with transformer network, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_107",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_108@0",
            "content": "Florian Lux, Julia Koch, Antje Schweitzer, Ngoc Vu, The IMS Toucan system for the Blizzard Challenge 2021, 2021, Proc. Blizzard Challenge Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_108",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_109@0",
            "content": "David Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, Lori Levin, PanPhon: A Resource for Mapping IPA Segments to Articulatory Feature Vectors, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_109",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_110@0",
            "content": "Eliya Nachmani, Adam Polyak, Yaniv Taigman, Lior Wolf, Fitting new speakers based on a short untranscribed sample, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_110",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_111@0",
            "content": "UNKNOWN, None, 2017, Voxceleb: a large-scale speaker identification dataset, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_111",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_112@0",
            "content": "Arsha Nagrani, Joon Chung, Weidi Xie, Andrew Zisserman, Voxceleb: Large-scale speaker verification in the wild, 2019, Computer Science and Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_112",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_113@0",
            "content": "UNKNOWN, None, 2021, Adapting TTS models For New Speakers using Transfer Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_113",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_114@0",
            "content": "Kyubyong Park, Thomas Mulc, CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages, 2019, Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_114",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_115@0",
            "content": "Ryan Prenger, Rafael Valle, Bryan Catanzaro, WaveGlow: A flow-based generative network for speech synthesis, 2019, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_115",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_116@0",
            "content": "Pascal Puchtler, Johannes Wirth, Ren\u00e9 Peinl, Hui-audio-corpus-german: A high quality tts dataset, 2021, German Conference on Artificial Intelligence (K\u00fcnstliche Intelligenz), Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_116",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_117@0",
            "content": "Aravind Rajeswaran, Chelsea Finn, M Sham, Sergey Kakade,  Levine, Meta-learning with implicit gradients, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_117",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_118@0",
            "content": "Kanishka Rao, Fuchun Peng, Ha\u015fim Sak, Fran\u00e7oise Beaufays, Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks, 2015, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_118",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_119",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_120@0",
            "content": "Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu, FastSpeech 2: Fast and High-Quality End-to-End Text to Speech, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_120",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_121@0",
            "content": "Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu, FastSpeech: fast, robust and controllable text to speech, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_121",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_122@0",
            "content": "Marc Schr\u00f6der, J\u00fcrgen Trouvain, The German text-to-speech synthesis system MARY: A tool for research, development and teaching, 2003, International Journal of Speech Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_122",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_123@0",
            "content": "UNKNOWN, None, 2018, Natural TTS Synthesis by Conditioning WaveNet on mel Spectrogram Predictions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_123",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_124@0",
            "content": "J Kevin, Rafael Shih, Rohan Valle, Adrian Badlani, Wei Lancucki, Bryan Ping,  Catanzaro, RAD-TTS: Parallel Flow-Based TTS with Robust Alignment Learning and Diverse Synthesis, 2021, ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_124",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_125@0",
            "content": "UNKNOWN, None, 2020, 2020. Phonological Features for 0-Shot Multilingual Speech Synthesis, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_125",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_126@0",
            "content": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara, Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_126",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_127@0",
            "content": "Jason Taylor, Korin Richmond, Enhancing Sequence-to-Sequence Text-to-Speech with Morphology, 2020, INTERSPEECH, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_127",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_128@0",
            "content": "Paul Taylor, Hidden Markov models for grapheme to phoneme conversion, 2005, Ninth European Conference on Speech Communication and Technology, Citeseer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_128",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_129@0",
            "content": "A\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, WaveNet: A Generative Model for Raw Audio, 2016, 9th ISCA Speech Synthesis Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_129",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_130@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_130",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_131@0",
            "content": "Ngoc Vu, Tanja Schultz, Multilingual multilayer perceptron for rapid language adaptation between and across language families, 2013, Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_131",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_132@0",
            "content": "UNKNOWN, None, 2017, Tacotron: Towards End-to-End Speech Synthesis. Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_132",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_133@0",
            "content": "Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson , Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, Tsubasa Ochiai, ESPnet: End-to-end speech processing toolkit, 2018, Proceedings of Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_133",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_134@0",
            "content": "Dan Wells, Pilar Oplustil-Gallegos, Simon King, The CSTR entry to the Blizzard Challenge 2021, 2021, Proc. Blizzard Challenge Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_134",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_135@0",
            "content": "Reiner Wilhelms-Tricarico, Brian Mottershead, Rattima Nitisaroj, Michael Baumgartner, John Reichenbach, Gary Marple, The lessac technologies system for blizzard challenge, 2011, Blizzard Challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_135",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_136@0",
            "content": "UNKNOWN, None, 2020, LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_136",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_137@0",
            "content": "Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim, Parallel waveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram, 2020, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_137",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_138@0",
            "content": "Jingzhou Yang, Lei He, Towards Universal Text-to-Speech, 2020, INTERSPEECH, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_138",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "203-ARR_v2_139@0",
            "content": "Jing-Xuan Zhang, Zhen-Hua Ling, Li-Rong Dai, Forward attention in sequence-to-sequence acoustic modeling for speech synthesis, 2018, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "203-ARR_v2_139",
            "start": 0,
            "end": 225,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_1",
            "tgt_ix": "203-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_1",
            "tgt_ix": "203-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_2",
            "tgt_ix": "203-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_4",
            "tgt_ix": "203-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_7",
            "tgt_ix": "203-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_8",
            "tgt_ix": "203-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_11",
            "tgt_ix": "203-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_13",
            "tgt_ix": "203-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_15",
            "tgt_ix": "203-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_15",
            "tgt_ix": "203-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_15",
            "tgt_ix": "203-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_17",
            "tgt_ix": "203-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_18",
            "tgt_ix": "203-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_18",
            "tgt_ix": "203-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_21",
            "tgt_ix": "203-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_18",
            "tgt_ix": "203-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_23",
            "tgt_ix": "203-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_25",
            "tgt_ix": "203-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_24",
            "tgt_ix": "203-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_24",
            "tgt_ix": "203-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_24",
            "tgt_ix": "203-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_18",
            "tgt_ix": "203-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_26",
            "tgt_ix": "203-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_30",
            "tgt_ix": "203-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_31",
            "tgt_ix": "203-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_32",
            "tgt_ix": "203-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_33",
            "tgt_ix": "203-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_34",
            "tgt_ix": "203-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_35",
            "tgt_ix": "203-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_36",
            "tgt_ix": "203-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_37",
            "tgt_ix": "203-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_38",
            "tgt_ix": "203-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_39",
            "tgt_ix": "203-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_39",
            "tgt_ix": "203-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_42",
            "tgt_ix": "203-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_40",
            "tgt_ix": "203-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_40",
            "tgt_ix": "203-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_40",
            "tgt_ix": "203-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_40",
            "tgt_ix": "203-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_39",
            "tgt_ix": "203-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_44",
            "tgt_ix": "203-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_44",
            "tgt_ix": "203-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_44",
            "tgt_ix": "203-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_46",
            "tgt_ix": "203-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_48",
            "tgt_ix": "203-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_49",
            "tgt_ix": "203-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_51",
            "tgt_ix": "203-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_53",
            "tgt_ix": "203-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_54",
            "tgt_ix": "203-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_55",
            "tgt_ix": "203-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_56",
            "tgt_ix": "203-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_58",
            "tgt_ix": "203-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_57",
            "tgt_ix": "203-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_57",
            "tgt_ix": "203-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_57",
            "tgt_ix": "203-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_57",
            "tgt_ix": "203-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_62",
            "tgt_ix": "203-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_63",
            "tgt_ix": "203-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_65",
            "tgt_ix": "203-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_68",
            "tgt_ix": "203-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_69",
            "tgt_ix": "203-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_70",
            "tgt_ix": "203-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "203-ARR_v2_0",
            "tgt_ix": "203-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_1",
            "tgt_ix": "203-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_2",
            "tgt_ix": "203-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_2",
            "tgt_ix": "203-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_2",
            "tgt_ix": "203-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_3",
            "tgt_ix": "203-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_4",
            "tgt_ix": "203-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_4",
            "tgt_ix": "203-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_4",
            "tgt_ix": "203-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_4",
            "tgt_ix": "203-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_5",
            "tgt_ix": "203-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_6",
            "tgt_ix": "203-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_7",
            "tgt_ix": "203-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_7",
            "tgt_ix": "203-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_7",
            "tgt_ix": "203-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_7",
            "tgt_ix": "203-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_8",
            "tgt_ix": "203-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_8",
            "tgt_ix": "203-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_9",
            "tgt_ix": "203-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_10",
            "tgt_ix": "203-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_11",
            "tgt_ix": "203-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_12",
            "tgt_ix": "203-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_13",
            "tgt_ix": "203-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_13",
            "tgt_ix": "203-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_14",
            "tgt_ix": "203-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_15",
            "tgt_ix": "203-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_16",
            "tgt_ix": "203-ARR_v2_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_17",
            "tgt_ix": "203-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_17",
            "tgt_ix": "203-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_17",
            "tgt_ix": "203-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_17",
            "tgt_ix": "203-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_18",
            "tgt_ix": "203-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_19",
            "tgt_ix": "203-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_20",
            "tgt_ix": "203-ARR_v2_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_21",
            "tgt_ix": "203-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_21",
            "tgt_ix": "203-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_22",
            "tgt_ix": "203-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_23",
            "tgt_ix": "203-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_24",
            "tgt_ix": "203-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_25",
            "tgt_ix": "203-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_25",
            "tgt_ix": "203-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_25",
            "tgt_ix": "203-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_25",
            "tgt_ix": "203-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_26",
            "tgt_ix": "203-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_26",
            "tgt_ix": "203-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_26",
            "tgt_ix": "203-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_26",
            "tgt_ix": "203-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_27",
            "tgt_ix": "203-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_28",
            "tgt_ix": "203-ARR_v2_28@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_29",
            "tgt_ix": "203-ARR_v2_29@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_30",
            "tgt_ix": "203-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_31",
            "tgt_ix": "203-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_32",
            "tgt_ix": "203-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_33",
            "tgt_ix": "203-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_34",
            "tgt_ix": "203-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_35",
            "tgt_ix": "203-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_36",
            "tgt_ix": "203-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_37",
            "tgt_ix": "203-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_37",
            "tgt_ix": "203-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_37",
            "tgt_ix": "203-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_38",
            "tgt_ix": "203-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_39",
            "tgt_ix": "203-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_40",
            "tgt_ix": "203-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_41",
            "tgt_ix": "203-ARR_v2_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_42",
            "tgt_ix": "203-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_43",
            "tgt_ix": "203-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_44",
            "tgt_ix": "203-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_45",
            "tgt_ix": "203-ARR_v2_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_46",
            "tgt_ix": "203-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_46",
            "tgt_ix": "203-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_47",
            "tgt_ix": "203-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_48",
            "tgt_ix": "203-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_48",
            "tgt_ix": "203-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_48",
            "tgt_ix": "203-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_49",
            "tgt_ix": "203-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_49",
            "tgt_ix": "203-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_49",
            "tgt_ix": "203-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_50",
            "tgt_ix": "203-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_51",
            "tgt_ix": "203-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_51",
            "tgt_ix": "203-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_51",
            "tgt_ix": "203-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_51",
            "tgt_ix": "203-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_52",
            "tgt_ix": "203-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_53",
            "tgt_ix": "203-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_53",
            "tgt_ix": "203-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_53",
            "tgt_ix": "203-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_53",
            "tgt_ix": "203-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_54",
            "tgt_ix": "203-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_55",
            "tgt_ix": "203-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_56",
            "tgt_ix": "203-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_56",
            "tgt_ix": "203-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_57",
            "tgt_ix": "203-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_58",
            "tgt_ix": "203-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_58",
            "tgt_ix": "203-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_58",
            "tgt_ix": "203-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_59",
            "tgt_ix": "203-ARR_v2_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_60",
            "tgt_ix": "203-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_61",
            "tgt_ix": "203-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_62",
            "tgt_ix": "203-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_62",
            "tgt_ix": "203-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_62",
            "tgt_ix": "203-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_62",
            "tgt_ix": "203-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_63",
            "tgt_ix": "203-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_64",
            "tgt_ix": "203-ARR_v2_64@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_65",
            "tgt_ix": "203-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_65",
            "tgt_ix": "203-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_65",
            "tgt_ix": "203-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_65",
            "tgt_ix": "203-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_66",
            "tgt_ix": "203-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_67",
            "tgt_ix": "203-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_68",
            "tgt_ix": "203-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_68",
            "tgt_ix": "203-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_69",
            "tgt_ix": "203-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_69",
            "tgt_ix": "203-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_70",
            "tgt_ix": "203-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_70",
            "tgt_ix": "203-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_70",
            "tgt_ix": "203-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_71",
            "tgt_ix": "203-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_71",
            "tgt_ix": "203-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_71",
            "tgt_ix": "203-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_71",
            "tgt_ix": "203-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_72",
            "tgt_ix": "203-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_73",
            "tgt_ix": "203-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_74",
            "tgt_ix": "203-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_75",
            "tgt_ix": "203-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_76",
            "tgt_ix": "203-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_77",
            "tgt_ix": "203-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_78",
            "tgt_ix": "203-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_79",
            "tgt_ix": "203-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_80",
            "tgt_ix": "203-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_81",
            "tgt_ix": "203-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_82",
            "tgt_ix": "203-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_83",
            "tgt_ix": "203-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_84",
            "tgt_ix": "203-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_85",
            "tgt_ix": "203-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_86",
            "tgt_ix": "203-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_87",
            "tgt_ix": "203-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_88",
            "tgt_ix": "203-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_89",
            "tgt_ix": "203-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_90",
            "tgt_ix": "203-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_91",
            "tgt_ix": "203-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_92",
            "tgt_ix": "203-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_93",
            "tgt_ix": "203-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_94",
            "tgt_ix": "203-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_95",
            "tgt_ix": "203-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_96",
            "tgt_ix": "203-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_97",
            "tgt_ix": "203-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_98",
            "tgt_ix": "203-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_99",
            "tgt_ix": "203-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_100",
            "tgt_ix": "203-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_101",
            "tgt_ix": "203-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_102",
            "tgt_ix": "203-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_103",
            "tgt_ix": "203-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_104",
            "tgt_ix": "203-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_105",
            "tgt_ix": "203-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_106",
            "tgt_ix": "203-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_107",
            "tgt_ix": "203-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_108",
            "tgt_ix": "203-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_109",
            "tgt_ix": "203-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_110",
            "tgt_ix": "203-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_111",
            "tgt_ix": "203-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_112",
            "tgt_ix": "203-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_113",
            "tgt_ix": "203-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_114",
            "tgt_ix": "203-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_115",
            "tgt_ix": "203-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_116",
            "tgt_ix": "203-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_117",
            "tgt_ix": "203-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_118",
            "tgt_ix": "203-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_119",
            "tgt_ix": "203-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_120",
            "tgt_ix": "203-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_121",
            "tgt_ix": "203-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_122",
            "tgt_ix": "203-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_123",
            "tgt_ix": "203-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_124",
            "tgt_ix": "203-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_125",
            "tgt_ix": "203-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_126",
            "tgt_ix": "203-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_127",
            "tgt_ix": "203-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_128",
            "tgt_ix": "203-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_129",
            "tgt_ix": "203-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_130",
            "tgt_ix": "203-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_131",
            "tgt_ix": "203-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_132",
            "tgt_ix": "203-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_133",
            "tgt_ix": "203-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_134",
            "tgt_ix": "203-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_135",
            "tgt_ix": "203-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_136",
            "tgt_ix": "203-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_137",
            "tgt_ix": "203-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_138",
            "tgt_ix": "203-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "203-ARR_v2_139",
            "tgt_ix": "203-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 873,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "203-ARR",
        "version": 2
    }
}