{
    "nodes": [
        {
            "ix": "383-ARR_v1_0",
            "content": "Jam or Cream First? 1 Modeling Ambiguity in Neural Machine Translation with SCONES",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_2",
            "content": "The softmax layer in neural machine translation is designed to model the distribution over mutually exclusive tokens. Machine translation, however, is intrinsically uncertain: the same source sentence can have multiple semantically equivalent translations. Therefore, we propose to replace the softmax activation with a multi-label classification layer that can model ambiguity more effectively. We call our loss function Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label output layer can still be trained on single reference training data using the SCONES loss function. SCONES yields consistent BLEU score gains across six translation directions, particularly for mediumresource language pairs and small beam sizes. By using smaller beam sizes we can speed up inference by a factor of 3.9x and still match or improve the softmax BLEU score. Furthermore, we demonstrate that SCONES can be used to train NMT models that assign the highest probability to adequate translations, thus mitigating the \"beam search curse\". Additional experiments on synthetic language pairs with varying levels of uncertainty suggest that the improvements from SCONES can be attributed to better handling of ambiguity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "383-ARR_v1_4",
            "content": "Conventional neural machine translation (NMT) models learn the probability P (y|x) of the target sentence y given the source sentence x ( Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014). This framework implies that there is a single best translation for a given source sentence: if there were multiple valid translations y 1 and y 2 they would need to share probability mass (e.g. P (y 1 |x) = 0.5 and P (y 2 |x) = 0.5), but such a distribution could also represent model uncertainty, i.e. the case when either y 1 or y 2 are correct translations. Therefore, learning a single distribution over all target language sentences does not allow the model to naturally express intrinsic uncertainty 2 (Pad\u00f3 et al., 2009;Dreyer and Marcu, 2012;Ott et al., 2018), the nature of the translation task to allow multiple semantically equivalent translations for a given source sentence. Single distributions over all sequences represent uncertainty by assigning probabilities, but they cannot distinguish between different kinds of uncertainty (e.g. model uncertainty versus intrinsic uncertainty).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_5",
            "content": "Therefore, in this work we frame machine translation as a multi-label classification task (Tsoumakas and Katakis, 2007;Zhang and Zhou, 2014). Rather than learning a single distribution P (y|x) over all target sentences y for a source sentence x, we learn binary classifiers for each sentence pair (x, y) that indicate whether or not y is a valid translation of x. In this framework, intrinsic uncertainty can be represented by setting the probabilities of two (or more) correct translations y 1 and y 2 to 1 simultaneously. The probabilities for each translation are computed using separate binary classifiers, and thus there is no requirement that the probabilities sum to one over all translations. In practice, the probability of a complete translation is decomposed into a product of the token-level probabilities. Thus we replace the softmax output layer in Transformer models (Vaswani et al., 2017) with sigmoid activations that assign a probability between 0 and 1 to each token in the vocabulary at each time step. We propose a loss function, Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES) that allows us to train our models on single reference training data. Our work is inspired by noise-contrastive estimation (NCE) Mnih and Teh, 2012). Unlike NCE, whose primary goal was to efficiently train models over large vo-cabularies, our motivation for SCONES is to model non-exclusive outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_6",
            "content": "We demonstrate multiple benefits of training NMT models using SCONES when compared to standard cross-entropy with regular softmax. We report consistent BLEU score gains between 1%-9% across six different translation directions. SCONES with greedy search typically outperforms softmax with beam search, resulting in inference speed-ups of up to 3.9x compared to softmax without any degradation in BLEU score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_7",
            "content": "SCONES mitigates some of the pathologies of traditional NMT models. Softmax-based models have been shown to assign the highest probability to either empty or inadequate translations (modes) (Stahlberg and Byrne, 2019;Eikema and Aziz, 2020). This behavior manifests itself as the \"beam search curse\" (Koehn and Knowles, 2017): increasing the beam size may lead to worse translation quality. We show that SCONES can be used to train models that a) assign the highest probability to adequate translations and b) do not suffer from the beam search curse.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_8",
            "content": "Finally, we use SCONES to train models on synthetic translation pairs that we generate by sampling from the IBM Model 3 (Brown et al., 1993). By varying the sampling temperature, we control the level of ambiguity in the language pair. We show that SCONES is effective in improving the adequacy of the highest probability translation for highly ambiguous translation pairs, confirming our intuition that SCONES can handle intrinsic uncertainty well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_9",
            "content": "Training NMT models with SCONES",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "383-ARR_v1_10",
            "content": "We denote the (subword) vocabulary as V = {w 1 , . . . , w |V| }, the special end-of-sentence symbol as w 1 = </s>, the source sentence as x = x 1 , . . . , x |x| \u2208 V * , a translation as y = y 1 , . . . , y |y| \u2208 V * , and a translation prefix as y \u2264i = y 1 , . . . , y i . We use a center dot \"\u2022\" for string concatenations. Unlike conventional NMT that models a single distribution P (y|x) over all target language sentences, SCONES learns a separate binary classifier for each sentence pair (x, y). We define a Boolean function t(\u2022, \u2022) that indicates whether y is a valid translation of x: t(x, y) := true if y is a translation of x false otherwise .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_11",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_12",
            "content": "We do not model t(\u2022, \u2022) directly. To guide decoding, we learn variables z x,y which generalize t(\u2022, \u2022) to translation prefixes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_13",
            "content": "z x,y := 1 \u2203y \u2208 V * : t(x, y \u2022 y ) = true 0 otherwise ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_14",
            "content": "(2) i.e. z x,y is a binary label for the pair (x, y) consisting of source sentence x and the translation prefix y: z x,y = 1 iff. y is a prefix of a valid translation of x. We decompose its probability as a product of conditionals to facilitate left-to-right beam decoding: 3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_15",
            "content": "P (z x,y = 1|x) = |y| i=1 P (z x,y \u2264i = 1|z x,y <i = 1, x) = |y| i=1 P (z x,y \u2264i = 1|x, y <i ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_16",
            "content": "(3) We assign the conditional probabilities by applying the sigmoid activation function \u03c3(\u2022) to the logits:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_17",
            "content": "P (z x,y <i \u2022w = 1|x, y <i ) = \u03c3(f (x, y <i ) w ), (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_18",
            "content": "where w \u2208 V is a single token, f (x, y <i ) \u2208 R |V| are the logits at time step i, and f (x, y <i ) w is the logit corresponding to token w. The only architectural difference to a standard NMT model is the output activation: instead of the softmax function that yields a single distribution over the full vocabulary, we use multiple sigmoid activations in each logit component to define separate Bernoulli distributions for each item in the vocabulary (Fig. 1). However, using such a multi-label classification view requires a different training loss function because, unlike the logits from a softmax, the logits in Eq. 4 do not provide a normalized distribution over the vocabulary. An additional challenge is that existing MT training datasets typically do not provide more than one reference translation. Our SCONES loss function aims to balance two tokenlevel objectives using a scaling factor \u03b1 \u2208 R + :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_19",
            "content": "L(x, y) = 1 |y| |y| i=1 L SCONES (x, y, i),(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_20",
            "content": "where L + (\u2022) aims to increase the log-probability P (z x,y \u2264i = 1|x, y <i ) of the gold label y i since it is a valid extension of the translation prefix y <i :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_21",
            "content": "L SCONES (x, y, i) = L + (x, y, i) + \u03b1L \u2212 (x, y, i).(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_22",
            "content": "L + (x, y, i) = \u2212 log P (z x,y \u2264i = 1|x, y <i ) = \u2212 log \u03c3(f (x, y <i ) y i ).(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_23",
            "content": "L \u2212 (\u2022) is designed to reduce the probability P (z x,y <i \u2022w = 1|x, y <i ) for all labels w except for the gold label y i :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_24",
            "content": "L \u2212 (x, y, i) = \u2212 w\u2208V\\{y i } log P (z x,y <i \u2022w = 0|x, y <i ) = \u2212 w\u2208V\\{y i } log(1 \u2212 \u03c3(f (x, y <i ) w )).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_25",
            "content": "(8) Appendix C provides an implementation of SCONES in JAX (Bradbury et al., 2018). 4 During inference we search for the translation y * that ends with </s> and has the highest probability of being a translation of x:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_26",
            "content": "y * = arg max y\u2208{w\u2022</s>|w\u2208V * } P (z x,y = 1|x) Eqs. 3, 4 = arg max y\u2208{w\u2022</s>|w\u2208V * } |y| i=1 log \u03c3(f (x, y <i ) y i ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_27",
            "content": "(9) We approximate this decision rule with vanilla beam search. The same inference code is used for both our softmax baselines and the SCONEStrained models. The only difference is that the logits from SCONES models are transformed by a sigmoid instead of a softmax activation, i.e. no summation over the full vocabulary is necessary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_28",
            "content": "Relation to noise-contrastive estimation Our SCONES loss function is related to noisecontrastive estimation (NCE) Mnih and Teh, 2012) because both methods reformulate next word prediction as a multi-label classification problem, and both losses have a \"positive\" component for the gold label, and a \"negative\" component for other labels. 5 Unlike NCE, the negative loss component (L \u2212 (\u2022)) in SCONES does not require sampling from a noise distribution as it makes use of all tokens in the vocabulary besides the gold token. This is possi-",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_29",
            "content": "Beam search (beam size = 4) de-en en-de fi-en en-fi lt-en en-lt de-en en-de fi-en en-fi lt-en en-lt",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_30",
            "content": ". improvement +2.7 \u2021 +1.2 +2.8 \u2020 +5.4 \u2021 +5.3 \u2021 +8.5 \u2021 +1.7 \u2020 +0.9 +2.7 \u2020 +5.5 \u2021 +7.4 \u2021 +5.7",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_31",
            "content": "Table 3: BLEU score gains from SCONES over our NMT softmax baselines with tuned \u03b1-values (Table 4). Using a paired bootstrap method (Koehn, 2004), we highlight improvements that are statistically significant either at a .05 level ( \u2020) or a .01 level ( \u2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_32",
            "content": "Language pair \u03b1 de-en 0.5 en-de 0.5 fi-en 0.7 en-fi 1.0 lt-en 0.7 en-lt 0.9 ble because we operate on a limited 32K subword vocabulary whereas NCE is typically used to efficiently train language models with much larger word-level vocabularies (Mnih and Teh, 2012). NCE has a \"self-normalization\" property Pihlaja et al., 2010;Mnih and Teh, 2012;Goldberger and Melamud, 2018) which can reduce computation by avoiding the expensive partition function for distributions over the full vocabulary. To do so, NCE uses the multi-label classification task as a proxy problem. By contrast, in SCONES, the multi-label classification perspective is used to express the intrinsic uncertainty in MT and is not simply a proxy for the full softmax. Thus the primary motivation for SCONES is not self-normalization over the full vocabulary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_33",
            "content": "Experimental setup",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "383-ARR_v1_34",
            "content": "In this work our focus is to compare NMT models trained with SCONES with well-trained standard softmax-based models. Thus we keep our setup simple, reproducible, and computationally economical. We trained Transformer models (Table 1) in six translation directions -German-English (deen), Finnish-English (en-fi), Lithuanian-English (lt-en), and the reverse directions -on the WMT19 (Barrault et al., 2019) training sets as provided by TensorFlow Datasets. 6 We selected these language pairs to experiment with different training set sizes (Table 2). The training sets were filtered using language ID and simple length-based heuristics, and split into subwords using joint 32K SentencePiece (Kudo and Richardson, 2018) models. All our models were trained until convergence on the development set (between 100K and 700K training steps) using the LAMB (You et al., 2020) optimizer in JAX (Bradbury et al., 2018). Our softmax baselines are trained by minimizing cross-entropy without label smoothing. Our multi-way NMT models are trained by minimizing the SCONES loss function from Sec. 2, also without label smoothing. We evaluate our models on the WMT19 test sets (Barrault et al., 2019) with SacreBLEU (Post, 2018), 7 using the WMT18 test sets as development sets to tune \u03b1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_35",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "383-ARR_v1_36",
            "content": "Translation quality",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "383-ARR_v1_37",
            "content": "Table 3 compares our SCONES-based NMT systems with the softmax baselines when \u03b1 is tuned based on the BLEU score on the development set (Table 4). SCONES yields consistent improvements across the board. For four of six language pairs (all except en-de and fi-en), SCONES with greedy search is even able to outperform the softmax models with beam search. The language pairs with fewer resources (fi\u2194en, lt\u2194en) benefit from SCONES training much more than the high-resource language pairs (de\u2194en).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_38",
            "content": "Decoding speed",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "383-ARR_v1_39",
            "content": "Our softmax-based models reach their (near) optimum BLEU score with a beam size of around 4. Most of our SCONES models can achieve similar or better BLEU scores with greedy search. Replacing beam-4 search with greedy search corresponds to a 3.9x speed-up (2.76 \u2192 10.64 sentences per second) on an entry-level NVIDIA Quadro P1000 GPU with a batch size of 4. 8 Fig. 2 shows the BLEU scores for all six translation directions as a function of decoding speed. Most of the speed-ups are due to choosing a smaller beam size and not due to SCONES avoiding the normalization over the full vocabulary. We expect further speed-ups from SCONES when comparing models with larger vocabularies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_40",
            "content": "Mitigating the beam search curse",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "383-ARR_v1_41",
            "content": "One of the most irksome pathologies of traditional softmax-based NMT models is the \"beam search curse\" (Koehn and Knowles, 2017): larger beam sizes improve the log-probability of the translations, but the translation quality gets worse. This happens because with large beam sizes, the model prefers translations that are too short. This phenomenon has been linked to the local normalization in sequence models (Sountsov and Sarawagi, 2016;Murray and Chiang, 2018) and poor model calibration (Kumar and Sarawagi, 2019). Stahlberg and Byrne (2019) showed that modes are often empty and suggested that the inherent bias of the model towards short translations is often obscured by beam search errors. Anonymous (2022) provided strong evidence that this length deficiency is due to the intrinsic uncertainty of the MT task. Given that models trained with SCONES explicitly take into account inherent uncertainty, we ran an experiment to determine whether these models are more robust to the beam search curse compared to softmax trained models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_42",
            "content": "Beam search (beam size = 4)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_43",
            "content": "Exact search de-en en-de fi-en en-fi lt-en en-lt de-en en-de fi-en en-fi lt-en en-lt Fig. 3 plots the BLEU score as a function of the beam size. The sharp decline of the green curve for large beam sizes reflects the beam search curse for the softmax baseline. SCONES seems to be less affected at larger beam sizes, particularly for small \u03b1values: the BLEU score for SCONES with \u03b1 0.2 (solid purple curve) is stable for beam sizes greater than 100. Fig. 4, which displays the length ratio (the hypothesis length divided by the reference length) versus beam size, suggests that the differences in BLEU trajectories arise due to translation lengths. Translations obtained using softmax become abruptly shorter at higher beam sizes whereas for SCONES with \u03b1 = 0.2, there is no such steep decrease in length. To study the impact of \u03b1 in the absence of beam search errors we ran the exact depth-first search algorithm of Stahlberg and Byrne (2019) to find the translation with global highest probability. 9 The adequacy of the translations found by exact search depends heavily on \u03b1 (Fig. 5). With exact search, small \u03b1-values yield adequate translations, but \u03b1 \u2248 1.0 performs similar to the softmax baseline: the BLEU score drops because hypotheses are too short. Table 5 shows that SCONES with \u03b1 = 0.2 consistently outperforms the softmax baselines by a large margin with exact search. Fig. 6 sheds some light on why SCONES with small \u03b1 does not prefer empty translations. 9 The maximum number of explored states per sentence was set to 1M. This threshold was reached for less than 1.45% of the German-English sentences. See Appendix A for other language directions. A small \u03b1 leads to a larger gap between the logprobabilities of the exact search translation and the empty translation that arises from higher logprobabilities for the exact-search translation along with smaller variances. Intuitively, a small \u03b1 reduces the importance of the negative loss component L \u2212 (\u2022) in Eq. 6, and thus biases each binary classifier towards predicting the true label. els: the percentage of search errors remains at a relatively high level of around 20% even for very large beam sizes. Increasing the beam size is most effective in reducing the number of search errors for SCONES with a small value of \u03b1. However, a small \u03b1 does not always yield the best overall BLEU score (Fig. 3). Taken together, these observations provide an insight into model errors in NMT: If we describe the \"model error\" as the mismatch between the global most likely translation and an adequate translation (following Stahlberg and Byrne ( 2019)), a small \u03b1 would simultaneously lead to both fewer search errors (Fig. 7) and fewer model errors (Tab. 5). Counter-intuitively, however, BLEU scores peak at slightly higher \u03b1-values (Tab. 4). A more sophisticated notion of model errors and search errors is needed to understand the complex inherent biases of beam search for neural sequence-to-sequence models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_44",
            "content": "Reducing the number of beam search errors",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "383-ARR_v1_45",
            "content": "Experiments with synthetic language pairs",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "383-ARR_v1_46",
            "content": "Our main motivation for SCONES is to equip the model to naturally represent intrinsic uncertainty, i.e. the existence of multiple correct target sentences for the same source sentence. To examine the characteristics of SCONES as a function of uncertainty, we generated synthetic language pairs that differ by the level of ambiguity. For this purpose, we trained an IBM-3 model (Brown et al., 1993) on the German-English training data after subword segmentation using MGIZA (Gao and Vogel, 2008). IBM-3 is a generative symbolic model that describes the translation process from one language into another with a generative story, and was popular for finding word alignments for statistical (phrase-based) machine translation (Koehn, 2009).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_47",
            "content": "The generative story consists of different steps such as distortion (word reordering), fertility (1:n word mappings), and lexical translation (word-to-word translation) that describe the translation process.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_48",
            "content": "The parameters of IBM-3 define probability distributions for each step. In this work we do not use IBM-3 for finding word alignments. Instead, for the original German sentences we sample synthetic English-like translations from the model with different sampling temperatures to control the ambiguity levels of the translation task. A low sampling temperature generates sentence pairs that still capture some of the characteristics of MT such as word reorderings, but the mapping is mostly deterministic (i.e. the same source token is almost always translated to the same target token). A high temperature corresponds to more randomness, i.e. more intrinsic uncertainty. Appendix B contains more details about sampling from IBM-3. We train NMT models using either softmax or SCONES on the synthetic corpora. Fig. 8 shows that softmax and SCONES perform similarly using beam search: high IBM-3 sampling temperature translation tasks are less predictable, and thus lead to lower BLEU scores. The difference between both approaches becomes clear with exact search (Fig. 9). While the translations with the global highest probability for high IBM-3 sampling temperatures are heavily degraded for softmax and SCONES with \u03b1 = 1, the drop is much less dramatic for SCONES with \u03b1 = 0.2 (solid purple curve). Setting \u03b1 to a low value enables the model to assign its highest probability to adequate translations, even when the translation task is highly uncertain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_49",
            "content": "Related work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "383-ARR_v1_50",
            "content": "Our approach draws insights from multi-label classification (MLC) (Tsoumakas and Katakis, 2007;Zhou, 2006, 2014). One of the earliest approaches for MLC was to transform the problem into multiple binary classification problems while ignoring the correlations between labels (Boutell et al., 2004). More recent work has modeled MLC in the sequence-to-sequence framework with a decoder that generates the labels sequentially, thus preserving the inter-label correlations (Yang et al., 2018). Most prior work in MLC focuses on classification and is not directly applicable to MT. In contrast, our training strategy is tailored for sequence-to-sequence problems. Unlike prior work (Yang et al., 2018), SCONES allows us to perform MLC style training with any underlying NMT architecture by simply changing the loss function. By jointly training all label-specific binary classifiers, our strategy is able to account for label correlations. used an MLC objective to improve machine translation. Unlike our approach, they attempted to predict all words in the target sentence with a bag-of-words loss function. We formulate the next word prediction at each time step as a MLC problem to handle intrinsic uncertainty, but our models are predicting ordered target sequences, not bags of words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_51",
            "content": "The speed-ups from SCONES can be partially attributed to avoiding the normalization of the output over the full vocabulary. The same idea motivated earlier work on self-normalized training Mnih and Teh, 2012;Devlin et al., 2014;Goldberger and Melamud, 2018). As described in Sec. 2, unlike work on self-normalization, SCONES does not try to approximate a distribution over the full vocabulary. Rather, its output consists of multiple binary classifiers that do not share probability mass by design to be able to better represent intrinsic uncertainty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_52",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "383-ARR_v1_53",
            "content": "Machine translation is a task with high intrinsic uncertainty: a source sentence can have multiple valid translations. We demonstrated that NMT models and specifically Transformers, can learn to model mutually non-exclusive target sentences from single-label training data using our SCONES loss function. Rather than learn a single distribution over all target sentences, SCONES learns multiple binary classifiers that indicate whether or not a target sentence is a valid translation of the source sentence. SCONES yields improved translation quality over conventional softmax-based models for six different translation directions, or (alternatively) speed-ups of up to 3.9x without any degradation in translation performance. We showed that SCONES can be tuned to mitigate the beam search curse and the problem of inadequate and empty modes in standard NMT. Our experiments on synthetic language translation suggest that, unlike softmaxtrained models, SCONES models are able to assign their highest probability to adequate translations even when the underlying task is highly ambiguous.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_54",
            "content": "The SCONES loss function is easy to implement. Adapting standard softmax-based sequenceto-sequence architectures such as Transformers requires only replacing the cross-entropy loss function with SCONES and the softmax with sigmoid activations. The remaining parts of the training and inference pipelines can be kept unchanged. SCONES can be potentially useful in handling uncertainty for a variety of ambiguous NLP problems beyond translation, such as generation and dialog. We expect this work to encourage research on modeling techniques that can address ambiguity in much better ways compared to current models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_55",
            "content": "The exact search algorithm of Stahlberg and Byrne (2019) we used in the paper is guaranteed to find the global best translation. Its runtime, however, varies greatly between language pairs and source sentences. Therefore, we limit the number of explored states per sentence by 1M to keep the decoding time under control. If the 1M threshold is reached, the optimality of the found translation is not guaranteed anymore. Fortunately, for most of our models and test sets, exact search was able to find and verify the global best translation earlier. Table 6 lists the runs for which a fraction of the sentences did not terminate before 1M steps. In these rare cases, we use the best translation found thus far by exact search as an approximation to the global best translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_56",
            "content": "The parameters of the IBM-3 model (Brown et al., 1993)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_57",
            "content": "(\u03b1 = 0.2) 5.31% synthetic-0.1 Softmax 1.05% synthetic-0.1 SCONES (\u03b1 = 0.2) 0.55% synthetic-0.1 SCONES (\u03b1 = 0.5) 1.10% synthetic-0.1 SCONES (\u03b1 = 1.0) 1.25% synthetic-0.2 Softmax 1.00% synthetic-0.2 SCONES (\u03b1 = 0.2) 5.10% synthetic-0.2 SCONES (\u03b1 = 0.5) 7.65% synthetic-0.2 SCONES (\u03b1 = 1.0) 2.65% synthetic-0.3 Softmax 0.10% synthetic-0.3 SCONES (\u03b1 = 0.2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_58",
            "content": "12.6% synthetic-0.3 SCONES (\u03b1 = 0.5) 17.3% synthetic-0.3 SCONES (\u03b1 = 1.0)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_59",
            "content": "1.80% synthetic-0.5 SCONES (\u03b1 = 0.2) 25.0% synthetic-0.5 SCONES (\u03b1 = 0.5) 25.2% synthetic-0.7 SCONES (\u03b1 = 0.2) 25.9% synthetic-0.7 SCONES (\u03b1 = 0.5) 20.3% process produces the target language sentence y from a source language sentence x (Knight, 1999):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_60",
            "content": "1. For each source word x i indexed by i = 1, 2, . . . , |x|, choose the fertility \u03c6 i with probability n(\u03c6 i |x i ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_61",
            "content": "2. Choose the number \u03c6 0 of \"spurious\" target words to be generated from x 0 = NULL, using probability p 1 and the sum of fertilities from step 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_62",
            "content": "Let",
            "ntype": "title",
            "meta": {
                "section": "3."
            }
        },
        {
            "ix": "383-ARR_v1_63",
            "content": "m = |x| i=0 \u03c6 i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_64",
            "content": "4. For each i = 0, 1, 2, . . . , |x| and each k = 1, 2, . . . , \u03c6 i , choose a target word \u03c4 ik with probability t(\u03c4 ik |x i ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_65",
            "content": "5. For each i = 1, 2, . . . , |x| and each k = 1, 2, . . . , \u03c6 i , choose a target position \u03c0 ik with probability d(\u03c0 ik |i, |x|, m).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_66",
            "content": "6. For each k = 1, 2, . . . , \u03c6 0 , choose a position \u03c0 0k from the \u03c6 0 \u2212 k + 1 remaining vacant positions in 1, 2, . . . , m, for a total probability of 1 \u03c6 0 ! . 7. Output the target sentence with words \u03c4 ik in positions \u03c0 ik (0",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_67",
            "content": "\u2264 i \u2264 |x|, 1 \u2264 k \u2264 \u03c6 i ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_68",
            "content": "First, we estimate the IBM-3 model parameters using the MGIZA (Gao and Vogel, 2008) l o s s = a * ( j n p . sum ( a l l _ f a l s e _ x e n t , a x i s =\u22121) \u2212 t g t _ f a l s e _ x e n t ) + t g t _ t r u e _ x e n t 20 w e i g h t s = j n p . where ( t a r g e t s > 0 , 1 , 0 ) . a s t y p e ( j n p . f l o a t 3 2 ) # PAD ID i s 0 . 21 r e t u r n l o s s * w e i g h t s / w e i g h t s . sum ( )",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_69",
            "content": "Figure 10: JAX implementation of the SCONES loss function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_70",
            "content": "for numerical stability. The JAX implementation generalizes the SCONES loss defined in the main paper in Eq. 6 with a label smoothing (Szegedy et al., 2016) factor \u03bb \u2208 [0, 1] (l in Fig. 10) such that the positive loss component L + (\u2022) becomes the following cross-entropy:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_71",
            "content": "L + (x, y, i) = \u2212 (1 \u2212 \u03bb) log P (z x,y \u2264i = 1|x, y <i ) \u2212 \u03bb log P (z x,y \u2264i = 0|x, y <i ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_72",
            "content": "(11) Similarly, the negative loss component L \u2212 (\u2022) with label smoothing can be written as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_73",
            "content": "L \u2212 (x, y, i) = \u2212 w\u2208V\\{y i }",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_74",
            "content": "(1 \u2212 \u03bb) log P (z x,y <i \u2022w = 0|x, y <i ) + \u03bb log P (z x,y <i \u2022w = 1|x, y <i ) .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_75",
            "content": "(12) The label smoothing extension is provided for the sake of completeness -we did not use label smoothing in any of the experiments in the main paper since it did not yield improvements in our setups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "383-ARR_v1_76",
            "content": "UNKNOWN, None, , Anonymous. 2022. Anonymous. (under ARR review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Anonymous. 2022. Anonymous. (under ARR review",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_77",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Lo\u00efc Barrault",
                    "Ond\u0159ej Bojar",
                    "Marta Costa-Juss\u00e0",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Philipp Koehn",
                    "Shervin Malmasi",
                    "Christof Monz",
                    "Mathias M\u00fcller"
                ],
                "title": "Findings of the 2019 conference on machine translation (WMT19)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_78",
            "content": "Matthew Boutell, Jiebo Luo, Xipeng Shen, Christopher Brown, Learning multilabel scene classification, 2004, Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Matthew Boutell",
                    "Jiebo Luo",
                    "Xipeng Shen",
                    "Christopher Brown"
                ],
                "title": "Learning multilabel scene classification",
                "pub_date": "2004",
                "pub_title": "Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_79",
            "content": "UNKNOWN, None, 2018, JAX: Composable transformations of Python+NumPy programs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "JAX: Composable transformations of Python+NumPy programs",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_80",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "F Peter",
                    "Stephen Brown",
                    "Vincent Pietra",
                    "Robert Della Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_81",
            "content": "UNKNOWN, None, 2009, Aleatory or epistemic? Does it matter? Structural safety, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Aleatory or epistemic? Does it matter? Structural safety",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_82",
            "content": "Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Fast and robust neural network joint models for statistical machine translation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Rabih Zbib",
                    "Zhongqiang Huang",
                    "Thomas Lamar",
                    "Richard Schwartz",
                    "John Makhoul"
                ],
                "title": "Fast and robust neural network joint models for statistical machine translation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_83",
            "content": "Markus Dreyer, Daniel Marcu, HyTER: Meaning-equivalent semantics for translation evaluation, 2012, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Markus Dreyer",
                    "Daniel Marcu"
                ],
                "title": "HyTER: Meaning-equivalent semantics for translation evaluation",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_84",
            "content": "Bryan Eikema, Wilker Aziz, Is MAP decoding all you need? the inadequacy of the mode in neural machine translation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Bryan Eikema",
                    "Wilker Aziz"
                ],
                "title": "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_85",
            "content": "Qin Gao, Stephan Vogel, Parallel implementations of word alignment tool, 2008, Software Engineering, Testing, and Quality Assurance for Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Qin Gao",
                    "Stephan Vogel"
                ],
                "title": "Parallel implementations of word alignment tool",
                "pub_date": "2008",
                "pub_title": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_86",
            "content": "Jacob Goldberger, Oren Melamud, Selfnormalization properties of language modeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jacob Goldberger",
                    "Oren Melamud"
                ],
                "title": "Selfnormalization properties of language modeling",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_87",
            "content": "Michael Gutmann, Aapo Hyv\u00e4rinen, Noisecontrastive estimation: A new estimation principle for unnormalized statistical models, 2010, Proceedings of the thirteenth international conference on artificial intelligence and statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Michael Gutmann",
                    "Aapo Hyv\u00e4rinen"
                ],
                "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
                "pub_date": "2010",
                "pub_title": "Proceedings of the thirteenth international conference on artificial intelligence and statistics",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_88",
            "content": "Nal Kalchbrenner, Phil Blunsom, Recurrent continuous translation models, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Nal Kalchbrenner",
                    "Phil Blunsom"
                ],
                "title": "Recurrent continuous translation models",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_89",
            "content": "Kevin Knight, A statistical mt tutorial workbook, 1999, Prepared for the 1999 JHU Summer Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Kevin Knight"
                ],
                "title": "A statistical mt tutorial workbook",
                "pub_date": "1999",
                "pub_title": "Prepared for the 1999 JHU Summer Workshop",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_90",
            "content": "Philipp Koehn, Statistical significance tests for machine translation evaluation, 2004, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Philipp Koehn"
                ],
                "title": "Statistical significance tests for machine translation evaluation",
                "pub_date": "2004",
                "pub_title": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_91",
            "content": "UNKNOWN, None, 2009, Statistical machine translation, Cambridge University Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Statistical machine translation",
                "pub": "Cambridge University Press"
            }
        },
        {
            "ix": "383-ARR_v1_92",
            "content": "Philipp Koehn, Rebecca Knowles, Six challenges for neural machine translation, 2017, Proceedings of the First Workshop on Neural Machine Translation, Vancouver. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Philipp Koehn",
                    "Rebecca Knowles"
                ],
                "title": "Six challenges for neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the First Workshop on Neural Machine Translation",
                "pub": "Vancouver. Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_93",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Taku Kudo",
                    "John Richardson"
                ],
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_94",
            "content": "UNKNOWN, None, 2019, Calibration of encoder decoder models for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Calibration of encoder decoder models for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_95",
            "content": "Shuming Ma, Xu Sun, Yizhong Wang, Junyang Lin, Bag-of-words as target for neural machine translation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Shuming Ma",
                    "Xu Sun",
                    "Yizhong Wang",
                    "Junyang Lin"
                ],
                "title": "Bag-of-words as target for neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "383-ARR_v1_96",
            "content": "Andriy Mnih, Yee Whye Teh, A fast and simple algorithm for training neural probabilistic language models, 2012, Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, Omnipress.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Andriy Mnih",
                    "Yee Whye Teh"
                ],
                "title": "A fast and simple algorithm for training neural probabilistic language models",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12",
                "pub": "Omnipress"
            }
        },
        {
            "ix": "383-ARR_v1_97",
            "content": "Kenton Murray, David Chiang, Correcting length bias in neural machine translation, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Kenton Murray",
                    "David Chiang"
                ],
                "title": "Correcting length bias in neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_98",
            "content": "Myle Ott, Michael Auli, David Grangier, Marc'aurelio Ranzato, Analyzing uncertainty in neural machine translation, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Myle Ott",
                    "Michael Auli",
                    "David Grangier",
                    "Marc'aurelio Ranzato"
                ],
                "title": "Analyzing uncertainty in neural machine translation",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "383-ARR_v1_99",
            "content": "Sebastian Pad\u00f3, Daniel Cer, Michel Galley, Dan Jurafsky, Christopher D Manning, Measuring machine translation quality as semantic equivalence: A metric based on entailment features, 2009, Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Sebastian Pad\u00f3",
                    "Daniel Cer",
                    "Michel Galley",
                    "Dan Jurafsky",
                    "Christopher D Manning"
                ],
                "title": "Measuring machine translation quality as semantic equivalence: A metric based on entailment features",
                "pub_date": "2009",
                "pub_title": "Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_100",
            "content": "Miika Pihlaja, Michael Gutmann, Aapo Hyv\u00e4rinen, A family of computationally efficient and simple estimators for unnormalized statistical models, 2010, Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI'10, AUAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Miika Pihlaja",
                    "Michael Gutmann",
                    "Aapo Hyv\u00e4rinen"
                ],
                "title": "A family of computationally efficient and simple estimators for unnormalized statistical models",
                "pub_date": "2010",
                "pub_title": "Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI'10",
                "pub": "AUAI Press"
            }
        },
        {
            "ix": "383-ARR_v1_101",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_102",
            "content": "Pavel Sountsov, Sunita Sarawagi, Length bias in encoder decoder models and a case for global conditioning, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Pavel Sountsov",
                    "Sunita Sarawagi"
                ],
                "title": "Length bias in encoder decoder models and a case for global conditioning",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "383-ARR_v1_103",
            "content": "Felix Stahlberg, Bill Byrne, On NMT search errors and model errors: Cat got your tongue?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Felix Stahlberg",
                    "Bill Byrne"
                ],
                "title": "On NMT search errors and model errors: Cat got your tongue?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_104",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "Quoc V Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_105",
            "content": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Rethinking the inception architecture for computer vision, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Christian Szegedy",
                    "Vincent Vanhoucke",
                    "Sergey Ioffe",
                    "Jon Shlens",
                    "Zbigniew Wojna"
                ],
                "title": "Rethinking the inception architecture for computer vision",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_106",
            "content": ", Grigorios Tsoumakas and Ioannis Katakis, 2007, Int J Data Warehousing and Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [],
                "title": "Grigorios Tsoumakas and Ioannis Katakis",
                "pub_date": "2007",
                "pub_title": "Int J Data Warehousing and Mining",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_107",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "383-ARR_v1_108",
            "content": "Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, Houfeng Wang, SGM: Sequence generation model for multi-label classification, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Pengcheng Yang",
                    "Xu Sun",
                    "Wei Li",
                    "Shuming Ma",
                    "Wei Wu",
                    "Houfeng Wang"
                ],
                "title": "SGM: Sequence generation model for multi-label classification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_109",
            "content": "Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh, Large batch optimization for deep learning: Training BERT in 76 minutes, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Yang You",
                    "Jing Li",
                    "Sashank Reddi",
                    "Jonathan Hseu",
                    "Sanjiv Kumar",
                    "Srinadh Bhojanapalli",
                    "Xiaodan Song",
                    "James Demmel",
                    "Kurt Keutzer",
                    "Cho-Jui Hsieh"
                ],
                "title": "Large batch optimization for deep learning: Training BERT in 76 minutes",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_110",
            "content": "Min-Ling Zhang, Zhi-Hua Zhou, Multilabel neural networks with applications to functional genomics and text categorization, 2006, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Min-Ling Zhang",
                    "Zhi-Hua Zhou"
                ],
                "title": "Multilabel neural networks with applications to functional genomics and text categorization",
                "pub_date": "2006",
                "pub_title": "IEEE Transactions on Knowledge and Data Engineering",
                "pub": null
            }
        },
        {
            "ix": "383-ARR_v1_111",
            "content": "Min-Ling Zhang, Zhi-Hua Zhou, A review on multi-label learning algorithms, 2014, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Min-Ling Zhang",
                    "Zhi-Hua Zhou"
                ],
                "title": "A review on multi-label learning algorithms",
                "pub_date": "2014",
                "pub_title": "IEEE Transactions on Knowledge and Data Engineering",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "383-ARR_v1_0@0",
            "content": "Jam or Cream First? 1 Modeling Ambiguity in Neural Machine Translation with SCONES",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_0",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@0",
            "content": "The softmax layer in neural machine translation is designed to model the distribution over mutually exclusive tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@1",
            "content": "Machine translation, however, is intrinsically uncertain: the same source sentence can have multiple semantically equivalent translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 118,
            "end": 255,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@2",
            "content": "Therefore, we propose to replace the softmax activation with a multi-label classification layer that can model ambiguity more effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 257,
            "end": 394,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@3",
            "content": "We call our loss function Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 396,
            "end": 493,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@4",
            "content": "We show that the multi-label output layer can still be trained on single reference training data using the SCONES loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 495,
            "end": 622,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@5",
            "content": "SCONES yields consistent BLEU score gains across six translation directions, particularly for mediumresource language pairs and small beam sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 624,
            "end": 768,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@6",
            "content": "By using smaller beam sizes we can speed up inference by a factor of 3.9x and still match or improve the softmax BLEU score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 770,
            "end": 893,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@7",
            "content": "Furthermore, we demonstrate that SCONES can be used to train NMT models that assign the highest probability to adequate translations, thus mitigating the \"beam search curse\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 895,
            "end": 1068,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_2@8",
            "content": "Additional experiments on synthetic language pairs with varying levels of uncertainty suggest that the improvements from SCONES can be attributed to better handling of ambiguity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_2",
            "start": 1070,
            "end": 1247,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_4@0",
            "content": "Conventional neural machine translation (NMT) models learn the probability P (y|x) of the target sentence y given the source sentence x ( Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_4",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_4@1",
            "content": "This framework implies that there is a single best translation for a given source sentence: if there were multiple valid translations y 1 and y 2 they would need to share probability mass (e.g. P (y 1 |x) = 0.5 and P (y 2 |x) = 0.5), but such a distribution could also represent model uncertainty, i.e. the case when either y 1 or y 2 are correct translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_4",
            "start": 194,
            "end": 553,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_4@2",
            "content": "Therefore, learning a single distribution over all target language sentences does not allow the model to naturally express intrinsic uncertainty 2 (Pad\u00f3 et al., 2009;Dreyer and Marcu, 2012;Ott et al., 2018), the nature of the translation task to allow multiple semantically equivalent translations for a given source sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_4",
            "start": 555,
            "end": 880,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_4@3",
            "content": "Single distributions over all sequences represent uncertainty by assigning probabilities, but they cannot distinguish between different kinds of uncertainty (e.g. model uncertainty versus intrinsic uncertainty).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_4",
            "start": 882,
            "end": 1092,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@0",
            "content": "Therefore, in this work we frame machine translation as a multi-label classification task (Tsoumakas and Katakis, 2007;Zhang and Zhou, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@1",
            "content": "Rather than learning a single distribution P (y|x) over all target sentences y for a source sentence x, we learn binary classifiers for each sentence pair (x, y) that indicate whether or not y is a valid translation of x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 142,
            "end": 362,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@2",
            "content": "In this framework, intrinsic uncertainty can be represented by setting the probabilities of two (or more) correct translations y 1 and y 2 to 1 simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 364,
            "end": 522,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@3",
            "content": "The probabilities for each translation are computed using separate binary classifiers, and thus there is no requirement that the probabilities sum to one over all translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 524,
            "end": 699,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@4",
            "content": "In practice, the probability of a complete translation is decomposed into a product of the token-level probabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 701,
            "end": 817,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@5",
            "content": "Thus we replace the softmax output layer in Transformer models (Vaswani et al., 2017) with sigmoid activations that assign a probability between 0 and 1 to each token in the vocabulary at each time step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 819,
            "end": 1021,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@6",
            "content": "We propose a loss function, Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES) that allows us to train our models on single reference training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 1023,
            "end": 1191,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@7",
            "content": "Our work is inspired by noise-contrastive estimation (NCE) Mnih and Teh, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 1193,
            "end": 1271,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_5@8",
            "content": "Unlike NCE, whose primary goal was to efficiently train models over large vo-cabularies, our motivation for SCONES is to model non-exclusive outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_5",
            "start": 1273,
            "end": 1421,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_6@0",
            "content": "We demonstrate multiple benefits of training NMT models using SCONES when compared to standard cross-entropy with regular softmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_6",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_6@1",
            "content": "We report consistent BLEU score gains between 1%-9% across six different translation directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_6",
            "start": 131,
            "end": 226,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_6@2",
            "content": "SCONES with greedy search typically outperforms softmax with beam search, resulting in inference speed-ups of up to 3.9x compared to softmax without any degradation in BLEU score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_6",
            "start": 228,
            "end": 406,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_7@0",
            "content": "SCONES mitigates some of the pathologies of traditional NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_7",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_7@1",
            "content": "Softmax-based models have been shown to assign the highest probability to either empty or inadequate translations (modes) (Stahlberg and Byrne, 2019;Eikema and Aziz, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_7",
            "start": 68,
            "end": 239,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_7@2",
            "content": "This behavior manifests itself as the \"beam search curse\" (Koehn and Knowles, 2017): increasing the beam size may lead to worse translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_7",
            "start": 241,
            "end": 388,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_7@3",
            "content": "We show that SCONES can be used to train models that a) assign the highest probability to adequate translations and b) do not suffer from the beam search curse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_7",
            "start": 390,
            "end": 549,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_8@0",
            "content": "Finally, we use SCONES to train models on synthetic translation pairs that we generate by sampling from the IBM Model 3 (Brown et al., 1993).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_8",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_8@1",
            "content": "By varying the sampling temperature, we control the level of ambiguity in the language pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_8",
            "start": 142,
            "end": 233,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_8@2",
            "content": "We show that SCONES is effective in improving the adequacy of the highest probability translation for highly ambiguous translation pairs, confirming our intuition that SCONES can handle intrinsic uncertainty well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_8",
            "start": 235,
            "end": 447,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_9@0",
            "content": "Training NMT models with SCONES",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_9",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_10@0",
            "content": "We denote the (subword) vocabulary as V = {w 1 , . . . , w |V| }, the special end-of-sentence symbol as w 1 = </s>, the source sentence as x = x 1 , . . . , x |x| \u2208 V * , a translation as y = y 1 , . . . , y |y| \u2208 V * , and a translation prefix as y \u2264i = y 1 , . . . , y i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_10",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_10@1",
            "content": "We use a center dot \"\u2022\" for string concatenations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_10",
            "start": 275,
            "end": 324,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_10@2",
            "content": "Unlike conventional NMT that models a single distribution P (y|x) over all target language sentences, SCONES learns a separate binary classifier for each sentence pair (x, y).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_10",
            "start": 326,
            "end": 500,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_10@3",
            "content": "We define a Boolean function t(\u2022, \u2022) that indicates whether y is a valid translation of x: t(x, y) := true if y is a translation of x false otherwise .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_10",
            "start": 502,
            "end": 652,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_11@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_11",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_12@0",
            "content": "We do not model t(\u2022, \u2022) directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_12",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_12@1",
            "content": "To guide decoding, we learn variables z x,y which generalize t(\u2022, \u2022) to translation prefixes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_12",
            "start": 34,
            "end": 126,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_13@0",
            "content": "z x,y := 1 \u2203y \u2208 V * : t(x, y \u2022 y ) = true 0 otherwise ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_13",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_14@0",
            "content": "(2) i.e. z x,y is a binary label for the pair (x, y) consisting of source sentence x and the translation prefix y: z x,y = 1 iff. y is a prefix of a valid translation of x. We decompose its probability as a product of conditionals to facilitate left-to-right beam decoding: 3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_14",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_15@0",
            "content": "P (z x,y = 1|x) = |y| i=1 P (z x,y \u2264i = 1|z x,y <i = 1, x) = |y| i=1 P (z x,y \u2264i = 1|x, y <i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_15",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_16@0",
            "content": "(3) We assign the conditional probabilities by applying the sigmoid activation function \u03c3(\u2022) to the logits:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_16",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_17@0",
            "content": "P (z x,y <i \u2022w = 1|x, y <i ) = \u03c3(f (x, y <i ) w ), (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_17",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_18@0",
            "content": "where w \u2208 V is a single token, f (x, y <i ) \u2208 R |V| are the logits at time step i, and f (x, y <i ) w is the logit corresponding to token w.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_18",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_18@1",
            "content": "The only architectural difference to a standard NMT model is the output activation: instead of the softmax function that yields a single distribution over the full vocabulary, we use multiple sigmoid activations in each logit component to define separate Bernoulli distributions for each item in the vocabulary (Fig. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_18",
            "start": 141,
            "end": 460,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_18@2",
            "content": "However, using such a multi-label classification view requires a different training loss function because, unlike the logits from a softmax, the logits in Eq. 4 do not provide a normalized distribution over the vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_18",
            "start": 462,
            "end": 683,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_18@3",
            "content": "An additional challenge is that existing MT training datasets typically do not provide more than one reference translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_18",
            "start": 685,
            "end": 807,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_18@4",
            "content": "Our SCONES loss function aims to balance two tokenlevel objectives using a scaling factor \u03b1 \u2208 R + :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_18",
            "start": 809,
            "end": 907,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_19@0",
            "content": "L(x, y) = 1 |y| |y| i=1 L SCONES (x, y, i),(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_19",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_20@0",
            "content": "where L + (\u2022) aims to increase the log-probability P (z x,y \u2264i = 1|x, y <i ) of the gold label y i since it is a valid extension of the translation prefix y <i :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_20",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_21@0",
            "content": "L SCONES (x, y, i) = L + (x, y, i) + \u03b1L \u2212 (x, y, i).(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_21",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_22@0",
            "content": "L + (x, y, i) = \u2212 log P (z x,y \u2264i = 1|x, y <i ) = \u2212 log \u03c3(f (x, y <i ) y i ).(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_22",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_23@0",
            "content": "L \u2212 (\u2022) is designed to reduce the probability P (z x,y <i \u2022w = 1|x, y <i ) for all labels w except for the gold label y i :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_23",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_24@0",
            "content": "L \u2212 (x, y, i) = \u2212 w\u2208V\\{y i } log P (z x,y <i \u2022w = 0|x, y <i ) = \u2212 w\u2208V\\{y i } log(1 \u2212 \u03c3(f (x, y <i ) w )).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_24",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_25@0",
            "content": "(8) Appendix C provides an implementation of SCONES in JAX (Bradbury et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_25",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_25@1",
            "content": "4 During inference we search for the translation y * that ends with </s> and has the highest probability of being a translation of x:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_25",
            "start": 84,
            "end": 216,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_26@0",
            "content": "y * = arg max y\u2208{w\u2022</s>|w\u2208V * } P (z x,y = 1|x) Eqs. 3, 4 = arg max y\u2208{w\u2022</s>|w\u2208V * } |y| i=1 log \u03c3(f (x, y <i ) y i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_26",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_27@0",
            "content": "(9) We approximate this decision rule with vanilla beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_27",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_27@1",
            "content": "The same inference code is used for both our softmax baselines and the SCONEStrained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_27",
            "start": 64,
            "end": 155,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_27@2",
            "content": "The only difference is that the logits from SCONES models are transformed by a sigmoid instead of a softmax activation, i.e. no summation over the full vocabulary is necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_27",
            "start": 157,
            "end": 332,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_28@0",
            "content": "Relation to noise-contrastive estimation Our SCONES loss function is related to noisecontrastive estimation (NCE) Mnih and Teh, 2012) because both methods reformulate next word prediction as a multi-label classification problem, and both losses have a \"positive\" component for the gold label, and a \"negative\" component for other labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_28",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_28@1",
            "content": "5 Unlike NCE, the negative loss component (L \u2212 (\u2022)) in SCONES does not require sampling from a noise distribution as it makes use of all tokens in the vocabulary besides the gold token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_28",
            "start": 338,
            "end": 522,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_28@2",
            "content": "This is possi-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_28",
            "start": 524,
            "end": 537,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_29@0",
            "content": "Beam search (beam size = 4) de-en en-de fi-en en-fi lt-en en-lt de-en en-de fi-en en-fi lt-en en-lt",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_29",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_30@0",
            "content": ". improvement +2.7 \u2021 +1.2 +2.8 \u2020 +5.4 \u2021 +5.3 \u2021 +8.5 \u2021 +1.7 \u2020 +0.9 +2.7 \u2020 +5.5 \u2021 +7.4 \u2021 +5.7",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_30",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_31@0",
            "content": "Table 3: BLEU score gains from SCONES over our NMT softmax baselines with tuned \u03b1-values (Table 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_31",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_31@1",
            "content": "Using a paired bootstrap method (Koehn, 2004), we highlight improvements that are statistically significant either at a .05 level ( \u2020) or a .01 level ( \u2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_31",
            "start": 100,
            "end": 254,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_32@0",
            "content": "Language pair \u03b1 de-en 0.5 en-de 0.5 fi-en 0.7 en-fi 1.0 lt-en 0.7 en-lt 0.9 ble because we operate on a limited 32K subword vocabulary whereas NCE is typically used to efficiently train language models with much larger word-level vocabularies (Mnih and Teh, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_32",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_32@1",
            "content": "NCE has a \"self-normalization\" property Pihlaja et al., 2010;Mnih and Teh, 2012;Goldberger and Melamud, 2018) which can reduce computation by avoiding the expensive partition function for distributions over the full vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_32",
            "start": 265,
            "end": 491,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_32@2",
            "content": "To do so, NCE uses the multi-label classification task as a proxy problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_32",
            "start": 493,
            "end": 566,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_32@3",
            "content": "By contrast, in SCONES, the multi-label classification perspective is used to express the intrinsic uncertainty in MT and is not simply a proxy for the full softmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_32",
            "start": 568,
            "end": 732,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_32@4",
            "content": "Thus the primary motivation for SCONES is not self-normalization over the full vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_32",
            "start": 734,
            "end": 823,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_33@0",
            "content": "Experimental setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_33",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@0",
            "content": "In this work our focus is to compare NMT models trained with SCONES with well-trained standard softmax-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@1",
            "content": "Thus we keep our setup simple, reproducible, and computationally economical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 117,
            "end": 192,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@2",
            "content": "We trained Transformer models (Table 1) in six translation directions -German-English (deen), Finnish-English (en-fi), Lithuanian-English (lt-en), and the reverse directions -on the WMT19 (Barrault et al., 2019) training sets as provided by TensorFlow Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 194,
            "end": 454,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@3",
            "content": "6 We selected these language pairs to experiment with different training set sizes (Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 456,
            "end": 548,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@4",
            "content": "The training sets were filtered using language ID and simple length-based heuristics, and split into subwords using joint 32K SentencePiece (Kudo and Richardson, 2018) models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 550,
            "end": 724,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@5",
            "content": "All our models were trained until convergence on the development set (between 100K and 700K training steps) using the LAMB (You et al., 2020) optimizer in JAX (Bradbury et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 726,
            "end": 908,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@6",
            "content": "Our softmax baselines are trained by minimizing cross-entropy without label smoothing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 910,
            "end": 995,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@7",
            "content": "Our multi-way NMT models are trained by minimizing the SCONES loss function from Sec. 2, also without label smoothing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 997,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_34@8",
            "content": "We evaluate our models on the WMT19 test sets (Barrault et al., 2019) with SacreBLEU (Post, 2018), 7 using the WMT18 test sets as development sets to tune \u03b1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_34",
            "start": 1116,
            "end": 1272,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_35@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_35",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_36@0",
            "content": "Translation quality",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_36",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_37@0",
            "content": "Table 3 compares our SCONES-based NMT systems with the softmax baselines when \u03b1 is tuned based on the BLEU score on the development set (Table 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_37",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_37@1",
            "content": "SCONES yields consistent improvements across the board.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_37",
            "start": 147,
            "end": 201,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_37@2",
            "content": "For four of six language pairs (all except en-de and fi-en), SCONES with greedy search is even able to outperform the softmax models with beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_37",
            "start": 203,
            "end": 352,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_37@3",
            "content": "The language pairs with fewer resources (fi\u2194en, lt\u2194en) benefit from SCONES training much more than the high-resource language pairs (de\u2194en).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_37",
            "start": 354,
            "end": 493,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_38@0",
            "content": "Decoding speed",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_38",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@0",
            "content": "Our softmax-based models reach their (near) optimum BLEU score with a beam size of around 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@1",
            "content": "Most of our SCONES models can achieve similar or better BLEU scores with greedy search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 93,
            "end": 179,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@2",
            "content": "Replacing beam-4 search with greedy search corresponds to a 3.9x speed-up (2.76 \u2192 10.64 sentences per second) on an entry-level NVIDIA Quadro P1000 GPU with a batch size of 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 181,
            "end": 355,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@3",
            "content": "8 Fig. 2 shows the BLEU scores for all six translation directions as a function of decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 357,
            "end": 454,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@4",
            "content": "Most of the speed-ups are due to choosing a smaller beam size and not due to SCONES avoiding the normalization over the full vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 456,
            "end": 591,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_39@5",
            "content": "We expect further speed-ups from SCONES when comparing models with larger vocabularies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_39",
            "start": 593,
            "end": 679,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_40@0",
            "content": "Mitigating the beam search curse",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_40",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@0",
            "content": "One of the most irksome pathologies of traditional softmax-based NMT models is the \"beam search curse\" (Koehn and Knowles, 2017): larger beam sizes improve the log-probability of the translations, but the translation quality gets worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@1",
            "content": "This happens because with large beam sizes, the model prefers translations that are too short.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 237,
            "end": 330,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@2",
            "content": "This phenomenon has been linked to the local normalization in sequence models (Sountsov and Sarawagi, 2016;Murray and Chiang, 2018) and poor model calibration (Kumar and Sarawagi, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 332,
            "end": 517,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@3",
            "content": "Stahlberg and Byrne (2019) showed that modes are often empty and suggested that the inherent bias of the model towards short translations is often obscured by beam search errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 519,
            "end": 696,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@4",
            "content": "Anonymous (2022) provided strong evidence that this length deficiency is due to the intrinsic uncertainty of the MT task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 698,
            "end": 818,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_41@5",
            "content": "Given that models trained with SCONES explicitly take into account inherent uncertainty, we ran an experiment to determine whether these models are more robust to the beam search curse compared to softmax trained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_41",
            "start": 820,
            "end": 1039,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_42@0",
            "content": "Beam search (beam size = 4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_42",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@0",
            "content": "Exact search de-en en-de fi-en en-fi lt-en en-lt de-en en-de fi-en en-fi lt-en en-lt Fig. 3 plots the BLEU score as a function of the beam size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@1",
            "content": "The sharp decline of the green curve for large beam sizes reflects the beam search curse for the softmax baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 145,
            "end": 258,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@2",
            "content": "SCONES seems to be less affected at larger beam sizes, particularly for small \u03b1values: the BLEU score for SCONES with \u03b1 0.2 (solid purple curve) is stable for beam sizes greater than 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 260,
            "end": 446,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@3",
            "content": "Fig. 4, which displays the length ratio (the hypothesis length divided by the reference length) versus beam size, suggests that the differences in BLEU trajectories arise due to translation lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 448,
            "end": 645,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@4",
            "content": "Translations obtained using softmax become abruptly shorter at higher beam sizes whereas for SCONES with \u03b1 = 0.2, there is no such steep decrease in length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 647,
            "end": 802,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@5",
            "content": "To study the impact of \u03b1 in the absence of beam search errors we ran the exact depth-first search algorithm of Stahlberg and Byrne (2019) to find the translation with global highest probability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 804,
            "end": 997,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@6",
            "content": "9 The adequacy of the translations found by exact search depends heavily on \u03b1 (Fig. 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 999,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@7",
            "content": "With exact search, small \u03b1-values yield adequate translations, but \u03b1 \u2248 1.0 performs similar to the softmax baseline: the BLEU score drops because hypotheses are too short.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1087,
            "end": 1257,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@8",
            "content": "Table 5 shows that SCONES with \u03b1 = 0.2 consistently outperforms the softmax baselines by a large margin with exact search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1259,
            "end": 1380,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@9",
            "content": "Fig. 6 sheds some light on why SCONES with small \u03b1 does not prefer empty translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1382,
            "end": 1467,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@10",
            "content": "9 The maximum number of explored states per sentence was set to 1M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1469,
            "end": 1535,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@11",
            "content": "This threshold was reached for less than 1.45% of the German-English sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1537,
            "end": 1615,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@12",
            "content": "See Appendix A for other language directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1617,
            "end": 1661,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@13",
            "content": "A small \u03b1 leads to a larger gap between the logprobabilities of the exact search translation and the empty translation that arises from higher logprobabilities for the exact-search translation along with smaller variances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1663,
            "end": 1884,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@14",
            "content": "Intuitively, a small \u03b1 reduces the importance of the negative loss component L \u2212 (\u2022) in Eq. 6, and thus biases each binary classifier towards predicting the true label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 1886,
            "end": 2053,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@15",
            "content": "els: the percentage of search errors remains at a relatively high level of around 20% even for very large beam sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2055,
            "end": 2171,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@16",
            "content": "Increasing the beam size is most effective in reducing the number of search errors for SCONES with a small value of \u03b1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2173,
            "end": 2290,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@17",
            "content": "However, a small \u03b1 does not always yield the best overall BLEU score (Fig. 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2292,
            "end": 2369,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@18",
            "content": "Taken together, these observations provide an insight into model errors in NMT: If we describe the \"model error\" as the mismatch between the global most likely translation and an adequate translation (following Stahlberg and Byrne ( 2019)), a small \u03b1 would simultaneously lead to both fewer search errors (Fig. 7) and fewer model errors (Tab. 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2371,
            "end": 2716,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@19",
            "content": "Counter-intuitively, however, BLEU scores peak at slightly higher \u03b1-values (Tab. 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2718,
            "end": 2801,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_43@20",
            "content": "A more sophisticated notion of model errors and search errors is needed to understand the complex inherent biases of beam search for neural sequence-to-sequence models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_43",
            "start": 2803,
            "end": 2970,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_44@0",
            "content": "Reducing the number of beam search errors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_44",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_45@0",
            "content": "Experiments with synthetic language pairs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_45",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_46@0",
            "content": "Our main motivation for SCONES is to equip the model to naturally represent intrinsic uncertainty, i.e. the existence of multiple correct target sentences for the same source sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_46",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_46@1",
            "content": "To examine the characteristics of SCONES as a function of uncertainty, we generated synthetic language pairs that differ by the level of ambiguity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_46",
            "start": 185,
            "end": 331,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_46@2",
            "content": "For this purpose, we trained an IBM-3 model (Brown et al., 1993) on the German-English training data after subword segmentation using MGIZA (Gao and Vogel, 2008).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_46",
            "start": 333,
            "end": 494,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_46@3",
            "content": "IBM-3 is a generative symbolic model that describes the translation process from one language into another with a generative story, and was popular for finding word alignments for statistical (phrase-based) machine translation (Koehn, 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_46",
            "start": 496,
            "end": 736,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_47@0",
            "content": "The generative story consists of different steps such as distortion (word reordering), fertility (1:n word mappings), and lexical translation (word-to-word translation) that describe the translation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_47",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@0",
            "content": "The parameters of IBM-3 define probability distributions for each step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@1",
            "content": "In this work we do not use IBM-3 for finding word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 72,
            "end": 132,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@2",
            "content": "Instead, for the original German sentences we sample synthetic English-like translations from the model with different sampling temperatures to control the ambiguity levels of the translation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 134,
            "end": 330,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@3",
            "content": "A low sampling temperature generates sentence pairs that still capture some of the characteristics of MT such as word reorderings, but the mapping is mostly deterministic (i.e. the same source token is almost always translated to the same target token).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 332,
            "end": 584,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@4",
            "content": "A high temperature corresponds to more randomness, i.e. more intrinsic uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 586,
            "end": 668,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@5",
            "content": "Appendix B contains more details about sampling from IBM-3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 670,
            "end": 728,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@6",
            "content": "We train NMT models using either softmax or SCONES on the synthetic corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 730,
            "end": 805,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@7",
            "content": "Fig. 8 shows that softmax and SCONES perform similarly using beam search: high IBM-3 sampling temperature translation tasks are less predictable, and thus lead to lower BLEU scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 807,
            "end": 987,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@8",
            "content": "The difference between both approaches becomes clear with exact search (Fig. 9).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 989,
            "end": 1068,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@9",
            "content": "While the translations with the global highest probability for high IBM-3 sampling temperatures are heavily degraded for softmax and SCONES with \u03b1 = 1, the drop is much less dramatic for SCONES with \u03b1 = 0.2 (solid purple curve).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 1070,
            "end": 1297,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_48@10",
            "content": "Setting \u03b1 to a low value enables the model to assign its highest probability to adequate translations, even when the translation task is highly uncertain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_48",
            "start": 1299,
            "end": 1452,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_49@0",
            "content": "Related work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_49",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@0",
            "content": "Our approach draws insights from multi-label classification (MLC) (Tsoumakas and Katakis, 2007;Zhou, 2006, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@1",
            "content": "One of the earliest approaches for MLC was to transform the problem into multiple binary classification problems while ignoring the correlations between labels (Boutell et al., 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 114,
            "end": 296,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@2",
            "content": "More recent work has modeled MLC in the sequence-to-sequence framework with a decoder that generates the labels sequentially, thus preserving the inter-label correlations (Yang et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 298,
            "end": 488,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@3",
            "content": "Most prior work in MLC focuses on classification and is not directly applicable to MT. In contrast, our training strategy is tailored for sequence-to-sequence problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 490,
            "end": 657,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@4",
            "content": "Unlike prior work (Yang et al., 2018), SCONES allows us to perform MLC style training with any underlying NMT architecture by simply changing the loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 659,
            "end": 818,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@5",
            "content": "By jointly training all label-specific binary classifiers, our strategy is able to account for label correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 820,
            "end": 933,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@6",
            "content": "used an MLC objective to improve machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 935,
            "end": 987,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@7",
            "content": "Unlike our approach, they attempted to predict all words in the target sentence with a bag-of-words loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 989,
            "end": 1102,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_50@8",
            "content": "We formulate the next word prediction at each time step as a MLC problem to handle intrinsic uncertainty, but our models are predicting ordered target sequences, not bags of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_50",
            "start": 1104,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_51@0",
            "content": "The speed-ups from SCONES can be partially attributed to avoiding the normalization of the output over the full vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_51",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_51@1",
            "content": "The same idea motivated earlier work on self-normalized training Mnih and Teh, 2012;Devlin et al., 2014;Goldberger and Melamud, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_51",
            "start": 124,
            "end": 257,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_51@2",
            "content": "As described in Sec. 2, unlike work on self-normalization, SCONES does not try to approximate a distribution over the full vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_51",
            "start": 259,
            "end": 392,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_51@3",
            "content": "Rather, its output consists of multiple binary classifiers that do not share probability mass by design to be able to better represent intrinsic uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_51",
            "start": 394,
            "end": 550,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_52@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_52",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@0",
            "content": "Machine translation is a task with high intrinsic uncertainty: a source sentence can have multiple valid translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@1",
            "content": "We demonstrated that NMT models and specifically Transformers, can learn to model mutually non-exclusive target sentences from single-label training data using our SCONES loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 119,
            "end": 303,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@2",
            "content": "Rather than learn a single distribution over all target sentences, SCONES learns multiple binary classifiers that indicate whether or not a target sentence is a valid translation of the source sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 305,
            "end": 506,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@3",
            "content": "SCONES yields improved translation quality over conventional softmax-based models for six different translation directions, or (alternatively) speed-ups of up to 3.9x without any degradation in translation performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 508,
            "end": 725,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@4",
            "content": "We showed that SCONES can be tuned to mitigate the beam search curse and the problem of inadequate and empty modes in standard NMT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 727,
            "end": 857,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_53@5",
            "content": "Our experiments on synthetic language translation suggest that, unlike softmaxtrained models, SCONES models are able to assign their highest probability to adequate translations even when the underlying task is highly ambiguous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_53",
            "start": 859,
            "end": 1086,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_54@0",
            "content": "The SCONES loss function is easy to implement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_54",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_54@1",
            "content": "Adapting standard softmax-based sequenceto-sequence architectures such as Transformers requires only replacing the cross-entropy loss function with SCONES and the softmax with sigmoid activations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_54",
            "start": 47,
            "end": 242,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_54@2",
            "content": "The remaining parts of the training and inference pipelines can be kept unchanged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_54",
            "start": 244,
            "end": 325,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_54@3",
            "content": "SCONES can be potentially useful in handling uncertainty for a variety of ambiguous NLP problems beyond translation, such as generation and dialog.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_54",
            "start": 327,
            "end": 473,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_54@4",
            "content": "We expect this work to encourage research on modeling techniques that can address ambiguity in much better ways compared to current models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_54",
            "start": 475,
            "end": 613,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@0",
            "content": "The exact search algorithm of Stahlberg and Byrne (2019) we used in the paper is guaranteed to find the global best translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@1",
            "content": "Its runtime, however, varies greatly between language pairs and source sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 129,
            "end": 209,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@2",
            "content": "Therefore, we limit the number of explored states per sentence by 1M to keep the decoding time under control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 211,
            "end": 319,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@3",
            "content": "If the 1M threshold is reached, the optimality of the found translation is not guaranteed anymore.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 321,
            "end": 418,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@4",
            "content": "Fortunately, for most of our models and test sets, exact search was able to find and verify the global best translation earlier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 420,
            "end": 547,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@5",
            "content": "Table 6 lists the runs for which a fraction of the sentences did not terminate before 1M steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 549,
            "end": 643,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_55@6",
            "content": "In these rare cases, we use the best translation found thus far by exact search as an approximation to the global best translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_55",
            "start": 645,
            "end": 775,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_56@0",
            "content": "The parameters of the IBM-3 model (Brown et al., 1993)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_56",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_57@0",
            "content": "(\u03b1 = 0.2) 5.31% synthetic-0.1 Softmax 1.05% synthetic-0.1 SCONES (\u03b1 = 0.2) 0.55% synthetic-0.1 SCONES (\u03b1 = 0.5) 1.10% synthetic-0.1 SCONES (\u03b1 = 1.0) 1.25% synthetic-0.2 Softmax 1.00% synthetic-0.2 SCONES (\u03b1 = 0.2) 5.10% synthetic-0.2 SCONES (\u03b1 = 0.5) 7.65% synthetic-0.2 SCONES (\u03b1 = 1.0) 2.65% synthetic-0.3 Softmax 0.10% synthetic-0.3 SCONES (\u03b1 = 0.2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_57",
            "start": 0,
            "end": 351,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_58@0",
            "content": "12.6% synthetic-0.3 SCONES (\u03b1 = 0.5) 17.3% synthetic-0.3 SCONES (\u03b1 = 1.0)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_58",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_59@0",
            "content": "1.80% synthetic-0.5 SCONES (\u03b1 = 0.2) 25.0% synthetic-0.5 SCONES (\u03b1 = 0.5) 25.2% synthetic-0.7 SCONES (\u03b1 = 0.2) 25.9% synthetic-0.7 SCONES (\u03b1 = 0.5) 20.3% process produces the target language sentence y from a source language sentence x (Knight, 1999):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_59",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_60@0",
            "content": "1. For each source word x i indexed by i = 1, 2, . . . , |x|, choose the fertility \u03c6 i with probability n(\u03c6 i |x i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_60",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_61@0",
            "content": "2. Choose the number \u03c6 0 of \"spurious\" target words to be generated from x 0 = NULL, using probability p 1 and the sum of fertilities from step 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_61",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_62@0",
            "content": "Let",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_62",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_63@0",
            "content": "m = |x| i=0 \u03c6 i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_63",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_64@0",
            "content": "4. For each i = 0, 1, 2, . . . , |x| and each k = 1, 2, . . . , \u03c6 i , choose a target word \u03c4 ik with probability t(\u03c4 ik |x i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_64",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_65@0",
            "content": "5. For each i = 1, 2, . . . , |x| and each k = 1, 2, . . . , \u03c6 i , choose a target position \u03c0 ik with probability d(\u03c0 ik |i, |x|, m).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_65",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_66@0",
            "content": "6. For each k = 1, 2, . . . , \u03c6 0 , choose a position \u03c0 0k from the \u03c6 0 \u2212 k + 1 remaining vacant positions in 1, 2, . . . , m, for a total probability of 1 \u03c6 0 !",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_66",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_66@1",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_66",
            "start": 162,
            "end": 162,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_66@2",
            "content": "7. Output the target sentence with words \u03c4 ik in positions \u03c0 ik (0",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_66",
            "start": 164,
            "end": 229,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_67@0",
            "content": "\u2264 i \u2264 |x|, 1 \u2264 k \u2264 \u03c6 i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_67",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@0",
            "content": "First, we estimate the IBM-3 model parameters using the MGIZA (Gao and Vogel, 2008) l o s s = a * ( j n p .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@1",
            "content": "sum ( a l l _ f a l s e _ x e n t , a x i s =\u22121) \u2212 t g t _ f a l s e _ x e n t ) + t g t _ t r u e _ x e n t 20 w e i g h t s = j n p .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 108,
            "end": 242,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@2",
            "content": "where ( t a r g e t s > 0 , 1 , 0 ) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 244,
            "end": 280,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@3",
            "content": "a s t y p e ( j n p . f l o a t 3 2 ) # PAD ID i s 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 282,
            "end": 335,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@4",
            "content": "21 r e t u r n l o s s * w e i g h t s / w e i g h t s .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 337,
            "end": 392,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_68@5",
            "content": "sum ( )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_68",
            "start": 394,
            "end": 400,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_69@0",
            "content": "Figure 10: JAX implementation of the SCONES loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_69",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_70@0",
            "content": "for numerical stability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_70",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_70@1",
            "content": "The JAX implementation generalizes the SCONES loss defined in the main paper in Eq. 6 with a label smoothing (Szegedy et al., 2016) factor \u03bb \u2208 [0, 1] (l in Fig. 10) such that the positive loss component L + (\u2022) becomes the following cross-entropy:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_70",
            "start": 25,
            "end": 271,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_71@0",
            "content": "L + (x, y, i) = \u2212 (1 \u2212 \u03bb) log P (z x,y \u2264i = 1|x, y <i ) \u2212 \u03bb log P (z x,y \u2264i = 0|x, y <i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_71",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_72@0",
            "content": "(11) Similarly, the negative loss component L \u2212 (\u2022) with label smoothing can be written as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_72",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_73@0",
            "content": "L \u2212 (x, y, i) = \u2212 w\u2208V\\{y i }",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_73",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_74@0",
            "content": "(1 \u2212 \u03bb) log P (z x,y <i \u2022w = 0|x, y <i ) + \u03bb log P (z x,y <i \u2022w = 1|x, y <i ) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_74",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_75@0",
            "content": "(12) The label smoothing extension is provided for the sake of completeness -we did not use label smoothing in any of the experiments in the main paper since it did not yield improvements in our setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_75",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_76@0",
            "content": "UNKNOWN, None, , Anonymous. 2022. Anonymous. (under ARR review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_76",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_77@0",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_77",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_78@0",
            "content": "Matthew Boutell, Jiebo Luo, Xipeng Shen, Christopher Brown, Learning multilabel scene classification, 2004, Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_78",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2018, JAX: Composable transformations of Python+NumPy programs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_79",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_80@0",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_80",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_81@0",
            "content": "UNKNOWN, None, 2009, Aleatory or epistemic? Does it matter? Structural safety, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_81",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_82@0",
            "content": "Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Fast and robust neural network joint models for statistical machine translation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_82",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_83@0",
            "content": "Markus Dreyer, Daniel Marcu, HyTER: Meaning-equivalent semantics for translation evaluation, 2012, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_83",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_84@0",
            "content": "Bryan Eikema, Wilker Aziz, Is MAP decoding all you need? the inadequacy of the mode in neural machine translation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_84",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_85@0",
            "content": "Qin Gao, Stephan Vogel, Parallel implementations of word alignment tool, 2008, Software Engineering, Testing, and Quality Assurance for Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_85",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_86@0",
            "content": "Jacob Goldberger, Oren Melamud, Selfnormalization properties of language modeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_86",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_87@0",
            "content": "Michael Gutmann, Aapo Hyv\u00e4rinen, Noisecontrastive estimation: A new estimation principle for unnormalized statistical models, 2010, Proceedings of the thirteenth international conference on artificial intelligence and statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_87",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_88@0",
            "content": "Nal Kalchbrenner, Phil Blunsom, Recurrent continuous translation models, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_88",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_89@0",
            "content": "Kevin Knight, A statistical mt tutorial workbook, 1999, Prepared for the 1999 JHU Summer Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_89",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_90@0",
            "content": "Philipp Koehn, Statistical significance tests for machine translation evaluation, 2004, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_90",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2009, Statistical machine translation, Cambridge University Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_91",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_92@0",
            "content": "Philipp Koehn, Rebecca Knowles, Six challenges for neural machine translation, 2017, Proceedings of the First Workshop on Neural Machine Translation, Vancouver. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_92",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_93@0",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_93",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2019, Calibration of encoder decoder models for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_94",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_95@0",
            "content": "Shuming Ma, Xu Sun, Yizhong Wang, Junyang Lin, Bag-of-words as target for neural machine translation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_95",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_96@0",
            "content": "Andriy Mnih, Yee Whye Teh, A fast and simple algorithm for training neural probabilistic language models, 2012, Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, Omnipress.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_96",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_97@0",
            "content": "Kenton Murray, David Chiang, Correcting length bias in neural machine translation, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_97",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_98@0",
            "content": "Myle Ott, Michael Auli, David Grangier, Marc'aurelio Ranzato, Analyzing uncertainty in neural machine translation, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_98",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_99@0",
            "content": "Sebastian Pad\u00f3, Daniel Cer, Michel Galley, Dan Jurafsky, Christopher D Manning, Measuring machine translation quality as semantic equivalence: A metric based on entailment features, 2009, Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_99",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_100@0",
            "content": "Miika Pihlaja, Michael Gutmann, Aapo Hyv\u00e4rinen, A family of computationally efficient and simple estimators for unnormalized statistical models, 2010, Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI'10, AUAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_100",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_101@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_101",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_102@0",
            "content": "Pavel Sountsov, Sunita Sarawagi, Length bias in encoder decoder models and a case for global conditioning, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_102",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_103@0",
            "content": "Felix Stahlberg, Bill Byrne, On NMT search errors and model errors: Cat got your tongue?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_103",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_104@0",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_104",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_105@0",
            "content": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Rethinking the inception architecture for computer vision, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_105",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_106@0",
            "content": ", Grigorios Tsoumakas and Ioannis Katakis, 2007, Int J Data Warehousing and Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_106",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_107@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_107",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_108@0",
            "content": "Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, Houfeng Wang, SGM: Sequence generation model for multi-label classification, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_108",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_109@0",
            "content": "Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh, Large batch optimization for deep learning: Training BERT in 76 minutes, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_109",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_110@0",
            "content": "Min-Ling Zhang, Zhi-Hua Zhou, Multilabel neural networks with applications to functional genomics and text categorization, 2006, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_110",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "383-ARR_v1_111@0",
            "content": "Min-Ling Zhang, Zhi-Hua Zhou, A review on multi-label learning algorithms, 2014, IEEE Transactions on Knowledge and Data Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "383-ARR_v1_111",
            "start": 0,
            "end": 134,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_1",
            "tgt_ix": "383-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_1",
            "tgt_ix": "383-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_4",
            "tgt_ix": "383-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_6",
            "tgt_ix": "383-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_7",
            "tgt_ix": "383-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_8",
            "tgt_ix": "383-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_10",
            "tgt_ix": "383-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_11",
            "tgt_ix": "383-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_12",
            "tgt_ix": "383-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_13",
            "tgt_ix": "383-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_14",
            "tgt_ix": "383-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_15",
            "tgt_ix": "383-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_16",
            "tgt_ix": "383-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_17",
            "tgt_ix": "383-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_19",
            "tgt_ix": "383-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_20",
            "tgt_ix": "383-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_21",
            "tgt_ix": "383-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_22",
            "tgt_ix": "383-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_23",
            "tgt_ix": "383-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_24",
            "tgt_ix": "383-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_25",
            "tgt_ix": "383-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_26",
            "tgt_ix": "383-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_27",
            "tgt_ix": "383-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_29",
            "tgt_ix": "383-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_30",
            "tgt_ix": "383-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_31",
            "tgt_ix": "383-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_28",
            "tgt_ix": "383-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_33",
            "tgt_ix": "383-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_33",
            "tgt_ix": "383-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_36",
            "tgt_ix": "383-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_36",
            "tgt_ix": "383-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_37",
            "tgt_ix": "383-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_38",
            "tgt_ix": "383-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_38",
            "tgt_ix": "383-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_42",
            "tgt_ix": "383-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_40",
            "tgt_ix": "383-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_40",
            "tgt_ix": "383-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_40",
            "tgt_ix": "383-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_40",
            "tgt_ix": "383-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_44",
            "tgt_ix": "383-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_46",
            "tgt_ix": "383-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_47",
            "tgt_ix": "383-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_45",
            "tgt_ix": "383-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_45",
            "tgt_ix": "383-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_45",
            "tgt_ix": "383-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_45",
            "tgt_ix": "383-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_49",
            "tgt_ix": "383-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_49",
            "tgt_ix": "383-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_49",
            "tgt_ix": "383-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_51",
            "tgt_ix": "383-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_56",
            "tgt_ix": "383-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_57",
            "tgt_ix": "383-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_58",
            "tgt_ix": "383-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_59",
            "tgt_ix": "383-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_60",
            "tgt_ix": "383-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_33",
            "tgt_ix": "383-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_61",
            "tgt_ix": "383-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_63",
            "tgt_ix": "383-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_64",
            "tgt_ix": "383-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_65",
            "tgt_ix": "383-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_66",
            "tgt_ix": "383-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_67",
            "tgt_ix": "383-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_69",
            "tgt_ix": "383-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_70",
            "tgt_ix": "383-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_71",
            "tgt_ix": "383-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_72",
            "tgt_ix": "383-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_73",
            "tgt_ix": "383-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_74",
            "tgt_ix": "383-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "383-ARR_v1_0",
            "tgt_ix": "383-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_1",
            "tgt_ix": "383-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_2",
            "tgt_ix": "383-ARR_v1_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_3",
            "tgt_ix": "383-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_4",
            "tgt_ix": "383-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_4",
            "tgt_ix": "383-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_4",
            "tgt_ix": "383-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_4",
            "tgt_ix": "383-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_5",
            "tgt_ix": "383-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_6",
            "tgt_ix": "383-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_6",
            "tgt_ix": "383-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_6",
            "tgt_ix": "383-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_7",
            "tgt_ix": "383-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_7",
            "tgt_ix": "383-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_7",
            "tgt_ix": "383-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_7",
            "tgt_ix": "383-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_8",
            "tgt_ix": "383-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_8",
            "tgt_ix": "383-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_8",
            "tgt_ix": "383-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_9",
            "tgt_ix": "383-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_10",
            "tgt_ix": "383-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_10",
            "tgt_ix": "383-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_10",
            "tgt_ix": "383-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_10",
            "tgt_ix": "383-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_11",
            "tgt_ix": "383-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_12",
            "tgt_ix": "383-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_12",
            "tgt_ix": "383-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_13",
            "tgt_ix": "383-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_14",
            "tgt_ix": "383-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_15",
            "tgt_ix": "383-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_16",
            "tgt_ix": "383-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_17",
            "tgt_ix": "383-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_18",
            "tgt_ix": "383-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_19",
            "tgt_ix": "383-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_20",
            "tgt_ix": "383-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_21",
            "tgt_ix": "383-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_22",
            "tgt_ix": "383-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_23",
            "tgt_ix": "383-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_24",
            "tgt_ix": "383-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_25",
            "tgt_ix": "383-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_25",
            "tgt_ix": "383-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_26",
            "tgt_ix": "383-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_27",
            "tgt_ix": "383-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_27",
            "tgt_ix": "383-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_27",
            "tgt_ix": "383-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_28",
            "tgt_ix": "383-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_28",
            "tgt_ix": "383-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_28",
            "tgt_ix": "383-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_29",
            "tgt_ix": "383-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_30",
            "tgt_ix": "383-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_31",
            "tgt_ix": "383-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_31",
            "tgt_ix": "383-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_32",
            "tgt_ix": "383-ARR_v1_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_33",
            "tgt_ix": "383-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_34",
            "tgt_ix": "383-ARR_v1_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_35",
            "tgt_ix": "383-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_36",
            "tgt_ix": "383-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_37",
            "tgt_ix": "383-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_37",
            "tgt_ix": "383-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_37",
            "tgt_ix": "383-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_37",
            "tgt_ix": "383-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_38",
            "tgt_ix": "383-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_39",
            "tgt_ix": "383-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_40",
            "tgt_ix": "383-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_41",
            "tgt_ix": "383-ARR_v1_41@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_42",
            "tgt_ix": "383-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@18",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@19",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_43",
            "tgt_ix": "383-ARR_v1_43@20",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_44",
            "tgt_ix": "383-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_45",
            "tgt_ix": "383-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_46",
            "tgt_ix": "383-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_46",
            "tgt_ix": "383-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_46",
            "tgt_ix": "383-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_46",
            "tgt_ix": "383-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_47",
            "tgt_ix": "383-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_48",
            "tgt_ix": "383-ARR_v1_48@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_49",
            "tgt_ix": "383-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_50",
            "tgt_ix": "383-ARR_v1_50@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_51",
            "tgt_ix": "383-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_51",
            "tgt_ix": "383-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_51",
            "tgt_ix": "383-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_51",
            "tgt_ix": "383-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_52",
            "tgt_ix": "383-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_53",
            "tgt_ix": "383-ARR_v1_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_54",
            "tgt_ix": "383-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_55",
            "tgt_ix": "383-ARR_v1_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_56",
            "tgt_ix": "383-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_57",
            "tgt_ix": "383-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_58",
            "tgt_ix": "383-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_59",
            "tgt_ix": "383-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_60",
            "tgt_ix": "383-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_61",
            "tgt_ix": "383-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_62",
            "tgt_ix": "383-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_63",
            "tgt_ix": "383-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_64",
            "tgt_ix": "383-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_65",
            "tgt_ix": "383-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_66",
            "tgt_ix": "383-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_66",
            "tgt_ix": "383-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_66",
            "tgt_ix": "383-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_67",
            "tgt_ix": "383-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_68",
            "tgt_ix": "383-ARR_v1_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_69",
            "tgt_ix": "383-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_70",
            "tgt_ix": "383-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_70",
            "tgt_ix": "383-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_71",
            "tgt_ix": "383-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_72",
            "tgt_ix": "383-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_73",
            "tgt_ix": "383-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_74",
            "tgt_ix": "383-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_75",
            "tgt_ix": "383-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_76",
            "tgt_ix": "383-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_77",
            "tgt_ix": "383-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_78",
            "tgt_ix": "383-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_79",
            "tgt_ix": "383-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_80",
            "tgt_ix": "383-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_81",
            "tgt_ix": "383-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_82",
            "tgt_ix": "383-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_83",
            "tgt_ix": "383-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_84",
            "tgt_ix": "383-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_85",
            "tgt_ix": "383-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_86",
            "tgt_ix": "383-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_87",
            "tgt_ix": "383-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_88",
            "tgt_ix": "383-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_89",
            "tgt_ix": "383-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_90",
            "tgt_ix": "383-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_91",
            "tgt_ix": "383-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_92",
            "tgt_ix": "383-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_93",
            "tgt_ix": "383-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_94",
            "tgt_ix": "383-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_95",
            "tgt_ix": "383-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_96",
            "tgt_ix": "383-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_97",
            "tgt_ix": "383-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_98",
            "tgt_ix": "383-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_99",
            "tgt_ix": "383-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_100",
            "tgt_ix": "383-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_101",
            "tgt_ix": "383-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_102",
            "tgt_ix": "383-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_103",
            "tgt_ix": "383-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_104",
            "tgt_ix": "383-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_105",
            "tgt_ix": "383-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_106",
            "tgt_ix": "383-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_107",
            "tgt_ix": "383-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_108",
            "tgt_ix": "383-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_109",
            "tgt_ix": "383-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_110",
            "tgt_ix": "383-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "383-ARR_v1_111",
            "tgt_ix": "383-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1151,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "383-ARR",
        "version": 1
    }
}