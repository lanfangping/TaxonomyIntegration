{
    "nodes": [
        {
            "ix": "429-ARR_v1_0",
            "content": "Speeding Up Entmax",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_2",
            "content": "Softmax is the de facto standard for normalizing logits in modern neural networks for language processing. However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation. \u03b1entmax of Peters et al. ( 2019) solves this problem, but is unfortunately slower than softmax.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_3",
            "content": "In this paper, we propose an alternative to \u03b1entmax, which keeps its virtuous characteristics, but is as fast as optimized softmax and achieves on par or better performance in machine translation task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "429-ARR_v1_5",
            "content": "Sparseness of vector representations is a desirable trait in neural network models for natural language processing (NLP): words (subwords) are discrete objects by their nature, and, accordingly, are encoded by one-hot embeddings at the input and output of neural networks. However, to predict a categorical response in neural models, softmax is most often used, which produces a dense probability distribution, i.e. every category (word/subword) receives a non-zero probability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_6",
            "content": "Recent studies suggest that it is this output density that poses problems when the trained NLP model is used for inference. For example, in the case of text generation, unconstrained sampling from a trained language model results in poor quality of the resulting text (Holtzman et al., 2020). In neural machine translation (NMT), exact decoding from a trained model often results in empty text (Stahlberg and Byrne, 2019). 1 To get around these problems, constrained decoding techniques have been proposed, most of which artificially impose sparsity on softmax prediction. For example, Fan et al. (2018) propose to sample from the top-k probable words, and Holtzman et al. (2020) propose to sample from the most probable words, which comprise the cumulative probability p. While these methods are effective, they are ad-hoc solutions that lead to a mismatch between how the model is trained and how it is used at inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_7",
            "content": "In this regard, the works on sparse alternatives to softmax stand apart since they allow us to make inference from the model in the same way than it was trained. Some of the most successful and elegant solutions are sparsemax (Martins and Astudillo, 2016) and its generalization \u03b1-entmax (Peters et al., 2019). When coupled with suitable losses, these transformations are not inferior to softmax, and sometimes even surpass it as measured with final performance metrics on a number of tasks. A problem with these transformations however is that they are significantly slower than softmax when the number of categories (vocabulary size) is tens of thousands, as in the case of text generation. This is because \u03b1-entmax transformation-in its original formulation-requires sorting over the logits. 2 In this work, we ask the question: is it possible to obtain a sparse output like that of \u03b1-entmax, but without its degradation in computational speed? Our answer is affirmative-we propose a sparse output transformation that \u2022 is on par or superior to softmax and \u03b1-entmax in the NMT tasks,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_8",
            "content": "\u2022 works as fast as softmax during training and at inference, \u2022 gives the same training dynamics as \u03b1-entmax (in training steps).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_9",
            "content": "The most surprising thing is that such a transformation is simply a shifted ReLU raised to power 1 \u03b1\u22121 , which we call \u03b1-ReLU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_10",
            "content": "The rest of the paper is organised as follows. In Sect. 2 we motivate the choice of \u03b1-ReLU as the output transformation, and also select an appropriate loss function. In Sect. 3 we experimentally confirm our claims about performance and output speed of \u03b1-ReLU in the NMT task. Sect. 4 is devoted to a comparative analysis of \u03b1-ReLU and \u03b1-entmax in terms of sparsity, ability to solve the empty translation problem, and training dynamics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_11",
            "content": "\u03b1-ReLU at Output",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "429-ARR_v1_12",
            "content": "Our departure point is the \u03b1-entmax transformation of Peters et al. (2019) which can be defined for",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_13",
            "content": "z \u2208 R d as \u03b1-entmax i (z) = [(\u03b1 \u2212 1)z i \u2212 \u03c4 (z)] 1 \u03b1\u22121 + ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_14",
            "content": "where [x] + := max{x, 0}, and \u03c4 : R d \u2192 R is the (unique) function that satisfies j [(\u03b1 \u2212 1)z j \u2212 \u03c4 (z)]",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_15",
            "content": "1 \u03b1\u22121 + = 1 for any z.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_16",
            "content": "It is this threshold \u03c4 that makes the computation of \u03b1-entmax slow, because one needs to sort the components of z to find \u03c4 (Peters et al., 2019, Alg. 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_17",
            "content": "As we can see, the threshold \u03c4 is only needed to ensure that \u03b1-entmax(z) is a probability distribution. We loosen this constraint, and only require non-negative weights, which is sufficient for most uses. Consider then a transformation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_18",
            "content": "\u03b1-ReLU i (z) := [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 + , (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_19",
            "content": "where \u03c4 is a constant that does not depend on z. In order to force \u03b1-ReLU(z)-applied to the logits zto converge to the one-hot vector e y of the gold label y we need to adjust the corresponding loss. This can easily be done by feeding the logits z and the output \u03b1-ReLU(z) into the following loss, which we call \u03b1-ReLU loss. (Tsallis, 1988). The rationale for coupling \u03b1-ReLU with the loss (2) is the following Lemma 1. For any \u03c4 \u2208 R, the gradient of the \u03b1-ReLU loss (2) is given by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_20",
            "content": "(z, y) = (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1\u22121 1 + H \u03b1 [\u03b1-ReLU(z)], (2) where H \u03b1 [p] := 1 \u03b1(\u03b1\u22121) 1 \u2212 j p \u03b1 j , \u03b1 = 1, is the Tsallis \u03b1-entropy",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_21",
            "content": "\u2207 z (z, y) = \u03b1-ReLU(z) \u2212 e y .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_22",
            "content": "Proof. The proof is in Appendix B.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_23",
            "content": "By Lemma 1, gradient-based minimization of indeed forces \u03b1-ReLU(z) \u2192 e y . Notice that this is similar to what happens when the softmax normalization is coupled with the cross-entropy loss or when \u03b1-entmax is coupled with the entmax loss. In both cases differentiating the loss with respect to logits gives p\u2212e y , where p is either softmax(z) or \u03b1-entmax(z) (Martins and Astudillo, 2016;Peters et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_24",
            "content": "Remark. Recall that \u03b1-entmax is a generalization of sparsemax. For example, 2-entmax is essentially sparsemax, and for \u03b1 \u2208 (1, 2) we get a smoothed version of sparsemax. Similarly, \u03b1-ReLU is a kind of generalization of ReLU. So, the standard ReLU is 2-ReLU (with \u03c4 = 0), and for \u03b1 \u2208 (1, 2) we get a smoothed ReLU (see Fig. 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_25",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "429-ARR_v1_26",
            "content": "In theory, nothing prevents \u03b1-ReLU from learning what \u03b1-entmax is learning. However, in practice we can have a different picture, because training is conditioned by many factors-the size of the dataset, the architecture of the neural network, the optimization algorithm, etc. In this section, we compare \u03b1-ReLU empirically with \u03b1-entmax (as well as with sparsemax and softmax), assuming all other factors are fixed. The goal of these experiments is to evaluate the consequences of using \u03b1-ReLU as drop-in replacement for \u03b1-entmax.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_27",
            "content": "We test \u03b1-ReLU at output in a neural machine translation task (Sutskever et al., 2014) clearer metric of the quality of the generated textthe BLEU score (Papineni et al., 2002). As in open-ended text generation, at each prediction step, the NMT system needs to make a choice from all words (subwords) of the vocabulary, the size of which can reach several tens of thousands. Therefore, the sparsity of the output distribution becomes critical in such setups, since it can explicitly prevent the occurrence of most of the words that are inappropriate in the context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_28",
            "content": "Setup",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "429-ARR_v1_29",
            "content": "Data. We conduct experiments on three datasets of varied sizes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_30",
            "content": "\u2022 IWSLT'14 De\u2192En (Cettolo et al.) We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 10K merge operations on IWSLT, 40K merge operations on WMT En\u2192De, and 60K merge operations on WMT En\u2192Ru. We report detokenized casesensitive BLEU with SacreBLEU (Post, 2018). 4 Hyperparameters \u03b1 and \u03c4 . In all experiments we set \u03b1 = 1.5, because this value was recommended by Peters et al. (2019); Peters and Martins (2021) as the middle ground between \u03b1 = 1 (softmax) and \u03b1 = 2 (sparsmax).",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_31",
            "content": "The value for \u03c4 is chosen as follows: we run the first batch through a non-trained neural network, which has 1.5-entmax at the output, in the forward direction and determine the average \u03c4 value across the batch. This value is then used to train the 1.5-ReLU network. Our preliminary experiments have shown that 1.5-ReLU convergence is sensitive to the \u03c4 value, and that having output close to the probability distribution early in the learning phase works well with the rest of hyperparameters which are set to their default values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_32",
            "content": "Training. We trained the Transformer Base (Vaswani et al., 2017) using the OpenNMT-py 2.0 toolkit (Klein et al., 2017). Optimization details are in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_33",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "429-ARR_v1_34",
            "content": "The results are given in Table 1. Reported are test BLEU scores for best checkpoints which are selected based on validation BLEU. We observe that the 1.5-ReLU performs on par with 1.5-entmax or better, while sparsemax is inferior to all others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_35",
            "content": "Training Time. Fig. 2&3 show the training dynamics in training steps and in wall time on WMT'14 En\u2192De. Despite the closeness of performance in intermediate steps and at the end of training, we see that on the larger datasets 1.5-entmax is slower in wall time than softmax and 1.5-ReLU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_36",
            "content": "To speed up the learning process, Peters et al. ( 2019) recommended limiting the number of sorted logits in the \u03b1-entmax to the k largest logits. We tried this on the WMT'14 En\u2192De dataset using k = 100, which is the default value in the author's implementation of \u03b1-entmax. 5 The resulting training dynamics in absolute time is shown as a dashed curve in Fig. 3 (middle). As we can see, partial sorting indeed speeds up the learning process, and at the same time does not harm the quality of the translation. But in the end, learning is still slower than in the case of 1.5-ReLU. Of course, one can try to select such k that the speed of calculating the 1.5-entmax will be as close as possible to the speed of 1.5-ReLU without losing quality, but this requires additional efforts on the part of the user, and this must be done for each case separately.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_37",
            "content": "In this regard, 1.5-ReLU does not require additional fine-tuning, converges as fast as softmax in absolute time and performs on par or better. Thus 1.5-ReLU combines all three desired properties: computation speed, task performance, and sparsity of output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_38",
            "content": "Inference Time. We measured inference time of translating the WMT En\u2192Ru test data with the different strategies and with different beam sizes. The results-normalized by the smallest valueare shown in Fig. 4. As can be seen the relative difference seems independent of the beam size: softmax is almost twice faster than 1.5-entmax (with full sorting over the logits). Even though the softmax version is optimized through the softmax CUDA kernel, it performs equivalent to the 1.5-ReLU model in terms of computation speed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_39",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "429-ARR_v1_40",
            "content": "Empty Translations",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "429-ARR_v1_41",
            "content": "We remind the reader that the cat got your tongue problem (Stahlberg and Byrne, 2019) is one of the main motivations for using sparse transformations when generating text. As Peters and Martins (2021) have shown, 1.5-entmax successfully tackles this problem by significantly lowering the proportion of cases where an empty string is more likely than the beam search hypothesis. For 1.5-ReLU, we also calculated this proportion, and compared it with the proportions for softmax and sparsemax (Table 2). As we see, 1.5-ReLU also successfully tackles the cat got your tongue problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_42",
            "content": "Sparsity",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "429-ARR_v1_43",
            "content": "To compare the sparsity of 1.5-ReLU and 1.5entmax we depict in Fig. 5 the distributions of the number of zero components after applying these transformations (recall that for softmax all components are always nonzero). Since we constructed the \u03b1-ReLU in such way that it mimics the \u03b1entmax (at least in the early stages of training), we expected that these two transformations would have similar properties, including sparsity. However, this is not the case: as we can see, the 1.5-ReLU is significantly less sparse than the 1.5-entmax. It is noteworthy that lower sparsity in this case correlates with a better performance in the translation task (see Table 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_44",
            "content": "Impact of \u03c4",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "429-ARR_v1_45",
            "content": "The selection of \u03c4 was described in Section 3.1. However, the question arises: does the described approach lead to the choice of the optimal \u03c4 ? To find out, we trained the \u03b1-ReLU models for \u03c4 \u2208 {0, 0.1, 0.2, ..., 0.9, 1, 2, 5, 10} on the IWSLT data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_46",
            "content": "Note that all of these \u03c4 's have led to almost the same result at the end of the training (as predicted by Lemma 1). In Fig. 6, we present the dynamics of early training only for \u03c4 \u2208 {0, 0.1, 0.2, 0.3, 5, 10}, since the curves for \u03c4 \u2208 {0.4, ..., 0.9, 1, 2} practically coincided with the optimal curve corresponding to \u03c4 = 0.3. Note that our \u03c4 selection method gave a value of 0.33, thus we have no evidence against the adequacy of our method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_47",
            "content": "Estimation of \u03c4 without data",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "429-ARR_v1_48",
            "content": "On closer inspection, we noticed that the preentmax logits in the untrained Transformer model are distributed according to the normal law, regardless of what data is supplied to the input, Shapiro-Wilk test, p-value > 0.15. This allows us, using asymptotic theory, to estimate \u03c4 as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_49",
            "content": "\u03c4 = d model 2(d model + d vocab ) \u2022 \u03a6 \u22121 (1 \u2212 p * ),(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_50",
            "content": "where d model is the size of hidden representations, d vocab is the vocabulary size for a target language, \u03a6 \u22121 (\u2022) is the probit function and p * is the solution of a non-linear equation that involves functions related to the standard normal distribution (see Appendix B.2 for details). Table 3 compares the \u03c4 calculated by running data through an untrained model with the estimate \u03c4 obtained from (3). As we can see, \u03c4 practically coincides with \u03c4 with an accuracy of two decimal places. Unfortunately, the formula (3) is not universal: it is only true for the Transformer architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_51",
            "content": "Self-normalization",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "429-ARR_v1_52",
            "content": "The attentive reader may have noticed that the output of \u03b1-ReLU is not normalized, i.e. the components of \u03b1-ReLU(z) do not have to sum up to 1. Accordingly, the question arises: how correct is it to compare translation scores at different steps of the beam-search decoding if the conditional probabilities are not normalized? However, the comparison is possible if the \u03b1-ReLU(z) components add up to approximately the same number, i.e. if the model is self-normalizing. To check this, we ran the trained \u03b1-ReLU model on the IWSLT and WMT'14 test sets, and looked at the distribution of i \u03b1-ReLU i (z) at each decoding step. The results are shown in Fig. 7. As we can see, the sum of the \u03b1-ReLU(z) components concentrates well around its mean \u2248 1.24 (IWSLT) and 1.09 (WMT'14), which might indicate that the model indeed has a self-normalization property.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_53",
            "content": "Training Dynamics",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "429-ARR_v1_54",
            "content": "As we noted in Sect. 3.2, the training dynamics are similar in all three cases (softmax, 1.5entmax, 1.5-ReLU) when time is measured in training steps. Here we attempt to explain this phenomenon through the recently proposed Neural Tangent Kernel (NTK) approach of Jacot et al. (2018). Roughly speaking, the NTK theory suggests that a sufficiently wide neural network trains like a kernel regression. We use this theory to show (in Appendix B.3) that in all three cases the logits z(x, t) for a training instance x at a training step t evolve (approximately) according to the same differential equation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_55",
            "content": "dz dt = \u2212E (x ,y ) [K \u03c3 (x, x ) \u2022 (\u03c3(z ) \u2212 e y )],(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_56",
            "content": "where expectation is over training examples (x , y ), \u03c3(\u2022) is one of the transformations considered (softmax, \u03b1-entmax, or \u03b1-ReLU), and K \u03c3 (x, x ) \u2208 R d\u00d7d is a positive semi-definite matrix that depends on \u03c3. The Equation ( 4) is a non-linear matrix differential equation which in general cannot be solved analytically. However, it has an equilibrium point z(x, t) such that E (x ,y ) [K \u03c3 (x, x ) \u2022 (\u03c3(z ) \u2212 e y )] = 0, thus its solution converges to this point as t \u2192 \u221e. This similarity in the evolution of \u03c3(z) implies the similarity in the evolution of the perfomance metric-such as BLEU-accross all three transformations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_57",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.7"
            }
        },
        {
            "ix": "429-ARR_v1_58",
            "content": "Although the BLEU metric (Papineni et al., 2002) has stood the test of time, it is still an automated assessment of translation quality. To double-check the reliability of the results from Table 1, we decided to manually evaluate the translations from the WMT'13 En\u2192Ru test split. To do this, we followed the human evaluation setup from (Berard et al., 2019). We formed two random samples of 135 instances each and gave them to two annotators. 45 instances were shared across two samples in order to calculate inter-annotator agreement. Each instance consists of an original sentence in English and 4 candidate translations into Russian (reference, softmax, entmax, \u03b1-ReLU). The annotators were to rate each translation on a 4-point scale. For annotation instructions, see Appendix C. The order of candidate translations was shuffled for each instance, so the annotators did not know which sentence is from which model. Nevertheless, the annotator always had a good chance of guessing which translation was the reference one, due to the large difference in quality between human and machine translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_59",
            "content": "The results of human evaluation are shown in Table 4. Cohen's \u03ba = 0.56, indicating moderate agreement between annotators. As we can see, all three models give approximately the same translation quality, and all three are significantly inferior to the reference translation. This is generally consistent with the results of 1.5-ReLU and 1.5-entmax in Table 1, but at the same time casts doubt on the softmax lag behind 1.5-ReLU and 1.5-entmax as the BLEU metric suggests.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_60",
            "content": "In Appendix D we give a few examples where 1.5-ReLU translates better than 1.5-entmax and vice versa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_61",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "429-ARR_v1_62",
            "content": "Sparse seq2seq models. Our proposed \u03b1-ReLU transformation is based on the \u03b1-entmax transformation of Peters et al. (2019), which in turn is a generalization of the sparsemax transformation (Martins and Astudillo, 2016). In our work, we study sparseness at the output of a neural network. Nevertheless, there are a number of works aimed at sparsification within a neural network. For example, Malaviya et al. (2018); Peters et al. (2019);Correia et al. (2019) show that sparsemax and \u03b1entmax can replace softmax in the attention mechanism with some success. A recent work of Zhang et al. (2021) attempted to replace softmax with a component-wise ReLU in the attention mechanism. Unfortunately, in its pure form, this replacement leads to the inability of the model to learn at all, since its loss function does not decrease during optimization. The authors solve this problem by adding a normalizing layer on top of the attention layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_63",
            "content": "These and other works (Zhang et al., 2019) state that sparsity in the weights of attention produces more interpretable patterns. However, Meister et al. (2021) questioned this claim and were unable to find clear evidence to support it. Therefore, in this work, we focused on the application of \u03b1-ReLU to the output of the transformer model, and not to the mechanism of attention, but at the same time we do not deny the possibility of studying the latter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_64",
            "content": "Self-normalization. Self-normalizing training aims to bypass the need of normalization during inference time. This is done by tweaking the learning mechanism so that the sum of all predictions sums (approximately) to a constant value. Theoretical work on why this works is poorly understood (Andreas et al., 2015) but early work in neural machine translation has shown its empirical value. Vaswani et al. (2013) achieves that by using noisecontrastive estimation (the neural model is used to re-rank the output of a hierarchical phrase-based machine translation system). Noise-contrastive estimation is also the standard training mechanism for word2vec (more popular than the alternative hierarchical softmax), which also eschews any expensive normalization. Differently, Devlin et al. (2014) changes the training loss to include a factor that encourages the normalizing factor to be 1. At inference time, this is just assumed and decoding time is reported to achieve a 15x speed-up.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_65",
            "content": "Limitations and Risks",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "429-ARR_v1_66",
            "content": "We believe that the main limitations of our work are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_67",
            "content": "\u2022 \u03b1-ReLU's output is still not a probability distribution, as required by the classical formulation of a probabilistic classification model. \u2022 \u03c4 evaluation requires either running the data through an untrained model with \u03b1-entmax at the output, or deriving a formula similar to (3) for each individual architecture. \u2022 Our approach only works for the case when \u03b1-ReLU is used at the output of the model, but it is not clear how to use it as an alternative to softmax/\u03b1-entmax in the attention layer.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_68",
            "content": "The last mentioned limitation leads to the potential risk of inability to learn if \u03b1-ReLU is misused in the intermediate layers of the neural network such as attention layers. The experiments of Zhang et al. (2021) using vanilla ReLU (2-ReLU with \u03c4 = 0 in our notation) instead of softmax to produce attention weights lead to a divergence of the loss function of the Transformer model. This translates into a waste of energy, especially when training large models on large datasets. Therefore, we believe that in the future, a preliminary mathematical analysis and/or experiments with small models on small datasets should be carried out as to why the unnormalized distribution of attention weights leads to the inability of the model to learn.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_69",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "429-ARR_v1_70",
            "content": "It seems that the sparsity of the output is natural for (sub)word prediction models. Nevertheless, sparsity does not have to come with slowdown of computations, as our work shows. The proposed transformation, \u03b1-ReLU, gives a sparse output, shows competitive performance, and is as fast as softmax.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_71",
            "content": "The reduced dependency on the vocabulary size seems particularly important in translation, where neural models are moving more and more towards multi-lingual ones, which in general have a much higher vocabulary size in order to accommodate enough tokens for all languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_72",
            "content": "A natural extension of this work will be the evaluation of \u03b1-ReLU in the problem of open-ended text generation, as well as a replacement for softmax in the attention layers of Transformer models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_73",
            "content": "\u2022 Architecture: Transformer, embedding size 512, 6 layers, 8 heads, hidden size 1024, shared vocabulary. \u2022 Batch size: 4096 tokens (with gradient accumulation for 8 steps). \u2022 Optimizer: ADAM, \u03b2 1 = 0.9, \u03b2 2 = 0.998, noam decay, learning rate 2.0, 4000 warmup steps. \u2022 Dropout: 0.3 \u2022 No label smoothing.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_74",
            "content": "\u2022 Architecture: Transformer, embedding size 512, 6 layers, 8 heads, hidden size 2048, shared vocabulary of 40K tokens, shared embeddings and decoder embeddings. \u2022 Batch size: 4096 tokens (with gradient accumulation for 4 steps). \u2022 Optimizer: ADAM, \u03b2 1 = 0.9, \u03b2 2 = 0.998, noam decay, learning rate 2.0, 8000 warmup steps, average decay 0.0005. We do not report CO 2 consumption, as experiments were run in different countries, making aggregate statistics difficult to compute. The largest experiment (on WMT'13), were run in [MASKED], which benefits from a very low CO 2 emission intensity in its electrical mix.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_75",
            "content": "Notation. We let R denote the real numbers. Bold-faced lowercase letters (x) denote vectors in Euclidean space, bold-faced uppercase letters (A) denote matrices, plain-faced lowercase letters (x) denote scalars, \u2022 denotes the Euclidean norm: x :=",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_76",
            "content": "x x. The gradient of f : R d \u2192 R is denoted by \u2207f . The Jacobian of z \u2192 g(z) is denoted by J g (z). Also, we denote ReLU(x) := [x] + := max{x, 0}, [d] := {1, . . . , d}, \u2206 d\u22121 := {p \u2208 R d | i p i = 1, p i \u2265 0}, e y := (0, . . . , 0, 1, 0, . . . , 0) where 1 is at y th position.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_77",
            "content": "First, let us calculate the Jacobian of the mapping z \u2192 \u03b1-ReLU(z). Recall that",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_78",
            "content": "\u03b1-ReLU i (z) := [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 + .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_79",
            "content": "Therefore, the partial derivatives are given by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_80",
            "content": "\u2202[\u03b1-ReLU i (z)] \u2202z i = 1 \u03b1 \u2212 1 \u2022 [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 \u22121 + \u2022 (\u03b1 \u2212 1) = [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 2\u2212\u03b1 \u03b1\u22121 + , = [\u03b1-ReLU i (z)] 2\u2212\u03b1 \u2202[\u03b1-ReLU i (z)] \u2202z j = 0. i = j",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_81",
            "content": "Thus, the Jacobian can be written concisely as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_82",
            "content": "J \u03b1-ReLU (z) = diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 },(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_83",
            "content": "where raising to power is done component-wise (i.e. x \u03b2 = [x \u03b2 1 , . . . , x \u03b2 d ]), and diag[x] is a diagonal matrix with x on its diagonal.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_84",
            "content": "Recall the definition of the Tsallis \u03b1-entropy:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_85",
            "content": "H \u03b1 [p] := 1 \u03b1(\u03b1 \u2212 1) \uf8eb \uf8ed 1 \u2212 j p \u03b1 j \uf8f6 \uf8f8 . Its gradient w.r.t. p is \u2207 p H \u03b1 [p] = \u2212 1 \u03b1 \u2212 1 p \u03b1\u22121 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_86",
            "content": "Combining this with (5), and using the chain rule, we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_87",
            "content": "\u2207 z H \u03b1 [\u03b1-ReLU(z)] = [J \u03b1-ReLU (z)] \u2022 \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] \u03b1\u22121 = diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 } \u2022 \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] \u03b1\u22121 = \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] 2\u2212\u03b1 [\u03b1-ReLU(z)] \u03b1\u22121 = \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z),(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_88",
            "content": "where is the Hadamard product (element-wise multiplication), and we used diag[x] \u2022 y = x y. Taking into account (6), the gradient of the \u03b1-ReLU loss (2) w.r.t. z is",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_89",
            "content": "\u2207 z (z, y) = \u2207 z (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1 \u2212 1 1 + \u2207 z H \u03b1 [\u03b1-ReLU(z)] = (\u03b1-ReLU(z) \u2212 e y ) + J \u03b1-ReLU(z) z \u2212 \u03c4 \u03b1 \u2212 1 1 \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = (\u03b1-ReLU(z) \u2212 e y ) + 1 \u03b1 \u2212 1 diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 } [(\u03b1 \u2212 1)z \u2212 \u03c4 1] \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = (\u03b1-ReLU(z) \u2212 e y ) + 1 \u03b1 \u2212 1 [(\u03b1 \u2212 1)z \u2212 \u03c4 1] 2\u2212\u03b1 \u03b1\u22121 + [(\u03b1 \u2212 1)z \u2212 \u03c4 1] \u03b1-ReLU(z) \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = \u03b1-ReLU(z) \u2212 e y ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_90",
            "content": "where in the fourth line we used",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_91",
            "content": "[x] \u03b2 + x = [x] \u03b2 + [x] + = [x] \u03b2+1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_92",
            "content": "+ . This concludes the proof. and thus",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_93",
            "content": "S(k) \u2248 \u03c3 2 p \u2212 \u2212\u03c6(\u03a6 \u22121 (x)) \u2022 \u03a6 \u22121 (x) + x x=p x= \u2212 [m(p)] 2 = \u03c3 2 s(p),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_94",
            "content": "where s(p) is defined by (8). Hence, finding k \u2208 [d] that satisfies ( 10) is (approximately) equivalent to finding p \u2208 (0, 1) that satisfies",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_95",
            "content": "\u03c3\u03a6 \u22121 (1 \u2212 p) = \u03c3m(p) \u2212 4 \u2022 p \u2212 \u03c3 2 s(p) \u21d4 \u03a6 \u22121 (1 \u2212 p) = m(p) \u2212 4 \u03c3 2 \u2022 p \u2212 s(p).(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_96",
            "content": "Let p * be the solution of (11). Then, taking into account (9), we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_97",
            "content": "\u03c4 (z) \u2248 \u03c3m(p * ) 2 \u2212 p * \u2212 \u03c3 2 s(p * ) 4 = \u03c3 2 m(p * ) \u2212 4 \u03c3 2 \u2022 p * \u2212 s(p * ) = \u03c3 2 \u03a6 \u22121 (1 \u2212 p * ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_98",
            "content": "which concludes the proof.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_99",
            "content": "Lemma 3. Let z = Wx be a pre-softmax vector of logits in the OpenNMT-py (Klein et al., 2017) implementation of the Transformer model (Vaswani et al., 2017). Then for any input, in a non-trained model the logits z 1 , . . . , z d are distributed according to the normal distribution N 0,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_100",
            "content": "E[w ij ] = 0, Var[w ij ] = (2a) 2 12 = a 2 3 = 2 d model + d vocab (12",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_101",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_102",
            "content": "Since x is the result of a layer normalization (Ba et al., 2016), we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_103",
            "content": "1 d model d model j=1 x j = 0, 1 d model d model j=1 x 2 j = 1(13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_104",
            "content": "Therefore, from ( 12) and ( 13), we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_105",
            "content": "E[z i ] = E \uf8ee \uf8f0 d model j=1 w ij x j \uf8f9 \uf8fb = d model j=1 E[w ij ] \u2022 x j = 0, Var[z i ] = Var \uf8ee \uf8f0 d model j=1 w ij x j \uf8f9 \uf8fb = 2 d model + d vocab d model j=1 x 2 j = 2 \u2022 d model d model + d vocab .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_106",
            "content": "Being a sum of independent random variables, by the Central Limit Theorem, each z i tends to normal distribution with the mean and variance above.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_107",
            "content": "We provide derivation for the case of \u03b1-ReLU. Extension to \u03b1-entmax and softmax is done analogously. Let x \u2208 R n 0 be the input vector. We define a feedforward neural network with L \u2212 1 hidden layers recursively:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_108",
            "content": "h (0) = x z (k) = 1 \u221a n k\u22121 W (k\u22121) h (k\u22121) , h (k) = \u03c3(z (k) ), k = 1, . . . , L \u2212 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_109",
            "content": "where W (k\u22121) \u2208 R n k \u00d7n k\u22121 is the weight matrix in the k th hidden layer, and \u03c3(\u2022) is a nonlinear activation function applied element-wise. We consider the case of a multi-label classification, i.e. the output layer is a vector z",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_110",
            "content": ":= z (L) \u2208 R d ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_111",
            "content": "which is fed into the \u03b1-ReLU loss:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_112",
            "content": "(z, y) = (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1 \u2212 1 1 + H \u03b1 [\u03b1-ReLU(z)],(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_113",
            "content": "where H \u03b1 [p] := 1 \u03b1(\u03b1\u22121) j (p j \u2212 p \u03b1 j ), \u03b1 = 1, is the Tsallis \u03b1-entropy (Tsallis, 1988). Given a training sample S := {(x, y)} learning is performed by minimizing the training error",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_114",
            "content": "L := E (x,y)\u223cS [ (z(x), y)](15",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_115",
            "content": "where K(x, x ) \u2208 R d\u00d7d is a positive semidefinite matrix, and z := z(x , t).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_116",
            "content": "Proof. From ( 15) and Lemma 1 we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_117",
            "content": "\u2207 z L = \u2207 z E (x ,y )\u223cS [ (z , y)] = \u2207 z (z, y) = \u03b1-ReLU(z) \u2212 e y ,(17)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_118",
            "content": "where we denoted z := z(x, t) and z := z(x , t) for shorthand. Now, consider the gradient descent update",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_119",
            "content": "\u03b8 t+\u03b7 = \u03b8 t \u2212 \u03b7\u2207 \u03b8 L \u21d4 \u03b8 t+\u03b7 \u2212 \u03b8 t \u03b7 = \u2212\u2207 \u03b8 L, (18",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_120",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_121",
            "content": "where \u03b7 is the learning rate. Taking the limit in (18) as \u03b7 \u2192 0, we have:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_122",
            "content": "d\u03b8 dt = \u2212\u2207 \u03b8 L = \u2212 E (x ,y )\u223cS [J z (\u03b8) \u2022 \u2207 z L],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_123",
            "content": "where the last equality is due to the chain rule. Combining this with (17), we get",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_124",
            "content": "d\u03b8 dt = \u2212 E (x ,y )\u223cS [J z (\u03b8) \u2022 (\u03b1-ReLU(z ) \u2212 e y )](19)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_125",
            "content": "Applying the chain rule again, and using ( 19), we have",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_126",
            "content": "dz dt = J z (\u03b8) \u2022 d\u03b8 dt = \u2212 E (x ,y )\u223cS [J z (\u03b8)J z (\u03b8) K(x,x ;\u03b8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_127",
            "content": "\u2022(\u03b1-ReLU(z ) \u2212 e y )].",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_128",
            "content": "The quantity K(x, x ; \u03b8) was named the Neural Tangent Kernel by Jacot et al. (2018). They also showed (see their Theorem 1) that K(x, x ; \u03b8) \u2192 K(x, x ) as n 1 , . . . , n L\u22121 \u2192 \u221e, where K(x, x ) \u2208 R d\u00d7d is the deterministric kernel that does not depend on \u03b8. This concludes the proof.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_129",
            "content": "You are shown a reference sentence and several candidate translations. Please indicate, for each, on a 4-point scale, how much of the meaning is represented in the translation, ignoring the language quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_130",
            "content": "Imagine you are a forgiving reader, ignoring any error that does not prevent you from getting the meaning of the text. So please ignore language oddities, typographic errors and the like. (This is difficult but key to us!)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_131",
            "content": "The scale of meaning preservation is: 4 = Everything / 3 = Most / 2 = Little / 1 = None",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_132",
            "content": "As we are interested in comparing system's output, you can refine your judgement using + or \u2212, e.g. 3+.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_133",
            "content": "When you do not know, simply leave empty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_134",
            "content": "For instance, given the reference sentence \"This restaurant is beautiful and the staff is very friendly\", valid judgements for different translations are provided in Table 6. We insist that evaluating by meaning differs from a natural intuitive evaluation. Provided the meaning is not impacted, we want to ignore the language quality, the punctuation, the casing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "429-ARR_v1_135",
            "content": "Jacob Andreas, Maxim Rabinovich, Dan Michael I Jordan,  Klein, On the accuracy of selfnormalized log-linear models, 2015, Proceedings of the 28th International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Jacob Andreas",
                    "Maxim Rabinovich",
                    "Dan Michael I Jordan",
                    " Klein"
                ],
                "title": "On the accuracy of selfnormalized log-linear models",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 28th International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_136",
            "content": "C Barry, N Arnold, H Balakrishnan,  Nagaraja, A First Course in Order Statistics, 2008, Classics in Applied Mathematics). Society for Industrial and Applied Mathematics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "C Barry",
                    "N Arnold",
                    "H Balakrishnan",
                    " Nagaraja"
                ],
                "title": "A First Course in Order Statistics",
                "pub_date": "2008",
                "pub_title": "Classics in Applied Mathematics). Society for Industrial and Applied Mathematics",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_137",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_138",
            "content": "Alexandre Berard, Ioan Calapodescu, Marc Dymetman, Claude Roux, Jean-Luc Meunier, Vassilina Nikoulina, Machine translation of restaurant reviews: New corpus for domain adaptation and robustness, 2019, Proceedings of the 3rd Workshop on Neural Generation and Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Alexandre Berard",
                    "Ioan Calapodescu",
                    "Marc Dymetman",
                    "Claude Roux",
                    "Jean-Luc Meunier",
                    "Vassilina Nikoulina"
                ],
                "title": "Machine translation of restaurant reviews: New corpus for domain adaptation and robustness",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 3rd Workshop on Neural Generation and Translation",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_139",
            "content": "UNKNOWN, None, 2013, Proceedings of the Eighth Workshop on Statistical Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Proceedings of the Eighth Workshop on Statistical Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_140",
            "content": "Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ales Tamchyna, Findings of the 2014 workshop on statistical machine translation, 2014-06-26, Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ondrej Bojar",
                    "Christian Buck",
                    "Christian Federmann",
                    "Barry Haddow",
                    "Philipp Koehn",
                    "Johannes Leveling",
                    "Christof Monz",
                    "Pavel Pecina",
                    "Matt Post",
                    "Herve Saint-Amand",
                    "Radu Soricut",
                    "Lucia Specia",
                    "Ales Tamchyna"
                ],
                "title": "Findings of the 2014 workshop on statistical machine translation",
                "pub_date": "2014-06-26",
                "pub_title": "Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_141",
            "content": "UNKNOWN, None, 2014, Report on the 11th iwslt evaluation campaign, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Report on the 11th iwslt evaluation campaign",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_142",
            "content": "M Gon\u00e7alo, Vlad Correia, Andr\u00e9 Niculae,  Martins, Adaptively sparse transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "M Gon\u00e7alo",
                    "Vlad Correia",
                    "Andr\u00e9 Niculae",
                    " Martins"
                ],
                "title": "Adaptively sparse transformers",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "429-ARR_v1_143",
            "content": "Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Fast and robust neural network joint models for statistical machine translation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jacob Devlin",
                    "Rabih Zbib",
                    "Zhongqiang Huang",
                    "Thomas Lamar",
                    "Richard Schwartz",
                    "John Makhoul"
                ],
                "title": "Fast and robust neural network joint models for statistical machine translation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "429-ARR_v1_144",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Angela Fan",
                    "Mike Lewis",
                    "Yann Dauphin"
                ],
                "title": "Hierarchical neural story generation",
                "pub_date": "2018-07-15",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "429-ARR_v1_145",
            "content": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, The curious case of neural text degeneration, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Ari Holtzman",
                    "Jan Buys",
                    "Li Du",
                    "Maxwell Forbes",
                    "Yejin Choi"
                ],
                "title": "The curious case of neural text degeneration",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_146",
            "content": "Arthur Jacot, Cl\u00e9ment Hongler, Franck Gabriel, Neural tangent kernel: Convergence and generalization in neural networks, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Arthur Jacot",
                    "Cl\u00e9ment Hongler",
                    "Franck Gabriel"
                ],
                "title": "Neural tangent kernel: Convergence and generalization in neural networks",
                "pub_date": "2018-12-03",
                "pub_title": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_147",
            "content": "Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush, OpenNMT: Opensource toolkit for neural machine translation, 2017, Proceedings of ACL 2017, System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Guillaume Klein",
                    "Yoon Kim",
                    "Yuntian Deng",
                    "Jean Senellart",
                    "Alexander Rush"
                ],
                "title": "OpenNMT: Opensource toolkit for neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of ACL 2017, System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "429-ARR_v1_148",
            "content": "Chaitanya Malaviya, Pedro Ferreira, Andr\u00e9 Martins, Sparse and constrained attention for neural machine translation, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Chaitanya Malaviya",
                    "Pedro Ferreira",
                    "Andr\u00e9 Martins"
                ],
                "title": "Sparse and constrained attention for neural machine translation",
                "pub_date": "2018-07-15",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "429-ARR_v1_149",
            "content": "F Andr\u00e9, Ram\u00f3n Martins,  Fernandez Astudillo, From softmax to sparsemax: A sparse model of attention and multi-label classification, 2016-06-19, Proceedings of the 33nd International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "F Andr\u00e9",
                    "Ram\u00f3n Martins",
                    " Fernandez Astudillo"
                ],
                "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
                "pub_date": "2016-06-19",
                "pub_title": "Proceedings of the 33nd International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_150",
            "content": "Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell, Is sparse attention more interpretable?, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Clara Meister",
                    "Stefan Lazov",
                    "Isabelle Augenstein",
                    "Ryan Cotterell"
                ],
                "title": "Is sparse attention more interpretable?",
                "pub_date": "2021-08-01",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "429-ARR_v1_151",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002-07-06, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002-07-06",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "ACL"
            }
        },
        {
            "ix": "429-ARR_v1_152",
            "content": "Ben Peters, F Andr\u00e9,  Martins, Smoothing and shrinking the sparse seq2seq search space, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Ben Peters",
                    "F Andr\u00e9",
                    " Martins"
                ],
                "title": "Smoothing and shrinking the sparse seq2seq search space",
                "pub_date": "2021-06-06",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_153",
            "content": "Ben Peters, Vlad Niculae, Andr\u00e9 Martins, Sparse sequence-to-sequence models, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Ben Peters",
                    "Vlad Niculae",
                    "Andr\u00e9 Martins"
                ],
                "title": "Sparse sequence-to-sequence models",
                "pub_date": "2019-07-28",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_154",
            "content": "UNKNOWN, None, 2018, A call for clarity in reporting bleu scores, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "A call for clarity in reporting bleu scores",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_155",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016-08-07",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_156",
            "content": "Felix Stahlberg, Bill Byrne, On NMT search errors and model errors: Cat got your tongue?, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Felix Stahlberg",
                    "Bill Byrne"
                ],
                "title": "On NMT search errors and model errors: Cat got your tongue?",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "429-ARR_v1_157",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "V Quoc",
                    " Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_158",
            "content": "Constantino Tsallis, Possible generalization of boltzmann-gibbs statistics, 1988, Journal of statistical physics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Constantino Tsallis"
                ],
                "title": "Possible generalization of boltzmann-gibbs statistics",
                "pub_date": "1988",
                "pub_title": "Journal of statistical physics",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_159",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_160",
            "content": "Ashish Vaswani, Yinggong Zhao, Victoria Fossum, David Chiang, Decoding with large-scale neural language models improves translation, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ashish Vaswani",
                    "Yinggong Zhao",
                    "Victoria Fossum",
                    "David Chiang"
                ],
                "title": "Decoding with large-scale neural language models improves translation",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "429-ARR_v1_161",
            "content": "Biao Zhang, Ivan Titov, Rico Sennrich, Sparse attention with linear units, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Biao Zhang",
                    "Ivan Titov",
                    "Rico Sennrich"
                ],
                "title": "Sparse attention with linear units",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "429-ARR_v1_162",
            "content": "Jiajun Zhang, Yang Zhao, Haoran Li, Chengqing Zong, Attention with sparsity regularization for neural machine translation and summarization, 2019, IEEE ACM Trans. Audio Speech Lang. Process, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Jiajun Zhang",
                    "Yang Zhao",
                    "Haoran Li",
                    "Chengqing Zong"
                ],
                "title": "Attention with sparsity regularization for neural machine translation and summarization",
                "pub_date": "2019",
                "pub_title": "IEEE ACM Trans. Audio Speech Lang. Process",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "429-ARR_v1_0@0",
            "content": "Speeding Up Entmax",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_0",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_2@0",
            "content": "Softmax is the de facto standard for normalizing logits in modern neural networks for language processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_2",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_2@1",
            "content": "However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_2",
            "start": 107,
            "end": 315,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_2@2",
            "content": "\u03b1entmax of Peters et al. ( 2019) solves this problem, but is unfortunately slower than softmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_2",
            "start": 317,
            "end": 411,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_3@0",
            "content": "In this paper, we propose an alternative to \u03b1entmax, which keeps its virtuous characteristics, but is as fast as optimized softmax and achieves on par or better performance in machine translation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_3",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_5@0",
            "content": "Sparseness of vector representations is a desirable trait in neural network models for natural language processing (NLP): words (subwords) are discrete objects by their nature, and, accordingly, are encoded by one-hot embeddings at the input and output of neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_5",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_5@1",
            "content": "However, to predict a categorical response in neural models, softmax is most often used, which produces a dense probability distribution, i.e. every category (word/subword) receives a non-zero probability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_5",
            "start": 273,
            "end": 477,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@0",
            "content": "Recent studies suggest that it is this output density that poses problems when the trained NLP model is used for inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@1",
            "content": "For example, in the case of text generation, unconstrained sampling from a trained language model results in poor quality of the resulting text (Holtzman et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 124,
            "end": 291,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@2",
            "content": "In neural machine translation (NMT), exact decoding from a trained model often results in empty text (Stahlberg and Byrne, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 293,
            "end": 421,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@3",
            "content": "1 To get around these problems, constrained decoding techniques have been proposed, most of which artificially impose sparsity on softmax prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 423,
            "end": 571,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@4",
            "content": "For example, Fan et al. (2018) propose to sample from the top-k probable words, and Holtzman et al. (2020) propose to sample from the most probable words, which comprise the cumulative probability p.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 573,
            "end": 771,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_6@5",
            "content": "While these methods are effective, they are ad-hoc solutions that lead to a mismatch between how the model is trained and how it is used at inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_6",
            "start": 773,
            "end": 922,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@0",
            "content": "In this regard, the works on sparse alternatives to softmax stand apart since they allow us to make inference from the model in the same way than it was trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@1",
            "content": "Some of the most successful and elegant solutions are sparsemax (Martins and Astudillo, 2016) and its generalization \u03b1-entmax (Peters et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 162,
            "end": 309,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@2",
            "content": "When coupled with suitable losses, these transformations are not inferior to softmax, and sometimes even surpass it as measured with final performance metrics on a number of tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 311,
            "end": 490,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@3",
            "content": "A problem with these transformations however is that they are significantly slower than softmax when the number of categories (vocabulary size) is tens of thousands, as in the case of text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 492,
            "end": 691,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@4",
            "content": "This is because \u03b1-entmax transformation-in its original formulation-requires sorting over the logits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 693,
            "end": 793,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@5",
            "content": "2 In this work, we ask the question: is it possible to obtain a sparse output like that of \u03b1-entmax, but without its degradation in computational speed?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 795,
            "end": 946,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_7@6",
            "content": "Our answer is affirmative-we propose a sparse output transformation that \u2022 is on par or superior to softmax and \u03b1-entmax in the NMT tasks,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_7",
            "start": 948,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_8@0",
            "content": "\u2022 works as fast as softmax during training and at inference, \u2022 gives the same training dynamics as \u03b1-entmax (in training steps).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_8",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_9@0",
            "content": "The most surprising thing is that such a transformation is simply a shifted ReLU raised to power 1 \u03b1\u22121 , which we call \u03b1-ReLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_9",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@0",
            "content": "The rest of the paper is organised as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@1",
            "content": "In Sect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 47,
            "end": 54,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@2",
            "content": "2 we motivate the choice of \u03b1-ReLU as the output transformation, and also select an appropriate loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 56,
            "end": 165,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@3",
            "content": "In Sect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 167,
            "end": 174,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@4",
            "content": "3 we experimentally confirm our claims about performance and output speed of \u03b1-ReLU in the NMT task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 176,
            "end": 275,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@5",
            "content": "Sect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 277,
            "end": 281,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_10@6",
            "content": "4 is devoted to a comparative analysis of \u03b1-ReLU and \u03b1-entmax in terms of sparsity, ability to solve the empty translation problem, and training dynamics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_10",
            "start": 283,
            "end": 436,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_11@0",
            "content": "\u03b1-ReLU at Output",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_11",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_12@0",
            "content": "Our departure point is the \u03b1-entmax transformation of Peters et al. (2019) which can be defined for",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_12",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_13@0",
            "content": "z \u2208 R d as \u03b1-entmax i (z) = [(\u03b1 \u2212 1)z i \u2212 \u03c4 (z)] 1 \u03b1\u22121 + ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_13",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_14@0",
            "content": "where [x] + := max{x, 0}, and \u03c4 : R d \u2192 R is the (unique) function that satisfies j [(\u03b1 \u2212 1)z j \u2212 \u03c4 (z)]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_14",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_15@0",
            "content": "1 \u03b1\u22121 + = 1 for any z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_15",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_16@0",
            "content": "It is this threshold \u03c4 that makes the computation of \u03b1-entmax slow, because one needs to sort the components of z to find \u03c4 (Peters et al., 2019, Alg. 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_16",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_17@0",
            "content": "As we can see, the threshold \u03c4 is only needed to ensure that \u03b1-entmax(z) is a probability distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_17",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_17@1",
            "content": "We loosen this constraint, and only require non-negative weights, which is sufficient for most uses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_17",
            "start": 104,
            "end": 203,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_17@2",
            "content": "Consider then a transformation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_17",
            "start": 205,
            "end": 234,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_18@0",
            "content": "\u03b1-ReLU i (z) := [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 + , (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_18",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_19@0",
            "content": "where \u03c4 is a constant that does not depend on z. In order to force \u03b1-ReLU(z)-applied to the logits zto converge to the one-hot vector e y of the gold label y we need to adjust the corresponding loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_19",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_19@1",
            "content": "This can easily be done by feeding the logits z and the output \u03b1-ReLU(z) into the following loss, which we call \u03b1-ReLU loss. (Tsallis, 1988).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_19",
            "start": 200,
            "end": 340,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_19@2",
            "content": "The rationale for coupling \u03b1-ReLU with the loss (2) is the following Lemma 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_19",
            "start": 342,
            "end": 418,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_19@3",
            "content": "For any \u03c4 \u2208 R, the gradient of the \u03b1-ReLU loss (2) is given by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_19",
            "start": 420,
            "end": 481,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_20@0",
            "content": "(z, y) = (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1\u22121 1 + H \u03b1 [\u03b1-ReLU(z)], (2) where H \u03b1 [p] := 1 \u03b1(\u03b1\u22121) 1 \u2212 j p \u03b1 j , \u03b1 = 1, is the Tsallis \u03b1-entropy",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_20",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_21@0",
            "content": "\u2207 z (z, y) = \u03b1-ReLU(z) \u2212 e y .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_21",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_22@0",
            "content": "Proof.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_22",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_22@1",
            "content": "The proof is in Appendix B.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_22",
            "start": 7,
            "end": 35,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_23@0",
            "content": "By Lemma 1, gradient-based minimization of indeed forces \u03b1-ReLU(z) \u2192 e y .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_23",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_23@1",
            "content": "Notice that this is similar to what happens when the softmax normalization is coupled with the cross-entropy loss or when \u03b1-entmax is coupled with the entmax loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_23",
            "start": 75,
            "end": 237,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_23@2",
            "content": "In both cases differentiating the loss with respect to logits gives p\u2212e y , where p is either softmax(z) or \u03b1-entmax(z) (Martins and Astudillo, 2016;Peters et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_23",
            "start": 239,
            "end": 408,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_24@0",
            "content": "Remark. Recall that \u03b1-entmax is a generalization of sparsemax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_24",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_24@1",
            "content": "For example, 2-entmax is essentially sparsemax, and for \u03b1 \u2208 (1, 2) we get a smoothed version of sparsemax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_24",
            "start": 63,
            "end": 168,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_24@2",
            "content": "Similarly, \u03b1-ReLU is a kind of generalization of ReLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_24",
            "start": 170,
            "end": 223,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_24@3",
            "content": "So, the standard ReLU is 2-ReLU (with \u03c4 = 0), and for \u03b1 \u2208 (1, 2) we get a smoothed ReLU (see Fig. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_24",
            "start": 225,
            "end": 325,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_25@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_25",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_26@0",
            "content": "In theory, nothing prevents \u03b1-ReLU from learning what \u03b1-entmax is learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_26",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_26@1",
            "content": "However, in practice we can have a different picture, because training is conditioned by many factors-the size of the dataset, the architecture of the neural network, the optimization algorithm, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_26",
            "start": 76,
            "end": 274,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_26@2",
            "content": "In this section, we compare \u03b1-ReLU empirically with \u03b1-entmax (as well as with sparsemax and softmax), assuming all other factors are fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_26",
            "start": 276,
            "end": 414,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_26@3",
            "content": "The goal of these experiments is to evaluate the consequences of using \u03b1-ReLU as drop-in replacement for \u03b1-entmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_26",
            "start": 416,
            "end": 529,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_27@0",
            "content": "We test \u03b1-ReLU at output in a neural machine translation task (Sutskever et al., 2014) clearer metric of the quality of the generated textthe BLEU score (Papineni et al., 2002).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_27",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_27@1",
            "content": "As in open-ended text generation, at each prediction step, the NMT system needs to make a choice from all words (subwords) of the vocabulary, the size of which can reach several tens of thousands.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_27",
            "start": 178,
            "end": 373,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_27@2",
            "content": "Therefore, the sparsity of the output distribution becomes critical in such setups, since it can explicitly prevent the occurrence of most of the words that are inappropriate in the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_27",
            "start": 375,
            "end": 564,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_28@0",
            "content": "Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_28",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_29@0",
            "content": "Data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_29",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_29@1",
            "content": "We conduct experiments on three datasets of varied sizes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_29",
            "start": 6,
            "end": 62,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_30@0",
            "content": "\u2022 IWSLT'14 De\u2192En (Cettolo et al.) We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 10K merge operations on IWSLT, 40K merge operations on WMT En\u2192De, and 60K merge operations on WMT En\u2192Ru. We report detokenized casesensitive BLEU with SacreBLEU (Post, 2018). 4 Hyperparameters \u03b1 and \u03c4 . In all experiments we set \u03b1 = 1.5, because this value was recommended by Peters et al. (2019); Peters and Martins (2021) as the middle ground between \u03b1 = 1 (softmax) and \u03b1 = 2 (sparsmax).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_30",
            "start": 0,
            "end": 518,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_31@0",
            "content": "The value for \u03c4 is chosen as follows: we run the first batch through a non-trained neural network, which has 1.5-entmax at the output, in the forward direction and determine the average \u03c4 value across the batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_31",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_31@1",
            "content": "This value is then used to train the 1.5-ReLU network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_31",
            "start": 212,
            "end": 265,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_31@2",
            "content": "Our preliminary experiments have shown that 1.5-ReLU convergence is sensitive to the \u03c4 value, and that having output close to the probability distribution early in the learning phase works well with the rest of hyperparameters which are set to their default values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_31",
            "start": 267,
            "end": 531,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_32@0",
            "content": "Training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_32",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_32@1",
            "content": "We trained the Transformer Base (Vaswani et al., 2017) using the OpenNMT-py 2.0 toolkit (Klein et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_32",
            "start": 10,
            "end": 118,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_32@2",
            "content": "Optimization details are in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_32",
            "start": 120,
            "end": 158,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_33@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_33",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_34@0",
            "content": "The results are given in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_34",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_34@1",
            "content": "Reported are test BLEU scores for best checkpoints which are selected based on validation BLEU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_34",
            "start": 34,
            "end": 128,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_34@2",
            "content": "We observe that the 1.5-ReLU performs on par with 1.5-entmax or better, while sparsemax is inferior to all others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_34",
            "start": 130,
            "end": 243,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_35@0",
            "content": "Training Time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_35",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_35@1",
            "content": "Fig. 2&3 show the training dynamics in training steps and in wall time on WMT'14 En\u2192De.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_35",
            "start": 15,
            "end": 101,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_35@2",
            "content": "Despite the closeness of performance in intermediate steps and at the end of training, we see that on the larger datasets 1.5-entmax is slower in wall time than softmax and 1.5-ReLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_35",
            "start": 103,
            "end": 284,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_36@0",
            "content": "To speed up the learning process, Peters et al. ( 2019) recommended limiting the number of sorted logits in the \u03b1-entmax to the k largest logits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_36",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_36@1",
            "content": "We tried this on the WMT'14 En\u2192De dataset using k = 100, which is the default value in the author's implementation of \u03b1-entmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_36",
            "start": 146,
            "end": 272,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_36@2",
            "content": "5 The resulting training dynamics in absolute time is shown as a dashed curve in Fig. 3 (middle).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_36",
            "start": 274,
            "end": 370,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_36@3",
            "content": "As we can see, partial sorting indeed speeds up the learning process, and at the same time does not harm the quality of the translation. But in the end, learning is still slower than in the case of 1.5-ReLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_36",
            "start": 372,
            "end": 578,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_36@4",
            "content": "Of course, one can try to select such k that the speed of calculating the 1.5-entmax will be as close as possible to the speed of 1.5-ReLU without losing quality, but this requires additional efforts on the part of the user, and this must be done for each case separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_36",
            "start": 580,
            "end": 851,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_37@0",
            "content": "In this regard, 1.5-ReLU does not require additional fine-tuning, converges as fast as softmax in absolute time and performs on par or better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_37",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_37@1",
            "content": "Thus 1.5-ReLU combines all three desired properties: computation speed, task performance, and sparsity of output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_37",
            "start": 143,
            "end": 255,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_38@0",
            "content": "Inference Time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_38",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_38@1",
            "content": "We measured inference time of translating the WMT En\u2192Ru test data with the different strategies and with different beam sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_38",
            "start": 16,
            "end": 141,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_38@2",
            "content": "The results-normalized by the smallest valueare shown in Fig. 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_38",
            "start": 143,
            "end": 206,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_38@3",
            "content": "As can be seen the relative difference seems independent of the beam size: softmax is almost twice faster than 1.5-entmax (with full sorting over the logits).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_38",
            "start": 208,
            "end": 365,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_38@4",
            "content": "Even though the softmax version is optimized through the softmax CUDA kernel, it performs equivalent to the 1.5-ReLU model in terms of computation speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_38",
            "start": 367,
            "end": 519,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_39@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_39",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_40@0",
            "content": "Empty Translations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_40",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_41@0",
            "content": "We remind the reader that the cat got your tongue problem (Stahlberg and Byrne, 2019) is one of the main motivations for using sparse transformations when generating text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_41",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_41@1",
            "content": "As Peters and Martins (2021) have shown, 1.5-entmax successfully tackles this problem by significantly lowering the proportion of cases where an empty string is more likely than the beam search hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_41",
            "start": 172,
            "end": 376,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_41@2",
            "content": "For 1.5-ReLU, we also calculated this proportion, and compared it with the proportions for softmax and sparsemax (Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_41",
            "start": 378,
            "end": 500,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_41@3",
            "content": "As we see, 1.5-ReLU also successfully tackles the cat got your tongue problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_41",
            "start": 502,
            "end": 579,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_42@0",
            "content": "Sparsity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_42",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_43@0",
            "content": "To compare the sparsity of 1.5-ReLU and 1.5entmax we depict in Fig. 5 the distributions of the number of zero components after applying these transformations (recall that for softmax all components are always nonzero).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_43",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_43@1",
            "content": "Since we constructed the \u03b1-ReLU in such way that it mimics the \u03b1entmax (at least in the early stages of training), we expected that these two transformations would have similar properties, including sparsity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_43",
            "start": 219,
            "end": 426,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_43@2",
            "content": "However, this is not the case: as we can see, the 1.5-ReLU is significantly less sparse than the 1.5-entmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_43",
            "start": 428,
            "end": 535,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_43@3",
            "content": "It is noteworthy that lower sparsity in this case correlates with a better performance in the translation task (see Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_43",
            "start": 537,
            "end": 661,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_44@0",
            "content": "Impact of \u03c4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_44",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_45@0",
            "content": "The selection of \u03c4 was described in Section 3.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_45",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_45@1",
            "content": "However, the question arises: does the described approach lead to the choice of the optimal \u03c4 ? To find out, we trained the \u03b1-ReLU models for \u03c4 \u2208 {0, 0.1, 0.2, ..., 0.9, 1, 2, 5, 10} on the IWSLT data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_45",
            "start": 49,
            "end": 249,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_46@0",
            "content": "Note that all of these \u03c4 's have led to almost the same result at the end of the training (as predicted by Lemma 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_46",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_46@1",
            "content": "In Fig. 6, we present the dynamics of early training only for \u03c4 \u2208 {0, 0.1, 0.2, 0.3, 5, 10}, since the curves for \u03c4 \u2208 {0.4, ..., 0.9, 1, 2} practically coincided with the optimal curve corresponding to \u03c4 = 0.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_46",
            "start": 117,
            "end": 326,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_46@2",
            "content": "Note that our \u03c4 selection method gave a value of 0.33, thus we have no evidence against the adequacy of our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_46",
            "start": 328,
            "end": 442,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_47@0",
            "content": "Estimation of \u03c4 without data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_47",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_48@0",
            "content": "On closer inspection, we noticed that the preentmax logits in the untrained Transformer model are distributed according to the normal law, regardless of what data is supplied to the input, Shapiro-Wilk test, p-value > 0.15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_48",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_48@1",
            "content": "This allows us, using asymptotic theory, to estimate \u03c4 as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_48",
            "start": 224,
            "end": 280,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_49@0",
            "content": "\u03c4 = d model 2(d model + d vocab ) \u2022 \u03a6 \u22121 (1 \u2212 p * ),(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_49",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_50@0",
            "content": "where d model is the size of hidden representations, d vocab is the vocabulary size for a target language, \u03a6 \u22121 (\u2022) is the probit function and p * is the solution of a non-linear equation that involves functions related to the standard normal distribution (see Appendix B.2 for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_50",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_50@1",
            "content": "Table 3 compares the \u03c4 calculated by running data through an untrained model with the estimate \u03c4 obtained from (3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_50",
            "start": 288,
            "end": 402,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_50@2",
            "content": "As we can see, \u03c4 practically coincides with \u03c4 with an accuracy of two decimal places.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_50",
            "start": 404,
            "end": 488,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_50@3",
            "content": "Unfortunately, the formula (3) is not universal: it is only true for the Transformer architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_50",
            "start": 490,
            "end": 587,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_51@0",
            "content": "Self-normalization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_51",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_52@0",
            "content": "The attentive reader may have noticed that the output of \u03b1-ReLU is not normalized, i.e. the components of \u03b1-ReLU(z) do not have to sum up to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_52",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_52@1",
            "content": "Accordingly, the question arises: how correct is it to compare translation scores at different steps of the beam-search decoding if the conditional probabilities are not normalized? However, the comparison is possible if the \u03b1-ReLU(z) components add up to approximately the same number, i.e. if the model is self-normalizing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_52",
            "start": 144,
            "end": 468,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_52@2",
            "content": "To check this, we ran the trained \u03b1-ReLU model on the IWSLT and WMT'14 test sets, and looked at the distribution of i \u03b1-ReLU i (z) at each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_52",
            "start": 470,
            "end": 622,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_52@3",
            "content": "The results are shown in Fig. 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_52",
            "start": 624,
            "end": 655,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_52@4",
            "content": "As we can see, the sum of the \u03b1-ReLU(z) components concentrates well around its mean \u2248 1.24 (IWSLT) and 1.09 (WMT'14), which might indicate that the model indeed has a self-normalization property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_52",
            "start": 657,
            "end": 852,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_53@0",
            "content": "Training Dynamics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_53",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_54@0",
            "content": "As we noted in Sect. 3.2, the training dynamics are similar in all three cases (softmax, 1.5entmax, 1.5-ReLU) when time is measured in training steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_54",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_54@1",
            "content": "Here we attempt to explain this phenomenon through the recently proposed Neural Tangent Kernel (NTK) approach of Jacot et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_54",
            "start": 151,
            "end": 283,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_54@2",
            "content": "Roughly speaking, the NTK theory suggests that a sufficiently wide neural network trains like a kernel regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_54",
            "start": 285,
            "end": 398,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_54@3",
            "content": "We use this theory to show (in Appendix B.3) that in all three cases the logits z(x, t) for a training instance x at a training step t evolve (approximately) according to the same differential equation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_54",
            "start": 400,
            "end": 600,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_55@0",
            "content": "dz dt = \u2212E (x ,y ) [K \u03c3 (x, x ) \u2022 (\u03c3(z ) \u2212 e y )],(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_55",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_56@0",
            "content": "where expectation is over training examples (x , y ), \u03c3(\u2022) is one of the transformations considered (softmax, \u03b1-entmax, or \u03b1-ReLU), and K \u03c3 (x, x ) \u2208 R d\u00d7d is a positive semi-definite matrix that depends on \u03c3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_56",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_56@1",
            "content": "The Equation ( 4) is a non-linear matrix differential equation which in general cannot be solved analytically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_56",
            "start": 210,
            "end": 319,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_56@2",
            "content": "However, it has an equilibrium point z(x, t) such that E (x ,y ) [K \u03c3 (x, x ) \u2022 (\u03c3(z ) \u2212 e y )] = 0, thus its solution converges to this point as t \u2192 \u221e. This similarity in the evolution of \u03c3(z) implies the similarity in the evolution of the perfomance metric-such as BLEU-accross all three transformations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_56",
            "start": 321,
            "end": 626,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_57@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_57",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@0",
            "content": "Although the BLEU metric (Papineni et al., 2002) has stood the test of time, it is still an automated assessment of translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@1",
            "content": "To double-check the reliability of the results from Table 1, we decided to manually evaluate the translations from the WMT'13 En\u2192Ru test split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 137,
            "end": 279,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@2",
            "content": "To do this, we followed the human evaluation setup from (Berard et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 281,
            "end": 358,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@3",
            "content": "We formed two random samples of 135 instances each and gave them to two annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 360,
            "end": 442,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@4",
            "content": "45 instances were shared across two samples in order to calculate inter-annotator agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 444,
            "end": 535,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@5",
            "content": "Each instance consists of an original sentence in English and 4 candidate translations into Russian (reference, softmax, entmax, \u03b1-ReLU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 537,
            "end": 673,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@6",
            "content": "The annotators were to rate each translation on a 4-point scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 675,
            "end": 738,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@7",
            "content": "For annotation instructions, see Appendix C. The order of candidate translations was shuffled for each instance, so the annotators did not know which sentence is from which model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 740,
            "end": 918,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_58@8",
            "content": "Nevertheless, the annotator always had a good chance of guessing which translation was the reference one, due to the large difference in quality between human and machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_58",
            "start": 920,
            "end": 1102,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_59@0",
            "content": "The results of human evaluation are shown in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_59",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_59@1",
            "content": "Cohen's \u03ba = 0.56, indicating moderate agreement between annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_59",
            "start": 54,
            "end": 120,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_59@2",
            "content": "As we can see, all three models give approximately the same translation quality, and all three are significantly inferior to the reference translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_59",
            "start": 122,
            "end": 272,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_59@3",
            "content": "This is generally consistent with the results of 1.5-ReLU and 1.5-entmax in Table 1, but at the same time casts doubt on the softmax lag behind 1.5-ReLU and 1.5-entmax as the BLEU metric suggests.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_59",
            "start": 274,
            "end": 469,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_60@0",
            "content": "In Appendix D we give a few examples where 1.5-ReLU translates better than 1.5-entmax and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_60",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_61@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_61",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@0",
            "content": "Sparse seq2seq models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@1",
            "content": "Our proposed \u03b1-ReLU transformation is based on the \u03b1-entmax transformation of Peters et al. (2019), which in turn is a generalization of the sparsemax transformation (Martins and Astudillo, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 23,
            "end": 218,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@2",
            "content": "In our work, we study sparseness at the output of a neural network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 220,
            "end": 286,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@3",
            "content": "Nevertheless, there are a number of works aimed at sparsification within a neural network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 288,
            "end": 377,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@4",
            "content": "For example, Malaviya et al. (2018); Peters et al. (2019);Correia et al. (2019) show that sparsemax and \u03b1entmax can replace softmax in the attention mechanism with some success.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 379,
            "end": 555,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@5",
            "content": "A recent work of Zhang et al. (2021) attempted to replace softmax with a component-wise ReLU in the attention mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 557,
            "end": 676,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@6",
            "content": "Unfortunately, in its pure form, this replacement leads to the inability of the model to learn at all, since its loss function does not decrease during optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 678,
            "end": 842,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_62@7",
            "content": "The authors solve this problem by adding a normalizing layer on top of the attention layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_62",
            "start": 844,
            "end": 934,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_63@0",
            "content": "These and other works (Zhang et al., 2019) state that sparsity in the weights of attention produces more interpretable patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_63",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_63@1",
            "content": "However, Meister et al. (2021) questioned this claim and were unable to find clear evidence to support it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_63",
            "start": 129,
            "end": 234,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_63@2",
            "content": "Therefore, in this work, we focused on the application of \u03b1-ReLU to the output of the transformer model, and not to the mechanism of attention, but at the same time we do not deny the possibility of studying the latter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_63",
            "start": 236,
            "end": 454,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@0",
            "content": "Self-normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@1",
            "content": "Self-normalizing training aims to bypass the need of normalization during inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 20,
            "end": 108,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@2",
            "content": "This is done by tweaking the learning mechanism so that the sum of all predictions sums (approximately) to a constant value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 110,
            "end": 233,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@3",
            "content": "Theoretical work on why this works is poorly understood (Andreas et al., 2015) but early work in neural machine translation has shown its empirical value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 235,
            "end": 388,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@4",
            "content": "Vaswani et al. (2013) achieves that by using noisecontrastive estimation (the neural model is used to re-rank the output of a hierarchical phrase-based machine translation system).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 390,
            "end": 569,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@5",
            "content": "Noise-contrastive estimation is also the standard training mechanism for word2vec (more popular than the alternative hierarchical softmax), which also eschews any expensive normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 571,
            "end": 757,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@6",
            "content": "Differently, Devlin et al. (2014) changes the training loss to include a factor that encourages the normalizing factor to be 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 759,
            "end": 885,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_64@7",
            "content": "At inference time, this is just assumed and decoding time is reported to achieve a 15x speed-up.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_64",
            "start": 887,
            "end": 982,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_65@0",
            "content": "Limitations and Risks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_65",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_66@0",
            "content": "We believe that the main limitations of our work are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_66",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_67@0",
            "content": "\u2022 \u03b1-ReLU's output is still not a probability distribution, as required by the classical formulation of a probabilistic classification model. \u2022 \u03c4 evaluation requires either running the data through an untrained model with \u03b1-entmax at the output, or deriving a formula similar to (3) for each individual architecture. \u2022 Our approach only works for the case when \u03b1-ReLU is used at the output of the model, but it is not clear how to use it as an alternative to softmax/\u03b1-entmax in the attention layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_67",
            "start": 0,
            "end": 497,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_68@0",
            "content": "The last mentioned limitation leads to the potential risk of inability to learn if \u03b1-ReLU is misused in the intermediate layers of the neural network such as attention layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_68",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_68@1",
            "content": "The experiments of Zhang et al. (2021) using vanilla ReLU (2-ReLU with \u03c4 = 0 in our notation) instead of softmax to produce attention weights lead to a divergence of the loss function of the Transformer model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_68",
            "start": 176,
            "end": 384,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_68@2",
            "content": "This translates into a waste of energy, especially when training large models on large datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_68",
            "start": 386,
            "end": 481,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_68@3",
            "content": "Therefore, we believe that in the future, a preliminary mathematical analysis and/or experiments with small models on small datasets should be carried out as to why the unnormalized distribution of attention weights leads to the inability of the model to learn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_68",
            "start": 483,
            "end": 743,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_69@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_69",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_70@0",
            "content": "It seems that the sparsity of the output is natural for (sub)word prediction models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_70",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_70@1",
            "content": "Nevertheless, sparsity does not have to come with slowdown of computations, as our work shows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_70",
            "start": 85,
            "end": 178,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_70@2",
            "content": "The proposed transformation, \u03b1-ReLU, gives a sparse output, shows competitive performance, and is as fast as softmax.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_70",
            "start": 180,
            "end": 296,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_71@0",
            "content": "The reduced dependency on the vocabulary size seems particularly important in translation, where neural models are moving more and more towards multi-lingual ones, which in general have a much higher vocabulary size in order to accommodate enough tokens for all languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_71",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_72@0",
            "content": "A natural extension of this work will be the evaluation of \u03b1-ReLU in the problem of open-ended text generation, as well as a replacement for softmax in the attention layers of Transformer models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_72",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_73@0",
            "content": "\u2022 Architecture: Transformer, embedding size 512, 6 layers, 8 heads, hidden size 1024, shared vocabulary. \u2022 Batch size: 4096 tokens (with gradient accumulation for 8 steps). \u2022 Optimizer: ADAM, \u03b2 1 = 0.9, \u03b2 2 = 0.998, noam decay, learning rate 2.0, 4000 warmup steps. \u2022 Dropout: 0.3 \u2022 No label smoothing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_73",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_74@0",
            "content": "\u2022 Architecture: Transformer, embedding size 512, 6 layers, 8 heads, hidden size 2048, shared vocabulary of 40K tokens, shared embeddings and decoder embeddings. \u2022 Batch size: 4096 tokens (with gradient accumulation for 4 steps). \u2022 Optimizer: ADAM, \u03b2 1 = 0.9, \u03b2 2 = 0.998, noam decay, learning rate 2.0, 8000 warmup steps, average decay 0.0005. We do not report CO 2 consumption, as experiments were run in different countries, making aggregate statistics difficult to compute. The largest experiment (on WMT'13), were run in [MASKED], which benefits from a very low CO 2 emission intensity in its electrical mix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_74",
            "start": 0,
            "end": 611,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_75@0",
            "content": "Notation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_75",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_75@1",
            "content": "We let R denote the real numbers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_75",
            "start": 10,
            "end": 42,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_75@2",
            "content": "Bold-faced lowercase letters (x) denote vectors in Euclidean space, bold-faced uppercase letters (A) denote matrices, plain-faced lowercase letters (x) denote scalars, \u2022 denotes the Euclidean norm: x :=",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_75",
            "start": 44,
            "end": 245,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_76@0",
            "content": "x x. The gradient of f : R d \u2192 R is denoted by \u2207f .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_76",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_76@1",
            "content": "The Jacobian of z \u2192 g(z) is denoted by J g (z).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_76",
            "start": 52,
            "end": 98,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_76@2",
            "content": "Also, we denote ReLU(x) := [x] + := max{x, 0}, [d] := {1, . . . , d}, \u2206 d\u22121 := {p \u2208 R d | i p i = 1, p i \u2265 0}, e y := (0, . . . , 0, 1, 0, . . . , 0) where 1 is at y th position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_76",
            "start": 100,
            "end": 277,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_77@0",
            "content": "First, let us calculate the Jacobian of the mapping z \u2192 \u03b1-ReLU(z).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_77",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_77@1",
            "content": "Recall that",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_77",
            "start": 67,
            "end": 77,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_78@0",
            "content": "\u03b1-ReLU i (z) := [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 + .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_78",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_79@0",
            "content": "Therefore, the partial derivatives are given by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_79",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_80@0",
            "content": "\u2202[\u03b1-ReLU i (z)] \u2202z i = 1 \u03b1 \u2212 1 \u2022 [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 1 \u03b1\u22121 \u22121 + \u2022 (\u03b1 \u2212 1) = [(\u03b1 \u2212 1)z i \u2212 \u03c4 ] 2\u2212\u03b1 \u03b1\u22121 + , = [\u03b1-ReLU i (z)] 2\u2212\u03b1 \u2202[\u03b1-ReLU i (z)] \u2202z j = 0. i = j",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_80",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_81@0",
            "content": "Thus, the Jacobian can be written concisely as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_81",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_82@0",
            "content": "J \u03b1-ReLU (z) = diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 },(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_82",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_83@0",
            "content": "where raising to power is done component-wise (i.e. x \u03b2 = [x \u03b2 1 , . . . , x \u03b2 d ]), and diag[x] is a diagonal matrix with x on its diagonal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_83",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_84@0",
            "content": "Recall the definition of the Tsallis \u03b1-entropy:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_84",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_85@0",
            "content": "H \u03b1 [p] := 1 \u03b1(\u03b1 \u2212 1) \uf8eb \uf8ed 1 \u2212 j p \u03b1 j \uf8f6 \uf8f8 . Its gradient w.r.t. p is \u2207 p H \u03b1 [p] = \u2212 1 \u03b1 \u2212 1 p \u03b1\u22121 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_85",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_86@0",
            "content": "Combining this with (5), and using the chain rule, we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_86",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_87@0",
            "content": "\u2207 z H \u03b1 [\u03b1-ReLU(z)] = [J \u03b1-ReLU (z)] \u2022 \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] \u03b1\u22121 = diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 } \u2022 \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] \u03b1\u22121 = \u2212 1 \u03b1 \u2212 1 [\u03b1-ReLU(z)] 2\u2212\u03b1 [\u03b1-ReLU(z)] \u03b1\u22121 = \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z),(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_87",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_88@0",
            "content": "where is the Hadamard product (element-wise multiplication), and we used diag[x] \u2022 y = x y.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_88",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_88@1",
            "content": "Taking into account (6), the gradient of the \u03b1-ReLU loss (2) w.r.t. z is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_88",
            "start": 92,
            "end": 163,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_89@0",
            "content": "\u2207 z (z, y) = \u2207 z (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1 \u2212 1 1 + \u2207 z H \u03b1 [\u03b1-ReLU(z)] = (\u03b1-ReLU(z) \u2212 e y ) + J \u03b1-ReLU(z) z \u2212 \u03c4 \u03b1 \u2212 1 1 \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = (\u03b1-ReLU(z) \u2212 e y ) + 1 \u03b1 \u2212 1 diag{[\u03b1-ReLU(z)] 2\u2212\u03b1 } [(\u03b1 \u2212 1)z \u2212 \u03c4 1] \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = (\u03b1-ReLU(z) \u2212 e y ) + 1 \u03b1 \u2212 1 [(\u03b1 \u2212 1)z \u2212 \u03c4 1] 2\u2212\u03b1 \u03b1\u22121 + [(\u03b1 \u2212 1)z \u2212 \u03c4 1] \u03b1-ReLU(z) \u2212 1 \u03b1 \u2212 1 \u03b1-ReLU(z) = \u03b1-ReLU(z) \u2212 e y ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_89",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_90@0",
            "content": "where in the fourth line we used",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_90",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_91@0",
            "content": "[x] \u03b2 + x = [x] \u03b2 + [x] + = [x] \u03b2+1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_91",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_92@0",
            "content": "+ . This concludes the proof. and thus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_92",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_93@0",
            "content": "S(k) \u2248 \u03c3 2 p \u2212 \u2212\u03c6(\u03a6 \u22121 (x)) \u2022 \u03a6 \u22121 (x) + x x=p x= \u2212 [m(p)] 2 = \u03c3 2 s(p),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_93",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_94@0",
            "content": "where s(p) is defined by (8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_94",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_94@1",
            "content": "Hence, finding k \u2208 [d] that satisfies ( 10) is (approximately) equivalent to finding p \u2208 (0, 1) that satisfies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_94",
            "start": 30,
            "end": 139,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_95@0",
            "content": "\u03c3\u03a6 \u22121 (1 \u2212 p) = \u03c3m(p) \u2212 4 \u2022 p \u2212 \u03c3 2 s(p) \u21d4 \u03a6 \u22121 (1 \u2212 p) = m(p) \u2212 4 \u03c3 2 \u2022 p \u2212 s(p).(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_95",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_96@0",
            "content": "Let p * be the solution of (11).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_96",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_96@1",
            "content": "Then, taking into account (9), we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_96",
            "start": 33,
            "end": 70,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_97@0",
            "content": "\u03c4 (z) \u2248 \u03c3m(p * ) 2 \u2212 p * \u2212 \u03c3 2 s(p * ) 4 = \u03c3 2 m(p * ) \u2212 4 \u03c3 2 \u2022 p * \u2212 s(p * ) = \u03c3 2 \u03a6 \u22121 (1 \u2212 p * ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_97",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_98@0",
            "content": "which concludes the proof.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_98",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_99@0",
            "content": "Lemma 3. Let z = Wx be a pre-softmax vector of logits in the OpenNMT-py (Klein et al., 2017) implementation of the Transformer model (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_99",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_99@1",
            "content": "Then for any input, in a non-trained model the logits z 1 , . . . , z d are distributed according to the normal distribution N 0,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_99",
            "start": 157,
            "end": 285,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_100@0",
            "content": "E[w ij ] = 0, Var[w ij ] = (2a) 2 12 = a 2 3 = 2 d model + d vocab (12",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_100",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_101@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_101",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_102@0",
            "content": "Since x is the result of a layer normalization (Ba et al., 2016), we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_102",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_103@0",
            "content": "1 d model d model j=1 x j = 0, 1 d model d model j=1 x 2 j = 1(13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_103",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_104@0",
            "content": "Therefore, from ( 12) and ( 13), we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_104",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_105@0",
            "content": "E[z i ] = E \uf8ee \uf8f0 d model j=1 w ij x j \uf8f9 \uf8fb = d model j=1 E[w ij ] \u2022 x j = 0, Var[z i ] = Var \uf8ee \uf8f0 d model j=1 w ij x j \uf8f9 \uf8fb = 2 d model + d vocab d model j=1 x 2 j = 2 \u2022 d model d model + d vocab .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_105",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_106@0",
            "content": "Being a sum of independent random variables, by the Central Limit Theorem, each z i tends to normal distribution with the mean and variance above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_106",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_107@0",
            "content": "We provide derivation for the case of \u03b1-ReLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_107",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_107@1",
            "content": "Extension to \u03b1-entmax and softmax is done analogously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_107",
            "start": 46,
            "end": 99,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_107@2",
            "content": "Let x \u2208 R n 0 be the input vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_107",
            "start": 101,
            "end": 134,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_107@3",
            "content": "We define a feedforward neural network with L \u2212 1 hidden layers recursively:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_107",
            "start": 136,
            "end": 211,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_108@0",
            "content": "h (0) = x z (k) = 1 \u221a n k\u22121 W (k\u22121) h (k\u22121) , h (k) = \u03c3(z (k) ), k = 1, . . . , L \u2212 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_108",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_109@0",
            "content": "where W (k\u22121) \u2208 R n k \u00d7n k\u22121 is the weight matrix in the k th hidden layer, and \u03c3(\u2022) is a nonlinear activation function applied element-wise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_109",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_109@1",
            "content": "We consider the case of a multi-label classification, i.e. the output layer is a vector z",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_109",
            "start": 142,
            "end": 230,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_110@0",
            "content": ":= z (L) \u2208 R d ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_110",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_111@0",
            "content": "which is fed into the \u03b1-ReLU loss:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_111",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_112@0",
            "content": "(z, y) = (\u03b1-ReLU(z) \u2212 e y ) z \u2212 \u03c4 \u03b1 \u2212 1 1 + H \u03b1 [\u03b1-ReLU(z)],(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_112",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_113@0",
            "content": "where H \u03b1 [p] := 1 \u03b1(\u03b1\u22121) j (p j \u2212 p \u03b1 j ), \u03b1 = 1, is the Tsallis \u03b1-entropy (Tsallis, 1988).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_113",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_113@1",
            "content": "Given a training sample S := {(x, y)} learning is performed by minimizing the training error",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_113",
            "start": 93,
            "end": 184,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_114@0",
            "content": "L := E (x,y)\u223cS [ (z(x), y)](15",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_114",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_115@0",
            "content": "where K(x, x ) \u2208 R d\u00d7d is a positive semidefinite matrix, and z := z(x , t).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_115",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_116@0",
            "content": "Proof.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_116",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_116@1",
            "content": "From ( 15) and Lemma 1 we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_116",
            "start": 7,
            "end": 36,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_117@0",
            "content": "\u2207 z L = \u2207 z E (x ,y )\u223cS [ (z , y)] = \u2207 z (z, y) = \u03b1-ReLU(z) \u2212 e y ,(17)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_117",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_118@0",
            "content": "where we denoted z := z(x, t) and z := z(x , t) for shorthand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_118",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_118@1",
            "content": "Now, consider the gradient descent update",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_118",
            "start": 63,
            "end": 103,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_119@0",
            "content": "\u03b8 t+\u03b7 = \u03b8 t \u2212 \u03b7\u2207 \u03b8 L \u21d4 \u03b8 t+\u03b7 \u2212 \u03b8 t \u03b7 = \u2212\u2207 \u03b8 L, (18",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_119",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_120@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_120",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_121@0",
            "content": "where \u03b7 is the learning rate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_121",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_121@1",
            "content": "Taking the limit in (18) as \u03b7 \u2192 0, we have:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_121",
            "start": 30,
            "end": 72,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_122@0",
            "content": "d\u03b8 dt = \u2212\u2207 \u03b8 L = \u2212 E (x ,y )\u223cS [J z (\u03b8) \u2022 \u2207 z L],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_122",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_123@0",
            "content": "where the last equality is due to the chain rule.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_123",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_123@1",
            "content": "Combining this with (17), we get",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_123",
            "start": 50,
            "end": 81,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_124@0",
            "content": "d\u03b8 dt = \u2212 E (x ,y )\u223cS [J z (\u03b8) \u2022 (\u03b1-ReLU(z ) \u2212 e y )](19)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_124",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_125@0",
            "content": "Applying the chain rule again, and using ( 19), we have",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_125",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_126@0",
            "content": "dz dt = J z (\u03b8) \u2022 d\u03b8 dt = \u2212 E (x ,y )\u223cS [J z (\u03b8)J z (\u03b8) K(x,x ;\u03b8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_126",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_127@0",
            "content": "\u2022(\u03b1-ReLU(z ) \u2212 e y )].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_127",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_128@0",
            "content": "The quantity K(x, x ; \u03b8) was named the Neural Tangent Kernel by Jacot et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_128",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_128@1",
            "content": "They also showed (see their Theorem 1) that K(x, x ; \u03b8) \u2192 K(x, x ) as n 1 , . . . , n L\u22121 \u2192 \u221e, where K(x, x ) \u2208 R d\u00d7d is the deterministric kernel that does not depend on \u03b8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_128",
            "start": 85,
            "end": 257,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_128@2",
            "content": "This concludes the proof.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_128",
            "start": 259,
            "end": 283,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_129@0",
            "content": "You are shown a reference sentence and several candidate translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_129",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_129@1",
            "content": "Please indicate, for each, on a 4-point scale, how much of the meaning is represented in the translation, ignoring the language quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_129",
            "start": 71,
            "end": 206,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_130@0",
            "content": "Imagine you are a forgiving reader, ignoring any error that does not prevent you from getting the meaning of the text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_130",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_130@1",
            "content": "So please ignore language oddities, typographic errors and the like. (This is difficult but key to us!)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_130",
            "start": 119,
            "end": 221,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_131@0",
            "content": "The scale of meaning preservation is: 4 = Everything / 3 = Most / 2 = Little / 1 = None",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_131",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_132@0",
            "content": "As we are interested in comparing system's output, you can refine your judgement using + or \u2212, e.g. 3+.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_132",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_133@0",
            "content": "When you do not know, simply leave empty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_133",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_134@0",
            "content": "For instance, given the reference sentence \"This restaurant is beautiful and the staff is very friendly\", valid judgements for different translations are provided in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_134",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_134@1",
            "content": "We insist that evaluating by meaning differs from a natural intuitive evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_134",
            "start": 175,
            "end": 255,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_134@2",
            "content": "Provided the meaning is not impacted, we want to ignore the language quality, the punctuation, the casing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_134",
            "start": 257,
            "end": 362,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_135@0",
            "content": "Jacob Andreas, Maxim Rabinovich, Dan Michael I Jordan,  Klein, On the accuracy of selfnormalized log-linear models, 2015, Proceedings of the 28th International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_135",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_136@0",
            "content": "C Barry, N Arnold, H Balakrishnan,  Nagaraja, A First Course in Order Statistics, 2008, Classics in Applied Mathematics). Society for Industrial and Applied Mathematics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_136",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_137@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_137",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_138@0",
            "content": "Alexandre Berard, Ioan Calapodescu, Marc Dymetman, Claude Roux, Jean-Luc Meunier, Vassilina Nikoulina, Machine translation of restaurant reviews: New corpus for domain adaptation and robustness, 2019, Proceedings of the 3rd Workshop on Neural Generation and Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_138",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_139@0",
            "content": "UNKNOWN, None, 2013, Proceedings of the Eighth Workshop on Statistical Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_139",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_140@0",
            "content": "Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ales Tamchyna, Findings of the 2014 workshop on statistical machine translation, 2014-06-26, Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_140",
            "start": 0,
            "end": 363,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_141@0",
            "content": "UNKNOWN, None, 2014, Report on the 11th iwslt evaluation campaign, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_141",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_142@0",
            "content": "M Gon\u00e7alo, Vlad Correia, Andr\u00e9 Niculae,  Martins, Adaptively sparse transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_142",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_143@0",
            "content": "Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Fast and robust neural network joint models for statistical machine translation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_143",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_144@0",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_144",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_145@0",
            "content": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, The curious case of neural text degeneration, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_145",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_146@0",
            "content": "Arthur Jacot, Cl\u00e9ment Hongler, Franck Gabriel, Neural tangent kernel: Convergence and generalization in neural networks, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_146",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_147@0",
            "content": "Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush, OpenNMT: Opensource toolkit for neural machine translation, 2017, Proceedings of ACL 2017, System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_147",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_148@0",
            "content": "Chaitanya Malaviya, Pedro Ferreira, Andr\u00e9 Martins, Sparse and constrained attention for neural machine translation, 2018-07-15, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_148",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_149@0",
            "content": "F Andr\u00e9, Ram\u00f3n Martins,  Fernandez Astudillo, From softmax to sparsemax: A sparse model of attention and multi-label classification, 2016-06-19, Proceedings of the 33nd International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_149",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_150@0",
            "content": "Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell, Is sparse attention more interpretable?, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_150",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_151@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002-07-06, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_151",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_152@0",
            "content": "Ben Peters, F Andr\u00e9,  Martins, Smoothing and shrinking the sparse seq2seq search space, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_152",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_153@0",
            "content": "Ben Peters, Vlad Niculae, Andr\u00e9 Martins, Sparse sequence-to-sequence models, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_153",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_154@0",
            "content": "UNKNOWN, None, 2018, A call for clarity in reporting bleu scores, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_154",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_155@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_155",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_156@0",
            "content": "Felix Stahlberg, Bill Byrne, On NMT search errors and model errors: Cat got your tongue?, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_156",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_157@0",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_157",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_158@0",
            "content": "Constantino Tsallis, Possible generalization of boltzmann-gibbs statistics, 1988, Journal of statistical physics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_158",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_159@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_159",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_160@0",
            "content": "Ashish Vaswani, Yinggong Zhao, Victoria Fossum, David Chiang, Decoding with large-scale neural language models improves translation, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_160",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_161@0",
            "content": "Biao Zhang, Ivan Titov, Rico Sennrich, Sparse attention with linear units, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_161",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "429-ARR_v1_162@0",
            "content": "Jiajun Zhang, Yang Zhao, Haoran Li, Chengqing Zong, Attention with sparsity regularization for neural machine translation and summarization, 2019, IEEE ACM Trans. Audio Speech Lang. Process, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "429-ARR_v1_162",
            "start": 0,
            "end": 191,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_1",
            "tgt_ix": "429-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_1",
            "tgt_ix": "429-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_1",
            "tgt_ix": "429-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_2",
            "tgt_ix": "429-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_3",
            "tgt_ix": "429-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_5",
            "tgt_ix": "429-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_9",
            "tgt_ix": "429-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_12",
            "tgt_ix": "429-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_13",
            "tgt_ix": "429-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_14",
            "tgt_ix": "429-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_15",
            "tgt_ix": "429-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_16",
            "tgt_ix": "429-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_17",
            "tgt_ix": "429-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_18",
            "tgt_ix": "429-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_19",
            "tgt_ix": "429-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_20",
            "tgt_ix": "429-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_21",
            "tgt_ix": "429-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_22",
            "tgt_ix": "429-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_23",
            "tgt_ix": "429-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_24",
            "tgt_ix": "429-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_26",
            "tgt_ix": "429-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_27",
            "tgt_ix": "429-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_29",
            "tgt_ix": "429-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_31",
            "tgt_ix": "429-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_32",
            "tgt_ix": "429-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_34",
            "tgt_ix": "429-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_35",
            "tgt_ix": "429-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_37",
            "tgt_ix": "429-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_40",
            "tgt_ix": "429-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_40",
            "tgt_ix": "429-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_41",
            "tgt_ix": "429-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_42",
            "tgt_ix": "429-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_42",
            "tgt_ix": "429-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_43",
            "tgt_ix": "429-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_45",
            "tgt_ix": "429-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_44",
            "tgt_ix": "429-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_44",
            "tgt_ix": "429-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_44",
            "tgt_ix": "429-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_46",
            "tgt_ix": "429-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_48",
            "tgt_ix": "429-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_49",
            "tgt_ix": "429-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_47",
            "tgt_ix": "429-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_47",
            "tgt_ix": "429-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_47",
            "tgt_ix": "429-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_47",
            "tgt_ix": "429-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_50",
            "tgt_ix": "429-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_51",
            "tgt_ix": "429-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_51",
            "tgt_ix": "429-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_54",
            "tgt_ix": "429-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_55",
            "tgt_ix": "429-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_53",
            "tgt_ix": "429-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_53",
            "tgt_ix": "429-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_53",
            "tgt_ix": "429-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_53",
            "tgt_ix": "429-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_56",
            "tgt_ix": "429-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_59",
            "tgt_ix": "429-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_57",
            "tgt_ix": "429-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_57",
            "tgt_ix": "429-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_57",
            "tgt_ix": "429-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_57",
            "tgt_ix": "429-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_60",
            "tgt_ix": "429-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_63",
            "tgt_ix": "429-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_61",
            "tgt_ix": "429-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_61",
            "tgt_ix": "429-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_61",
            "tgt_ix": "429-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_61",
            "tgt_ix": "429-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_66",
            "tgt_ix": "429-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_65",
            "tgt_ix": "429-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_65",
            "tgt_ix": "429-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_65",
            "tgt_ix": "429-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_65",
            "tgt_ix": "429-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_68",
            "tgt_ix": "429-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_70",
            "tgt_ix": "429-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_71",
            "tgt_ix": "429-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_72",
            "tgt_ix": "429-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_75",
            "tgt_ix": "429-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_77",
            "tgt_ix": "429-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_78",
            "tgt_ix": "429-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_79",
            "tgt_ix": "429-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_80",
            "tgt_ix": "429-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_81",
            "tgt_ix": "429-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_82",
            "tgt_ix": "429-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_83",
            "tgt_ix": "429-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_84",
            "tgt_ix": "429-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_85",
            "tgt_ix": "429-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_86",
            "tgt_ix": "429-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_87",
            "tgt_ix": "429-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_88",
            "tgt_ix": "429-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_89",
            "tgt_ix": "429-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_90",
            "tgt_ix": "429-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_91",
            "tgt_ix": "429-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_92",
            "tgt_ix": "429-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_93",
            "tgt_ix": "429-ARR_v1_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_94",
            "tgt_ix": "429-ARR_v1_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_95",
            "tgt_ix": "429-ARR_v1_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_96",
            "tgt_ix": "429-ARR_v1_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_97",
            "tgt_ix": "429-ARR_v1_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_98",
            "tgt_ix": "429-ARR_v1_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_99",
            "tgt_ix": "429-ARR_v1_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_100",
            "tgt_ix": "429-ARR_v1_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_101",
            "tgt_ix": "429-ARR_v1_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_102",
            "tgt_ix": "429-ARR_v1_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_103",
            "tgt_ix": "429-ARR_v1_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_104",
            "tgt_ix": "429-ARR_v1_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_105",
            "tgt_ix": "429-ARR_v1_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_76",
            "tgt_ix": "429-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_107",
            "tgt_ix": "429-ARR_v1_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_108",
            "tgt_ix": "429-ARR_v1_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_109",
            "tgt_ix": "429-ARR_v1_110",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_110",
            "tgt_ix": "429-ARR_v1_111",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_111",
            "tgt_ix": "429-ARR_v1_112",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_112",
            "tgt_ix": "429-ARR_v1_113",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_113",
            "tgt_ix": "429-ARR_v1_114",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_114",
            "tgt_ix": "429-ARR_v1_115",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_115",
            "tgt_ix": "429-ARR_v1_116",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_116",
            "tgt_ix": "429-ARR_v1_117",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_117",
            "tgt_ix": "429-ARR_v1_118",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_118",
            "tgt_ix": "429-ARR_v1_119",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_119",
            "tgt_ix": "429-ARR_v1_120",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_120",
            "tgt_ix": "429-ARR_v1_121",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_121",
            "tgt_ix": "429-ARR_v1_122",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_122",
            "tgt_ix": "429-ARR_v1_123",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_123",
            "tgt_ix": "429-ARR_v1_124",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_124",
            "tgt_ix": "429-ARR_v1_125",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_125",
            "tgt_ix": "429-ARR_v1_126",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_126",
            "tgt_ix": "429-ARR_v1_127",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_110",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_111",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_112",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_113",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_114",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_115",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_116",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_117",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_118",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_119",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_120",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_121",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_122",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_123",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_124",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_125",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_126",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_127",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_128",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_106",
            "tgt_ix": "429-ARR_v1_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_129",
            "tgt_ix": "429-ARR_v1_130",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_130",
            "tgt_ix": "429-ARR_v1_131",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_131",
            "tgt_ix": "429-ARR_v1_132",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_132",
            "tgt_ix": "429-ARR_v1_133",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_133",
            "tgt_ix": "429-ARR_v1_134",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_129",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_130",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_131",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_132",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_133",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_134",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_128",
            "tgt_ix": "429-ARR_v1_129",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "429-ARR_v1_0",
            "tgt_ix": "429-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_1",
            "tgt_ix": "429-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_2",
            "tgt_ix": "429-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_2",
            "tgt_ix": "429-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_2",
            "tgt_ix": "429-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_3",
            "tgt_ix": "429-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_4",
            "tgt_ix": "429-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_5",
            "tgt_ix": "429-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_5",
            "tgt_ix": "429-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_6",
            "tgt_ix": "429-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_7",
            "tgt_ix": "429-ARR_v1_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_8",
            "tgt_ix": "429-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_9",
            "tgt_ix": "429-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_10",
            "tgt_ix": "429-ARR_v1_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_11",
            "tgt_ix": "429-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_12",
            "tgt_ix": "429-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_13",
            "tgt_ix": "429-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_14",
            "tgt_ix": "429-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_15",
            "tgt_ix": "429-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_16",
            "tgt_ix": "429-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_17",
            "tgt_ix": "429-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_17",
            "tgt_ix": "429-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_17",
            "tgt_ix": "429-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_18",
            "tgt_ix": "429-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_19",
            "tgt_ix": "429-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_19",
            "tgt_ix": "429-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_19",
            "tgt_ix": "429-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_19",
            "tgt_ix": "429-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_20",
            "tgt_ix": "429-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_21",
            "tgt_ix": "429-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_22",
            "tgt_ix": "429-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_22",
            "tgt_ix": "429-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_23",
            "tgt_ix": "429-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_23",
            "tgt_ix": "429-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_23",
            "tgt_ix": "429-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_24",
            "tgt_ix": "429-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_24",
            "tgt_ix": "429-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_24",
            "tgt_ix": "429-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_24",
            "tgt_ix": "429-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_25",
            "tgt_ix": "429-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_26",
            "tgt_ix": "429-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_26",
            "tgt_ix": "429-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_26",
            "tgt_ix": "429-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_26",
            "tgt_ix": "429-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_27",
            "tgt_ix": "429-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_27",
            "tgt_ix": "429-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_27",
            "tgt_ix": "429-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_28",
            "tgt_ix": "429-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_29",
            "tgt_ix": "429-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_29",
            "tgt_ix": "429-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_30",
            "tgt_ix": "429-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_31",
            "tgt_ix": "429-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_31",
            "tgt_ix": "429-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_31",
            "tgt_ix": "429-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_32",
            "tgt_ix": "429-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_32",
            "tgt_ix": "429-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_32",
            "tgt_ix": "429-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_33",
            "tgt_ix": "429-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_34",
            "tgt_ix": "429-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_34",
            "tgt_ix": "429-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_34",
            "tgt_ix": "429-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_35",
            "tgt_ix": "429-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_35",
            "tgt_ix": "429-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_35",
            "tgt_ix": "429-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_36",
            "tgt_ix": "429-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_37",
            "tgt_ix": "429-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_37",
            "tgt_ix": "429-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_38",
            "tgt_ix": "429-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_39",
            "tgt_ix": "429-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_40",
            "tgt_ix": "429-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_41",
            "tgt_ix": "429-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_41",
            "tgt_ix": "429-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_41",
            "tgt_ix": "429-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_41",
            "tgt_ix": "429-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_42",
            "tgt_ix": "429-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_43",
            "tgt_ix": "429-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_43",
            "tgt_ix": "429-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_43",
            "tgt_ix": "429-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_43",
            "tgt_ix": "429-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_44",
            "tgt_ix": "429-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_45",
            "tgt_ix": "429-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_45",
            "tgt_ix": "429-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_46",
            "tgt_ix": "429-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_46",
            "tgt_ix": "429-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_46",
            "tgt_ix": "429-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_47",
            "tgt_ix": "429-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_48",
            "tgt_ix": "429-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_48",
            "tgt_ix": "429-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_49",
            "tgt_ix": "429-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_50",
            "tgt_ix": "429-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_50",
            "tgt_ix": "429-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_50",
            "tgt_ix": "429-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_50",
            "tgt_ix": "429-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_51",
            "tgt_ix": "429-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_52",
            "tgt_ix": "429-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_53",
            "tgt_ix": "429-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_54",
            "tgt_ix": "429-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_54",
            "tgt_ix": "429-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_54",
            "tgt_ix": "429-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_54",
            "tgt_ix": "429-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_55",
            "tgt_ix": "429-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_56",
            "tgt_ix": "429-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_56",
            "tgt_ix": "429-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_56",
            "tgt_ix": "429-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_57",
            "tgt_ix": "429-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_58",
            "tgt_ix": "429-ARR_v1_58@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_59",
            "tgt_ix": "429-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_59",
            "tgt_ix": "429-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_59",
            "tgt_ix": "429-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_59",
            "tgt_ix": "429-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_60",
            "tgt_ix": "429-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_61",
            "tgt_ix": "429-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_62",
            "tgt_ix": "429-ARR_v1_62@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_63",
            "tgt_ix": "429-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_63",
            "tgt_ix": "429-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_63",
            "tgt_ix": "429-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_64",
            "tgt_ix": "429-ARR_v1_64@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_65",
            "tgt_ix": "429-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_66",
            "tgt_ix": "429-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_67",
            "tgt_ix": "429-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_68",
            "tgt_ix": "429-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_68",
            "tgt_ix": "429-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_68",
            "tgt_ix": "429-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_68",
            "tgt_ix": "429-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_69",
            "tgt_ix": "429-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_70",
            "tgt_ix": "429-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_70",
            "tgt_ix": "429-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_70",
            "tgt_ix": "429-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_71",
            "tgt_ix": "429-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_72",
            "tgt_ix": "429-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_73",
            "tgt_ix": "429-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_74",
            "tgt_ix": "429-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_75",
            "tgt_ix": "429-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_75",
            "tgt_ix": "429-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_75",
            "tgt_ix": "429-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_76",
            "tgt_ix": "429-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_76",
            "tgt_ix": "429-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_76",
            "tgt_ix": "429-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_77",
            "tgt_ix": "429-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_77",
            "tgt_ix": "429-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_78",
            "tgt_ix": "429-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_79",
            "tgt_ix": "429-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_80",
            "tgt_ix": "429-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_81",
            "tgt_ix": "429-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_82",
            "tgt_ix": "429-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_83",
            "tgt_ix": "429-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_84",
            "tgt_ix": "429-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_85",
            "tgt_ix": "429-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_86",
            "tgt_ix": "429-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_87",
            "tgt_ix": "429-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_88",
            "tgt_ix": "429-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_88",
            "tgt_ix": "429-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_89",
            "tgt_ix": "429-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_90",
            "tgt_ix": "429-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_91",
            "tgt_ix": "429-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_92",
            "tgt_ix": "429-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_93",
            "tgt_ix": "429-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_94",
            "tgt_ix": "429-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_94",
            "tgt_ix": "429-ARR_v1_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_95",
            "tgt_ix": "429-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_96",
            "tgt_ix": "429-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_96",
            "tgt_ix": "429-ARR_v1_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_97",
            "tgt_ix": "429-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_98",
            "tgt_ix": "429-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_99",
            "tgt_ix": "429-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_99",
            "tgt_ix": "429-ARR_v1_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_100",
            "tgt_ix": "429-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_101",
            "tgt_ix": "429-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_102",
            "tgt_ix": "429-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_103",
            "tgt_ix": "429-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_104",
            "tgt_ix": "429-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_105",
            "tgt_ix": "429-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_106",
            "tgt_ix": "429-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_107",
            "tgt_ix": "429-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_107",
            "tgt_ix": "429-ARR_v1_107@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_107",
            "tgt_ix": "429-ARR_v1_107@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_107",
            "tgt_ix": "429-ARR_v1_107@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_108",
            "tgt_ix": "429-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_109",
            "tgt_ix": "429-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_109",
            "tgt_ix": "429-ARR_v1_109@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_110",
            "tgt_ix": "429-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_111",
            "tgt_ix": "429-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_112",
            "tgt_ix": "429-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_113",
            "tgt_ix": "429-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_113",
            "tgt_ix": "429-ARR_v1_113@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_114",
            "tgt_ix": "429-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_115",
            "tgt_ix": "429-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_116",
            "tgt_ix": "429-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_116",
            "tgt_ix": "429-ARR_v1_116@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_117",
            "tgt_ix": "429-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_118",
            "tgt_ix": "429-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_118",
            "tgt_ix": "429-ARR_v1_118@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_119",
            "tgt_ix": "429-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_120",
            "tgt_ix": "429-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_121",
            "tgt_ix": "429-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_121",
            "tgt_ix": "429-ARR_v1_121@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_122",
            "tgt_ix": "429-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_123",
            "tgt_ix": "429-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_123",
            "tgt_ix": "429-ARR_v1_123@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_124",
            "tgt_ix": "429-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_125",
            "tgt_ix": "429-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_126",
            "tgt_ix": "429-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_127",
            "tgt_ix": "429-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_128",
            "tgt_ix": "429-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_128",
            "tgt_ix": "429-ARR_v1_128@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_128",
            "tgt_ix": "429-ARR_v1_128@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_129",
            "tgt_ix": "429-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_129",
            "tgt_ix": "429-ARR_v1_129@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_130",
            "tgt_ix": "429-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_130",
            "tgt_ix": "429-ARR_v1_130@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_131",
            "tgt_ix": "429-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_132",
            "tgt_ix": "429-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_133",
            "tgt_ix": "429-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_134",
            "tgt_ix": "429-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_134",
            "tgt_ix": "429-ARR_v1_134@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_134",
            "tgt_ix": "429-ARR_v1_134@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_135",
            "tgt_ix": "429-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_136",
            "tgt_ix": "429-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_137",
            "tgt_ix": "429-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_138",
            "tgt_ix": "429-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_139",
            "tgt_ix": "429-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_140",
            "tgt_ix": "429-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_141",
            "tgt_ix": "429-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_142",
            "tgt_ix": "429-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_143",
            "tgt_ix": "429-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_144",
            "tgt_ix": "429-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_145",
            "tgt_ix": "429-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_146",
            "tgt_ix": "429-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_147",
            "tgt_ix": "429-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_148",
            "tgt_ix": "429-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_149",
            "tgt_ix": "429-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_150",
            "tgt_ix": "429-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_151",
            "tgt_ix": "429-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_152",
            "tgt_ix": "429-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_153",
            "tgt_ix": "429-ARR_v1_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_154",
            "tgt_ix": "429-ARR_v1_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_155",
            "tgt_ix": "429-ARR_v1_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_156",
            "tgt_ix": "429-ARR_v1_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_157",
            "tgt_ix": "429-ARR_v1_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_158",
            "tgt_ix": "429-ARR_v1_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_159",
            "tgt_ix": "429-ARR_v1_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_160",
            "tgt_ix": "429-ARR_v1_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_161",
            "tgt_ix": "429-ARR_v1_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "429-ARR_v1_162",
            "tgt_ix": "429-ARR_v1_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1393,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "429-ARR",
        "version": 1
    }
}