{
    "nodes": [
        {
            "ix": "296-ARR_v1_0",
            "content": "Flooding-X: Improving BERT's Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_2",
            "content": "Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training. However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples. To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks. We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch. Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training. We experimentally show that our method improves Bert's resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "296-ARR_v1_4",
            "content": "Despite their impressive performances on various NLP tasks, deep neural networks such as BERT (Devlin et al., 2019) suffer a sharp decline facing deliberately constructed adversarial attacks (Zeng et al., 2021;Nie et al., 2020;Zang et al., 2020;Ren et al., 2019;Zhang et al., 2019). A line of works attempt to alleviate this problem by creating adversarially robust models via defense methods, including adversarial data augmentation (Chen et al., 2021;Si et al., 2021), regularizing (Wang et al., 2020a), and adversarial training (Wang et al., 2020b;Zhu et al., 2019;Madry et al., 2018). Data augmentation and adversarial training rely on extra adversarial examples generated either by handcrafting or conducting gradient ascent on the clean data for virtual adversarial samples. However, generating adversarial examples scales up the cost of training computationally, which makes vanilla adversarial training almost impractical on large-scale NLP tasks like QNLI (Questionanswering NLI). Increasing researches express their concern of the time-consuming property of standard adversarial training and offer cheaper but competitive alternatives by (i) replacing the perturbation generation with an extra generator network (Baluja and Fischer, 2017;Xiao et al., 2018), or by (ii) combining the gradient computation of clean data and perturbations into one backward pass (Shafahi et al., 2019). These approaches still rely on extra adversarial examples generated either by the model itself or by an extra module.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_5",
            "content": "In this work, we propose a novel method, Flooding-X, to largely improve adversarial robustness without any adversarial examples, maintaining the same computational cost as conventional BERT fine-tuning. The vanilla Flooding (Ishida et al., 2020) method is a practical regularization technique to boost model generalization by preventing further reduction of the training loss when it reaches a reasonably small value. It results in a model performing normal gradient descent when training loss is above the decided value but gradient ascent when below. By continuing to \"random walk\" with the same non-zero value as a \"virtual loss\", the model drifts into an area with a flat loss landscape that is claimed to lead to better generalization (Ishida et al., 2020). Interestingly, we find that Flooding method is also promising in increasing models' resistance to adversarial attacks. Despite the significant rise in robust accuracy, the so-called reasonably small value, which is a hyperparameter, takes effort to be found and varies for each dataset, which requires an overly extensive search among the numerous candidates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_6",
            "content": "In an attempt to narrow down the candidates of hyper-parameter, we propose gradient accordance as an informative criterion for optimal values that bring Flooding into effect, which is used as a building-block in Flooding-X. We measure how accordant the gradients of the batches are by analyzing how the gradient descent steps based on part of an epoch affect the loss of each batch. Gradient accordance is computationally friendly and is tractable during training process. Experiments on various tasks show a close relation between gradient accordance and overfitting. As a result, we propose gradient accordance as a reliable flooding criterion to make the training loss flood around the level when the model has nearly overfitted. That is to say, we leverage the training loss of the model right before overfitting as the value of flood level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_7",
            "content": "Flooding-X is especially useful and shows great advantage over adversarial training in terms of computational cost when the training dataset is relatively large. Experimental results demonstrate that our method achieves stated-of-the-art robust accuracy with BERT on various tasks and improves its robust accuracy by 100 to 400% without using any adversarial example, consuming any extra training time, or conducting overly extensive search for hyper-parameter. Our main contributions are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_8",
            "content": "1) We propose a novel method, Flooding-X, that achieves state-of-the-art robust accuracy for BERT on various tasks, which is adversarial-example-free and takes no more training time than fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_9",
            "content": "2) We propose a promising indicator, i.e. gradient accordance, to alleviate Flooding method from tedious search of the hyper-parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_10",
            "content": "3) We conduct comprehensive experiments on NLP tasks to illustrate the potential of Flooding for improving BERT's adversarial robustness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_11",
            "content": "Why Does Flooding Boost Adversarial",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "296-ARR_v1_12",
            "content": "Robustness?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_13",
            "content": "Vanilla Flooding",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "296-ARR_v1_14",
            "content": "We first describe the vanilla Flooding regularization method (Ishida et al., 2020) for alleviating overfitting via keeping training loss from reducing to zero. Under the main assumption that learning until zero loss is harmful, Ishida et al. (2020) propose Flooding to intentionally prevent further reduction of the training loss when it reaches a reasonably small value, which is called the flood level. Intuitively, this approach makes the training loss float around the pre-defined flood level and alter from normal mini-batch gradient descent to gradient ascent if the loss is below the flood level. With the constraint of flood level, the model will continue to \"random walk\" around the non-zero training loss, which is expected to reach a flat loss landscape.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_15",
            "content": "The algorithm of Flooding is defined as follow:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_16",
            "content": "J(\u03b8) = |J(\u03b8) \u2212 b| + b,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_17",
            "content": "where J denotes the original learning objective, and J represents the modified learning objective with flooding. The positive value b is the flood level specified by user, and \u03b8 is the model parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_18",
            "content": "Accordingly, the flooded empirical risk is then defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_19",
            "content": "R(f ) = | R(f ) \u2212 b| + b,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_20",
            "content": "within which R(f ) / R(f ) denotes the original / flooded empirical risk respectively, and f refers to the score function to be learned by the model. During the back propagation process, the gradient of R(f ) w.r.t. model parameters and R(f ) point to the same direction when R(f ) is above b but to the opposite direction when it is below b. As a result, model performs normal gradient descent when the learning objective is above the flood level, and gradient ascent when below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_21",
            "content": "Smooth Parameter Landscape Leads to Better Robustness",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "296-ARR_v1_22",
            "content": "According to the definition described in the previous section, Flooding does not make any difference to the training process when the loss is beyond the flood level. When the training loss approaches the flood level, on closer inspection, gradient descent and gradient ascent begin to alternate. Assume that the model with learning rate \u03b5 performs gradient descent for the n-th batch and then gradient ascent for batch n + 1, which results in:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_23",
            "content": "\u03b8 n = \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ), \u03b8 n+1 = \u03b8 n + \u03b5g(\u03b8 n ).(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_24",
            "content": "In the equations above, g(\u03b8) = \u2207 \u03b8 J(\u03b8) is the gradient of J(\u03b8) w.r.t. model parameters. We can then get",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_25",
            "content": "\u03b8 n+1 = \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) + \u03b5g \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_26",
            "content": "which is, by Taylor expansion, approximately equivalent to",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_27",
            "content": "\u2248 \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) + \u03b5 g(\u03b8 n\u22121 ) \u2212 \u03b5\u2207 \u03b8 g(\u03b8 n\u22121 )g(\u03b8 n\u22121 ) = \u03b8 n\u22121 \u2212 \u03b5 2 2 \u2207 \u03b8 \u2225g(\u03b8 n\u22121 )\u2225 2 .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_28",
            "content": "(5) Thus, theoretically, when the training loss is relatively low, the model alters into a new learning mode where the learning rate is \u03b5 2 /2 and the objective is to minimize \u2225g(\u03b8)\u2225 2 . Generally, the flooded model is guided into an area with a smooth parameter landscape that leads to better adversarial robustness (Prabhu et al., 2019;Yu et al., 2018;Li et al., 2018a). As is demonstrated in Figure 1, adversarial training brings about a smoother loss change to the model when the input embedding is perturbed by Gaussian random noise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_29",
            "content": "Achilles' Heel of Flooding",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "296-ARR_v1_30",
            "content": "Despite its potential in boosting model's resistance to adversarial attacks, the optimal flood level has to be searched by performing exhaustive search within a wide range at tiny steps, which is not easily at hand. A relatively large value of flood level lengthens the gradient steps and keeps the model from convergence, while a tiny value causes hardly any difference to the training process. The effect of Flooding deeply relies on the flood level, which, at the same time, is also sensitive to the subtle change of this hyper-parameter. Figure 2 reveals that even a slight change on the value of flood level can make a huge difference on the adversarial robustness of the so-trained model. In an attempt to ease the effort of searching and make the best of Flooding, we propose a promising and reliable criterion to narrow down the search space, which is described in detail in the next section.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_31",
            "content": "Figure 2: Influence of different flood levels on performance of the trained BERT on SST-2. The range marked in yellow is lined out by our proposed criterion , i.e., gradient accordance. The optimal value of flood level is guaranteed within the narrowed-down space.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_32",
            "content": "Gradient Accordance as a Criterion for Flooding",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "296-ARR_v1_33",
            "content": "Since Flooding is proposed as an attempt to avoid overfitting, we intuitively suppose that the optimal flood level would be found at the stage when the model is about to overfit. That is, we leverage the training loss before overfitting as the flood level. Inspired by influence function (Koh and Liang, 2017), we propose gradient accordance as a criterion for flooding, which is empirically proved to be reliable and indicative. We consider the effect of the model updated w.r.t. one epoch on each of its batches as a signal of overfitting. As is indicated by its name, this criterion measures the relation among the gradients of each batch on epoch level, evaluating whether the model updated on an epoch has the same positive effect on the batches on average. Now we provide the formal definition of gradient accordance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_34",
            "content": "Preliminaries",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "296-ARR_v1_35",
            "content": "We denote a model as a functional approximation f which is parameterized by \u03b8. Consider a training data point x with the ground truth label y, which results in a loss L(f (\u03b8, x), y). The gradient of the loss w.r.t. the parameters is thus",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_36",
            "content": "g = \u2207 \u03b8 L(f (\u03b8, x), y),(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_37",
            "content": "whose negation denotes the direction in which the parameters \u03b8 are updated to better correspond to the desired outputs on the training data (Fort et al., 2019). Now let's consider two data points x 1 and x 2 with their corresponding labels y 1 and y 2 . According to the definition above, the gradient of sample 1 is g 1 = \u2207 \u03b8 L(f (\u03b8, x 1 ), y 1 ). We try to inspect how the small change of \u03b8 in the direction \u2212g 1 influences the loss on sample x 1 or x 2 :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_38",
            "content": "\u2206L 1 =L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ), y 1 ) \u2212 L(f (\u03b8, x 1 ), y 1 ),(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_39",
            "content": "where f (\u03b8, x 1 ) can be expanded by Taylor expansion to be:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_40",
            "content": "f (\u03b8, x 1 ) = f (\u03b8 \u2212 \u03b5g 1 , x 1 ) + \u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_41",
            "content": "(8) Here, we refer to (\u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 )) as T (x 1 ); and by repeating the similar expansion we can get",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_42",
            "content": "L(f (\u03b8, x 1 ), y 1 ) = L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ) + T (x 1 ), y 1 ) = L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ), y 1 ) + \u2202L \u2202f T (x 1 ) + O(T 2 (x 1 )).(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_43",
            "content": "Equation ( 7) is thus equal to",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_44",
            "content": "\u2206L 1 = \u2212 \u2202L \u2202f T (x 1 ) \u2212 O(T 2 (x 1 )) = \u2212 \u2202L \u2202f (\u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 )) = \u2212\u03b5g 1 \u2022 g 1 \u2212 O(\u03b5 2 ).(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_45",
            "content": "Similarly, the change of the loss on x 2 caused by the gradient update by",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_46",
            "content": "x 1 is \u2206L 2 = \u2212\u03b5g 1 \u2022 g 2 \u2212 O(\u03b5 2 ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_47",
            "content": "Notably, \u2206L 1 is negative by definition since the model is updated with respect to x 1 and naturally leads to a decrease on its loss. The model updated on x 1 is considered to have a positive effect on x 2 if \u2206L 2 is also negative while an opposite effect if positive. The equations above demonstrate that this co-relation is equivalent to the overlap between the gradients of the two data points g 1 \u2022 g 2 , which we hereafter refer to as gradient accordance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_48",
            "content": "Coarse-Grained Gradient Accordance",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "296-ARR_v1_49",
            "content": "Data-point-level gradient accordance is too finegrained to be tractable in practice. Thus, we attempt to scale it up and result in coarse-grain gradient accordance at batch level, which is computationally tractable and still reliable as a criterion for overfitting. Consider a training batch B 0 with n samples X = {x 1 , x 2 , . . . , x n } and labels y = {y 1 , y 2 , . . . , y n } of k classes {c 1 , c 2 , . . . , c k }. These samples can be divided into k groups according to their labels",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_50",
            "content": "X = X 1 \u222aX 2 \u222a\u2022 \u2022 \u2022\u222aX k ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_51",
            "content": "and so are the labels y = k i=1 y i , where all the samples in X m belong to class c m . Thus, we have the sub-batch B 1 0 = {X 1 , y 1 }. We then define class accordance score of two sub-batches B 1 0 and B 2 0 of classes c 1 and c 2 as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_52",
            "content": "C(B 1 0 , B 2 0 ) = E[cos(g 1 , g 2 )],(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_53",
            "content": "where g 1 is the gradient of the training loss of sub-batch B 1 0 w.r.t. the model parameters, and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_54",
            "content": "cos(g 1 , g 2 ) = (g 1 /|g 1 |) \u2022 (g 2 /|g 2 |).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_55",
            "content": "Class accordance measures whether the gradient taken with respect to a sub-batch B 1 0 of class c 1 will also decrease the loss for samples in another sub-batch B 2 0 of class c 2 (Fort et al., 2019;Fu et al., 2020). Further consider that there are N batches in one training epoch and the training samples are of k classes. The batch accordance score between batches B s and B t is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_56",
            "content": "S batch accd (B s , B t ) = 1 k(k \u2212 1) k j=1 k i=1 i\u0338 =j C(B i s , B j t ). (12",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_57",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_58",
            "content": "Batch accordance quantifies the learning consistency of two batches by evaluating how the model updated on one batch affects the other. To be more specific, a positive batch accordance denotes that the measured two batches are under the same learning pace since the model updated according to each batch benefits them both. The gradient accordance of certain epoch (or a part of an epoch, namely the sub-epoch) is finally defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_59",
            "content": "S epoch accd = 1 N (N \u2212 1) N j=i+1 N \u22121 i=1 S batch accd (B s , B t ). (13",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_60",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_61",
            "content": "Gradient accordance scales the batch accordance score up from a measure of two batches to that of a sub-epoch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_62",
            "content": "Criterion for Flooding A positive gradient accordance means that the model performed gradient descent w.r.t. the certain epoch decreases the loss of its batches on average, indicating that the learning pace of most batches are in line with each other. A negative one means that the model has overfitted to some of the training batches since the update of one epoch increases the loss of its batches on average, which is right the stage we would like to identify for the model by gradient accordance. We assume that the optimal flood level lies in the range of the training loss of a model when it is about to overfit. In the following section, we empirically prove that gradient accordance is a reliable and promising criterion for flooding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_63",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "296-ARR_v1_64",
            "content": "In this section, we provide comprehensive analysis on Flooding-X through extensive experiments on five text classification datasets of various tasks and scales: SST (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), IMDB (Maas et al., 2011) and AG News (Zhang et al., 2015). We conduct experiments on BERTbase (Devlin et al., 2019) and compare robust accuracy of Flooding-X with other adversarial training algorithms to demonstrate its strength.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_65",
            "content": "Baseline Methods",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "296-ARR_v1_66",
            "content": "We compare our proposed Flooding-X with three adversarial training algorithms and one regularization method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_67",
            "content": "PGD Projected gradient descent (PGD, Madry et al., 2018) formulates adversarial training algorithms into solving a minimax problem that minimizes the empirical loss on adversarial examples that can lead to maximized adversarial risk.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_68",
            "content": "FreeLB Zhu et al. (2019) propose FreeLB to improve the generalization of language models. By adding adversarial perturbations to word embeddings, FreeLB generates virtual adversarial samples inside the region around input samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_69",
            "content": "TAVAT Token-Aware Virtual Adversarial Training (TAVAT, Li and Qiu, 2021) aims at fine-grained perturbations, leveraging a token-level accumulated perturbation vocabulary to initialize the perturbations better and constraining them within a token-level normalization ball.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_70",
            "content": "InfoBERT InfoBERT (Wang et al., 2020a) leverages two mutual-information-based regularizers for robust model training, suppressing noisy mutual information while increasing mutual information between local stable features and global features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_71",
            "content": "Attack Methods and Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "296-ARR_v1_72",
            "content": "Three well-received attack methods are leveraged via TextAttack (Morris et al., 2020) for an extensive comparison between our proposed method and baseline algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_73",
            "content": "TextFooler identifies the important words for target model and repeats replacing them with synonyms until the prediction of the model is altered. Similarly, TextBugger (Li et al., 2018b) also searches for important words and modifies them by choosing an optimal perturbation from the generated several kinds of perturbations. BERTAttack (Li et al., 2020) applies BERT in a semantic-preserving way to generate substitutes for the vulnerable words detected in the given input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_74",
            "content": "We consider four evaluation metrics to measure BERT's resistance to the mentioned adversarial attacks under different defence algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_75",
            "content": "Clean% The clean accuracy refers to the model's test accuracy on the original clean dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_76",
            "content": "Aua% Accuracy under attack measures the model's prediction accuracy on the adversarial data deliberately generated by certain attack method. A higher Aua% means a more robust model and a better defender.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_77",
            "content": "Suc% Attack success rate is evaluated by the ratio of the number of texts successfully perturbed by a specific attack method to the number of all the involved texts. Robust models are expected to score low at Suc%. #Query Number of queries denotes the average attempts the attacker queries the target model. The larger the number is, the harder the model is to be attacked.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_78",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "296-ARR_v1_79",
            "content": "All the baseline methods are re-implemented based on their open-released codes and the results are competing to those reported. We train our models on NVIDIA RTX 3090 and RTX 2080Ti GPUs, depending on the volume of the dataset involved. Most of the parameters such as learning rate and warm-up step are in line with vanilla BERT (Devlin et al., 2019) and the baseline methods. For all of the adversarial methods we set the training step to be 5 for a fair comparison, which is a trade-off between training cost and model performance . The clean accuracy (Clean%) is tested on the whole test dataset. The other three metrics (e.g., Aua%, Suc% and #Query) are evaluated on the whole test dataset for SST-2 and MRPC, and 800 randomly chosen samples for IMDB, AG NEWS, and QNLI. We train 10 epochs for each model on each dataset, among which the last epochs are selected for the comparison of adversarial robustness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_80",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "296-ARR_v1_81",
            "content": "The extensive results of all the above mentioned methods are summarized in Table 1. Generally, our Flooding-X method improves BERT by a large margin in terms of its resistance to adversarial attacks, surpassing the baseline adversarial training algorithms on most datasets under different attack methods. Under TextFooler attack , our algorithm reaches the best robust performance on four datasets: IMDB, AG News, SST-2, and MRPC. We observe that Flooding is more effective on smaller datasets than larger ones, since the smaller datasets with shorter training sentences are easier to be memorized by the neural network and are more likely to cause overfitting. On QNLI dataset where Flooding-X fails to win, the accuracy under attack is only 0.2 points lower than the 5-step PGD. This might be explained by the mild change in gradient accordance during training on QNLI dataset, in which case the precise stage of overfitting is hard to be identified. Though we believe that a better value of flood level exists and can further boost the performance, we refuse to take on the pattern of extensive hyper-parameter searching which is against the original purpose of Flooding-X. Notably, our method performs better than the baseline adversarial training methods by 5 to 20 points on average even without using any adversarial examples as training source, not to mention the vanilla BERT. Under most cases, our method remains the best performing algorithm facing BERTAttack (Li et al., 2020) and TextBugger (Li et al., 2018b). This proves that our method maintains effectiveness under different kinds of adversarial attacks. As a byproduct, the clean accuracy of our method is also the best among all the baseline methods, which is inherent to the vanilla Flooding that aims at better generalization. In the cases of AG News and QNLI, our re-implement the results of BERT fine-tuning to 97.0 and 91.6 respectively so Flooding-X does not surpass the reported performance, but still outperforming the baselines of our implementation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_82",
            "content": "Analysis and Discussion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "296-ARR_v1_83",
            "content": "In this section, we construct supplementary experiments to further analyze the effectiveness of Flooding-X and its building block, i.e., gradient accordance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_84",
            "content": "Does Gradient Accordance Capture",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "296-ARR_v1_85",
            "content": "Overfitting?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_86",
            "content": "Influence function (Koh and Liang, 2017) inspects the influence of one single training data on the model prediction and stiffness (Fort et al., 2019) measures how the model updated according to one sample affects the model prediction on another.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_87",
            "content": "Based on these two works, gradient accordance is proposed as a means for identifying model overfitting at sub-epoch level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_88",
            "content": "As seen in Figure 3, during training process, the turning point of gradient accordance from negative to positive closely matches the point when the test loss is about to increase, which is well received as a signal of overfitting. Since it is computationally intractable to calculate gradient accordance after trained on every single batch, we can only figure out the range where the model is about to overfit by computing gradient accordance at sub-epoch level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_89",
            "content": "How does Flooding-X Help with",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "296-ARR_v1_90",
            "content": "Robustness?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_91",
            "content": "Despite its outstanding performance of the last training epoch, we find that Flooding-X boosts the robustness of model at an earlier stage than standard fine-tuning and adversarial training methods like FreeLB. As is shown in Figure 4, Flooding-X",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_92",
            "content": "Time Consumption",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "296-ARR_v1_93",
            "content": "To further reveal the strength of Flooding-X besides its robustness performance, we compare its GPU training time consumption with baseline methods The adversarial examples are generated by replacing the original texts based on certain rules such as semantic similarity (Alzantot et al., 2018;Li et al., 2020). Ebrahimi et al. (2018) propose a perturbation strategy that conducts character insertion, deletion, and replacement. Jia and Liang (2017) mislead MRC models via a human-involved phrase generation method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_94",
            "content": "The mentioned algorithms of AT generates additional adversarial examples either by calculating gradients or by human force, which is computationally expensive and effort taking.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_95",
            "content": "Overfitting and Criterion Deep neural networks are shown to suffer from overfitting to training configurations and memorise training scenarios (Takeoka et al., 2021;Rodriguez et al., 2021;Roelofs et al., 2019;Werpachowski et al., 2019), which leads to poor generalization and vulnerability towards adversarial perturbations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_96",
            "content": "One way of identifying overfitting is to see whether the generalization gap, i.e., the test minus the training loss, is increasing or not (Goodfellow et al., 2016). Ishida et al. (2020) further decompose the situation of the generalization gap increasing into two stages: The first stage is when training and test losses are both decreasing, but the former is decreasing faster then the latter. The next stage is when the training loss is decreasing but the test loss is increasing, after which the training loss continues to approach zero and memorize the training data completely Belkin et al., 2018;Arpit et al., 2017). Derived from influence function (Koh and Liang, 2017), Fort et al. (2019) propose the concept of Stiffness as a new perspective of generalization. They measure how stiff a network is by looking at how a small gradient step in the network parameters on one example affects the loss on another example. This criterion carries is theoretically proved to have a close relation with generalization and overfitting. However, from the practical perspective, it is computationally intractable to compute the stiffness between every single sample during the process of standard training where thousands of samples are involved in one batch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_97",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "296-ARR_v1_98",
            "content": "In this work, we propose Flooding-X as an efficient and computational-friendly algorithm for improving BERT's resistance to adversarial attacks. We first theoretically prove that the vanilla Flooding method is able to boost model's adversarial robustness by leading it into a smooth parameter landscape. We further propose a promising and computationally tractable criterion, Gradient Accordance, to detect when the model is about to overfit and accordingly narrow down the hyperparameter space for Flooding with an optimal flood level guaranteed. Experimental results prove that gradient accordance is closely related with the phenomenon of overfitting, equipped with which Flooding-X beats the well-received adversarial training methods and achieves state-of-the-art performances on various NLP tasks facing different textual attack methods. This implies that adversarial examples, either generated by gradient-based algorithms or human efforts, are not a must for the improvement of adversarial robustness. We call for further exploring and deeper understanding in the nature of adversarial robustness and attacks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "296-ARR_v1_99",
            "content": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Generating natural language adversarial examples, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Moustafa Alzantot",
                    "Yash Sharma",
                    "Ahmed Elgohary",
                    "Bo-Jhang Ho",
                    "Mani Srivastava",
                    "Kai-Wei Chang"
                ],
                "title": "Generating natural language adversarial examples",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_100",
            "content": "Devansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, S Maxinder, Tegan Kanwal, Asja Maharaj, Aaron Fischer, Yoshua Courville,  Bengio, A closer look at memorization in deep networks, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Devansh Arpit",
                    "Stanis\u0142aw Jastrz\u0119bski",
                    "Nicolas Ballas",
                    "David Krueger",
                    "Emmanuel Bengio",
                    "S Maxinder",
                    "Tegan Kanwal",
                    "Asja Maharaj",
                    "Aaron Fischer",
                    "Yoshua Courville",
                    " Bengio"
                ],
                "title": "A closer look at memorization in deep networks",
                "pub_date": "2017",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "296-ARR_v1_101",
            "content": "UNKNOWN, None, 2017, Adversarial transformation networks: Learning to generate adversarial examples, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Adversarial transformation networks: Learning to generate adversarial examples",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_102",
            "content": "Mikhail Belkin, Daniel Hsu, Partha Mitra, Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Mikhail Belkin",
                    "Daniel Hsu",
                    "Partha Mitra"
                ],
                "title": "Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 32nd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_103",
            "content": "Guandan Chen, Kai Fan, Kaibo Zhang, Boxing Chen, and Zhongqiang Huang. 2021. Manifold adversarial augmentation for neural machine translation, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Guandan Chen",
                    "Kai Fan",
                    "Kaibo Zhang"
                ],
                "title": "Boxing Chen, and Zhongqiang Huang. 2021. Manifold adversarial augmentation for neural machine translation",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_104",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_105",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, Proceedings of the Third International Workshop on Paraphrasing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "B William",
                    "Chris Dolan",
                    " Brockett"
                ],
                "title": "Automatically constructing a corpus of sentential paraphrases",
                "pub_date": "2005",
                "pub_title": "Proceedings of the Third International Workshop on Paraphrasing",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_106",
            "content": "Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, HotFlip: White-box adversarial examples for text classification, 2018, Proceedings of the 56th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Javid Ebrahimi",
                    "Anyi Rao",
                    "Daniel Lowd",
                    "Dejing Dou"
                ],
                "title": "HotFlip: White-box adversarial examples for text classification",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_107",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_108",
            "content": "UNKNOWN, None, 2019, Stiffness: A new perspective on generalization in neural networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Stiffness: A new perspective on generalization in neural networks",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_109",
            "content": "Jinlan Fu, Pengfei Liu, Qi Zhang, Rethinking generalization of neural models: A named entity recognition case study, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jinlan Fu",
                    "Pengfei Liu",
                    "Qi Zhang"
                ],
                "title": "Rethinking generalization of neural models: A named entity recognition case study",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_110",
            "content": "UNKNOWN, None, 2016, Deep learning, MIT press.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Deep learning",
                "pub": "MIT press"
            }
        },
        {
            "ix": "296-ARR_v1_111",
            "content": "UNKNOWN, None, 2014, Explaining and harnessing adversarial examples, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Explaining and harnessing adversarial examples",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_112",
            "content": "Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama, Do we need zero training loss after achieving zero training error, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Takashi Ishida",
                    "Ikko Yamane",
                    "Tomoya Sakai",
                    "Gang Niu",
                    "Masashi Sugiyama"
                ],
                "title": "Do we need zero training loss after achieving zero training error",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "296-ARR_v1_113",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Adversarial examples for evaluating reading comprehension systems",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_114",
            "content": "Di Jin, Zhijing Jin, Joey Zhou, Peter Szolovits, Is bert really robust? a strong baseline for natural language attack on text classification and entailment, 2020, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Di Jin",
                    "Zhijing Jin",
                    "Joey Zhou",
                    "Peter Szolovits"
                ],
                "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_115",
            "content": "Wei Pang, Percy Koh,  Liang, Understanding black-box predictions via influence functions, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Wei Pang",
                    "Percy Koh",
                    " Liang"
                ],
                "title": "Understanding black-box predictions via influence functions",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 34th International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_116",
            "content": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein, Visualizing the loss landscape of neural nets, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Hao Li",
                    "Zheng Xu",
                    "Gavin Taylor",
                    "Christoph Studer",
                    "Tom Goldstein"
                ],
                "title": "Visualizing the loss landscape of neural nets",
                "pub_date": "2018",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_117",
            "content": "UNKNOWN, None, 2018, Textbugger: Generating adversarial text against real-world applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Textbugger: Generating adversarial text against real-world applications",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_118",
            "content": "Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu, BERT-ATTACK: Adversarial attack against BERT using BERT, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Linyang Li",
                    "Ruotian Ma",
                    "Qipeng Guo",
                    "Xiangyang Xue",
                    "Xipeng Qiu"
                ],
                "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_119",
            "content": "Linyang Li, Xipeng Qiu, Token-aware virtual adversarial training in natural language understanding, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Linyang Li",
                    "Xipeng Qiu"
                ],
                "title": "Token-aware virtual adversarial training in natural language understanding",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_120",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Andrew Maas",
                    "Raymond Daly",
                    "Peter Pham",
                    "Dan Huang",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Learning word vectors for sentiment analysis",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_121",
            "content": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, Towards deep learning models resistant to adversarial attacks, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Aleksander Madry",
                    "Aleksandar Makelov",
                    "Ludwig Schmidt",
                    "Dimitris Tsipras",
                    "Adrian Vladu"
                ],
                "title": "Towards deep learning models resistant to adversarial attacks",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_122",
            "content": "John Morris, Eli Lifland, Jin Yoo, Jake Grigsby, Di Jin, Yanjun Qi, TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "John Morris",
                    "Eli Lifland",
                    "Jin Yoo",
                    "Jake Grigsby",
                    "Di Jin",
                    "Yanjun Qi"
                ],
                "title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_123",
            "content": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Adversarial NLI: A new benchmark for natural language understanding, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yixin Nie",
                    "Adina Williams",
                    "Emily Dinan",
                    "Mohit Bansal",
                    "Jason Weston",
                    "Douwe Kiela"
                ],
                "title": "Adversarial NLI: A new benchmark for natural language understanding",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_124",
            "content": "UNKNOWN, None, 2019, Understanding adversarial robustness through loss landscape geometries, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Understanding adversarial robustness through loss landscape geometries",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_125",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100,000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_126",
            "content": "Yihe Shuhuai Ren, Kun Deng, Wanxiang He,  Che, Generating natural language adversarial examples through probability weighted word saliency, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yihe Shuhuai Ren",
                    "Kun Deng",
                    "Wanxiang He",
                    " Che"
                ],
                "title": "Generating natural language adversarial examples through probability weighted word saliency",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_127",
            "content": "Pedro Rodriguez, Joe Barrow, Alexander Hoyle, John Lalor, Robin Jia, Jordan Boyd-Graber, Evaluation examples are not equally informative: How should that change NLP leaderboards?, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Pedro Rodriguez",
                    "Joe Barrow",
                    "Alexander Hoyle",
                    "John Lalor",
                    "Robin Jia",
                    "Jordan Boyd-Graber"
                ],
                "title": "Evaluation examples are not equally informative: How should that change NLP leaderboards?",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "296-ARR_v1_128",
            "content": "Rebecca Roelofs, Sara Fridovich-Keil, John Miller, Vaishaal Shankar, Moritz Hardt, Benjamin Recht, Ludwig Schmidt, A meta-analysis of overfitting in machine learning, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Rebecca Roelofs",
                    "Sara Fridovich-Keil",
                    "John Miller",
                    "Vaishaal Shankar",
                    "Moritz Hardt",
                    "Benjamin Recht",
                    "Ludwig Schmidt"
                ],
                "title": "A meta-analysis of overfitting in machine learning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_129",
            "content": "UNKNOWN, None, 2019, Adversarial training for free! Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Adversarial training for free! Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_130",
            "content": "Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun, Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Chenglei Si",
                    "Zhengyan Zhang",
                    "Fanchao Qi",
                    "Zhiyuan Liu",
                    "Yasheng Wang",
                    "Qun Liu",
                    "Maosong Sun"
                ],
                "title": "Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_131",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_132",
            "content": "Kunihiro Takeoka, Kosuke Akimoto, Masafumi Oyamada, Low-resource taxonomy enrichment with pretrained language models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Kunihiro Takeoka",
                    "Kosuke Akimoto",
                    "Masafumi Oyamada"
                ],
                "title": "Low-resource taxonomy enrichment with pretrained language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_133",
            "content": "Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu, Infobert: Improving robustness of language models from an information theoretic perspective, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Boxin Wang",
                    "Shuohang Wang",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Ruoxi Jia",
                    "Bo Li",
                    "Jingjing Liu"
                ],
                "title": "Infobert: Improving robustness of language models from an information theoretic perspective",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_134",
            "content": "UNKNOWN, None, 2020, Adversarial training with fast gradient projection method against synonym substitution based text attacks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Adversarial training with fast gradient projection method against synonym substitution based text attacks",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_135",
            "content": "Roman Werpachowski, Andr\u00e1s Gy\u00f6rgy, Csaba Szepesv\u00e1ri, Detecting overfitting via adversarial examples, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Roman Werpachowski",
                    "Andr\u00e1s Gy\u00f6rgy",
                    "Csaba Szepesv\u00e1ri"
                ],
                "title": "Detecting overfitting via adversarial examples",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_136",
            "content": "Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song, Generating adversarial examples with adversarial networks, 2018, Proceedings of the 27th International Joint Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Chaowei Xiao",
                    "Bo Li",
                    "Jun-Yan Zhu",
                    "Warren He",
                    "Mingyan Liu",
                    "Dawn Song"
                ],
                "title": "Generating adversarial examples with adversarial networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Joint Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_137",
            "content": "UNKNOWN, None, 2018, Interpreting adversarial robustness: A view from decision surface in input space, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Interpreting adversarial robustness: A view from decision surface in input space",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_138",
            "content": "Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun, Word-level textual adversarial attacking as combinatorial optimization, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Yuan Zang",
                    "Fanchao Qi",
                    "Chenghao Yang",
                    "Zhiyuan Liu",
                    "Meng Zhang",
                    "Qun Liu",
                    "Maosong Sun"
                ],
                "title": "Word-level textual adversarial attacking as combinatorial optimization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_139",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "296-ARR_v1_140",
            "content": "Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan Liu, Maosong Sun, OpenAttack: An open-source textual adversarial attack toolkit, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Guoyang Zeng",
                    "Fanchao Qi",
                    "Qianrui Zhou",
                    "Tingji Zhang",
                    "Zixian Ma",
                    "Bairu Hou",
                    "Yuan Zang",
                    "Zhiyuan Liu",
                    "Maosong Sun"
                ],
                "title": "OpenAttack: An open-source textual adversarial attack toolkit",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_141",
            "content": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning (still) requires rethinking generalization, 2021, Communications of the ACM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Chiyuan Zhang",
                    "Samy Bengio",
                    "Moritz Hardt",
                    "Benjamin Recht",
                    "Oriol Vinyals"
                ],
                "title": "Understanding deep learning (still) requires rethinking generalization",
                "pub_date": "2021",
                "pub_title": "Communications of the ACM",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_142",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "296-ARR_v1_143",
            "content": "Yuan Zhang, Jason Baldridge, Luheng He, PAWS: Paraphrase adversaries from word scrambling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Yuan Zhang",
                    "Jason Baldridge",
                    "Luheng He"
                ],
                "title": "PAWS: Paraphrase adversaries from word scrambling",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "296-ARR_v1_144",
            "content": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu, Freelb: Enhanced adversarial training for natural language understanding, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Chen Zhu",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Siqi Sun",
                    "Tom Goldstein",
                    "Jingjing Liu"
                ],
                "title": "Freelb: Enhanced adversarial training for natural language understanding",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "296-ARR_v1_0@0",
            "content": "Flooding-X: Improving BERT's Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_0",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@0",
            "content": "Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@1",
            "content": "However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 115,
            "end": 350,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@2",
            "content": "To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 352,
            "end": 505,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@3",
            "content": "We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 507,
            "end": 727,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@4",
            "content": "Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 729,
            "end": 909,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_2@5",
            "content": "We experimentally show that our method improves Bert's resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_2",
            "start": 911,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@0",
            "content": "Despite their impressive performances on various NLP tasks, deep neural networks such as BERT (Devlin et al., 2019) suffer a sharp decline facing deliberately constructed adversarial attacks (Zeng et al., 2021;Nie et al., 2020;Zang et al., 2020;Ren et al., 2019;Zhang et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@1",
            "content": "A line of works attempt to alleviate this problem by creating adversarially robust models via defense methods, including adversarial data augmentation (Chen et al., 2021;Si et al., 2021), regularizing (Wang et al., 2020a), and adversarial training (Wang et al., 2020b;Zhu et al., 2019;Madry et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 283,
            "end": 587,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@2",
            "content": "Data augmentation and adversarial training rely on extra adversarial examples generated either by handcrafting or conducting gradient ascent on the clean data for virtual adversarial samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 589,
            "end": 779,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@3",
            "content": "However, generating adversarial examples scales up the cost of training computationally, which makes vanilla adversarial training almost impractical on large-scale NLP tasks like QNLI (Questionanswering NLI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 781,
            "end": 988,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@4",
            "content": "Increasing researches express their concern of the time-consuming property of standard adversarial training and offer cheaper but competitive alternatives by (i) replacing the perturbation generation with an extra generator network (Baluja and Fischer, 2017;Xiao et al., 2018), or by (ii) combining the gradient computation of clean data and perturbations into one backward pass (Shafahi et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 990,
            "end": 1391,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_4@5",
            "content": "These approaches still rely on extra adversarial examples generated either by the model itself or by an extra module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_4",
            "start": 1393,
            "end": 1509,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@0",
            "content": "In this work, we propose a novel method, Flooding-X, to largely improve adversarial robustness without any adversarial examples, maintaining the same computational cost as conventional BERT fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@1",
            "content": "The vanilla Flooding (Ishida et al., 2020) method is a practical regularization technique to boost model generalization by preventing further reduction of the training loss when it reaches a reasonably small value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 203,
            "end": 416,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@2",
            "content": "It results in a model performing normal gradient descent when training loss is above the decided value but gradient ascent when below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 418,
            "end": 551,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@3",
            "content": "By continuing to \"random walk\" with the same non-zero value as a \"virtual loss\", the model drifts into an area with a flat loss landscape that is claimed to lead to better generalization (Ishida et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 553,
            "end": 761,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@4",
            "content": "Interestingly, we find that Flooding method is also promising in increasing models' resistance to adversarial attacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 763,
            "end": 880,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_5@5",
            "content": "Despite the significant rise in robust accuracy, the so-called reasonably small value, which is a hyperparameter, takes effort to be found and varies for each dataset, which requires an overly extensive search among the numerous candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_5",
            "start": 882,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@0",
            "content": "In an attempt to narrow down the candidates of hyper-parameter, we propose gradient accordance as an informative criterion for optimal values that bring Flooding into effect, which is used as a building-block in Flooding-X.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@1",
            "content": "We measure how accordant the gradients of the batches are by analyzing how the gradient descent steps based on part of an epoch affect the loss of each batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 224,
            "end": 381,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@2",
            "content": "Gradient accordance is computationally friendly and is tractable during training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 383,
            "end": 471,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@3",
            "content": "Experiments on various tasks show a close relation between gradient accordance and overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 473,
            "end": 567,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@4",
            "content": "As a result, we propose gradient accordance as a reliable flooding criterion to make the training loss flood around the level when the model has nearly overfitted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 569,
            "end": 731,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_6@5",
            "content": "That is to say, we leverage the training loss of the model right before overfitting as the value of flood level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_6",
            "start": 733,
            "end": 844,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_7@0",
            "content": "Flooding-X is especially useful and shows great advantage over adversarial training in terms of computational cost when the training dataset is relatively large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_7",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_7@1",
            "content": "Experimental results demonstrate that our method achieves stated-of-the-art robust accuracy with BERT on various tasks and improves its robust accuracy by 100 to 400% without using any adversarial example, consuming any extra training time, or conducting overly extensive search for hyper-parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_7",
            "start": 162,
            "end": 460,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_7@2",
            "content": "Our main contributions are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_7",
            "start": 462,
            "end": 499,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_8@0",
            "content": "1) We propose a novel method, Flooding-X, that achieves state-of-the-art robust accuracy for BERT on various tasks, which is adversarial-example-free and takes no more training time than fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_8",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_9@0",
            "content": "2) We propose a promising indicator, i.e. gradient accordance, to alleviate Flooding method from tedious search of the hyper-parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_9",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_10@0",
            "content": "3) We conduct comprehensive experiments on NLP tasks to illustrate the potential of Flooding for improving BERT's adversarial robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_10",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_11@0",
            "content": "Why Does Flooding Boost Adversarial",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_11",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_12@0",
            "content": "Robustness?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_12",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_13@0",
            "content": "Vanilla Flooding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_13",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_14@0",
            "content": "We first describe the vanilla Flooding regularization method (Ishida et al., 2020) for alleviating overfitting via keeping training loss from reducing to zero.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_14",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_14@1",
            "content": "Under the main assumption that learning until zero loss is harmful, Ishida et al. (2020) propose Flooding to intentionally prevent further reduction of the training loss when it reaches a reasonably small value, which is called the flood level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_14",
            "start": 160,
            "end": 403,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_14@2",
            "content": "Intuitively, this approach makes the training loss float around the pre-defined flood level and alter from normal mini-batch gradient descent to gradient ascent if the loss is below the flood level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_14",
            "start": 405,
            "end": 602,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_14@3",
            "content": "With the constraint of flood level, the model will continue to \"random walk\" around the non-zero training loss, which is expected to reach a flat loss landscape.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_14",
            "start": 604,
            "end": 764,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_15@0",
            "content": "The algorithm of Flooding is defined as follow:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_15",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_16@0",
            "content": "J(\u03b8) = |J(\u03b8) \u2212 b| + b,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_16",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_17@0",
            "content": "where J denotes the original learning objective, and J represents the modified learning objective with flooding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_17",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_17@1",
            "content": "The positive value b is the flood level specified by user, and \u03b8 is the model parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_17",
            "start": 113,
            "end": 200,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_18@0",
            "content": "Accordingly, the flooded empirical risk is then defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_18",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_19@0",
            "content": "R(f ) = | R(f ) \u2212 b| + b,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_19",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_20@0",
            "content": "within which R(f ) / R(f ) denotes the original / flooded empirical risk respectively, and f refers to the score function to be learned by the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_20",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_20@1",
            "content": "During the back propagation process, the gradient of R(f ) w.r.t. model parameters and R(f ) point to the same direction when R(f ) is above b but to the opposite direction when it is below b.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_20",
            "start": 150,
            "end": 341,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_20@2",
            "content": "As a result, model performs normal gradient descent when the learning objective is above the flood level, and gradient ascent when below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_20",
            "start": 343,
            "end": 479,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_21@0",
            "content": "Smooth Parameter Landscape Leads to Better Robustness",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_21",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_22@0",
            "content": "According to the definition described in the previous section, Flooding does not make any difference to the training process when the loss is beyond the flood level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_22",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_22@1",
            "content": "When the training loss approaches the flood level, on closer inspection, gradient descent and gradient ascent begin to alternate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_22",
            "start": 166,
            "end": 294,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_22@2",
            "content": "Assume that the model with learning rate \u03b5 performs gradient descent for the n-th batch and then gradient ascent for batch n + 1, which results in:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_22",
            "start": 296,
            "end": 442,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_23@0",
            "content": "\u03b8 n = \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ), \u03b8 n+1 = \u03b8 n + \u03b5g(\u03b8 n ).(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_23",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_24@0",
            "content": "In the equations above, g(\u03b8) = \u2207 \u03b8 J(\u03b8) is the gradient of J(\u03b8) w.r.t. model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_24",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_24@1",
            "content": "We can then get",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_24",
            "start": 89,
            "end": 103,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_25@0",
            "content": "\u03b8 n+1 = \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) + \u03b5g \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_25",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_26@0",
            "content": "which is, by Taylor expansion, approximately equivalent to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_26",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_27@0",
            "content": "\u2248 \u03b8 n\u22121 \u2212 \u03b5g(\u03b8 n\u22121 ) + \u03b5 g(\u03b8 n\u22121 ) \u2212 \u03b5\u2207 \u03b8 g(\u03b8 n\u22121 )g(\u03b8 n\u22121 ) = \u03b8 n\u22121 \u2212 \u03b5 2 2 \u2207 \u03b8 \u2225g(\u03b8 n\u22121 )\u2225 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_27",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_28@0",
            "content": "(5) Thus, theoretically, when the training loss is relatively low, the model alters into a new learning mode where the learning rate is \u03b5 2 /2 and the objective is to minimize \u2225g(\u03b8)\u2225 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_28",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_28@1",
            "content": "Generally, the flooded model is guided into an area with a smooth parameter landscape that leads to better adversarial robustness (Prabhu et al., 2019;Yu et al., 2018;Li et al., 2018a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_28",
            "start": 187,
            "end": 371,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_28@2",
            "content": "As is demonstrated in Figure 1, adversarial training brings about a smoother loss change to the model when the input embedding is perturbed by Gaussian random noise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_28",
            "start": 373,
            "end": 537,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_29@0",
            "content": "Achilles' Heel of Flooding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_29",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_30@0",
            "content": "Despite its potential in boosting model's resistance to adversarial attacks, the optimal flood level has to be searched by performing exhaustive search within a wide range at tiny steps, which is not easily at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_30",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_30@1",
            "content": "A relatively large value of flood level lengthens the gradient steps and keeps the model from convergence, while a tiny value causes hardly any difference to the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_30",
            "start": 216,
            "end": 394,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_30@2",
            "content": "The effect of Flooding deeply relies on the flood level, which, at the same time, is also sensitive to the subtle change of this hyper-parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_30",
            "start": 396,
            "end": 540,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_30@3",
            "content": "Figure 2 reveals that even a slight change on the value of flood level can make a huge difference on the adversarial robustness of the so-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_30",
            "start": 542,
            "end": 693,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_30@4",
            "content": "In an attempt to ease the effort of searching and make the best of Flooding, we propose a promising and reliable criterion to narrow down the search space, which is described in detail in the next section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_30",
            "start": 695,
            "end": 899,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_31@0",
            "content": "Figure 2: Influence of different flood levels on performance of the trained BERT on SST-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_31",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_31@1",
            "content": "The range marked in yellow is lined out by our proposed criterion , i.e., gradient accordance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_31",
            "start": 91,
            "end": 184,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_31@2",
            "content": "The optimal value of flood level is guaranteed within the narrowed-down space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_31",
            "start": 186,
            "end": 263,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_32@0",
            "content": "Gradient Accordance as a Criterion for Flooding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_32",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@0",
            "content": "Since Flooding is proposed as an attempt to avoid overfitting, we intuitively suppose that the optimal flood level would be found at the stage when the model is about to overfit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@1",
            "content": "That is, we leverage the training loss before overfitting as the flood level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 179,
            "end": 255,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@2",
            "content": "Inspired by influence function (Koh and Liang, 2017), we propose gradient accordance as a criterion for flooding, which is empirically proved to be reliable and indicative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 257,
            "end": 428,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@3",
            "content": "We consider the effect of the model updated w.r.t. one epoch on each of its batches as a signal of overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 430,
            "end": 540,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@4",
            "content": "As is indicated by its name, this criterion measures the relation among the gradients of each batch on epoch level, evaluating whether the model updated on an epoch has the same positive effect on the batches on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 542,
            "end": 761,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_33@5",
            "content": "Now we provide the formal definition of gradient accordance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_33",
            "start": 763,
            "end": 822,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_34@0",
            "content": "Preliminaries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_34",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_35@0",
            "content": "We denote a model as a functional approximation f which is parameterized by \u03b8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_35",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_35@1",
            "content": "Consider a training data point x with the ground truth label y, which results in a loss L(f (\u03b8, x), y).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_35",
            "start": 79,
            "end": 181,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_35@2",
            "content": "The gradient of the loss w.r.t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_35",
            "start": 183,
            "end": 213,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_35@3",
            "content": "the parameters is thus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_35",
            "start": 215,
            "end": 236,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_36@0",
            "content": "g = \u2207 \u03b8 L(f (\u03b8, x), y),(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_36",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_37@0",
            "content": "whose negation denotes the direction in which the parameters \u03b8 are updated to better correspond to the desired outputs on the training data (Fort et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_37",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_37@1",
            "content": "Now let's consider two data points x 1 and x 2 with their corresponding labels y 1 and y 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_37",
            "start": 161,
            "end": 252,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_37@2",
            "content": "According to the definition above, the gradient of sample 1 is g 1 = \u2207 \u03b8 L(f (\u03b8, x 1 ), y 1 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_37",
            "start": 254,
            "end": 347,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_37@3",
            "content": "We try to inspect how the small change of \u03b8 in the direction \u2212g 1 influences the loss on sample x 1 or x 2 :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_37",
            "start": 349,
            "end": 456,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_38@0",
            "content": "\u2206L 1 =L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ), y 1 ) \u2212 L(f (\u03b8, x 1 ), y 1 ),(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_38",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_39@0",
            "content": "where f (\u03b8, x 1 ) can be expanded by Taylor expansion to be:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_39",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_40@0",
            "content": "f (\u03b8, x 1 ) = f (\u03b8 \u2212 \u03b5g 1 , x 1 ) + \u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_40",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_41@0",
            "content": "(8) Here, we refer to (\u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 )) as T (x 1 ); and by repeating the similar expansion we can get",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_41",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_42@0",
            "content": "L(f (\u03b8, x 1 ), y 1 ) = L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ) + T (x 1 ), y 1 ) = L(f (\u03b8 \u2212 \u03b5g 1 , x 1 ), y 1 ) + \u2202L \u2202f T (x 1 ) + O(T 2 (x 1 )).(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_42",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_43@0",
            "content": "Equation ( 7) is thus equal to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_43",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_44@0",
            "content": "\u2206L 1 = \u2212 \u2202L \u2202f T (x 1 ) \u2212 O(T 2 (x 1 )) = \u2212 \u2202L \u2202f (\u03b5g 1 \u2202f \u2202\u03b8 + O(\u03b5 2 )) = \u2212\u03b5g 1 \u2022 g 1 \u2212 O(\u03b5 2 ).(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_44",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_45@0",
            "content": "Similarly, the change of the loss on x 2 caused by the gradient update by",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_45",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_46@0",
            "content": "x 1 is \u2206L 2 = \u2212\u03b5g 1 \u2022 g 2 \u2212 O(\u03b5 2 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_46",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_47@0",
            "content": "Notably, \u2206L 1 is negative by definition since the model is updated with respect to x 1 and naturally leads to a decrease on its loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_47",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_47@1",
            "content": "The model updated on x 1 is considered to have a positive effect on x 2 if \u2206L 2 is also negative while an opposite effect if positive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_47",
            "start": 134,
            "end": 267,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_47@2",
            "content": "The equations above demonstrate that this co-relation is equivalent to the overlap between the gradients of the two data points g 1 \u2022 g 2 , which we hereafter refer to as gradient accordance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_47",
            "start": 269,
            "end": 459,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_48@0",
            "content": "Coarse-Grained Gradient Accordance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_48",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_49@0",
            "content": "Data-point-level gradient accordance is too finegrained to be tractable in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_49",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_49@1",
            "content": "Thus, we attempt to scale it up and result in coarse-grain gradient accordance at batch level, which is computationally tractable and still reliable as a criterion for overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_49",
            "start": 85,
            "end": 264,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_49@2",
            "content": "Consider a training batch B 0 with n samples X = {x 1 , x 2 , . . . , x n } and labels y = {y 1 , y 2 , . . . , y n } of k classes {c 1 , c 2 , . . . , c k }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_49",
            "start": 266,
            "end": 423,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_49@3",
            "content": "These samples can be divided into k groups according to their labels",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_49",
            "start": 425,
            "end": 492,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_50@0",
            "content": "X = X 1 \u222aX 2 \u222a\u2022 \u2022 \u2022\u222aX k ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_50",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_51@0",
            "content": "and so are the labels y = k i=1 y i , where all the samples in X m belong to class c m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_51",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_51@1",
            "content": "Thus, we have the sub-batch B 1 0 = {X 1 , y 1 }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_51",
            "start": 89,
            "end": 137,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_51@2",
            "content": "We then define class accordance score of two sub-batches B 1 0 and B 2 0 of classes c 1 and c 2 as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_51",
            "start": 139,
            "end": 237,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_52@0",
            "content": "C(B 1 0 , B 2 0 ) = E[cos(g 1 , g 2 )],(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_52",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_53@0",
            "content": "where g 1 is the gradient of the training loss of sub-batch B 1 0 w.r.t. the model parameters, and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_53",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_54@0",
            "content": "cos(g 1 , g 2 ) = (g 1 /|g 1 |) \u2022 (g 2 /|g 2 |).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_54",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_55@0",
            "content": "Class accordance measures whether the gradient taken with respect to a sub-batch B 1 0 of class c 1 will also decrease the loss for samples in another sub-batch B 2 0 of class c 2 (Fort et al., 2019;Fu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_55",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_55@1",
            "content": "Further consider that there are N batches in one training epoch and the training samples are of k classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_55",
            "start": 217,
            "end": 322,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_55@2",
            "content": "The batch accordance score between batches B s and B t is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_55",
            "start": 324,
            "end": 391,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_56@0",
            "content": "S batch accd (B s , B t ) = 1 k(k \u2212 1) k j=1 k i=1 i\u0338 =j C(B i s , B j t ). (12",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_56",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_57@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_57",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_58@0",
            "content": "Batch accordance quantifies the learning consistency of two batches by evaluating how the model updated on one batch affects the other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_58",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_58@1",
            "content": "To be more specific, a positive batch accordance denotes that the measured two batches are under the same learning pace since the model updated according to each batch benefits them both.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_58",
            "start": 136,
            "end": 322,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_58@2",
            "content": "The gradient accordance of certain epoch (or a part of an epoch, namely the sub-epoch) is finally defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_58",
            "start": 324,
            "end": 431,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_59@0",
            "content": "S epoch accd = 1 N (N \u2212 1) N j=i+1 N \u22121 i=1 S batch accd (B s , B t ). (13",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_59",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_60@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_60",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_61@0",
            "content": "Gradient accordance scales the batch accordance score up from a measure of two batches to that of a sub-epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_61",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_62@0",
            "content": "Criterion for Flooding A positive gradient accordance means that the model performed gradient descent w.r.t. the certain epoch decreases the loss of its batches on average, indicating that the learning pace of most batches are in line with each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_62",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_62@1",
            "content": "A negative one means that the model has overfitted to some of the training batches since the update of one epoch increases the loss of its batches on average, which is right the stage we would like to identify for the model by gradient accordance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_62",
            "start": 252,
            "end": 498,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_62@2",
            "content": "We assume that the optimal flood level lies in the range of the training loss of a model when it is about to overfit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_62",
            "start": 500,
            "end": 616,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_62@3",
            "content": "In the following section, we empirically prove that gradient accordance is a reliable and promising criterion for flooding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_62",
            "start": 618,
            "end": 740,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_63@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_63",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_64@0",
            "content": "In this section, we provide comprehensive analysis on Flooding-X through extensive experiments on five text classification datasets of various tasks and scales: SST (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), IMDB (Maas et al., 2011) and AG News (Zhang et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_64",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_64@1",
            "content": "We conduct experiments on BERTbase (Devlin et al., 2019) and compare robust accuracy of Flooding-X with other adversarial training algorithms to demonstrate its strength.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_64",
            "start": 311,
            "end": 480,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_65@0",
            "content": "Baseline Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_65",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_66@0",
            "content": "We compare our proposed Flooding-X with three adversarial training algorithms and one regularization method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_66",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_67@0",
            "content": "PGD Projected gradient descent (PGD, Madry et al., 2018) formulates adversarial training algorithms into solving a minimax problem that minimizes the empirical loss on adversarial examples that can lead to maximized adversarial risk.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_67",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_68@0",
            "content": "FreeLB Zhu et al. (2019) propose FreeLB to improve the generalization of language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_68",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_68@1",
            "content": "By adding adversarial perturbations to word embeddings, FreeLB generates virtual adversarial samples inside the region around input samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_68",
            "start": 90,
            "end": 229,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_69@0",
            "content": "TAVAT Token-Aware Virtual Adversarial Training (TAVAT, Li and Qiu, 2021) aims at fine-grained perturbations, leveraging a token-level accumulated perturbation vocabulary to initialize the perturbations better and constraining them within a token-level normalization ball.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_69",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_70@0",
            "content": "InfoBERT InfoBERT (Wang et al., 2020a) leverages two mutual-information-based regularizers for robust model training, suppressing noisy mutual information while increasing mutual information between local stable features and global features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_70",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_71@0",
            "content": "Attack Methods and Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_71",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_72@0",
            "content": "Three well-received attack methods are leveraged via TextAttack (Morris et al., 2020) for an extensive comparison between our proposed method and baseline algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_72",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_73@0",
            "content": "TextFooler identifies the important words for target model and repeats replacing them with synonyms until the prediction of the model is altered.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_73",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_73@1",
            "content": "Similarly, TextBugger (Li et al., 2018b) also searches for important words and modifies them by choosing an optimal perturbation from the generated several kinds of perturbations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_73",
            "start": 146,
            "end": 324,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_73@2",
            "content": "BERTAttack (Li et al., 2020) applies BERT in a semantic-preserving way to generate substitutes for the vulnerable words detected in the given input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_73",
            "start": 326,
            "end": 473,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_74@0",
            "content": "We consider four evaluation metrics to measure BERT's resistance to the mentioned adversarial attacks under different defence algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_74",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_75@0",
            "content": "Clean% The clean accuracy refers to the model's test accuracy on the original clean dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_75",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_76@0",
            "content": "Aua% Accuracy under attack measures the model's prediction accuracy on the adversarial data deliberately generated by certain attack method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_76",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_76@1",
            "content": "A higher Aua% means a more robust model and a better defender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_76",
            "start": 141,
            "end": 202,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_77@0",
            "content": "Suc% Attack success rate is evaluated by the ratio of the number of texts successfully perturbed by a specific attack method to the number of all the involved texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_77",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_77@1",
            "content": "Robust models are expected to score low at Suc%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_77",
            "start": 166,
            "end": 213,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_77@2",
            "content": "#Query Number of queries denotes the average attempts the attacker queries the target model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_77",
            "start": 215,
            "end": 306,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_77@3",
            "content": "The larger the number is, the harder the model is to be attacked.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_77",
            "start": 308,
            "end": 372,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_78@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_78",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@0",
            "content": "All the baseline methods are re-implemented based on their open-released codes and the results are competing to those reported.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@1",
            "content": "We train our models on NVIDIA RTX 3090 and RTX 2080Ti GPUs, depending on the volume of the dataset involved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 128,
            "end": 235,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@2",
            "content": "Most of the parameters such as learning rate and warm-up step are in line with vanilla BERT (Devlin et al., 2019) and the baseline methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 237,
            "end": 375,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@3",
            "content": "For all of the adversarial methods we set the training step to be 5 for a fair comparison, which is a trade-off between training cost and model performance .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 377,
            "end": 533,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@4",
            "content": "The clean accuracy (Clean%) is tested on the whole test dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 535,
            "end": 598,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@5",
            "content": "The other three metrics (e.g., Aua%, Suc% and #Query) are evaluated on the whole test dataset for SST-2 and MRPC, and 800 randomly chosen samples for IMDB, AG NEWS, and QNLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 600,
            "end": 773,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_79@6",
            "content": "We train 10 epochs for each model on each dataset, among which the last epochs are selected for the comparison of adversarial robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_79",
            "start": 775,
            "end": 911,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_80@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_80",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@0",
            "content": "The extensive results of all the above mentioned methods are summarized in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@1",
            "content": "Generally, our Flooding-X method improves BERT by a large margin in terms of its resistance to adversarial attacks, surpassing the baseline adversarial training algorithms on most datasets under different attack methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 84,
            "end": 303,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@2",
            "content": "Under TextFooler attack , our algorithm reaches the best robust performance on four datasets: IMDB, AG News, SST-2, and MRPC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 305,
            "end": 429,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@3",
            "content": "We observe that Flooding is more effective on smaller datasets than larger ones, since the smaller datasets with shorter training sentences are easier to be memorized by the neural network and are more likely to cause overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 431,
            "end": 660,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@4",
            "content": "On QNLI dataset where Flooding-X fails to win, the accuracy under attack is only 0.2 points lower than the 5-step PGD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 662,
            "end": 779,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@5",
            "content": "This might be explained by the mild change in gradient accordance during training on QNLI dataset, in which case the precise stage of overfitting is hard to be identified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 781,
            "end": 951,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@6",
            "content": "Though we believe that a better value of flood level exists and can further boost the performance, we refuse to take on the pattern of extensive hyper-parameter searching which is against the original purpose of Flooding-X.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 953,
            "end": 1175,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@7",
            "content": "Notably, our method performs better than the baseline adversarial training methods by 5 to 20 points on average even without using any adversarial examples as training source, not to mention the vanilla BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 1177,
            "end": 1384,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@8",
            "content": "Under most cases, our method remains the best performing algorithm facing BERTAttack (Li et al., 2020) and TextBugger (Li et al., 2018b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 1386,
            "end": 1522,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@9",
            "content": "This proves that our method maintains effectiveness under different kinds of adversarial attacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 1524,
            "end": 1620,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@10",
            "content": "As a byproduct, the clean accuracy of our method is also the best among all the baseline methods, which is inherent to the vanilla Flooding that aims at better generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 1622,
            "end": 1796,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_81@11",
            "content": "In the cases of AG News and QNLI, our re-implement the results of BERT fine-tuning to 97.0 and 91.6 respectively so Flooding-X does not surpass the reported performance, but still outperforming the baselines of our implementation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_81",
            "start": 1798,
            "end": 2027,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_82@0",
            "content": "Analysis and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_82",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_83@0",
            "content": "In this section, we construct supplementary experiments to further analyze the effectiveness of Flooding-X and its building block, i.e., gradient accordance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_83",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_84@0",
            "content": "Does Gradient Accordance Capture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_84",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_85@0",
            "content": "Overfitting?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_85",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_86@0",
            "content": "Influence function (Koh and Liang, 2017) inspects the influence of one single training data on the model prediction and stiffness (Fort et al., 2019) measures how the model updated according to one sample affects the model prediction on another.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_86",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_87@0",
            "content": "Based on these two works, gradient accordance is proposed as a means for identifying model overfitting at sub-epoch level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_87",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_88@0",
            "content": "As seen in Figure 3, during training process, the turning point of gradient accordance from negative to positive closely matches the point when the test loss is about to increase, which is well received as a signal of overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_88",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_88@1",
            "content": "Since it is computationally intractable to calculate gradient accordance after trained on every single batch, we can only figure out the range where the model is about to overfit by computing gradient accordance at sub-epoch level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_88",
            "start": 231,
            "end": 461,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_89@0",
            "content": "How does Flooding-X Help with",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_89",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_90@0",
            "content": "Robustness?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_90",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_91@0",
            "content": "Despite its outstanding performance of the last training epoch, we find that Flooding-X boosts the robustness of model at an earlier stage than standard fine-tuning and adversarial training methods like FreeLB.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_91",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_91@1",
            "content": "As is shown in Figure 4, Flooding-X",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_91",
            "start": 211,
            "end": 245,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_92@0",
            "content": "Time Consumption",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_92",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_93@0",
            "content": "To further reveal the strength of Flooding-X besides its robustness performance, we compare its GPU training time consumption with baseline methods The adversarial examples are generated by replacing the original texts based on certain rules such as semantic similarity (Alzantot et al., 2018;Li et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_93",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_93@1",
            "content": "Ebrahimi et al. (2018) propose a perturbation strategy that conducts character insertion, deletion, and replacement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_93",
            "start": 311,
            "end": 426,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_93@2",
            "content": "Jia and Liang (2017) mislead MRC models via a human-involved phrase generation method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_93",
            "start": 428,
            "end": 513,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_94@0",
            "content": "The mentioned algorithms of AT generates additional adversarial examples either by calculating gradients or by human force, which is computationally expensive and effort taking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_94",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_95@0",
            "content": "Overfitting and Criterion Deep neural networks are shown to suffer from overfitting to training configurations and memorise training scenarios (Takeoka et al., 2021;Rodriguez et al., 2021;Roelofs et al., 2019;Werpachowski et al., 2019), which leads to poor generalization and vulnerability towards adversarial perturbations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_95",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@0",
            "content": "One way of identifying overfitting is to see whether the generalization gap, i.e., the test minus the training loss, is increasing or not (Goodfellow et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@1",
            "content": "Ishida et al. (2020) further decompose the situation of the generalization gap increasing into two stages: The first stage is when training and test losses are both decreasing, but the former is decreasing faster then the latter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 165,
            "end": 393,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@2",
            "content": "The next stage is when the training loss is decreasing but the test loss is increasing, after which the training loss continues to approach zero and memorize the training data completely Belkin et al., 2018;Arpit et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 395,
            "end": 621,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@3",
            "content": "Derived from influence function (Koh and Liang, 2017), Fort et al. (2019) propose the concept of Stiffness as a new perspective of generalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 623,
            "end": 768,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@4",
            "content": "They measure how stiff a network is by looking at how a small gradient step in the network parameters on one example affects the loss on another example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 770,
            "end": 922,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@5",
            "content": "This criterion carries is theoretically proved to have a close relation with generalization and overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 924,
            "end": 1031,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_96@6",
            "content": "However, from the practical perspective, it is computationally intractable to compute the stiffness between every single sample during the process of standard training where thousands of samples are involved in one batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_96",
            "start": 1033,
            "end": 1253,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_97@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_97",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@0",
            "content": "In this work, we propose Flooding-X as an efficient and computational-friendly algorithm for improving BERT's resistance to adversarial attacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@1",
            "content": "We first theoretically prove that the vanilla Flooding method is able to boost model's adversarial robustness by leading it into a smooth parameter landscape.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 145,
            "end": 302,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@2",
            "content": "We further propose a promising and computationally tractable criterion, Gradient Accordance, to detect when the model is about to overfit and accordingly narrow down the hyperparameter space for Flooding with an optimal flood level guaranteed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 304,
            "end": 546,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@3",
            "content": "Experimental results prove that gradient accordance is closely related with the phenomenon of overfitting, equipped with which Flooding-X beats the well-received adversarial training methods and achieves state-of-the-art performances on various NLP tasks facing different textual attack methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 548,
            "end": 842,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@4",
            "content": "This implies that adversarial examples, either generated by gradient-based algorithms or human efforts, are not a must for the improvement of adversarial robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 844,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_98@5",
            "content": "We call for further exploring and deeper understanding in the nature of adversarial robustness and attacks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_98",
            "start": 1010,
            "end": 1116,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_99@0",
            "content": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Generating natural language adversarial examples, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_99",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_100@0",
            "content": "Devansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, S Maxinder, Tegan Kanwal, Asja Maharaj, Aaron Fischer, Yoshua Courville,  Bengio, A closer look at memorization in deep networks, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_100",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2017, Adversarial transformation networks: Learning to generate adversarial examples, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_101",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_102@0",
            "content": "Mikhail Belkin, Daniel Hsu, Partha Mitra, Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_102",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_103@0",
            "content": "Guandan Chen, Kai Fan, Kaibo Zhang, Boxing Chen, and Zhongqiang Huang. 2021. Manifold adversarial augmentation for neural machine translation, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_103",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_104@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_104",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_105@0",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, Proceedings of the Third International Workshop on Paraphrasing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_105",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_106@0",
            "content": "Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, HotFlip: White-box adversarial examples for text classification, 2018, Proceedings of the 56th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_106",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_107@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_107",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_108@0",
            "content": "UNKNOWN, None, 2019, Stiffness: A new perspective on generalization in neural networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_108",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_109@0",
            "content": "Jinlan Fu, Pengfei Liu, Qi Zhang, Rethinking generalization of neural models: A named entity recognition case study, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_109",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2016, Deep learning, MIT press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_110",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_111@0",
            "content": "UNKNOWN, None, 2014, Explaining and harnessing adversarial examples, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_111",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_112@0",
            "content": "Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama, Do we need zero training loss after achieving zero training error, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_112",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_113@0",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_113",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_114@0",
            "content": "Di Jin, Zhijing Jin, Joey Zhou, Peter Szolovits, Is bert really robust? a strong baseline for natural language attack on text classification and entailment, 2020, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_114",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_115@0",
            "content": "Wei Pang, Percy Koh,  Liang, Understanding black-box predictions via influence functions, 2017, Proceedings of the 34th International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_115",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_116@0",
            "content": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein, Visualizing the loss landscape of neural nets, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_116",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_117@0",
            "content": "UNKNOWN, None, 2018, Textbugger: Generating adversarial text against real-world applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_117",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_118@0",
            "content": "Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu, BERT-ATTACK: Adversarial attack against BERT using BERT, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_118",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_119@0",
            "content": "Linyang Li, Xipeng Qiu, Token-aware virtual adversarial training in natural language understanding, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_119",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_120@0",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_120",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_121@0",
            "content": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, Towards deep learning models resistant to adversarial attacks, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_121",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_122@0",
            "content": "John Morris, Eli Lifland, Jin Yoo, Jake Grigsby, Di Jin, Yanjun Qi, TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_122",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_123@0",
            "content": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Adversarial NLI: A new benchmark for natural language understanding, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_123",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_124@0",
            "content": "UNKNOWN, None, 2019, Understanding adversarial robustness through loss landscape geometries, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_124",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_125@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_125",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_126@0",
            "content": "Yihe Shuhuai Ren, Kun Deng, Wanxiang He,  Che, Generating natural language adversarial examples through probability weighted word saliency, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_126",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_127@0",
            "content": "Pedro Rodriguez, Joe Barrow, Alexander Hoyle, John Lalor, Robin Jia, Jordan Boyd-Graber, Evaluation examples are not equally informative: How should that change NLP leaderboards?, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_127",
            "start": 0,
            "end": 361,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_128@0",
            "content": "Rebecca Roelofs, Sara Fridovich-Keil, John Miller, Vaishaal Shankar, Moritz Hardt, Benjamin Recht, Ludwig Schmidt, A meta-analysis of overfitting in machine learning, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_128",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_129@0",
            "content": "UNKNOWN, None, 2019, Adversarial training for free! Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_129",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_130@0",
            "content": "Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun, Better robustness by more coverage: Adversarial and mixup data augmentation for robust finetuning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_130",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_131@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_131",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_132@0",
            "content": "Kunihiro Takeoka, Kosuke Akimoto, Masafumi Oyamada, Low-resource taxonomy enrichment with pretrained language models, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_132",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_133@0",
            "content": "Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu, Infobert: Improving robustness of language models from an information theoretic perspective, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_133",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_134@0",
            "content": "UNKNOWN, None, 2020, Adversarial training with fast gradient projection method against synonym substitution based text attacks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_134",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_135@0",
            "content": "Roman Werpachowski, Andr\u00e1s Gy\u00f6rgy, Csaba Szepesv\u00e1ri, Detecting overfitting via adversarial examples, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_135",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_136@0",
            "content": "Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song, Generating adversarial examples with adversarial networks, 2018, Proceedings of the 27th International Joint Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_136",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_137@0",
            "content": "UNKNOWN, None, 2018, Interpreting adversarial robustness: A view from decision surface in input space, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_137",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_138@0",
            "content": "Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun, Word-level textual adversarial attacking as combinatorial optimization, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_138",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_139@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_139",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_140@0",
            "content": "Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan Liu, Maosong Sun, OpenAttack: An open-source textual adversarial attack toolkit, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_140",
            "start": 0,
            "end": 369,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_141@0",
            "content": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning (still) requires rethinking generalization, 2021, Communications of the ACM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_141",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_142@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_142",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_143@0",
            "content": "Yuan Zhang, Jason Baldridge, Luheng He, PAWS: Paraphrase adversaries from word scrambling, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_143",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "296-ARR_v1_144@0",
            "content": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu, Freelb: Enhanced adversarial training for natural language understanding, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "296-ARR_v1_144",
            "start": 0,
            "end": 202,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_1",
            "tgt_ix": "296-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_1",
            "tgt_ix": "296-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_7",
            "tgt_ix": "296-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_8",
            "tgt_ix": "296-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_9",
            "tgt_ix": "296-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_10",
            "tgt_ix": "296-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_12",
            "tgt_ix": "296-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_14",
            "tgt_ix": "296-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_15",
            "tgt_ix": "296-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_16",
            "tgt_ix": "296-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_17",
            "tgt_ix": "296-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_18",
            "tgt_ix": "296-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_19",
            "tgt_ix": "296-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_20",
            "tgt_ix": "296-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_22",
            "tgt_ix": "296-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_23",
            "tgt_ix": "296-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_24",
            "tgt_ix": "296-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_25",
            "tgt_ix": "296-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_26",
            "tgt_ix": "296-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_27",
            "tgt_ix": "296-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_28",
            "tgt_ix": "296-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_29",
            "tgt_ix": "296-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_29",
            "tgt_ix": "296-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_29",
            "tgt_ix": "296-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_31",
            "tgt_ix": "296-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_32",
            "tgt_ix": "296-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_32",
            "tgt_ix": "296-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_32",
            "tgt_ix": "296-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_35",
            "tgt_ix": "296-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_36",
            "tgt_ix": "296-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_37",
            "tgt_ix": "296-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_38",
            "tgt_ix": "296-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_39",
            "tgt_ix": "296-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_40",
            "tgt_ix": "296-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_41",
            "tgt_ix": "296-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_42",
            "tgt_ix": "296-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_43",
            "tgt_ix": "296-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_44",
            "tgt_ix": "296-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_45",
            "tgt_ix": "296-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_46",
            "tgt_ix": "296-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_32",
            "tgt_ix": "296-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_47",
            "tgt_ix": "296-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_49",
            "tgt_ix": "296-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_50",
            "tgt_ix": "296-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_51",
            "tgt_ix": "296-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_52",
            "tgt_ix": "296-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_53",
            "tgt_ix": "296-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_54",
            "tgt_ix": "296-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_55",
            "tgt_ix": "296-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_56",
            "tgt_ix": "296-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_57",
            "tgt_ix": "296-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_58",
            "tgt_ix": "296-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_59",
            "tgt_ix": "296-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_60",
            "tgt_ix": "296-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_61",
            "tgt_ix": "296-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_62",
            "tgt_ix": "296-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_64",
            "tgt_ix": "296-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_66",
            "tgt_ix": "296-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_67",
            "tgt_ix": "296-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_68",
            "tgt_ix": "296-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_69",
            "tgt_ix": "296-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_70",
            "tgt_ix": "296-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_72",
            "tgt_ix": "296-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_73",
            "tgt_ix": "296-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_74",
            "tgt_ix": "296-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_75",
            "tgt_ix": "296-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_76",
            "tgt_ix": "296-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_77",
            "tgt_ix": "296-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_78",
            "tgt_ix": "296-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_78",
            "tgt_ix": "296-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_80",
            "tgt_ix": "296-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_80",
            "tgt_ix": "296-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_83",
            "tgt_ix": "296-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_85",
            "tgt_ix": "296-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_86",
            "tgt_ix": "296-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_87",
            "tgt_ix": "296-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_88",
            "tgt_ix": "296-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_90",
            "tgt_ix": "296-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_89",
            "tgt_ix": "296-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_89",
            "tgt_ix": "296-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_89",
            "tgt_ix": "296-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_91",
            "tgt_ix": "296-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_93",
            "tgt_ix": "296-ARR_v1_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_94",
            "tgt_ix": "296-ARR_v1_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_95",
            "tgt_ix": "296-ARR_v1_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_97",
            "tgt_ix": "296-ARR_v1_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_97",
            "tgt_ix": "296-ARR_v1_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "296-ARR_v1_0",
            "tgt_ix": "296-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_1",
            "tgt_ix": "296-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_2",
            "tgt_ix": "296-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_3",
            "tgt_ix": "296-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_4",
            "tgt_ix": "296-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_5",
            "tgt_ix": "296-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_6",
            "tgt_ix": "296-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_7",
            "tgt_ix": "296-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_7",
            "tgt_ix": "296-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_7",
            "tgt_ix": "296-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_8",
            "tgt_ix": "296-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_9",
            "tgt_ix": "296-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_10",
            "tgt_ix": "296-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_11",
            "tgt_ix": "296-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_12",
            "tgt_ix": "296-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_13",
            "tgt_ix": "296-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_14",
            "tgt_ix": "296-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_14",
            "tgt_ix": "296-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_14",
            "tgt_ix": "296-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_14",
            "tgt_ix": "296-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_15",
            "tgt_ix": "296-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_16",
            "tgt_ix": "296-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_17",
            "tgt_ix": "296-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_17",
            "tgt_ix": "296-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_18",
            "tgt_ix": "296-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_19",
            "tgt_ix": "296-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_20",
            "tgt_ix": "296-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_20",
            "tgt_ix": "296-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_20",
            "tgt_ix": "296-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_21",
            "tgt_ix": "296-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_22",
            "tgt_ix": "296-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_22",
            "tgt_ix": "296-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_22",
            "tgt_ix": "296-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_23",
            "tgt_ix": "296-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_24",
            "tgt_ix": "296-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_24",
            "tgt_ix": "296-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_25",
            "tgt_ix": "296-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_26",
            "tgt_ix": "296-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_27",
            "tgt_ix": "296-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_28",
            "tgt_ix": "296-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_28",
            "tgt_ix": "296-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_28",
            "tgt_ix": "296-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_29",
            "tgt_ix": "296-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_30",
            "tgt_ix": "296-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_31",
            "tgt_ix": "296-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_31",
            "tgt_ix": "296-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_31",
            "tgt_ix": "296-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_32",
            "tgt_ix": "296-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_33",
            "tgt_ix": "296-ARR_v1_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_34",
            "tgt_ix": "296-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_35",
            "tgt_ix": "296-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_35",
            "tgt_ix": "296-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_35",
            "tgt_ix": "296-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_35",
            "tgt_ix": "296-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_36",
            "tgt_ix": "296-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_37",
            "tgt_ix": "296-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_37",
            "tgt_ix": "296-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_37",
            "tgt_ix": "296-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_37",
            "tgt_ix": "296-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_38",
            "tgt_ix": "296-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_39",
            "tgt_ix": "296-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_40",
            "tgt_ix": "296-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_41",
            "tgt_ix": "296-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_42",
            "tgt_ix": "296-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_43",
            "tgt_ix": "296-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_44",
            "tgt_ix": "296-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_45",
            "tgt_ix": "296-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_46",
            "tgt_ix": "296-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_47",
            "tgt_ix": "296-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_47",
            "tgt_ix": "296-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_47",
            "tgt_ix": "296-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_48",
            "tgt_ix": "296-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_49",
            "tgt_ix": "296-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_49",
            "tgt_ix": "296-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_49",
            "tgt_ix": "296-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_49",
            "tgt_ix": "296-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_50",
            "tgt_ix": "296-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_51",
            "tgt_ix": "296-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_51",
            "tgt_ix": "296-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_51",
            "tgt_ix": "296-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_52",
            "tgt_ix": "296-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_53",
            "tgt_ix": "296-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_54",
            "tgt_ix": "296-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_55",
            "tgt_ix": "296-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_55",
            "tgt_ix": "296-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_55",
            "tgt_ix": "296-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_56",
            "tgt_ix": "296-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_57",
            "tgt_ix": "296-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_58",
            "tgt_ix": "296-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_58",
            "tgt_ix": "296-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_58",
            "tgt_ix": "296-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_59",
            "tgt_ix": "296-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_60",
            "tgt_ix": "296-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_61",
            "tgt_ix": "296-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_62",
            "tgt_ix": "296-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_62",
            "tgt_ix": "296-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_62",
            "tgt_ix": "296-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_62",
            "tgt_ix": "296-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_63",
            "tgt_ix": "296-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_64",
            "tgt_ix": "296-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_64",
            "tgt_ix": "296-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_65",
            "tgt_ix": "296-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_66",
            "tgt_ix": "296-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_67",
            "tgt_ix": "296-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_68",
            "tgt_ix": "296-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_68",
            "tgt_ix": "296-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_69",
            "tgt_ix": "296-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_70",
            "tgt_ix": "296-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_71",
            "tgt_ix": "296-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_72",
            "tgt_ix": "296-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_73",
            "tgt_ix": "296-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_73",
            "tgt_ix": "296-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_73",
            "tgt_ix": "296-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_74",
            "tgt_ix": "296-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_75",
            "tgt_ix": "296-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_76",
            "tgt_ix": "296-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_76",
            "tgt_ix": "296-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_77",
            "tgt_ix": "296-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_77",
            "tgt_ix": "296-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_77",
            "tgt_ix": "296-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_77",
            "tgt_ix": "296-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_78",
            "tgt_ix": "296-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_79",
            "tgt_ix": "296-ARR_v1_79@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_80",
            "tgt_ix": "296-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_81",
            "tgt_ix": "296-ARR_v1_81@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_82",
            "tgt_ix": "296-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_83",
            "tgt_ix": "296-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_84",
            "tgt_ix": "296-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_85",
            "tgt_ix": "296-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_86",
            "tgt_ix": "296-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_87",
            "tgt_ix": "296-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_88",
            "tgt_ix": "296-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_88",
            "tgt_ix": "296-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_89",
            "tgt_ix": "296-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_90",
            "tgt_ix": "296-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_91",
            "tgt_ix": "296-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_91",
            "tgt_ix": "296-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_92",
            "tgt_ix": "296-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_93",
            "tgt_ix": "296-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_93",
            "tgt_ix": "296-ARR_v1_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_93",
            "tgt_ix": "296-ARR_v1_93@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_94",
            "tgt_ix": "296-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_95",
            "tgt_ix": "296-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_96",
            "tgt_ix": "296-ARR_v1_96@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_97",
            "tgt_ix": "296-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_98",
            "tgt_ix": "296-ARR_v1_98@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_99",
            "tgt_ix": "296-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_100",
            "tgt_ix": "296-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_101",
            "tgt_ix": "296-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_102",
            "tgt_ix": "296-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_103",
            "tgt_ix": "296-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_104",
            "tgt_ix": "296-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_105",
            "tgt_ix": "296-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_106",
            "tgt_ix": "296-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_107",
            "tgt_ix": "296-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_108",
            "tgt_ix": "296-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_109",
            "tgt_ix": "296-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_110",
            "tgt_ix": "296-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_111",
            "tgt_ix": "296-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_112",
            "tgt_ix": "296-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_113",
            "tgt_ix": "296-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_114",
            "tgt_ix": "296-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_115",
            "tgt_ix": "296-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_116",
            "tgt_ix": "296-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_117",
            "tgt_ix": "296-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_118",
            "tgt_ix": "296-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_119",
            "tgt_ix": "296-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_120",
            "tgt_ix": "296-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_121",
            "tgt_ix": "296-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_122",
            "tgt_ix": "296-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_123",
            "tgt_ix": "296-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_124",
            "tgt_ix": "296-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_125",
            "tgt_ix": "296-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_126",
            "tgt_ix": "296-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_127",
            "tgt_ix": "296-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_128",
            "tgt_ix": "296-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_129",
            "tgt_ix": "296-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_130",
            "tgt_ix": "296-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_131",
            "tgt_ix": "296-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_132",
            "tgt_ix": "296-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_133",
            "tgt_ix": "296-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_134",
            "tgt_ix": "296-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_135",
            "tgt_ix": "296-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_136",
            "tgt_ix": "296-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_137",
            "tgt_ix": "296-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_138",
            "tgt_ix": "296-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_139",
            "tgt_ix": "296-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_140",
            "tgt_ix": "296-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_141",
            "tgt_ix": "296-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_142",
            "tgt_ix": "296-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_143",
            "tgt_ix": "296-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "296-ARR_v1_144",
            "tgt_ix": "296-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1234,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "296-ARR",
        "version": 1
    }
}