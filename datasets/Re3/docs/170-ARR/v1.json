{
    "nodes": [
        {
            "ix": "170-ARR_v1_0",
            "content": "VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_2",
            "content": "Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Videogrounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "170-ARR_v1_4",
            "content": "Vision-language tasks have been studied to build intelligent systems that can perceive information from multiple modalities, such as images, videos, and text. Extended from image-grounded tasks, e.g. (Antol et al., 2015), recently Jang et al. (2017); Lei et al. (2018) propose to use video as the grounding features. This modification poses a significant challenge to previous image-based models with the additional temporal variance through video frames.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_5",
            "content": "Recently further develop videogrounded language research into the dialogue domain. In the proposed task, video-grounded dialogues, the dialogue agent is required to answer questions about a video over multiple dialogue turns. Using Figure 1 as an example, to answer",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_6",
            "content": "Figure 1: A sample video-grounded dialogue with a demonstration of a reasoning process questions correctly, a dialogue agent has to resolve references in dialogue context, e.g. \"he\" and \"it\", and identify the original entity, e.g. \"a boy\" and \"a backpack\". Besides, the agent also needs to identify the actions of these entities, e.g. \"carrying a backpack\" to retrieve information from the video.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_7",
            "content": "Current state-of-the-art approaches to videogrounded dialogue tasks, e.g. (Le et al., 2019b;Fan et al., 2019) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs. However, these approaches often assume the reasoning structure, including resolving references of entities and detecting the corresponding actions to retrieve visual cues, is implicitly learned. An explicit reasoning structure becomes more beneficial as the tasks complicate in two scenarios: video with complex spatial and temporal dynamics, and language inputs with sophisticated semantic dependencies, e.g. questions positioned in a dialogue context. These scenarios often challenge researchers to interpret model hidden layers, identify errors, and assess model reasoning capability.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_8",
            "content": "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016;Goyal et al., 2017;Feng et al., 2018;Serrano and Smith, 2019). Andreas et al. (2016b) propose neural module networks (NMNs) by decomposing a question into sub-sequences called program and assembling a network of neural operations. Motivated by this line of research, we propose a new approach, VGNMN, to video-grounded language tasks. Our approach benefits from integrating neural networks with a compositional reasoning structure to exploit low-level information signals in video. An example of the reasoning structure can be seen on the right side of Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_9",
            "content": "Video-grounded Neural Module Network (VGNMN) tackles video understanding through action and entity-paramterized NMNs to retrieve video features. We first decompose question into a set of entities and extract video features related to these entities. VGNMN then extracts the temporal steps by focusing on relevant actions that are associated with these entities. VGNMN is analogous to how human processes information by gradually retrieving signals from input modalities using a set of discrete subjects and their actions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_10",
            "content": "To tackle dialogue understanding, VGNMN is trained to resolve any co-reference in language inputs, e.g. questions in a dialogue context, to identify the unique entities in each dialogue. Previous approaches to video-grounded dialogues often obtain question global representations in relation to dialogue context. These approaches might be suitable to represent general semantics in open-domain dialogues (Serban et al., 2016). However, they are not ideal to detect fine-grained information in a video-grounded dialogue which frequently entails dependencies between questions and past dialogue turns in the form of entity references.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_11",
            "content": "In summary, our contributions include:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_12",
            "content": "\u2022 VGNMN, a neural module network-based approach for video-grounded dialogues. \u2022 The approach includes a modularized system that creates a reasoning pipeline parameterized by entity and action-based representations from both dialogue and video contexts. \u2022 Our experiments are conducted on the challenging benchmark for video-grounded dialogues, Audio-visual Scene-Aware Dialogues (AVSD) as well as TGIF-QA (Jang et al., 2017) for video QA task. \u2022 Our results indicate strong performance of VGNMN as well as improved model interpretability and robustness to difficult scenarios of dialogues, videos, and question structures.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_13",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_14",
            "content": "Video-Language Understanding",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "170-ARR_v1_15",
            "content": "The research of video-language understanding aims to develop a model's joint understanding capability of language, video, and their interactions. Recently, we have witnessed emerging techniques in video-language systems that exploit deep transformer-based architectures such as BERT (Devlin et al., 2019) for pretraining multimodal representations (Li et al., 2020a;Yang et al., 2020;Kim et al., 2021;Tang et al., 2021;Zellers et al., 2021) in very large-scale videolanguage datasets. While these systems can achieve impressive performance, they are not straightforward to apply in domains with limited data such as video-grounded dialogues. Moreover, as we shown in our qualitative examples, our approach facilitates better interpretability through the output of decoded functional programs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_16",
            "content": "Video-grounded Dialogues",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "170-ARR_v1_17",
            "content": "Extended from video QA, video-grounded dialogue is an emerging task that combines dialogue response generation and video-language understanding research. This task entails a novel requirement for models to learn dialogue semantics and decode entity co-references in questions. Nguyen et al. (2018); ; ; Sanabria et al. (2019); Le et al. (2019a,b) extend traditional QA models by adding dialogue history neural encoders. Kumar et al. (2019) enhances dialogue features with topic-level representations to express the general topic in each dialogue. Schwartz et al. (2019) treats each dialogue turn as an independent sequence and allows interaction between questions and each dialogue turn. Le et al. (2019b) encodes dialogue history as a sequence with embedding and positional representations. Different from prior work, we dissect the question sequence and explicitly detect and decode any entities and their references. Our approach also enables insights on how models extract deductive bias from dialogues to extract video information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_18",
            "content": "Neural Module Network",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "170-ARR_v1_19",
            "content": "Neural Module Network (NMN) (Andreas et al., 2016b,a) is introduced to address visual QA by decomposing questions into linguistic sub-structures, known as programs, to instantiate a network of neural modules. NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b;Hu et al., 2018;Han et al., 2019). Yi et al. (2018); Han et al. (2019); improve NMN models by decoupling visual-language understanding and visual concept learning. Our work is related to the recent work (Kottur et al., 2018;Jiang and Bansal, 2019;Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning. Our approach follows the previous approaches that learn to generate program structure and require no parser at evaluation time. Compared to prior work, we use NMN to learn dependencies between the composition in language inputs and the spatio-temporal dynamics in videos. Specifically, we propose to construct a reasoning structure from text, from which detected entities are used to extract visual information in the spatial space and detected actions are used to find visual information in the temporal space.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_20",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "170-ARR_v1_21",
            "content": "In this section, we present the design of our model. An overview of the model can be seen in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_22",
            "content": "Task Definition",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "170-ARR_v1_23",
            "content": "The input to the model consists of a dialogue D which is grounded on a video V. The input components include the question of current dialogue turn Q, dialogue history H, and the features of the input video, including visual and audio input. The output is a dialogue response, denoted as R. Each text input component is a sequence of words w 1 , ..., w m \u2208 V in , the input vocabulary. Similarly, the output response R is a sequence of tokens w 1 , ..., w n \u2208 V out , the output vocabulary. The objective of the task is the generation objective that output answers of the current dialogue turn t:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_24",
            "content": "Rt = arg max Rt P (R t |V, H t , Q t ; \u03b8) = arg max Rt L R n=1 P m (w n |R t,1:n\u22121 , V, H t , Q t ; \u03b8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_25",
            "content": "In a Video-QA task, the dialogue history H is simply absent and the output response is typically collapsed to a single-token response.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_26",
            "content": "Encoders",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "170-ARR_v1_27",
            "content": "Text Encoder. A text encoder is shared to encode text inputs, including dialogue history, questions, and captions. The text encoder converts each text sequence X = w 1 , ..., w m into a sequence of embeddings X \u2208 R m\u00d7d . We use a trainable embedding matrix to map token indices to vector representations of d dimensions through a mapping function \u03c6. These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization (Ba et al., 2016;Vaswani et al., 2017). The embedding and positional representations are combined through element-wise summation. The encoded dialogue history and question of the current turn are defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_28",
            "content": "H = Norm(\u03c6(H) + PE(H)) \u2208 R L H \u00d7d and Q = Norm(\u03c6(Q) + PE(Q)) \u2208 R L Q \u00d7d .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_29",
            "content": "Video Encoder. To encode video, we use pretrained models to extract visual and audio features. We denote F as the sampled video frames or video clips. For object-level visual features, we denote O as the maximum number of objects considered in each frame. The resulting output from a pretrained object detection model is Z obj \u2208 R F \u00d7O\u00d7d vis . We concatenate each object representation with the corresponding coordinates projected to d vis dimensions. We also make use of a CNN-based pretrained model to obtain features of temporal dimension Z cnn \u2208 R F \u00d7d vis . The audio feature is 1: Description of the modules and their functionalities. We denote P as the parameter to instantiate each module, H as the dialogue history, Q as the question of the current dialogue turn, and V as video input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_30",
            "content": "obtained through a pretrained audio model, Z aud \u2208 R F \u00d7d aud . We passed all video features through a linear transformation layer with ReLU activation to the same embedding dimension d.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_31",
            "content": "Neural Modules",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "170-ARR_v1_32",
            "content": "We introduce neural modules that are used to assemble an executable program constructed by the generated sequence from question parsers. We provide an overview of neural modules in Table 1 and demonstrate dialogue understanding and video understanding modules in Figure 3 and 4 respectively. Each module parameter, e.g. \"a backpack\", is extracted from the parsed program (See Section 3.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_33",
            "content": "For each parameter, we denote P \u2208 R d as the average pooling of component token embeddings. find(P,H)\u2192H ent . This module handles entity tracing by obtaining a distribution over tokens in the dialogue history. We use an entity-todialogue-history attention mechanism applied from an entity P i to all tokens in the dialogue history. Any neural network that learn to generate attention between two tensors is applicable .e.g. (Bahdanau et al., 2015;Vaswani et al., 2017). The attention matrix normalized by softmax, A find,i \u2208 R L H , is used to compute the weighted sum of dialogue history token representations. The output is combined with entity embedding P i to obtain contextual entity representation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_34",
            "content": "H ent,i \u2208 R d .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_35",
            "content": "summarize(H ent ,Q)\u2192Q ctx . For each contextual entity representation H ent,i , i = 1, ..., N ent , it is projected to L Q dimensions and is combined with question token embeddings through elementwise summation to obtain entity-aware question representation Q ent,i \u2208 R L Q \u00d7d . It is fed to a onedimensional CNN with max-pooling layer (Kim, 2014) to obtain a contextual entity-aware question representation. We denote the final output as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_36",
            "content": "Q ctx \u2208 R Nent\u00d7d .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_37",
            "content": "While previous models usually focus on global or token-level dependencies Le et al., 2019b) to encode question features, our modules compress fine-grained question representations at the entity level. Specifically, find and summarize modules can generate entitydependent local and global representations of question semantics. We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context (Serban et al., 2016;Vaswani et al., 2017) (Section 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_38",
            "content": "where(P,V)\u2192V ent . Similar to the find module, this module handles entity-based attention to the video input. However, the entity representation P , in this case, is parameterized by the original entity in dialogue rather than in question (See Section 3.4 for more description). Each entity P i is stacked to match the number of sampled video frames/clips F . An attention network is used to obtain entity-to-object attention matrix A where,i \u2208 R F \u00d7O . The attended feature are compressed through weighted sum pooling along the spatial dimension, resulting in V ent,i \u2208 R F \u00d7d , i = 1, ..., N ent .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_39",
            "content": "when(P,V ent )\u2192V ent+act . This module follows a similar architecture as the where module. However, the action parameter P i is stacked to match N ent dimensions. The attention matrix A when,i \u2208 R F is then used to compute the visual entity-action representations through weighted sum along the temporal dimension. We denote the output for all actions P i as and [; ] is the concatenation operation. Note that the parameter P here is extracted from questions, often as the type of questions e.g. \"what\" and \"how\". This eliminates the need to have different modules for different question types. However, we noted the current design may be challenged in rare cases in which an utterance contain numerous questions (refer to Figure 5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_40",
            "content": "V ent+act \u2208 R Nent\u00d7Nact\u00d7d describe(P,V ent+act )\u2192V ctx . This module is a linear transformation to compute V ctx = W desc T [V ent+act ; P stack ] \u2208 R Nent\u00d7Nact\u00d7d where W desc \u2208 R 2d\u00d7d , P stack is the stacked represen- tations of parameter embedding P to N ent \u00d7 N act dimensions,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_41",
            "content": "The exist module is used when the questions are \"yes/no\" questions. This module is a special case of describe module where the parameter P is simply the average pooled question embeddings. The above where module is applied to object-level features. For temporal-based features such as CNN-based and audio features, the same neural operation is applied along the temporal dimension. Each resulting entity-aware output is then incorporated to frame-level features through element-wise summation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_42",
            "content": "An advantage of our architecture is that it separates dialogue and video understanding. We adopt a transparent approach to solve linguistic entity references during the dialogue understanding phase. The resolved entities are fed to the video understanding phase to learn entity-action dynamics in the video. We show that our approach is robust when dialogue evolves to many turns and video extends over time (Please refer to Section 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_43",
            "content": "Question Parsers",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "170-ARR_v1_44",
            "content": "To learn compositional programs, we follow (Johnson et al., 2017a;Hu et al., 2017) and consider program generation as a sequence-tosequence task. We adopt a simple template \" param 1 module 1 param 2 module 2 ...\" as the target sequence. The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_45",
            "content": "The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding. Each parser is a vanilla Transformer decoder, including multi-head attention layers on questions and past dialogue turns (Please refer to Appendix A.1 for more technical details).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_46",
            "content": "Response Decoder",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "170-ARR_v1_47",
            "content": "System response is decoded by incorporating the dialogue context and video context outputs from the corresponding reasoning programs to target token representations. We follows a vanilla Transformer decoder architecture (Le et al., 2019b), which consists of 3 attention layers: self-attention to attend on existing tokens, attention to Q ctx from dialogue understanding program execution, and attention to V ctx from video understanding program execution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_48",
            "content": "A (1) res = Attention(R| j\u22121 0 , R| j\u22121 0 , R| j\u22121 0 ) \u2208 R j\u00d7d A (2) res = Attention(A (1) res , Q ctx , Q ctx ) \u2208 R j\u00d7d A (3) res = Attention(A (2) res , V ctx , V ctx ) \u2208 R j\u00d7d",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_49",
            "content": "Multimodal Fusion. For video features come from multiple modalities, visual and audio, the contextual features, denoted V ctx , is obtained through a weighted sum of component modalities, e.g. contextual visual features V vis ctx and contextual audio features V aud ctx . The scores S fusion to compute the weighted sum is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_50",
            "content": "S fusion = Softmax(W T fusion [Q stack ; V vis ctx ; V aud ctx ])",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_51",
            "content": "where Q stack is the mean pooling output of question embeddings Q which is then stacked to N ent + N act dimensions, and W fusion \u2208 R 3d\u00d72 are trainable model parameters. The resulting S fusion has a dimension of \u2208 R (Nent+Nact)\u00d72 . Response Generation. To generate response sequences, a special token \"_sos\" is concatenated as the first token w 0 . The decoded token w 1 is then appended to w 0 as input to decode w 2 and so on. Similarly to input source sequences, at decoding time step j, the input target sequence is encoded to obtain representations of system response R| j\u22121 0 . We combine vocabulary of input and output sequences and share the embedding matrix E \u2208 R |V|\u00d7d where V = V in \u2229 V out . During training time, we directly use the ground-truth responses as input to the decoder and optimize VGNMN with a cross-entropy loss to decode the next ground-truth tokens. During test time, responses are generated auto-regressively through beam search with beam size 5. Note that we apply the same procedure to generate reasoning programs from question parsers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_52",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "170-ARR_v1_53",
            "content": "Experimental Setup and Training We use the AVSD benchmark from the Dialogue System Technology Challenge 7 (DSTC7) .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_54",
            "content": "The benchmark consists of dialogues grounded on the Charades videos (Sigurdsson et al., 2016). Each dialogue contains up to 10 dialogue turns, each turn consists of a question and expected response about a given video. For visual features, we use the 3D CNN-based features from a pretrained I3D model (Carreira and Zisserman, 2017) and object-level features from a pretrained FasterRNN model (Ren et al., 2015b). The audio features are obtained from a pretrained VGGish model (Hershey et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_55",
            "content": "In the experiments with AVSD, we consider two settings: one with video summary and one without video summary as input. In the setting with video summary, the summary is concatenated to the dialogue history before the first dialogue turn. We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs. We also adapt VGNMN to the video QA benchmark TGIF-QA (Jang et al., 2017). For more details of data and training, please refer to Appendix B and C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_56",
            "content": "AVSD Results. We evaluate model performance by the objective metrics, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015), between each generated response and 6 reference gold responses. As seen in Table 2, our models outperform most of existing approaches. We observed that our approach did not outperform the GPT-based baselines (Li et al., 2020b;Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al., 2020b), even though our model did not rely on deep pretrained represen- .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_57",
            "content": "Robustness to video length: In Table 3a, we noted that the performance gap between VGNMN and (1) is quite distinct, with 7/10 cases of video ranges in which VGNMN outperforms. However, in lower ranges (i.e. 1-23 seconds) and higher ranges (37-75 seconds), VGNMN performs not as well as model (1). We observed that related factors might affect the discrepancy, such as the complexity of the questions for these short and long-range videos. Potentially, our question parser for the video understanding program needs to be improved (e.g. for tree-based programs) to retrieve information in these ranges.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_58",
            "content": "Robustness to dialogue turn: In Table 3b, we observed that model (1) performs better than model (2) overall, especially in higher turn positions, i.e. from the 4 th turn to 8 th turn. Interestingly, we noted some mixed results in very low turn position, i.e. the 2 nd and 3 rd turn, and very high turn position, i.e. the 10 th turn. Potentially, with a large dialogue turn position, the neural-based approach such as hierarchical RNN can better capture the global dependencies within dialogue context than the entity-based compositional NMN method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_59",
            "content": "Robustness to question structure: Finally, we compared performance of VGNMN with the no-NMN variant (1) in different cases of question structures: single-question vs. multiple-part structure. In single-question structures, we examined by the question types (e.g. yes/no, wh-questions). In multi-part structures, we further classified whether there are sentences preceding the question (e.g. \"1Sent+Que\") or there are smaller (sub-)questions (e.g. \"2SubQue\") within the question. In Table 3c, we observed that VGNMN has clearer performance gains in multi-part structures than singlequestion structures. In multi-part structures, we observed higher gaps between VGNMN and model (1) in highly complex cases e.g. \"2Sent+Que\" vs. \"1Sent+Que\". These observations indicate the robustness of VGNMN and the underlying compositionality principle to deal with complex question structures. We also noted that VGNMN is still susceptible to extremely long questions (\">2Sent+Que\") and future work is needed to address these scenarios.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_60",
            "content": "Interpretability. In Figure 5, we show both success and failure cases of generated responses and corresponding generated functional programs. In each example, we marked predicted outputs as incorrect if they do not match the ground-truth completely (even though the outputs might be partially correct). From Figure 5, we observe that in cases where generated dialogue programs and video programs match or are close to the gold labels, the model can generate generally correct responses. For cases where some module parameters do not exactly match but are closed to the gold labels, the model can still generate responses with the correct visual information (e.g. the 4 th turn in example B). In cases of wrong predicted responses, we can further look at how the model understands the questions based on predicted programs. In the 3 rd turn of example A, the output response is missing a minor detail as compared to the label response because the video program fails to capture \"rooftop\" as a where parameter. These subtle yet important details can determine whether output responses can fully address user queries. In the 3 rd turn of example B, the model wrongly identifies \"what room\" as a where parameter and subsequently generates a wrong response that it is \"a living room\". TGIF-QA Results. We report the result using the L2 loss in Count task and accuracy in other tasks. From Table 4, VGNMN outperforms the majority of the baseline models in all tasks by a large margin. Compared to AVSD experiments, the TGIF-QA experiments emphasize the video understanding ability of the models, removing the requirement for dialogue understanding and natural language generation. Since TGIF-QA questions follow a very specific question type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD. Using exact-match accuracy of parsed programs vs. label programs as a metric, our question parser can achieve a performance 81% to 94% accuracy in TGIF-QA vs. 41-45% in AVSD. The higher accuracy in decoding a reasoning structure translates to better adaptation between training and test time, resulting in higher performance gains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_61",
            "content": "Cascading Errors. Compared to prior approaches, we noted that VGNMN is a modularized system which may result in cascading errors to downstream modules. One major error is the error of generated programs which is used as parameters in neural modules. To gauge this error, we compare the performance of VGNMN between 2 cases: with generated programs and with groundtruth programs. From Table 5, we noticed some performance gaps between these cases. These observations imply that: (1) program generations and response generations are positively correlated and more accurate programs can lead to better responses; and (2) current question parsers are not perfect, resulting in wrong parameters to instantiate neural modules. Future work may focus on learning better question parsers or directly deploying a better off-the-shelf parser tool. Table 5: Comparison of VGNMN on AVSD (top) and TGIF-QA (bottom) when using generated (\"Gen.\") vs. ground-truth (\"GT\") programs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_62",
            "content": "For additional experiment results, qualitative samples, and analysis between model variants, refer to Appendix D and E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_63",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "170-ARR_v1_64",
            "content": "In this work, we introduce Video-grounded Neural Module Network (VGNMN). VGNMN consists of dialogue and video understanding neural modules, each of which performs entity and action-level operations on language and video components. Our comprehensive experiments on AVSD and TGIF-QA benchmarks show that our models can achieve competitive performance while promoting a compositional and interpretable learning approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_65",
            "content": "Broader Impacts",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "170-ARR_v1_66",
            "content": "During the duration of this work, there have been no ethical concerns regarding the model implementation, training, and testing. The data used in this work has been carefully reviewed and accordingly to the description from the original authors, we did not find any concerns on any significant biases. For any potential application or extension of this work, we would like to highlight some specific concerns. First, as the work is developed to build an intelligent dialogue agents, models should not be used with the intention to create fake human profiles for any harmful purposes (e.g. fishing or spreading fake news). For wider use of dialogue systems, the application of work might result in certain impacts to some stakeholders whose jobs may be affected by this application (e.g. customer service call agents). We hope any application should be carefully considered against these potential risks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_67",
            "content": "To learn compositional programs, we follow (Johnson et al., 2017a;Hu et al., 2017) and consider program generation as a sequence-tosequence task. We adopt a simple template \" param 1 module 1 param 2 module 2 ...\" as the target sequence. The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_68",
            "content": "The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding. Each parser is an attention-based Transformer decoder. The Transformer attention is a multi-head attention on query q, key k, and value v tensors, denoted as Attention(q, k, v). For each token in the q sequence , the distribution over tokens in the k sequence is used to obtain the weighted sum of the corresponding representations in the v sequence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_69",
            "content": "Attention(q, k, v) = softmax( qk T \u221a d k )v \u2208 R Lq\u00d7dq",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_70",
            "content": "Each attention is followed by a feed-forward network applied to each position identically. We exploit the multi-head and feed-forward architecture, which show good performance in NLP tasks such as NMT and QA (Vaswani et al., 2017;Dehghani et al., 2019), to efficiently incorporate contextual cues from dialogue components to parse question into reasoning programs. At decoding step 0, we simply use a special token _sos as the input to the parser. In each subsequent decoding step, we concatenate the prior input sequence with the generated token to decode in an auto-regressive manner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_71",
            "content": "We share the vocabulary sets of input and output components and thus, use the same embedding matrix. Given the encoded question Q, to decode the program for dialogue understanding, the contextual signals are integrated through 2 attention layers: one attention on previously generated tokens, and the other on question tokens. At time step j, we denote the output from an attention layer as A dial,j .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_72",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_73",
            "content": "dial = Attention(P dial | j\u22121 0 , P dial | j\u22121 0 , P dial | j\u22121 0 ) A (2) dial = Attention(A (1) dial , Q, Q) \u2208 R j\u00d7d",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_74",
            "content": "To generate programs for video understanding, the contextual signals are learned and incorporated in a similar manner. However, to exploit dialogue contextual cues, the execution output of dialogue understanding neural modules Q ctx is incorporated to each vector in P dial through an additional attention layer. This layer integrates the resolved entity information to decode the original entities for video understanding. It is equivalent to a reasoning process that converts the question from its original multi-turn semantics to single-turn semantics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_75",
            "content": "A (1) vid = Attention(P vid | j\u22121 0 , P vid | j\u22121 0 , P vid | j\u22121 0 ) A (2) vid = Attention(A (1) vid , Q, Q) \u2208 R j\u00d7d A (3) vid = Attention(A (2) vid , Q ctx , Q ctx ) \u2208 R j\u00d7d",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_76",
            "content": "Noted that in the neural modules described in Section 3.3, during training, we simply feed the ground-truth programs to optimize these modules. For instance, the neural module where received the ground truth entities P which is then used to instantiate the neural network and retrieve from video V . During test time, we decode the programs token by token through the question parsers, and feed the predicted entities P to neural modules. Note that we do not assume, and hence not train model to retrieve ground-truth locations of visual entities in videos. This strategy enables the applicability of VGNMN as we consider these entity annotations mostly unavailable in real-world systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_77",
            "content": "Different from AVSD, TGIF-QA contains a diverse set of QA tasks:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_78",
            "content": "We follow prior approaches (Hu et al., 2017(Hu et al., , 2018Kottur et al., 2018) by obtaining the annotations of the programs through a language parser (Hu et al., 2016) and a reference resolution model (Clark and Manning, 2016). During training, we directly use these as ground-truth labels of programs to train our models. The ground-truth responses are augmented with label smoothing technique (Szegedy et al., 2016). During inference time, we generate all programs and responses from given dialogues and videos. We run beam search to enumerate programs for dialogue and video understanding and dialogue responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_79",
            "content": "We use a training batch size of 32 and embedding dimension d = 128 in all experiments. Where Transformer attention is used, we fix the number of attention heads to 8 in all attention layers. In neural modules with MLP layers, the MLP network is fixed to 2 linear layers with a ReLU activation in between. In neural modules with CNN, we adopt a vanilla CNN architecture for text classification (without the last MLP layer) where the number of input channels is 1, the kernel sizes are {3, 4, 5}, and the number of output channels is d. We initialize models with uniform distribution (Glorot and Bengio, 2010). During training, we adopt the Adam optimizer (Kingma and Ba, 2015) and a decaying learning rate (Vaswani et al., 2017) where we fix the warm-up steps to 15K training steps. We employ dropout (Srivastava et al., 2014) of 0.2 at all networks except the last linear layers of question parsers and response decoder. We train models up to 50 epochs and select the best models based on the average loss per epoch in the validation set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_80",
            "content": "All models are trained in a V100 GPU with a capacity of 16GB. We approximated each training epoch took about 20 minutes to run. For each model experiment with VGNMN, we obtained at least 2 runs and reported the average results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_81",
            "content": "Optimization. We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_82",
            "content": "L = \u03b1L dial + \u03b2L vid + L res = \u03b1 j \u2212 log(P dial (P dial,j )) + \u03b2 l \u2212 log(P video (P video,l )) + n \u2212 log(P res (R n ))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_83",
            "content": "where P is the probability distribution of an output token. The probability is computed by passing output representations from the parsers and decoder to a linear layer W \u2208 R d\u00d7V with softmax activation. We share the parameters between W and embedding matrix E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_84",
            "content": "We experiment with several Non-NMN based variants of our models. As can be seen in Table 7, our approach to video and dialogue understanding through compositional reasoning programs exhibits better performance than non-compositional approaches. Compared to the approaches that directly process frame-level features in videos (Row B) or token-level features in dialogues (Row C, D), our full VGNMN (Row A) considers entitylevel and action-level information extraction and thus, avoids unnecessary and possibly noisy extraction. Compared to the approaches that obtain dialogue contextual cues through a hierarchical encoding architecture (Row E, F) such as (Serban et al., 2016;, VGNMN directly addresses the challenge of entity references in dialogues. As mentioned, we hypothesize that the hierarchical encoding architecture is more appropriate for less entity-sensitive dialogues such as chit-chat and open-domain dialogues.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_85",
            "content": "Experimenting with different ways to integrate dialogue context representations, we observe that adding an attention layer attending to question during response decoding (Row G) is not necessary. This can be explained as the representation Q ctx obtained from dialogue understanding program already contains contextual information of both dialogue history and question and question input is no longer needed in the decoding phase. Furthermore, we investigate the model sensitivity to natural language generation through its ability to construct linguistically correct programs and responses. To generate responses that are linguistically appropriate, VGNMN needs dialogue context representation Q ctx as input to the response decoder (Row H). The model also needs encoded question Q as input to the video understanding program parser to be able to decompose this sequence to entity and action module parameters (Row I).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_86",
            "content": "We extract the predicted programs and responses for some example dialogues in Figure 6, 7, 8, and 9 and report our observations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_87",
            "content": "\u2022 We observe that when the predicted programs are correct, the output responses generally match the ground-truth (See the 1 st and 2 nd turn in Figure 6, and the 1 st and 4 th turn in Figure 8) or close to the ground-truth responses (1 st turn in Figure 7). \u2022 When the output responses do not match the ground truth, we can understand the model mistakes by interpreting the predicted programs. For example, in the 3 rd turn in Figure 6, the output response describes a room because the predicted video program focuses on the entity \"what room\" instead of the entity \"an object\" in the question. Another example is the 3 rd turn in Figure 8 where the entity \"rooftop\" is missing in the video program.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_88",
            "content": "These mismatches can deviate the information retrieved from the video during video program execution, leading to wrong output responses with wrong visual contents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_89",
            "content": "\u2022 We also note that in some cases, one or both of the predicted programs are incorrect, but the predicted responses still match the groundtruth responses. This might be explained as the predicted module parameters are still close enough to the \"gold\" labels (e.g. 4 th turn in Figure 6). Sometimes, our model predicted programs that are more appropriate than the ground truth. For example, in the 2 nd turn in Figure 7, the program is added with a where module parameterized by the entity \"the shopping bag\" which was solved from the reference \"them\" mentioned in the question. \u2022 We observe that for complex questions that involve more than one queries (e.g. the 3 rd turn in Figure 8), it becomes more challenging to decode an appropriate video understanding program and generate responses that can address all queries. \u2022 In Figure 9, we demonstrate some output examples of VGNMN and compare with two baselines: Baseline and MTN (Le et al., 2019b). We noted that VGNMN can include important entities relevant to the current dialogue turn to construct output responses while other models might miss some entity details, e.g. \"them/dishes\" in example A and \"the magazine\" in example B. These small yet important details can determine the correctness of dialogue responses.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "170-ARR_v1_90",
            "content": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Analyzing the behavior of visual question answering models, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Aishwarya Agrawal",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Analyzing the behavior of visual question answering models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_91",
            "content": "Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh, Dhruv Batra, Anoop Cherian, Tim Marks, Chiori Hori, Audio-visual scene-aware dialog, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Huda Alamri",
                    "Vincent Cartillier",
                    "Abhishek Das",
                    "Jue Wang",
                    "Stefan Lee",
                    "Peter Anderson",
                    "Irfan Essa",
                    "Devi Parikh",
                    "Dhruv Batra",
                    "Anoop Cherian",
                    "Tim Marks",
                    "Chiori Hori"
                ],
                "title": "Audio-visual scene-aware dialog",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_92",
            "content": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Learning to compose neural networks for question answering, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jacob Andreas",
                    "Marcus Rohrbach",
                    "Trevor Darrell",
                    "Dan Klein"
                ],
                "title": "Learning to compose neural networks for question answering",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_93",
            "content": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Neural module networks, 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Andreas",
                    "Marcus Rohrbach",
                    "Trevor Darrell",
                    "Dan Klein"
                ],
                "title": "Neural module networks",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_94",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Vqa: Visual question answering, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Stanislaw Antol",
                    "Aishwarya Agrawal",
                    "Jiasen Lu",
                    "Margaret Mitchell",
                    "Dhruv Batra",
                    "Lawrence Zitnick",
                    "Devi Parikh"
                ],
                "title": "Vqa: Visual question answering",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE international conference on computer vision",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_95",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_96",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Dzmitry Bahdanau",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "Neural machine translation by jointly learning to align and translate",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_97",
            "content": "Satanjeev Banerjee, Alon Lavie, Meteor: An automatic metric for mt evaluation with improved correlation with human judgments, 2005, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_98",
            "content": "Joao Carreira, Andrew Zisserman, Quo vadis, action recognition? a new model and the kinetics dataset, 2017, proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Joao Carreira",
                    "Andrew Zisserman"
                ],
                "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
                "pub_date": "2017",
                "pub_title": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_99",
            "content": "Yun-Wei Chu, Kuan-Yen Lin, Chao-Chun Hsu, Lun-Wei Ku, Multi-step joint-modality attention network for scene-aware dialogue system, 2020, DSTC Workshop @ AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Yun-Wei Chu",
                    "Kuan-Yen Lin",
                    "Chao-Chun Hsu",
                    "Lun-Wei Ku"
                ],
                "title": "Multi-step joint-modality attention network for scene-aware dialogue system",
                "pub_date": "2020",
                "pub_title": "DSTC Workshop @ AAAI",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_100",
            "content": "Kevin Clark, Christopher Manning, Deep reinforcement learning for mention-ranking coreference models, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Kevin Clark",
                    "Christopher Manning"
                ],
                "title": "Deep reinforcement learning for mention-ranking coreference models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_101",
            "content": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, Universal transformers, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Mostafa Dehghani",
                    "Stephan Gouws",
                    "Oriol Vinyals",
                    "Jakob Uszkoreit",
                    "Lukasz Kaiser"
                ],
                "title": "Universal transformers",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_102",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "170-ARR_v1_103",
            "content": "Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, Heng Huang, Heterogeneous memory enhanced multimodal attention model for video question answering, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Chenyou Fan",
                    "Xiaofan Zhang",
                    "Shu Zhang",
                    "Wensheng Wang",
                    "Chi Zhang",
                    "Heng Huang"
                ],
                "title": "Heterogeneous memory enhanced multimodal attention model for video question answering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_104",
            "content": "Eric Shi Feng, Alvin Wallace, I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Pathologies of neural models make interpretations difficult, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Eric Shi Feng",
                    "Alvin Wallace",
                    "I Grissom",
                    "Mohit Iyyer",
                    "Pedro Rodriguez",
                    "Jordan Boyd-Graber"
                ],
                "title": "Pathologies of neural models make interpretations difficult",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_105",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Akira Fukui",
                    "Dong Park",
                    "Daylen Yang",
                    "Anna Rohrbach",
                    "Trevor Darrell",
                    "Marcus Rohrbach"
                ],
                "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_106",
            "content": "Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia, Motion-appearance co-memory networks for video question answering, 2018, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jiyang Gao",
                    "Runzhou Ge",
                    "Kan Chen",
                    "Ram Nevatia"
                ],
                "title": "Motion-appearance co-memory networks for video question answering",
                "pub_date": "2018",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_107",
            "content": "Lianli Gao, Pengpeng Zeng, Jingkuan Song, Yuan-Fang Li, Wu Liu, Tao Mei, Heng Tao Shen, Structured two-stream attention network for video question answering, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Lianli Gao",
                    "Pengpeng Zeng",
                    "Jingkuan Song",
                    "Yuan-Fang Li",
                    "Wu Liu",
                    "Tao Mei",
                    "Heng Tao Shen"
                ],
                "title": "Structured two-stream attention network for video question answering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_108",
            "content": "Xavier Glorot, Yoshua Bengio, Understanding the difficulty of training deep feedforward neural networks, 2010, Proceedings of the thirteenth international conference on artificial intelligence and statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Xavier Glorot",
                    "Yoshua Bengio"
                ],
                "title": "Understanding the difficulty of training deep feedforward neural networks",
                "pub_date": "2010",
                "pub_title": "Proceedings of the thirteenth international conference on artificial intelligence and statistics",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_109",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_110",
            "content": "Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, Neural module networks for reasoning over text, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Nitish Gupta",
                    "Kevin Lin",
                    "Dan Roth",
                    "Sameer Singh",
                    "Matt Gardner"
                ],
                "title": "Neural module networks for reasoning over text",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_111",
            "content": "Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, Jiajun Wu, Visual concept-metaconcept learning, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Chi Han",
                    "Jiayuan Mao",
                    "Chuang Gan",
                    "Josh Tenenbaum",
                    "Jiajun Wu"
                ],
                "title": "Visual concept-metaconcept learning",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_112",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                "title": "Deep residual learning for image recognition",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_113",
            "content": "Shawn Hershey, Sourish Chaudhuri, P Daniel,  Ellis, F Jort, Aren Gemmeke, Channing Jansen, Manoj Moore, Devin Plakal,  Platt, A Rif, Bryan Saurous,  Seybold, Cnn architectures for largescale audio classification, 2017, Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Shawn Hershey",
                    "Sourish Chaudhuri",
                    "P Daniel",
                    " Ellis",
                    "F Jort",
                    "Aren Gemmeke",
                    "Channing Jansen",
                    "Manoj Moore",
                    "Devin Plakal",
                    " Platt",
                    "A Rif",
                    "Bryan Saurous",
                    " Seybold"
                ],
                "title": "Cnn architectures for largescale audio classification",
                "pub_date": "2017",
                "pub_title": "Acoustics, Speech and Signal Processing",
                "pub": "IEEE"
            }
        },
        {
            "ix": "170-ARR_v1_114",
            "content": "C Hori, H Alamri, J Wang, G Wichern, T Hori, A Cherian, T Marks, V Cartillier, R Lopes, A Das, I Essa, D Batra, D Parikh, Endto-end audio visual scene-aware dialog using multimodal attention-based video features, 2019, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "C Hori",
                    "H Alamri",
                    "J Wang",
                    "G Wichern",
                    "T Hori",
                    "A Cherian",
                    "T Marks",
                    "V Cartillier",
                    "R Lopes",
                    "A Das",
                    "I Essa",
                    "D Batra",
                    "D Parikh"
                ],
                "title": "Endto-end audio visual scene-aware dialog using multimodal attention-based video features",
                "pub_date": "2019",
                "pub_title": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_115",
            "content": "Chiori Hori, Anoop Cherian, Tim Marks, Takaaki Hori, Joint student-teacher learning for audio-visual scene-aware dialog, 2019, Proc. Interspeech 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Chiori Hori",
                    "Anoop Cherian",
                    "Tim Marks",
                    "Takaaki Hori"
                ],
                "title": "Joint student-teacher learning for audio-visual scene-aware dialog",
                "pub_date": "2019",
                "pub_title": "Proc. Interspeech 2019",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_116",
            "content": "Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko, Explainable neural computation via stack neural module networks, 2018, Proceedings of the European conference on computer vision (ECCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Ronghang Hu",
                    "Jacob Andreas",
                    "Trevor Darrell",
                    "Kate Saenko"
                ],
                "title": "Explainable neural computation via stack neural module networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the European conference on computer vision (ECCV)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_117",
            "content": "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko, Learning to reason: End-to-end module networks for visual question answering, 2017, Proceedings of the IEEE International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Ronghang Hu",
                    "Jacob Andreas",
                    "Marcus Rohrbach",
                    "Trevor Darrell",
                    "Kate Saenko"
                ],
                "title": "Learning to reason: End-to-end module networks for visual question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE International Conference on Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_118",
            "content": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell, Natural language object retrieval, 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Ronghang Hu",
                    "Huazhe Xu",
                    "Marcus Rohrbach",
                    "Jiashi Feng",
                    "Kate Saenko",
                    "Trevor Darrell"
                ],
                "title": "Natural language object retrieval",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_119",
            "content": "Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, Chuang Gan, Locationaware graph convolutional networks for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Deng Huang",
                    "Peihao Chen",
                    "Runhao Zeng",
                    "Qing Du",
                    "Mingkui Tan",
                    "Chuang Gan"
                ],
                "title": "Locationaware graph convolutional networks for video question answering",
                "pub_date": "2020",
                "pub_title": "The AAAI Conference on Artificial Intelligence (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_120",
            "content": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim, Tgif-qa: Toward spatiotemporal reasoning in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Yunseok Jang",
                    "Yale Song",
                    "Youngjae Yu",
                    "Youngjin Kim",
                    "Gunhee Kim"
                ],
                "title": "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_121",
            "content": "Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, Yue Gao, Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Jianwen Jiang",
                    "Ziqiang Chen",
                    "Haojie Lin",
                    "Xibin Zhao",
                    "Yue Gao"
                ],
                "title": "Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering",
                "pub_date": "2020",
                "pub_title": "The AAAI Conference on Artificial Intelligence (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_122",
            "content": "Pin Jiang, Yahong Han, Reasoning with heterogeneous graph alignment for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Pin Jiang",
                    "Yahong Han"
                ],
                "title": "Reasoning with heterogeneous graph alignment for video question answering",
                "pub_date": "2020",
                "pub_title": "The AAAI Conference on Artificial Intelligence (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_123",
            "content": "Yichen Jiang, Mohit Bansal, Self-assembling modular networks for interpretable multi-hop reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Yichen Jiang",
                    "Mohit Bansal"
                ],
                "title": "Self-assembling modular networks for interpretable multi-hop reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_124",
            "content": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Justin Johnson",
                    "Bharath Hariharan",
                    "Laurens Van Der Maaten",
                    "Li Fei-Fei",
                    "Lawrence Zitnick",
                    "Ross Girshick"
                ],
                "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_125",
            "content": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Inferring and executing programs for visual reasoning, 2017, Proceedings of the IEEE International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Justin Johnson",
                    "Bharath Hariharan",
                    "Laurens Van Der Maaten",
                    "Judy Hoffman",
                    "Li Fei-Fei",
                    "Lawrence Zitnick",
                    "Ross Girshick"
                ],
                "title": "Inferring and executing programs for visual reasoning",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE International Conference on Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_126",
            "content": "Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang, Nojun Kwak, Self-supervised pretraining and contrastive representation learning for multiple-choice video qa, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Seonhoon Kim",
                    "Seohyeong Jeong",
                    "Eunbyul Kim",
                    "Inho Kang",
                    "Nojun Kwak"
                ],
                "title": "Self-supervised pretraining and contrastive representation learning for multiple-choice video qa",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_127",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Yoon Kim"
                ],
                "title": "Convolutional neural networks for sentence classification",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_128",
            "content": "P Diederick, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "P Diederick",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015",
                "pub_title": "International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_129",
            "content": "Satwik Kottur, M Jos\u00e9, Devi Moura,  Parikh, Visual coreference resolution in visual dialog using neural module networks, 2018, Proceedings of the European Conference on Computer Vision (ECCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Satwik Kottur",
                    "M Jos\u00e9",
                    "Devi Moura",
                    " Parikh"
                ],
                "title": "Visual coreference resolution in visual dialog using neural module networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_130",
            "content": "UNKNOWN, None, 2019, Leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_131",
            "content": "Hung Le, Doyen Hoi, N Sahoo,  Chen, End-to-end multimodal dialog systems with hierarchical multimodal attention on video features, 2019, DSTC7 at AAAI2019 workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Hung Le",
                    "Doyen Hoi",
                    "N Sahoo",
                    " Chen"
                ],
                "title": "End-to-end multimodal dialog systems with hierarchical multimodal attention on video features",
                "pub_date": "2019",
                "pub_title": "DSTC7 at AAAI2019 workshop",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_132",
            "content": "Hung Le, C Steven,  Hoi, Video-grounded dialogues with pretrained generation language models, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Hung Le",
                    "C Steven",
                    " Hoi"
                ],
                "title": "Video-grounded dialogues with pretrained generation language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_133",
            "content": "Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi, Multimodal transformer networks for end-toend video-grounded dialogue systems, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Hung Le",
                    "Doyen Sahoo",
                    "Nancy Chen",
                    "Steven Hoi"
                ],
                "title": "Multimodal transformer networks for end-toend video-grounded dialogue systems",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_134",
            "content": "Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi, BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Hung Le",
                    "Doyen Sahoo",
                    "Nancy Chen",
                    "Steven Hoi"
                ],
                "title": "BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_135",
            "content": "UNKNOWN, None, 2019, Learning to reason with relational video representation for question answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Learning to reason with relational video representation for question answering",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_136",
            "content": "UNKNOWN, None, 2020, Hierarchical conditional relation networks for video question answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Hierarchical conditional relation networks for video question answering",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_137",
            "content": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Soon Doo, Trung Kim, Kyomin Bui,  Jung, Dstc8-avsd: Multimodal semantic transformer network with retrieval style word generator, 2020, DSTC Workshop @ AAAI 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Hwanhee Lee",
                    "Seunghyun Yoon",
                    "Franck Dernoncourt",
                    "Soon Doo",
                    "Trung Kim",
                    "Kyomin Bui",
                    " Jung"
                ],
                "title": "Dstc8-avsd: Multimodal semantic transformer network with retrieval style word generator",
                "pub_date": "2020",
                "pub_title": "DSTC Workshop @ AAAI 2020",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_138",
            "content": "Chenyi Lei, Lei Wu, Dong Liu, Zhao Li, Guoxin Wang, Haihong Tang, Houqiang Li, Multiquestion learning for visual question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Chenyi Lei",
                    "Lei Wu",
                    "Dong Liu",
                    "Zhao Li",
                    "Guoxin Wang",
                    "Haihong Tang",
                    "Houqiang Li"
                ],
                "title": "Multiquestion learning for visual question answering",
                "pub_date": "2020",
                "pub_title": "The AAAI Conference on Artificial Intelligence (AAAI)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_139",
            "content": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara Berg, Mohit Bansal, Jingjing Liu, Less is more: Clipbert for video-and-language learning via sparse sampling, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Jie Lei",
                    "Linjie Li",
                    "Luowei Zhou",
                    "Zhe Gan",
                    "Tamara Berg",
                    "Mohit Bansal",
                    "Jingjing Liu"
                ],
                "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_140",
            "content": "Jie Lei, Licheng Yu, Mohit Bansal, Tamara Berg, TVQA: Localized, compositional video question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Jie Lei",
                    "Licheng Yu",
                    "Mohit Bansal",
                    "Tamara Berg"
                ],
                "title": "TVQA: Localized, compositional video question answering",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_141",
            "content": "Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu, HERO: Hierarchical encoder for Video+Language omnirepresentation pre-training, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Linjie Li",
                    "Yen-Chun Chen",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Licheng Yu",
                    "Jingjing Liu"
                ],
                "title": "HERO: Hierarchical encoder for Video+Language omnirepresentation pre-training",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_142",
            "content": "Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, Chuang Gan, Beyond rnns: Positional self-attention with co-attention for video question answering, 2019, The 33rd AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Xiangpeng Li",
                    "Jingkuan Song",
                    "Lianli Gao",
                    "Xianglong Liu",
                    "Wenbing Huang",
                    "Xiangnan He",
                    "Chuang Gan"
                ],
                "title": "Beyond rnns: Positional self-attention with co-attention for video question answering",
                "pub_date": "2019",
                "pub_title": "The 33rd AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_143",
            "content": "UNKNOWN, None, 2020, Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_144",
            "content": "UNKNOWN, None, 2004, Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": null,
                "title": null,
                "pub_date": "2004",
                "pub_title": "Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_145",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Jiayuan Mao",
                    "Chuang Gan",
                    "Pushmeet Kohli",
                    "Joshua Tenenbaum",
                    "Jiajun Wu"
                ],
                "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_146",
            "content": "Shikhar Dat Tien Nguyen, Hannes Sharma, Layla Schulz,  Asri, From film to video: Multiturn question answering with multi-modal context, 2018, AAAI 2019 Dialog System Technology Challenge (DSTC7) Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Shikhar Dat Tien Nguyen",
                    "Hannes Sharma",
                    "Layla Schulz",
                    " Asri"
                ],
                "title": "From film to video: Multiturn question answering with multi-modal context",
                "pub_date": "2018",
                "pub_title": "AAAI 2019 Dialog System Technology Challenge (DSTC7) Workshop",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_147",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "170-ARR_v1_148",
            "content": "Mengye Ren, Ryan Kiros, Richard Zemel, Exploring models and data for image question answering, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Mengye Ren",
                    "Ryan Kiros",
                    "Richard Zemel"
                ],
                "title": "Exploring models and data for image question answering",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_149",
            "content": "Kaiming Shaoqing Ren, Ross He, Jian Girshick,  Sun, Faster R-CNN: Towards real-time object detection with region proposal networks, 2015, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Kaiming Shaoqing Ren",
                    "Ross He",
                    "Jian Girshick",
                    " Sun"
                ],
                "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
                "pub_date": "2015",
                "pub_title": "Advances in Neural Information Processing Systems (NIPS)",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_150",
            "content": "Ramon Sanabria, Shruti Palaskar, Florian Metze, Cmu sinbad's submission for the dstc7 avsd challenge, 2019, DSTC7 at AAAI2019 workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Ramon Sanabria",
                    "Shruti Palaskar",
                    "Florian Metze"
                ],
                "title": "Cmu sinbad's submission for the dstc7 avsd challenge",
                "pub_date": "2019",
                "pub_title": "DSTC7 at AAAI2019 workshop",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_151",
            "content": "Idan Schwartz, Seunghak Yu, Tamir Hazan, Alexander Schwing, Factor graph attention, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Idan Schwartz",
                    "Seunghak Yu",
                    "Tamir Hazan",
                    "Alexander Schwing"
                ],
                "title": "Factor graph attention",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_152",
            "content": "Alessandro Iulian V Serban, Yoshua Sordoni, Aaron Bengio, Joelle Courville,  Pineau, Building end-to-end dialogue systems using generative hierarchical neural network models, 2016, Thirtieth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Alessandro Iulian V Serban",
                    "Yoshua Sordoni",
                    "Aaron Bengio",
                    "Joelle Courville",
                    " Pineau"
                ],
                "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
                "pub_date": "2016",
                "pub_title": "Thirtieth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_153",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Sofia Serrano",
                    "Noah Smith"
                ],
                "title": "Is attention interpretable?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_154",
            "content": "G\u00fcl Gunnar A Sigurdsson, Xiaolong Varol, Ali Wang, Ivan Farhadi, Abhinav Laptev,  Gupta, Hollywood in homes: Crowdsourcing data collection for activity understanding, 2016, European Conference on Computer Vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "G\u00fcl Gunnar A Sigurdsson",
                    "Xiaolong Varol",
                    "Ali Wang",
                    "Ivan Farhadi",
                    "Abhinav Laptev",
                    " Gupta"
                ],
                "title": "Hollywood in homes: Crowdsourcing data collection for activity understanding",
                "pub_date": "2016",
                "pub_title": "European Conference on Computer Vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "170-ARR_v1_155",
            "content": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: a simple way to prevent neural networks from overfitting, 2014, The Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Nitish Srivastava",
                    "Geoffrey Hinton",
                    "Alex Krizhevsky",
                    "Ilya Sutskever",
                    "Ruslan Salakhutdinov"
                ],
                "title": "Dropout: a simple way to prevent neural networks from overfitting",
                "pub_date": "2014",
                "pub_title": "The Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_156",
            "content": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Rethinking the inception architecture for computer vision, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Christian Szegedy",
                    "Vincent Vanhoucke",
                    "Sergey Ioffe",
                    "Jon Shlens",
                    "Zbigniew Wojna"
                ],
                "title": "Rethinking the inception architecture for computer vision",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_157",
            "content": "Zineng Tang, Jie Lei, Mohit Bansal, DeCEMBERT: Learning from noisy instructional videos via dense captions and entropy minimization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Zineng Tang",
                    "Jie Lei",
                    "Mohit Bansal"
                ],
                "title": "DeCEMBERT: Learning from noisy instructional videos via dense captions and entropy minimization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_158",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "170-ARR_v1_159",
            "content": "Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Cider: Consensus-based image description evaluation, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": [
                    "Ramakrishna Vedantam",
                    "Lawrence Zitnick",
                    "Devi Parikh"
                ],
                "title": "Cider: Consensus-based image description evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_160",
            "content": "Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura. 2020. Bert representations for video question answering, , Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": [
                    "Zekun Yang",
                    "Noa Garcia",
                    "Chenhui Chu",
                    "Mayu Otani"
                ],
                "title": "Yuta Nakashima, and Haruo Takemura. 2020. Bert representations for video question answering",
                "pub_date": null,
                "pub_title": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_161",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic vqa: Disentangling reasoning from vision and language understanding, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Kexin Yi",
                    "Jiajun Wu",
                    "Chuang Gan",
                    "Antonio Torralba",
                    "Pushmeet Kohli",
                    "Josh Tenenbaum"
                ],
                "title": "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding",
                "pub_date": "2018",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_162",
            "content": "Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim, End-to-end concept word detection for video captioning, retrieval, and question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": [
                    "Youngjae Yu",
                    "Hyungjin Ko",
                    "Jongwook Choi",
                    "Gunhee Kim"
                ],
                "title": "End-to-end concept word detection for video captioning, retrieval, and question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "170-ARR_v1_163",
            "content": "UNKNOWN, None, 2021, Merlot: Multimodal neural script knowledge models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b73",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Merlot: Multimodal neural script knowledge models",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "170-ARR_v1_0@0",
            "content": "VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_0",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@0",
            "content": "Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@1",
            "content": "However, very limited work on NMN has been studied in the video-grounded dialogue tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 136,
            "end": 223,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@2",
            "content": "These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 225,
            "end": 368,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@3",
            "content": "Motivated by recent NMN approaches on image-grounded tasks, we introduce Videogrounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 370,
            "end": 594,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@4",
            "content": "VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 596,
            "end": 762,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@5",
            "content": "The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 764,
            "end": 897,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_2@6",
            "content": "Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_2",
            "start": 899,
            "end": 1047,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_4@0",
            "content": "Vision-language tasks have been studied to build intelligent systems that can perceive information from multiple modalities, such as images, videos, and text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_4",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_4@1",
            "content": "Extended from image-grounded tasks, e.g. (Antol et al., 2015), recently Jang et al. (2017); Lei et al. (2018) propose to use video as the grounding features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_4",
            "start": 159,
            "end": 315,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_4@2",
            "content": "This modification poses a significant challenge to previous image-based models with the additional temporal variance through video frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_4",
            "start": 317,
            "end": 454,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_5@0",
            "content": "Recently further develop videogrounded language research into the dialogue domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_5",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_5@1",
            "content": "In the proposed task, video-grounded dialogues, the dialogue agent is required to answer questions about a video over multiple dialogue turns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_5",
            "start": 83,
            "end": 224,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_5@2",
            "content": "Using Figure 1 as an example, to answer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_5",
            "start": 226,
            "end": 264,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_6@0",
            "content": "Figure 1: A sample video-grounded dialogue with a demonstration of a reasoning process questions correctly, a dialogue agent has to resolve references in dialogue context, e.g. \"he\" and \"it\", and identify the original entity, e.g. \"a boy\" and \"a backpack\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_6",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_6@1",
            "content": "Besides, the agent also needs to identify the actions of these entities, e.g. \"carrying a backpack\" to retrieve information from the video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_6",
            "start": 257,
            "end": 395,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_7@0",
            "content": "Current state-of-the-art approaches to videogrounded dialogue tasks, e.g. (Le et al., 2019b;Fan et al., 2019) have achieved remarkable performance through the use of deep neural networks to retrieve grounding video signals based on language inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_7",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_7@1",
            "content": "However, these approaches often assume the reasoning structure, including resolving references of entities and detecting the corresponding actions to retrieve visual cues, is implicitly learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_7",
            "start": 249,
            "end": 442,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_7@2",
            "content": "An explicit reasoning structure becomes more beneficial as the tasks complicate in two scenarios: video with complex spatial and temporal dynamics, and language inputs with sophisticated semantic dependencies, e.g. questions positioned in a dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_7",
            "start": 444,
            "end": 701,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_7@3",
            "content": "These scenarios often challenge researchers to interpret model hidden layers, identify errors, and assess model reasoning capability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_7",
            "start": 703,
            "end": 835,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_8@0",
            "content": "Similar challenges have been observed in imagegrounded tasks in which deep neural networks exhibit shallow understanding capability as they exploit superficial visual cues (Agrawal et al., 2016;Goyal et al., 2017;Feng et al., 2018;Serrano and Smith, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_8",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_8@1",
            "content": "Andreas et al. (2016b) propose neural module networks (NMNs) by decomposing a question into sub-sequences called program and assembling a network of neural operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_8",
            "start": 257,
            "end": 423,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_8@2",
            "content": "Motivated by this line of research, we propose a new approach, VGNMN, to video-grounded language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_8",
            "start": 425,
            "end": 527,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_8@3",
            "content": "Our approach benefits from integrating neural networks with a compositional reasoning structure to exploit low-level information signals in video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_8",
            "start": 529,
            "end": 674,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_8@4",
            "content": "An example of the reasoning structure can be seen on the right side of Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_8",
            "start": 676,
            "end": 755,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_9@0",
            "content": "Video-grounded Neural Module Network (VGNMN) tackles video understanding through action and entity-paramterized NMNs to retrieve video features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_9",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_9@1",
            "content": "We first decompose question into a set of entities and extract video features related to these entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_9",
            "start": 145,
            "end": 248,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_9@2",
            "content": "VGNMN then extracts the temporal steps by focusing on relevant actions that are associated with these entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_9",
            "start": 250,
            "end": 360,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_9@3",
            "content": "VGNMN is analogous to how human processes information by gradually retrieving signals from input modalities using a set of discrete subjects and their actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_9",
            "start": 362,
            "end": 520,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_10@0",
            "content": "To tackle dialogue understanding, VGNMN is trained to resolve any co-reference in language inputs, e.g. questions in a dialogue context, to identify the unique entities in each dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_10",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_10@1",
            "content": "Previous approaches to video-grounded dialogues often obtain question global representations in relation to dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_10",
            "start": 187,
            "end": 311,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_10@2",
            "content": "These approaches might be suitable to represent general semantics in open-domain dialogues (Serban et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_10",
            "start": 313,
            "end": 425,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_10@3",
            "content": "However, they are not ideal to detect fine-grained information in a video-grounded dialogue which frequently entails dependencies between questions and past dialogue turns in the form of entity references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_10",
            "start": 427,
            "end": 631,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_11@0",
            "content": "In summary, our contributions include:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_11",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_12@0",
            "content": "\u2022 VGNMN, a neural module network-based approach for video-grounded dialogues. \u2022 The approach includes a modularized system that creates a reasoning pipeline parameterized by entity and action-based representations from both dialogue and video contexts. \u2022 Our experiments are conducted on the challenging benchmark for video-grounded dialogues, Audio-visual Scene-Aware Dialogues (AVSD) as well as TGIF-QA (Jang et al., 2017) for video QA task. \u2022 Our results indicate strong performance of VGNMN as well as improved model interpretability and robustness to difficult scenarios of dialogues, videos, and question structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_12",
            "start": 0,
            "end": 621,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_13@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_13",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_14@0",
            "content": "Video-Language Understanding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_14",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_15@0",
            "content": "The research of video-language understanding aims to develop a model's joint understanding capability of language, video, and their interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_15",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_15@1",
            "content": "Recently, we have witnessed emerging techniques in video-language systems that exploit deep transformer-based architectures such as BERT (Devlin et al., 2019) for pretraining multimodal representations (Li et al., 2020a;Yang et al., 2020;Kim et al., 2021;Tang et al., 2021;Zellers et al., 2021) in very large-scale videolanguage datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_15",
            "start": 146,
            "end": 483,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_15@2",
            "content": "While these systems can achieve impressive performance, they are not straightforward to apply in domains with limited data such as video-grounded dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_15",
            "start": 485,
            "end": 640,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_15@3",
            "content": "Moreover, as we shown in our qualitative examples, our approach facilitates better interpretability through the output of decoded functional programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_15",
            "start": 642,
            "end": 791,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_16@0",
            "content": "Video-grounded Dialogues",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_16",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@0",
            "content": "Extended from video QA, video-grounded dialogue is an emerging task that combines dialogue response generation and video-language understanding research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@1",
            "content": "This task entails a novel requirement for models to learn dialogue semantics and decode entity co-references in questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 154,
            "end": 275,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@2",
            "content": "Nguyen et al. (2018); ; ; Sanabria et al. (2019); Le et al. (2019a,b) extend traditional QA models by adding dialogue history neural encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 277,
            "end": 418,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@3",
            "content": "Kumar et al. (2019) enhances dialogue features with topic-level representations to express the general topic in each dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 420,
            "end": 545,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@4",
            "content": "Schwartz et al. (2019) treats each dialogue turn as an independent sequence and allows interaction between questions and each dialogue turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 547,
            "end": 686,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@5",
            "content": "Le et al. (2019b) encodes dialogue history as a sequence with embedding and positional representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 688,
            "end": 790,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@6",
            "content": "Different from prior work, we dissect the question sequence and explicitly detect and decode any entities and their references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 792,
            "end": 918,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_17@7",
            "content": "Our approach also enables insights on how models extract deductive bias from dialogues to extract video information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_17",
            "start": 920,
            "end": 1035,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_18@0",
            "content": "Neural Module Network",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_18",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@0",
            "content": "Neural Module Network (NMN) (Andreas et al., 2016b,a) is introduced to address visual QA by decomposing questions into linguistic sub-structures, known as programs, to instantiate a network of neural modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@1",
            "content": "NMN models have achieved success in synthetic image domains where a multi-step reasoning process is required (Johnson et al., 2017b;Hu et al., 2018;Han et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 209,
            "end": 374,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@2",
            "content": "Yi et al. (2018); Han et al. (2019); improve NMN models by decoupling visual-language understanding and visual concept learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 376,
            "end": 503,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@3",
            "content": "Our work is related to the recent work (Kottur et al., 2018;Jiang and Bansal, 2019;Gupta et al., 2020) that extended NMNs to image reasoning in dialogues and reading comprehension reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 505,
            "end": 694,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@4",
            "content": "Our approach follows the previous approaches that learn to generate program structure and require no parser at evaluation time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 696,
            "end": 822,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@5",
            "content": "Compared to prior work, we use NMN to learn dependencies between the composition in language inputs and the spatio-temporal dynamics in videos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 824,
            "end": 966,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_19@6",
            "content": "Specifically, we propose to construct a reasoning structure from text, from which detected entities are used to extract visual information in the spatial space and detected actions are used to find visual information in the temporal space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_19",
            "start": 968,
            "end": 1206,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_20@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_20",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_21@0",
            "content": "In this section, we present the design of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_21",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_21@1",
            "content": "An overview of the model can be seen in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_21",
            "start": 53,
            "end": 101,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_22@0",
            "content": "Task Definition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_22",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_23@0",
            "content": "The input to the model consists of a dialogue D which is grounded on a video V. The input components include the question of current dialogue turn Q, dialogue history H, and the features of the input video, including visual and audio input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_23",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_23@1",
            "content": "The output is a dialogue response, denoted as R. Each text input component is a sequence of words w 1 , ..., w m \u2208 V in , the input vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_23",
            "start": 241,
            "end": 383,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_23@2",
            "content": "Similarly, the output response R is a sequence of tokens w 1 , ..., w n \u2208 V out , the output vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_23",
            "start": 385,
            "end": 488,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_23@3",
            "content": "The objective of the task is the generation objective that output answers of the current dialogue turn t:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_23",
            "start": 490,
            "end": 594,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_24@0",
            "content": "Rt = arg max Rt P (R t |V, H t , Q t ; \u03b8) = arg max Rt L R n=1 P m (w n |R t,1:n\u22121 , V, H t , Q t ; \u03b8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_24",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_25@0",
            "content": "In a Video-QA task, the dialogue history H is simply absent and the output response is typically collapsed to a single-token response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_25",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_26@0",
            "content": "Encoders",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_26",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@0",
            "content": "Text Encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@1",
            "content": "A text encoder is shared to encode text inputs, including dialogue history, questions, and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 14,
            "end": 113,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@2",
            "content": "The text encoder converts each text sequence X = w 1 , ..., w m into a sequence of embeddings X \u2208 R m\u00d7d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 115,
            "end": 219,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@3",
            "content": "We use a trainable embedding matrix to map token indices to vector representations of d dimensions through a mapping function \u03c6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 221,
            "end": 348,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@4",
            "content": "These vectors are then integrated with ordering information of tokens through a positional encoding function with layer normalization (Ba et al., 2016;Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 350,
            "end": 522,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@5",
            "content": "The embedding and positional representations are combined through element-wise summation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 524,
            "end": 612,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_27@6",
            "content": "The encoded dialogue history and question of the current turn are defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_27",
            "start": 614,
            "end": 689,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_28@0",
            "content": "H = Norm(\u03c6(H) + PE(H)) \u2208 R L H \u00d7d and Q = Norm(\u03c6(Q) + PE(Q)) \u2208 R L Q \u00d7d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_28",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@0",
            "content": "Video Encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@1",
            "content": "To encode video, we use pretrained models to extract visual and audio features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 15,
            "end": 93,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@2",
            "content": "We denote F as the sampled video frames or video clips.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 95,
            "end": 149,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@3",
            "content": "For object-level visual features, we denote O as the maximum number of objects considered in each frame.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 151,
            "end": 254,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@4",
            "content": "The resulting output from a pretrained object detection model is Z obj \u2208 R F \u00d7O\u00d7d vis .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 256,
            "end": 342,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@5",
            "content": "We concatenate each object representation with the corresponding coordinates projected to d vis dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 344,
            "end": 450,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@6",
            "content": "We also make use of a CNN-based pretrained model to obtain features of temporal dimension Z cnn \u2208 R F \u00d7d vis .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 452,
            "end": 561,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@7",
            "content": "The audio feature is 1: Description of the modules and their functionalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 563,
            "end": 639,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_29@8",
            "content": "We denote P as the parameter to instantiate each module, H as the dialogue history, Q as the question of the current dialogue turn, and V as video input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_29",
            "start": 641,
            "end": 793,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_30@0",
            "content": "obtained through a pretrained audio model, Z aud \u2208 R F \u00d7d aud .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_30",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_30@1",
            "content": "We passed all video features through a linear transformation layer with ReLU activation to the same embedding dimension d.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_30",
            "start": 64,
            "end": 185,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_31@0",
            "content": "Neural Modules",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_31",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_32@0",
            "content": "We introduce neural modules that are used to assemble an executable program constructed by the generated sequence from question parsers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_32",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_32@1",
            "content": "We provide an overview of neural modules in Table 1 and demonstrate dialogue understanding and video understanding modules in Figure 3 and 4 respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_32",
            "start": 137,
            "end": 290,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_32@2",
            "content": "Each module parameter, e.g. \"a backpack\", is extracted from the parsed program (See Section 3.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_32",
            "start": 292,
            "end": 388,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@0",
            "content": "For each parameter, we denote P \u2208 R d as the average pooling of component token embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@1",
            "content": "find(P,H)\u2192H ent .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 92,
            "end": 108,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@2",
            "content": "This module handles entity tracing by obtaining a distribution over tokens in the dialogue history.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 110,
            "end": 208,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@3",
            "content": "We use an entity-todialogue-history attention mechanism applied from an entity P i to all tokens in the dialogue history. Any neural network that learn to generate attention between two tensors is applicable .e.g. (Bahdanau et al., 2015;Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 210,
            "end": 468,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@4",
            "content": "The attention matrix normalized by softmax, A find,i \u2208 R L H , is used to compute the weighted sum of dialogue history token representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 470,
            "end": 610,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_33@5",
            "content": "The output is combined with entity embedding P i to obtain contextual entity representation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_33",
            "start": 612,
            "end": 702,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_34@0",
            "content": "H ent,i \u2208 R d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_34",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_35@0",
            "content": "summarize(H ent ,Q)\u2192Q ctx .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_35",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_35@1",
            "content": "For each contextual entity representation H ent,i , i = 1, ..., N ent , it is projected to L Q dimensions and is combined with question token embeddings through elementwise summation to obtain entity-aware question representation Q ent,i \u2208 R L Q \u00d7d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_35",
            "start": 28,
            "end": 277,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_35@2",
            "content": "It is fed to a onedimensional CNN with max-pooling layer (Kim, 2014) to obtain a contextual entity-aware question representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_35",
            "start": 279,
            "end": 407,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_35@3",
            "content": "We denote the final output as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_35",
            "start": 409,
            "end": 437,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_36@0",
            "content": "Q ctx \u2208 R Nent\u00d7d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_36",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_37@0",
            "content": "While previous models usually focus on global or token-level dependencies Le et al., 2019b) to encode question features, our modules compress fine-grained question representations at the entity level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_37",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_37@1",
            "content": "Specifically, find and summarize modules can generate entitydependent local and global representations of question semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_37",
            "start": 201,
            "end": 325,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_37@2",
            "content": "We show that our modularized approach can achieve better performance and transparency than traditional approaches to encode dialogue context (Serban et al., 2016;Vaswani et al., 2017) (Section 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_37",
            "start": 327,
            "end": 522,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@0",
            "content": "where(P,V)\u2192V ent .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@1",
            "content": "Similar to the find module, this module handles entity-based attention to the video input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 19,
            "end": 108,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@2",
            "content": "However, the entity representation P , in this case, is parameterized by the original entity in dialogue rather than in question (See Section 3.4 for more description).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 110,
            "end": 277,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@3",
            "content": "Each entity P i is stacked to match the number of sampled video frames/clips F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 279,
            "end": 358,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@4",
            "content": "An attention network is used to obtain entity-to-object attention matrix A where,i \u2208 R F \u00d7O .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 360,
            "end": 452,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_38@5",
            "content": "The attended feature are compressed through weighted sum pooling along the spatial dimension, resulting in V ent,i \u2208 R F \u00d7d , i = 1, ..., N ent .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_38",
            "start": 454,
            "end": 598,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@0",
            "content": "when(P,V ent )\u2192V ent+act .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@1",
            "content": "This module follows a similar architecture as the where module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 27,
            "end": 89,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@2",
            "content": "However, the action parameter P i is stacked to match N ent dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 91,
            "end": 161,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@3",
            "content": "The attention matrix A when,i \u2208 R F is then used to compute the visual entity-action representations through weighted sum along the temporal dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 163,
            "end": 313,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@4",
            "content": "We denote the output for all actions P i as and [; ] is the concatenation operation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 315,
            "end": 398,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@5",
            "content": "Note that the parameter P here is extracted from questions, often as the type of questions e.g. \"what\" and \"how\". This eliminates the need to have different modules for different question types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 400,
            "end": 593,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_39@6",
            "content": "However, we noted the current design may be challenged in rare cases in which an utterance contain numerous questions (refer to Figure 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_39",
            "start": 595,
            "end": 732,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_40@0",
            "content": "V ent+act \u2208 R Nent\u00d7Nact\u00d7d describe(P,V ent+act )\u2192V ctx . This module is a linear transformation to compute V ctx = W desc T [V ent+act ; P stack ] \u2208 R Nent\u00d7Nact\u00d7d where W desc \u2208 R 2d\u00d7d , P stack is the stacked represen- tations of parameter embedding P to N ent \u00d7 N act dimensions,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_40",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_41@0",
            "content": "The exist module is used when the questions are \"yes/no\" questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_41",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_41@1",
            "content": "This module is a special case of describe module where the parameter P is simply the average pooled question embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_41",
            "start": 68,
            "end": 187,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_41@2",
            "content": "The above where module is applied to object-level features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_41",
            "start": 189,
            "end": 247,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_41@3",
            "content": "For temporal-based features such as CNN-based and audio features, the same neural operation is applied along the temporal dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_41",
            "start": 249,
            "end": 380,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_41@4",
            "content": "Each resulting entity-aware output is then incorporated to frame-level features through element-wise summation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_41",
            "start": 382,
            "end": 492,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_42@0",
            "content": "An advantage of our architecture is that it separates dialogue and video understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_42",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_42@1",
            "content": "We adopt a transparent approach to solve linguistic entity references during the dialogue understanding phase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_42",
            "start": 88,
            "end": 197,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_42@2",
            "content": "The resolved entities are fed to the video understanding phase to learn entity-action dynamics in the video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_42",
            "start": 199,
            "end": 306,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_42@3",
            "content": "We show that our approach is robust when dialogue evolves to many turns and video extends over time (Please refer to Section 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_42",
            "start": 308,
            "end": 435,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_43@0",
            "content": "Question Parsers",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_43",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_44@0",
            "content": "To learn compositional programs, we follow (Johnson et al., 2017a;Hu et al., 2017) and consider program generation as a sequence-tosequence task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_44",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_44@1",
            "content": "We adopt a simple template \" param 1 module 1 param 2 module 2 ...\" as the target sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_44",
            "start": 146,
            "end": 236,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_44@2",
            "content": "The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_44",
            "start": 238,
            "end": 358,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_45@0",
            "content": "The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_45",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_45@1",
            "content": "Each parser is a vanilla Transformer decoder, including multi-head attention layers on questions and past dialogue turns (Please refer to Appendix A.1 for more technical details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_45",
            "start": 134,
            "end": 312,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_46@0",
            "content": "Response Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_46",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_47@0",
            "content": "System response is decoded by incorporating the dialogue context and video context outputs from the corresponding reasoning programs to target token representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_47",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_47@1",
            "content": "We follows a vanilla Transformer decoder architecture (Le et al., 2019b), which consists of 3 attention layers: self-attention to attend on existing tokens, attention to Q ctx from dialogue understanding program execution, and attention to V ctx from video understanding program execution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_47",
            "start": 166,
            "end": 454,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_48@0",
            "content": "A (1) res = Attention(R| j\u22121 0 , R| j\u22121 0 , R| j\u22121 0 ) \u2208 R j\u00d7d A (2) res = Attention(A (1) res , Q ctx , Q ctx ) \u2208 R j\u00d7d A (3) res = Attention(A (2) res , V ctx , V ctx ) \u2208 R j\u00d7d",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_48",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_49@0",
            "content": "Multimodal Fusion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_49",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_49@1",
            "content": "For video features come from multiple modalities, visual and audio, the contextual features, denoted V ctx , is obtained through a weighted sum of component modalities, e.g. contextual visual features V vis ctx and contextual audio features V aud ctx .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_49",
            "start": 19,
            "end": 270,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_49@2",
            "content": "The scores S fusion to compute the weighted sum is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_49",
            "start": 272,
            "end": 333,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_50@0",
            "content": "S fusion = Softmax(W T fusion [Q stack ; V vis ctx ; V aud ctx ])",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_50",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@0",
            "content": "where Q stack is the mean pooling output of question embeddings Q which is then stacked to N ent + N act dimensions, and W fusion \u2208 R 3d\u00d72 are trainable model parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@1",
            "content": "The resulting S fusion has a dimension of \u2208 R (Nent+Nact)\u00d72 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 171,
            "end": 231,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@2",
            "content": "Response Generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 233,
            "end": 252,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@3",
            "content": "To generate response sequences, a special token \"_sos\" is concatenated as the first token w 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 254,
            "end": 348,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@4",
            "content": "The decoded token w 1 is then appended to w 0 as input to decode w 2 and so on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 350,
            "end": 428,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@5",
            "content": "Similarly to input source sequences, at decoding time step j, the input target sequence is encoded to obtain representations of system response R| j\u22121 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 430,
            "end": 583,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@6",
            "content": "We combine vocabulary of input and output sequences and share the embedding matrix E \u2208 R |V|\u00d7d where V = V in \u2229 V out .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 585,
            "end": 703,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@7",
            "content": "During training time, we directly use the ground-truth responses as input to the decoder and optimize VGNMN with a cross-entropy loss to decode the next ground-truth tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 705,
            "end": 877,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@8",
            "content": "During test time, responses are generated auto-regressively through beam search with beam size 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 879,
            "end": 975,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_51@9",
            "content": "Note that we apply the same procedure to generate reasoning programs from question parsers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_51",
            "start": 977,
            "end": 1067,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_52@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_52",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_53@0",
            "content": "Experimental Setup and Training We use the AVSD benchmark from the Dialogue System Technology Challenge 7 (DSTC7) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_53",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_54@0",
            "content": "The benchmark consists of dialogues grounded on the Charades videos (Sigurdsson et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_54",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_54@1",
            "content": "Each dialogue contains up to 10 dialogue turns, each turn consists of a question and expected response about a given video.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_54",
            "start": 95,
            "end": 217,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_54@2",
            "content": "For visual features, we use the 3D CNN-based features from a pretrained I3D model (Carreira and Zisserman, 2017) and object-level features from a pretrained FasterRNN model (Ren et al., 2015b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_54",
            "start": 219,
            "end": 411,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_54@3",
            "content": "The audio features are obtained from a pretrained VGGish model (Hershey et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_54",
            "start": 413,
            "end": 498,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_55@0",
            "content": "In the experiments with AVSD, we consider two settings: one with video summary and one without video summary as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_55",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_55@1",
            "content": "In the setting with video summary, the summary is concatenated to the dialogue history before the first dialogue turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_55",
            "start": 119,
            "end": 236,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_55@2",
            "content": "We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_55",
            "start": 238,
            "end": 357,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_55@3",
            "content": "We also adapt VGNMN to the video QA benchmark TGIF-QA (Jang et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_55",
            "start": 359,
            "end": 432,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_55@4",
            "content": "For more details of data and training, please refer to Appendix B and C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_55",
            "start": 434,
            "end": 505,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_56@0",
            "content": "AVSD Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_56",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_56@1",
            "content": "We evaluate model performance by the objective metrics, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015), between each generated response and 6 reference gold responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_56",
            "start": 14,
            "end": 263,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_56@2",
            "content": "As seen in Table 2, our models outperform most of existing approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_56",
            "start": 265,
            "end": 334,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_56@3",
            "content": "We observed that our approach did not outperform the GPT-based baselines (Li et al., 2020b;Le and Hoi, 2020) in the setting that allows video summary/caption input However, the performance of our model in the setting without video summary/caption input is on par with the GPT-based baseline (Li et al., 2020b), even though our model did not rely on deep pretrained represen- .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_56",
            "start": 336,
            "end": 711,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_57@0",
            "content": "Robustness to video length: In Table 3a, we noted that the performance gap between VGNMN and (1) is quite distinct, with 7/10 cases of video ranges in which VGNMN outperforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_57",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_57@1",
            "content": "However, in lower ranges (i.e. 1-23 seconds) and higher ranges (37-75 seconds), VGNMN performs not as well as model (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_57",
            "start": 176,
            "end": 295,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_57@2",
            "content": "We observed that related factors might affect the discrepancy, such as the complexity of the questions for these short and long-range videos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_57",
            "start": 297,
            "end": 437,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_57@3",
            "content": "Potentially, our question parser for the video understanding program needs to be improved (e.g. for tree-based programs) to retrieve information in these ranges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_57",
            "start": 439,
            "end": 599,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_58@0",
            "content": "Robustness to dialogue turn: In Table 3b, we observed that model (1) performs better than model (2) overall, especially in higher turn positions, i.e. from the 4 th turn to 8 th turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_58",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_58@1",
            "content": "Interestingly, we noted some mixed results in very low turn position, i.e. the 2 nd and 3 rd turn, and very high turn position, i.e. the 10 th turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_58",
            "start": 184,
            "end": 331,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_58@2",
            "content": "Potentially, with a large dialogue turn position, the neural-based approach such as hierarchical RNN can better capture the global dependencies within dialogue context than the entity-based compositional NMN method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_58",
            "start": 333,
            "end": 547,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@0",
            "content": "Robustness to question structure: Finally, we compared performance of VGNMN with the no-NMN variant (1) in different cases of question structures: single-question vs. multiple-part structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@1",
            "content": "In single-question structures, we examined by the question types (e.g. yes/no, wh-questions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 192,
            "end": 284,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@2",
            "content": "In multi-part structures, we further classified whether there are sentences preceding the question (e.g. \"1Sent+Que\") or there are smaller (sub-)questions (e.g. \"2SubQue\") within the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 286,
            "end": 477,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@3",
            "content": "In Table 3c, we observed that VGNMN has clearer performance gains in multi-part structures than singlequestion structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 479,
            "end": 600,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@4",
            "content": "In multi-part structures, we observed higher gaps between VGNMN and model (1) in highly complex cases e.g. \"2Sent+Que\" vs. \"1Sent+Que\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 602,
            "end": 736,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@5",
            "content": "These observations indicate the robustness of VGNMN and the underlying compositionality principle to deal with complex question structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 738,
            "end": 876,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_59@6",
            "content": "We also noted that VGNMN is still susceptible to extremely long questions (\">2Sent+Que\") and future work is needed to address these scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_59",
            "start": 878,
            "end": 1019,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@0",
            "content": "Interpretability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@1",
            "content": "In Figure 5, we show both success and failure cases of generated responses and corresponding generated functional programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 18,
            "end": 140,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@2",
            "content": "In each example, we marked predicted outputs as incorrect if they do not match the ground-truth completely (even though the outputs might be partially correct).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 142,
            "end": 301,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@3",
            "content": "From Figure 5, we observe that in cases where generated dialogue programs and video programs match or are close to the gold labels, the model can generate generally correct responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 303,
            "end": 485,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@4",
            "content": "For cases where some module parameters do not exactly match but are closed to the gold labels, the model can still generate responses with the correct visual information (e.g. the 4 th turn in example B).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 487,
            "end": 690,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@5",
            "content": "In cases of wrong predicted responses, we can further look at how the model understands the questions based on predicted programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 692,
            "end": 821,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@6",
            "content": "In the 3 rd turn of example A, the output response is missing a minor detail as compared to the label response because the video program fails to capture \"rooftop\" as a where parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 823,
            "end": 1007,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@7",
            "content": "These subtle yet important details can determine whether output responses can fully address user queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1009,
            "end": 1113,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@8",
            "content": "In the 3 rd turn of example B, the model wrongly identifies \"what room\" as a where parameter and subsequently generates a wrong response that it is \"a living room\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1115,
            "end": 1278,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@9",
            "content": "TGIF-QA Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1280,
            "end": 1295,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@10",
            "content": "We report the result using the L2 loss in Count task and accuracy in other tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1297,
            "end": 1377,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@11",
            "content": "From Table 4, VGNMN outperforms the majority of the baseline models in all tasks by a large margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1379,
            "end": 1477,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@12",
            "content": "Compared to AVSD experiments, the TGIF-QA experiments emphasize the video understanding ability of the models, removing the requirement for dialogue understanding and natural language generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1479,
            "end": 1673,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@13",
            "content": "Since TGIF-QA questions follow a very specific question type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1675,
            "end": 1856,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@14",
            "content": "Using exact-match accuracy of parsed programs vs. label programs as a metric, our question parser can achieve a performance 81% to 94% accuracy in TGIF-QA vs. 41-45% in AVSD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 1858,
            "end": 2031,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_60@15",
            "content": "The higher accuracy in decoding a reasoning structure translates to better adaptation between training and test time, resulting in higher performance gains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_60",
            "start": 2033,
            "end": 2188,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@0",
            "content": "Cascading Errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@1",
            "content": "Compared to prior approaches, we noted that VGNMN is a modularized system which may result in cascading errors to downstream modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 18,
            "end": 150,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@2",
            "content": "One major error is the error of generated programs which is used as parameters in neural modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 152,
            "end": 248,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@3",
            "content": "To gauge this error, we compare the performance of VGNMN between 2 cases: with generated programs and with groundtruth programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 250,
            "end": 377,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@4",
            "content": "From Table 5, we noticed some performance gaps between these cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 379,
            "end": 445,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@5",
            "content": "These observations imply that: (1) program generations and response generations are positively correlated and more accurate programs can lead to better responses; and (2) current question parsers are not perfect, resulting in wrong parameters to instantiate neural modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 447,
            "end": 719,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@6",
            "content": "Future work may focus on learning better question parsers or directly deploying a better off-the-shelf parser tool.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 721,
            "end": 835,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_61@7",
            "content": "Table 5: Comparison of VGNMN on AVSD (top) and TGIF-QA (bottom) when using generated (\"Gen.\") vs. ground-truth (\"GT\") programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_61",
            "start": 837,
            "end": 963,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_62@0",
            "content": "For additional experiment results, qualitative samples, and analysis between model variants, refer to Appendix D and E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_62",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_63@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_63",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_64@0",
            "content": "In this work, we introduce Video-grounded Neural Module Network (VGNMN).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_64",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_64@1",
            "content": "VGNMN consists of dialogue and video understanding neural modules, each of which performs entity and action-level operations on language and video components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_64",
            "start": 73,
            "end": 230,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_64@2",
            "content": "Our comprehensive experiments on AVSD and TGIF-QA benchmarks show that our models can achieve competitive performance while promoting a compositional and interpretable learning approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_64",
            "start": 232,
            "end": 417,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_65@0",
            "content": "Broader Impacts",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_65",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@0",
            "content": "During the duration of this work, there have been no ethical concerns regarding the model implementation, training, and testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@1",
            "content": "The data used in this work has been carefully reviewed and accordingly to the description from the original authors, we did not find any concerns on any significant biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 129,
            "end": 300,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@2",
            "content": "For any potential application or extension of this work, we would like to highlight some specific concerns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 302,
            "end": 408,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@3",
            "content": "First, as the work is developed to build an intelligent dialogue agents, models should not be used with the intention to create fake human profiles for any harmful purposes (e.g. fishing or spreading fake news).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 410,
            "end": 620,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@4",
            "content": "For wider use of dialogue systems, the application of work might result in certain impacts to some stakeholders whose jobs may be affected by this application (e.g. customer service call agents).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 622,
            "end": 816,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_66@5",
            "content": "We hope any application should be carefully considered against these potential risks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_66",
            "start": 818,
            "end": 902,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_67@0",
            "content": "To learn compositional programs, we follow (Johnson et al., 2017a;Hu et al., 2017) and consider program generation as a sequence-tosequence task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_67",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_67@1",
            "content": "We adopt a simple template \" param 1 module 1 param 2 module 2 ...\" as the target sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_67",
            "start": 146,
            "end": 236,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_67@2",
            "content": "The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_67",
            "start": 238,
            "end": 358,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_68@0",
            "content": "The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_68",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_68@1",
            "content": "Each parser is an attention-based Transformer decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_68",
            "start": 134,
            "end": 187,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_68@2",
            "content": "The Transformer attention is a multi-head attention on query q, key k, and value v tensors, denoted as Attention(q, k, v).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_68",
            "start": 189,
            "end": 310,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_68@3",
            "content": "For each token in the q sequence , the distribution over tokens in the k sequence is used to obtain the weighted sum of the corresponding representations in the v sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_68",
            "start": 312,
            "end": 483,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_69@0",
            "content": "Attention(q, k, v) = softmax( qk T \u221a d k )v \u2208 R Lq\u00d7dq",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_69",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_70@0",
            "content": "Each attention is followed by a feed-forward network applied to each position identically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_70",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_70@1",
            "content": "We exploit the multi-head and feed-forward architecture, which show good performance in NLP tasks such as NMT and QA (Vaswani et al., 2017;Dehghani et al., 2019), to efficiently incorporate contextual cues from dialogue components to parse question into reasoning programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_70",
            "start": 91,
            "end": 363,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_70@2",
            "content": "At decoding step 0, we simply use a special token _sos as the input to the parser.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_70",
            "start": 365,
            "end": 446,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_70@3",
            "content": "In each subsequent decoding step, we concatenate the prior input sequence with the generated token to decode in an auto-regressive manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_70",
            "start": 448,
            "end": 585,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_71@0",
            "content": "We share the vocabulary sets of input and output components and thus, use the same embedding matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_71",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_71@1",
            "content": "Given the encoded question Q, to decode the program for dialogue understanding, the contextual signals are integrated through 2 attention layers: one attention on previously generated tokens, and the other on question tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_71",
            "start": 101,
            "end": 325,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_71@2",
            "content": "At time step j, we denote the output from an attention layer as A dial,j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_71",
            "start": 327,
            "end": 400,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_72@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_72",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_73@0",
            "content": "dial = Attention(P dial | j\u22121 0 , P dial | j\u22121 0 , P dial | j\u22121 0 ) A (2) dial = Attention(A (1) dial , Q, Q) \u2208 R j\u00d7d",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_73",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_74@0",
            "content": "To generate programs for video understanding, the contextual signals are learned and incorporated in a similar manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_74",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_74@1",
            "content": "However, to exploit dialogue contextual cues, the execution output of dialogue understanding neural modules Q ctx is incorporated to each vector in P dial through an additional attention layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_74",
            "start": 119,
            "end": 311,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_74@2",
            "content": "This layer integrates the resolved entity information to decode the original entities for video understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_74",
            "start": 313,
            "end": 422,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_74@3",
            "content": "It is equivalent to a reasoning process that converts the question from its original multi-turn semantics to single-turn semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_74",
            "start": 424,
            "end": 554,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_75@0",
            "content": "A (1) vid = Attention(P vid | j\u22121 0 , P vid | j\u22121 0 , P vid | j\u22121 0 ) A (2) vid = Attention(A (1) vid , Q, Q) \u2208 R j\u00d7d A (3) vid = Attention(A (2) vid , Q ctx , Q ctx ) \u2208 R j\u00d7d",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_75",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_76@0",
            "content": "Noted that in the neural modules described in Section 3.3, during training, we simply feed the ground-truth programs to optimize these modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_76",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_76@1",
            "content": "For instance, the neural module where received the ground truth entities P which is then used to instantiate the neural network and retrieve from video V .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_76",
            "start": 144,
            "end": 298,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_76@2",
            "content": "During test time, we decode the programs token by token through the question parsers, and feed the predicted entities P to neural modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_76",
            "start": 300,
            "end": 437,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_76@3",
            "content": "Note that we do not assume, and hence not train model to retrieve ground-truth locations of visual entities in videos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_76",
            "start": 439,
            "end": 556,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_76@4",
            "content": "This strategy enables the applicability of VGNMN as we consider these entity annotations mostly unavailable in real-world systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_76",
            "start": 558,
            "end": 687,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_77@0",
            "content": "Different from AVSD, TGIF-QA contains a diverse set of QA tasks:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_77",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_78@0",
            "content": "We follow prior approaches (Hu et al., 2017(Hu et al., , 2018Kottur et al., 2018) by obtaining the annotations of the programs through a language parser (Hu et al., 2016) and a reference resolution model (Clark and Manning, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_78",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_78@1",
            "content": "During training, we directly use these as ground-truth labels of programs to train our models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_78",
            "start": 231,
            "end": 324,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_78@2",
            "content": "The ground-truth responses are augmented with label smoothing technique (Szegedy et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_78",
            "start": 326,
            "end": 420,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_78@3",
            "content": "During inference time, we generate all programs and responses from given dialogues and videos.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_78",
            "start": 422,
            "end": 515,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_78@4",
            "content": "We run beam search to enumerate programs for dialogue and video understanding and dialogue responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_78",
            "start": 517,
            "end": 617,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@0",
            "content": "We use a training batch size of 32 and embedding dimension d = 128 in all experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@1",
            "content": "Where Transformer attention is used, we fix the number of attention heads to 8 in all attention layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 87,
            "end": 189,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@2",
            "content": "In neural modules with MLP layers, the MLP network is fixed to 2 linear layers with a ReLU activation in between.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 191,
            "end": 303,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@3",
            "content": "In neural modules with CNN, we adopt a vanilla CNN architecture for text classification (without the last MLP layer) where the number of input channels is 1, the kernel sizes are {3, 4, 5}, and the number of output channels is d. We initialize models with uniform distribution (Glorot and Bengio, 2010).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 305,
            "end": 607,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@4",
            "content": "During training, we adopt the Adam optimizer (Kingma and Ba, 2015) and a decaying learning rate (Vaswani et al., 2017) where we fix the warm-up steps to 15K training steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 609,
            "end": 780,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@5",
            "content": "We employ dropout (Srivastava et al., 2014) of 0.2 at all networks except the last linear layers of question parsers and response decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 782,
            "end": 919,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_79@6",
            "content": "We train models up to 50 epochs and select the best models based on the average loss per epoch in the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_79",
            "start": 921,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_80@0",
            "content": "All models are trained in a V100 GPU with a capacity of 16GB.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_80",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_80@1",
            "content": "We approximated each training epoch took about 20 minutes to run.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_80",
            "start": 62,
            "end": 126,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_80@2",
            "content": "For each model experiment with VGNMN, we obtained at least 2 runs and reported the average results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_80",
            "start": 128,
            "end": 226,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_81@0",
            "content": "Optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_81",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_81@1",
            "content": "We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_81",
            "start": 14,
            "end": 133,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_82@0",
            "content": "L = \u03b1L dial + \u03b2L vid + L res = \u03b1 j \u2212 log(P dial (P dial,j )) + \u03b2 l \u2212 log(P video (P video,l )) + n \u2212 log(P res (R n ))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_82",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_83@0",
            "content": "where P is the probability distribution of an output token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_83",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_83@1",
            "content": "The probability is computed by passing output representations from the parsers and decoder to a linear layer W \u2208 R d\u00d7V with softmax activation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_83",
            "start": 60,
            "end": 202,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_83@2",
            "content": "We share the parameters between W and embedding matrix E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_83",
            "start": 204,
            "end": 260,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_84@0",
            "content": "We experiment with several Non-NMN based variants of our models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_84",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_84@1",
            "content": "As can be seen in Table 7, our approach to video and dialogue understanding through compositional reasoning programs exhibits better performance than non-compositional approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_84",
            "start": 65,
            "end": 243,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_84@2",
            "content": "Compared to the approaches that directly process frame-level features in videos (Row B) or token-level features in dialogues (Row C, D), our full VGNMN (Row A) considers entitylevel and action-level information extraction and thus, avoids unnecessary and possibly noisy extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_84",
            "start": 245,
            "end": 525,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_84@3",
            "content": "Compared to the approaches that obtain dialogue contextual cues through a hierarchical encoding architecture (Row E, F) such as (Serban et al., 2016;, VGNMN directly addresses the challenge of entity references in dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_84",
            "start": 527,
            "end": 750,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_84@4",
            "content": "As mentioned, we hypothesize that the hierarchical encoding architecture is more appropriate for less entity-sensitive dialogues such as chit-chat and open-domain dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_84",
            "start": 752,
            "end": 924,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_85@0",
            "content": "Experimenting with different ways to integrate dialogue context representations, we observe that adding an attention layer attending to question during response decoding (Row G) is not necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_85",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_85@1",
            "content": "This can be explained as the representation Q ctx obtained from dialogue understanding program already contains contextual information of both dialogue history and question and question input is no longer needed in the decoding phase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_85",
            "start": 196,
            "end": 429,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_85@2",
            "content": "Furthermore, we investigate the model sensitivity to natural language generation through its ability to construct linguistically correct programs and responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_85",
            "start": 431,
            "end": 590,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_85@3",
            "content": "To generate responses that are linguistically appropriate, VGNMN needs dialogue context representation Q ctx as input to the response decoder (Row H).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_85",
            "start": 592,
            "end": 741,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_85@4",
            "content": "The model also needs encoded question Q as input to the video understanding program parser to be able to decompose this sequence to entity and action module parameters (Row I).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_85",
            "start": 743,
            "end": 918,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_86@0",
            "content": "We extract the predicted programs and responses for some example dialogues in Figure 6, 7, 8, and 9 and report our observations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_86",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_87@0",
            "content": "\u2022 We observe that when the predicted programs are correct, the output responses generally match the ground-truth (See the 1 st and 2 nd turn in Figure 6, and the 1 st and 4 th turn in Figure 8) or close to the ground-truth responses (1 st turn in Figure 7). \u2022 When the output responses do not match the ground truth, we can understand the model mistakes by interpreting the predicted programs. For example, in the 3 rd turn in Figure 6, the output response describes a room because the predicted video program focuses on the entity \"what room\" instead of the entity \"an object\" in the question. Another example is the 3 rd turn in Figure 8 where the entity \"rooftop\" is missing in the video program.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_87",
            "start": 0,
            "end": 698,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_88@0",
            "content": "These mismatches can deviate the information retrieved from the video during video program execution, leading to wrong output responses with wrong visual contents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_88",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_89@0",
            "content": "\u2022 We also note that in some cases, one or both of the predicted programs are incorrect, but the predicted responses still match the groundtruth responses. This might be explained as the predicted module parameters are still close enough to the \"gold\" labels (e.g. 4 th turn in Figure 6). Sometimes, our model predicted programs that are more appropriate than the ground truth. For example, in the 2 nd turn in Figure 7, the program is added with a where module parameterized by the entity \"the shopping bag\" which was solved from the reference \"them\" mentioned in the question. \u2022 We observe that for complex questions that involve more than one queries (e.g. the 3 rd turn in Figure 8), it becomes more challenging to decode an appropriate video understanding program and generate responses that can address all queries. \u2022 In Figure 9, we demonstrate some output examples of VGNMN and compare with two baselines: Baseline and MTN (Le et al., 2019b). We noted that VGNMN can include important entities relevant to the current dialogue turn to construct output responses while other models might miss some entity details, e.g. \"them/dishes\" in example A and \"the magazine\" in example B. These small yet important details can determine the correctness of dialogue responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_89",
            "start": 0,
            "end": 1270,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_90@0",
            "content": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Analyzing the behavior of visual question answering models, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_90",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_91@0",
            "content": "Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh, Dhruv Batra, Anoop Cherian, Tim Marks, Chiori Hori, Audio-visual scene-aware dialog, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_91",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_92@0",
            "content": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Learning to compose neural networks for question answering, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_92",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_93@0",
            "content": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Neural module networks, 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_93",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_94@0",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Vqa: Visual question answering, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_94",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_95@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_95",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_96@0",
            "content": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_96",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_97@0",
            "content": "Satanjeev Banerjee, Alon Lavie, Meteor: An automatic metric for mt evaluation with improved correlation with human judgments, 2005, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_97",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_98@0",
            "content": "Joao Carreira, Andrew Zisserman, Quo vadis, action recognition? a new model and the kinetics dataset, 2017, proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_98",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_99@0",
            "content": "Yun-Wei Chu, Kuan-Yen Lin, Chao-Chun Hsu, Lun-Wei Ku, Multi-step joint-modality attention network for scene-aware dialogue system, 2020, DSTC Workshop @ AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_99",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_100@0",
            "content": "Kevin Clark, Christopher Manning, Deep reinforcement learning for mention-ranking coreference models, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_100",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_101@0",
            "content": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, Universal transformers, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_101",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_102@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_102",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_103@0",
            "content": "Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, Heng Huang, Heterogeneous memory enhanced multimodal attention model for video question answering, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_103",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_104@0",
            "content": "Eric Shi Feng, Alvin Wallace, I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Pathologies of neural models make interpretations difficult, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_104",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_105@0",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_105",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_106@0",
            "content": "Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia, Motion-appearance co-memory networks for video question answering, 2018, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_106",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_107@0",
            "content": "Lianli Gao, Pengpeng Zeng, Jingkuan Song, Yuan-Fang Li, Wu Liu, Tao Mei, Heng Tao Shen, Structured two-stream attention network for video question answering, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_107",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_108@0",
            "content": "Xavier Glorot, Yoshua Bengio, Understanding the difficulty of training deep feedforward neural networks, 2010, Proceedings of the thirteenth international conference on artificial intelligence and statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_108",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_109@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_109",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_110@0",
            "content": "Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, Neural module networks for reasoning over text, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_110",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_111@0",
            "content": "Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, Jiajun Wu, Visual concept-metaconcept learning, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_111",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_112@0",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_112",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_113@0",
            "content": "Shawn Hershey, Sourish Chaudhuri, P Daniel,  Ellis, F Jort, Aren Gemmeke, Channing Jansen, Manoj Moore, Devin Plakal,  Platt, A Rif, Bryan Saurous,  Seybold, Cnn architectures for largescale audio classification, 2017, Acoustics, Speech and Signal Processing, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_113",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_114@0",
            "content": "C Hori, H Alamri, J Wang, G Wichern, T Hori, A Cherian, T Marks, V Cartillier, R Lopes, A Das, I Essa, D Batra, D Parikh, Endto-end audio visual scene-aware dialog using multimodal attention-based video features, 2019, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_114",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_115@0",
            "content": "Chiori Hori, Anoop Cherian, Tim Marks, Takaaki Hori, Joint student-teacher learning for audio-visual scene-aware dialog, 2019, Proc. Interspeech 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_115",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_116@0",
            "content": "Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko, Explainable neural computation via stack neural module networks, 2018, Proceedings of the European conference on computer vision (ECCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_116",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_117@0",
            "content": "Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko, Learning to reason: End-to-end module networks for visual question answering, 2017, Proceedings of the IEEE International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_117",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_118@0",
            "content": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell, Natural language object retrieval, 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_118",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_119@0",
            "content": "Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, Chuang Gan, Locationaware graph convolutional networks for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_119",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_120@0",
            "content": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim, Tgif-qa: Toward spatiotemporal reasoning in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_120",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_121@0",
            "content": "Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, Yue Gao, Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_121",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_122@0",
            "content": "Pin Jiang, Yahong Han, Reasoning with heterogeneous graph alignment for video question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_122",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_123@0",
            "content": "Yichen Jiang, Mohit Bansal, Self-assembling modular networks for interpretable multi-hop reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_123",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_124@0",
            "content": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_124",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_125@0",
            "content": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Inferring and executing programs for visual reasoning, 2017, Proceedings of the IEEE International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_125",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_126@0",
            "content": "Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang, Nojun Kwak, Self-supervised pretraining and contrastive representation learning for multiple-choice video qa, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_126",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_127@0",
            "content": "Yoon Kim, Convolutional neural networks for sentence classification, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_127",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_128@0",
            "content": "P Diederick, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_128",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_129@0",
            "content": "Satwik Kottur, M Jos\u00e9, Devi Moura,  Parikh, Visual coreference resolution in visual dialog using neural module networks, 2018, Proceedings of the European Conference on Computer Vision (ECCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_129",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_130@0",
            "content": "UNKNOWN, None, 2019, Leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_130",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_131@0",
            "content": "Hung Le, Doyen Hoi, N Sahoo,  Chen, End-to-end multimodal dialog systems with hierarchical multimodal attention on video features, 2019, DSTC7 at AAAI2019 workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_131",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_132@0",
            "content": "Hung Le, C Steven,  Hoi, Video-grounded dialogues with pretrained generation language models, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_132",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_133@0",
            "content": "Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi, Multimodal transformer networks for end-toend video-grounded dialogue systems, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_133",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_134@0",
            "content": "Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi, BiST: Bi-directional spatio-temporal reasoning for video-grounded dialogues, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_134",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_135@0",
            "content": "UNKNOWN, None, 2019, Learning to reason with relational video representation for question answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_135",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_136@0",
            "content": "UNKNOWN, None, 2020, Hierarchical conditional relation networks for video question answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_136",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_137@0",
            "content": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Soon Doo, Trung Kim, Kyomin Bui,  Jung, Dstc8-avsd: Multimodal semantic transformer network with retrieval style word generator, 2020, DSTC Workshop @ AAAI 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_137",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_138@0",
            "content": "Chenyi Lei, Lei Wu, Dong Liu, Zhao Li, Guoxin Wang, Haihong Tang, Houqiang Li, Multiquestion learning for visual question answering, 2020, The AAAI Conference on Artificial Intelligence (AAAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_138",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_139@0",
            "content": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara Berg, Mohit Bansal, Jingjing Liu, Less is more: Clipbert for video-and-language learning via sparse sampling, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_139",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_140@0",
            "content": "Jie Lei, Licheng Yu, Mohit Bansal, Tamara Berg, TVQA: Localized, compositional video question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_140",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_141@0",
            "content": "Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu, HERO: Hierarchical encoder for Video+Language omnirepresentation pre-training, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_141",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_142@0",
            "content": "Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, Chuang Gan, Beyond rnns: Positional self-attention with co-attention for video question answering, 2019, The 33rd AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_142",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_143@0",
            "content": "UNKNOWN, None, 2020, Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_143",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_144@0",
            "content": "UNKNOWN, None, 2004, Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_144",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_145@0",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_145",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_146@0",
            "content": "Shikhar Dat Tien Nguyen, Hannes Sharma, Layla Schulz,  Asri, From film to video: Multiturn question answering with multi-modal context, 2018, AAAI 2019 Dialog System Technology Challenge (DSTC7) Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_146",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_147@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_147",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_148@0",
            "content": "Mengye Ren, Ryan Kiros, Richard Zemel, Exploring models and data for image question answering, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_148",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_149@0",
            "content": "Kaiming Shaoqing Ren, Ross He, Jian Girshick,  Sun, Faster R-CNN: Towards real-time object detection with region proposal networks, 2015, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_149",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_150@0",
            "content": "Ramon Sanabria, Shruti Palaskar, Florian Metze, Cmu sinbad's submission for the dstc7 avsd challenge, 2019, DSTC7 at AAAI2019 workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_150",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_151@0",
            "content": "Idan Schwartz, Seunghak Yu, Tamir Hazan, Alexander Schwing, Factor graph attention, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_151",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_152@0",
            "content": "Alessandro Iulian V Serban, Yoshua Sordoni, Aaron Bengio, Joelle Courville,  Pineau, Building end-to-end dialogue systems using generative hierarchical neural network models, 2016, Thirtieth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_152",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_153@0",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_153",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_154@0",
            "content": "G\u00fcl Gunnar A Sigurdsson, Xiaolong Varol, Ali Wang, Ivan Farhadi, Abhinav Laptev,  Gupta, Hollywood in homes: Crowdsourcing data collection for activity understanding, 2016, European Conference on Computer Vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_154",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_155@0",
            "content": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: a simple way to prevent neural networks from overfitting, 2014, The Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_155",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_156@0",
            "content": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Rethinking the inception architecture for computer vision, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_156",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_157@0",
            "content": "Zineng Tang, Jie Lei, Mohit Bansal, DeCEMBERT: Learning from noisy instructional videos via dense captions and entropy minimization, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_157",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_158@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_158",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_159@0",
            "content": "Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Cider: Consensus-based image description evaluation, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_159",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_160@0",
            "content": "Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura. 2020. Bert representations for video question answering, , Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_160",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_161@0",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic vqa: Disentangling reasoning from vision and language understanding, 2018, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_161",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_162@0",
            "content": "Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim, End-to-end concept word detection for video captioning, retrieval, and question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_162",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "170-ARR_v1_163@0",
            "content": "UNKNOWN, None, 2021, Merlot: Multimodal neural script knowledge models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "170-ARR_v1_163",
            "start": 0,
            "end": 72,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_1",
            "tgt_ix": "170-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_1",
            "tgt_ix": "170-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_4",
            "tgt_ix": "170-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_6",
            "tgt_ix": "170-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_7",
            "tgt_ix": "170-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_9",
            "tgt_ix": "170-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_10",
            "tgt_ix": "170-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_11",
            "tgt_ix": "170-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_5",
            "tgt_ix": "170-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_13",
            "tgt_ix": "170-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_14",
            "tgt_ix": "170-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_14",
            "tgt_ix": "170-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_15",
            "tgt_ix": "170-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_16",
            "tgt_ix": "170-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_16",
            "tgt_ix": "170-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_18",
            "tgt_ix": "170-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_18",
            "tgt_ix": "170-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_21",
            "tgt_ix": "170-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_23",
            "tgt_ix": "170-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_24",
            "tgt_ix": "170-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_22",
            "tgt_ix": "170-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_22",
            "tgt_ix": "170-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_22",
            "tgt_ix": "170-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_22",
            "tgt_ix": "170-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_25",
            "tgt_ix": "170-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_28",
            "tgt_ix": "170-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_30",
            "tgt_ix": "170-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_32",
            "tgt_ix": "170-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_34",
            "tgt_ix": "170-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_35",
            "tgt_ix": "170-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_36",
            "tgt_ix": "170-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_37",
            "tgt_ix": "170-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_40",
            "tgt_ix": "170-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_42",
            "tgt_ix": "170-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_44",
            "tgt_ix": "170-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_43",
            "tgt_ix": "170-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_43",
            "tgt_ix": "170-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_43",
            "tgt_ix": "170-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_45",
            "tgt_ix": "170-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_47",
            "tgt_ix": "170-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_48",
            "tgt_ix": "170-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_49",
            "tgt_ix": "170-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_50",
            "tgt_ix": "170-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_53",
            "tgt_ix": "170-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_54",
            "tgt_ix": "170-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_56",
            "tgt_ix": "170-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_57",
            "tgt_ix": "170-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_58",
            "tgt_ix": "170-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_62",
            "tgt_ix": "170-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_63",
            "tgt_ix": "170-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_63",
            "tgt_ix": "170-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_64",
            "tgt_ix": "170-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_67",
            "tgt_ix": "170-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_68",
            "tgt_ix": "170-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_69",
            "tgt_ix": "170-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_70",
            "tgt_ix": "170-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_72",
            "tgt_ix": "170-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_73",
            "tgt_ix": "170-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_74",
            "tgt_ix": "170-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_71",
            "tgt_ix": "170-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_75",
            "tgt_ix": "170-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_80",
            "tgt_ix": "170-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_81",
            "tgt_ix": "170-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_82",
            "tgt_ix": "170-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_77",
            "tgt_ix": "170-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_83",
            "tgt_ix": "170-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_86",
            "tgt_ix": "170-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_88",
            "tgt_ix": "170-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "170-ARR_v1_0",
            "tgt_ix": "170-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_1",
            "tgt_ix": "170-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_2",
            "tgt_ix": "170-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_3",
            "tgt_ix": "170-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_4",
            "tgt_ix": "170-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_4",
            "tgt_ix": "170-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_4",
            "tgt_ix": "170-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_5",
            "tgt_ix": "170-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_5",
            "tgt_ix": "170-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_5",
            "tgt_ix": "170-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_6",
            "tgt_ix": "170-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_6",
            "tgt_ix": "170-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_7",
            "tgt_ix": "170-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_7",
            "tgt_ix": "170-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_7",
            "tgt_ix": "170-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_7",
            "tgt_ix": "170-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_8",
            "tgt_ix": "170-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_9",
            "tgt_ix": "170-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_9",
            "tgt_ix": "170-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_9",
            "tgt_ix": "170-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_9",
            "tgt_ix": "170-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_10",
            "tgt_ix": "170-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_10",
            "tgt_ix": "170-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_10",
            "tgt_ix": "170-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_10",
            "tgt_ix": "170-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_11",
            "tgt_ix": "170-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_12",
            "tgt_ix": "170-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_13",
            "tgt_ix": "170-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_14",
            "tgt_ix": "170-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_15",
            "tgt_ix": "170-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_15",
            "tgt_ix": "170-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_15",
            "tgt_ix": "170-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_15",
            "tgt_ix": "170-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_16",
            "tgt_ix": "170-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_17",
            "tgt_ix": "170-ARR_v1_17@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_18",
            "tgt_ix": "170-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_19",
            "tgt_ix": "170-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_20",
            "tgt_ix": "170-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_21",
            "tgt_ix": "170-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_21",
            "tgt_ix": "170-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_22",
            "tgt_ix": "170-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_23",
            "tgt_ix": "170-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_23",
            "tgt_ix": "170-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_23",
            "tgt_ix": "170-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_23",
            "tgt_ix": "170-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_24",
            "tgt_ix": "170-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_25",
            "tgt_ix": "170-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_26",
            "tgt_ix": "170-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_27",
            "tgt_ix": "170-ARR_v1_27@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_28",
            "tgt_ix": "170-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_29",
            "tgt_ix": "170-ARR_v1_29@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_30",
            "tgt_ix": "170-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_30",
            "tgt_ix": "170-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_31",
            "tgt_ix": "170-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_32",
            "tgt_ix": "170-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_32",
            "tgt_ix": "170-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_32",
            "tgt_ix": "170-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_33",
            "tgt_ix": "170-ARR_v1_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_34",
            "tgt_ix": "170-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_35",
            "tgt_ix": "170-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_35",
            "tgt_ix": "170-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_35",
            "tgt_ix": "170-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_35",
            "tgt_ix": "170-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_36",
            "tgt_ix": "170-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_37",
            "tgt_ix": "170-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_37",
            "tgt_ix": "170-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_37",
            "tgt_ix": "170-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_38",
            "tgt_ix": "170-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_39",
            "tgt_ix": "170-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_40",
            "tgt_ix": "170-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_41",
            "tgt_ix": "170-ARR_v1_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_42",
            "tgt_ix": "170-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_42",
            "tgt_ix": "170-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_42",
            "tgt_ix": "170-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_42",
            "tgt_ix": "170-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_43",
            "tgt_ix": "170-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_44",
            "tgt_ix": "170-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_44",
            "tgt_ix": "170-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_44",
            "tgt_ix": "170-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_45",
            "tgt_ix": "170-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_45",
            "tgt_ix": "170-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_46",
            "tgt_ix": "170-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_47",
            "tgt_ix": "170-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_47",
            "tgt_ix": "170-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_48",
            "tgt_ix": "170-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_49",
            "tgt_ix": "170-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_49",
            "tgt_ix": "170-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_49",
            "tgt_ix": "170-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_50",
            "tgt_ix": "170-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_51",
            "tgt_ix": "170-ARR_v1_51@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_52",
            "tgt_ix": "170-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_53",
            "tgt_ix": "170-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_54",
            "tgt_ix": "170-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_54",
            "tgt_ix": "170-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_54",
            "tgt_ix": "170-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_54",
            "tgt_ix": "170-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_55",
            "tgt_ix": "170-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_56",
            "tgt_ix": "170-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_56",
            "tgt_ix": "170-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_56",
            "tgt_ix": "170-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_56",
            "tgt_ix": "170-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_57",
            "tgt_ix": "170-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_57",
            "tgt_ix": "170-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_57",
            "tgt_ix": "170-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_57",
            "tgt_ix": "170-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_58",
            "tgt_ix": "170-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_58",
            "tgt_ix": "170-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_58",
            "tgt_ix": "170-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_59",
            "tgt_ix": "170-ARR_v1_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_60",
            "tgt_ix": "170-ARR_v1_60@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_61",
            "tgt_ix": "170-ARR_v1_61@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_62",
            "tgt_ix": "170-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_63",
            "tgt_ix": "170-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_64",
            "tgt_ix": "170-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_64",
            "tgt_ix": "170-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_64",
            "tgt_ix": "170-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_65",
            "tgt_ix": "170-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_66",
            "tgt_ix": "170-ARR_v1_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_67",
            "tgt_ix": "170-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_67",
            "tgt_ix": "170-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_67",
            "tgt_ix": "170-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_68",
            "tgt_ix": "170-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_68",
            "tgt_ix": "170-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_68",
            "tgt_ix": "170-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_68",
            "tgt_ix": "170-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_69",
            "tgt_ix": "170-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_70",
            "tgt_ix": "170-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_70",
            "tgt_ix": "170-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_70",
            "tgt_ix": "170-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_70",
            "tgt_ix": "170-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_71",
            "tgt_ix": "170-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_71",
            "tgt_ix": "170-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_71",
            "tgt_ix": "170-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_72",
            "tgt_ix": "170-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_73",
            "tgt_ix": "170-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_74",
            "tgt_ix": "170-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_74",
            "tgt_ix": "170-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_74",
            "tgt_ix": "170-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_74",
            "tgt_ix": "170-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_75",
            "tgt_ix": "170-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_76",
            "tgt_ix": "170-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_77",
            "tgt_ix": "170-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_78",
            "tgt_ix": "170-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_79",
            "tgt_ix": "170-ARR_v1_79@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_80",
            "tgt_ix": "170-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_80",
            "tgt_ix": "170-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_80",
            "tgt_ix": "170-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_81",
            "tgt_ix": "170-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_81",
            "tgt_ix": "170-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_82",
            "tgt_ix": "170-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_83",
            "tgt_ix": "170-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_83",
            "tgt_ix": "170-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_83",
            "tgt_ix": "170-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_84",
            "tgt_ix": "170-ARR_v1_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_85",
            "tgt_ix": "170-ARR_v1_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_86",
            "tgt_ix": "170-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_87",
            "tgt_ix": "170-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_88",
            "tgt_ix": "170-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_89",
            "tgt_ix": "170-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_90",
            "tgt_ix": "170-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_91",
            "tgt_ix": "170-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_92",
            "tgt_ix": "170-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_93",
            "tgt_ix": "170-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_94",
            "tgt_ix": "170-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_95",
            "tgt_ix": "170-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_96",
            "tgt_ix": "170-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_97",
            "tgt_ix": "170-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_98",
            "tgt_ix": "170-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_99",
            "tgt_ix": "170-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_100",
            "tgt_ix": "170-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_101",
            "tgt_ix": "170-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_102",
            "tgt_ix": "170-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_103",
            "tgt_ix": "170-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_104",
            "tgt_ix": "170-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_105",
            "tgt_ix": "170-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_106",
            "tgt_ix": "170-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_107",
            "tgt_ix": "170-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_108",
            "tgt_ix": "170-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_109",
            "tgt_ix": "170-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_110",
            "tgt_ix": "170-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_111",
            "tgt_ix": "170-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_112",
            "tgt_ix": "170-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_113",
            "tgt_ix": "170-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_114",
            "tgt_ix": "170-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_115",
            "tgt_ix": "170-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_116",
            "tgt_ix": "170-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_117",
            "tgt_ix": "170-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_118",
            "tgt_ix": "170-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_119",
            "tgt_ix": "170-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_120",
            "tgt_ix": "170-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_121",
            "tgt_ix": "170-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_122",
            "tgt_ix": "170-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_123",
            "tgt_ix": "170-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_124",
            "tgt_ix": "170-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_125",
            "tgt_ix": "170-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_126",
            "tgt_ix": "170-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_127",
            "tgt_ix": "170-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_128",
            "tgt_ix": "170-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_129",
            "tgt_ix": "170-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_130",
            "tgt_ix": "170-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_131",
            "tgt_ix": "170-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_132",
            "tgt_ix": "170-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_133",
            "tgt_ix": "170-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_134",
            "tgt_ix": "170-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_135",
            "tgt_ix": "170-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_136",
            "tgt_ix": "170-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_137",
            "tgt_ix": "170-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_138",
            "tgt_ix": "170-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_139",
            "tgt_ix": "170-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_140",
            "tgt_ix": "170-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_141",
            "tgt_ix": "170-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_142",
            "tgt_ix": "170-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_143",
            "tgt_ix": "170-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_144",
            "tgt_ix": "170-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_145",
            "tgt_ix": "170-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_146",
            "tgt_ix": "170-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_147",
            "tgt_ix": "170-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_148",
            "tgt_ix": "170-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_149",
            "tgt_ix": "170-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_150",
            "tgt_ix": "170-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_151",
            "tgt_ix": "170-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_152",
            "tgt_ix": "170-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_153",
            "tgt_ix": "170-ARR_v1_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_154",
            "tgt_ix": "170-ARR_v1_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_155",
            "tgt_ix": "170-ARR_v1_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_156",
            "tgt_ix": "170-ARR_v1_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_157",
            "tgt_ix": "170-ARR_v1_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_158",
            "tgt_ix": "170-ARR_v1_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_159",
            "tgt_ix": "170-ARR_v1_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_160",
            "tgt_ix": "170-ARR_v1_160@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_161",
            "tgt_ix": "170-ARR_v1_161@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_162",
            "tgt_ix": "170-ARR_v1_162@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "170-ARR_v1_163",
            "tgt_ix": "170-ARR_v1_163@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1564,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "170-ARR",
        "version": 1
    }
}