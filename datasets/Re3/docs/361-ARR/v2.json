{
    "nodes": [
        {
            "ix": "361-ARR_v2_0",
            "content": "Image Retrieval from Contextual Descriptions",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_2",
            "content": "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we propose a new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGECODE). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on IMAGECODE. Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that IMAGECODE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences. We make code and dataset publicly available. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "361-ARR_v2_4",
            "content": "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker's intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998). These processes, more generally, \"The girl in blue is to the left of the girl in the middle with the purple shoes. The girl in blue is not obscured in any way.\" Frames 5-10 are left out for simplicity's sake. The target image, frame 3, is in green, whereas the incorrect frames are in red.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_5",
            "content": "fall under the umbrella term of pragmatics (Grice, 1957). Despite recent progress in multimodal systems, it remains unclear to which extent they can handle settings where context plays a major role, such as in real-world communication.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_6",
            "content": "To this end, we present a new challenge that requires multimodal models to leverage context to retrieve images from text. In particular, given a contextual description and a set of minimally contrastive candidate images, i.e. differing only in some details, the model has to retrieve the target image. In order to discriminate between similar images, human annotators naturally produce highly nuanced and grammatically complex descriptions. An example of our new challenging dataset, Image Retrieval from Contextual Descriptions (IMAGECODE), is shown in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_7",
            "content": "During the data collection process, sets of similar images are selected among static pictures from Open Images (Kuznetsova et al., 2020) and (a larger portion) among video frames from diverse domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_8",
            "content": "Including both types of images allows for diversifying the dataset while representing different degrees of visual similarity within each set. Next, we crowdsource a contextual description of a target image (presented together with the rest of the set) that contains only differences relevant for retrieval. After a filtering phase involving human retrievers, we obtain a large-scale dataset with 94,020 images and 21,202 descriptions associated with image sets of size 10.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_9",
            "content": "As a result of this annotation protocol, successfully completing the task requires models to integrate several kinds of context: i) the image set, as the descriptions often only make sense in the context of several other images and are not suitable as stand-alone captions. In fact, aspects of the image that are very salient and that therefore would normally be emphasized are not useful in our proposed task. Instead, the focus of our descriptions are finegrained details that help discriminate between images (see Figure 1); ii) the speaker's intention. Due to their high degree of image similarity, contextual descriptions may be literally true for multiple images; however, once the speaker's intention is taken into account, the correct image can be determined by virtue of pragmatics, i.e. Grice's maxim of quality 2 (see Figure 2, Figure 7); iii) temporal sequences: for video frames temporal reasoning is also required to compare different moments of an unfolding event.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_10",
            "content": "On our new dataset IMAGECODE, we benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, specifically ViLBERT (Lu et al., 2019) and UNITER (Chen et al., 2020) as two cross-encoder variants and CLIP as a strong biencoder (Radford et al., 2021). We report several findings. First, accuracy on static images is vastly superior than on video frames. Therefore, the degree of similarity among the candidate images has an overwhelming impact on retrieval performance. Second, all state-of-the-art models generally struggle with image retrieval from contextual descriptions, whereas humans consistently achieve high accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_11",
            "content": "Hence, we propose model variants capable of better taking context into account: i) once an imagedescription pair is encoded, we refine this representation by attending to the other images in the set;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_12",
            "content": "ii) we augment image encodings with temporal embeddings. Based on our results, models take advantage of this additional information fruitfully but only to a limited degree.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_13",
            "content": "Because of its challenging nature, due to the minimally contrastive images and complex descriptions, we believe that IMAGECODE will help make visio-linguistic models more context-aware and sensitive to fine-grained details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_14",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "361-ARR_v2_15",
            "content": "There is a long tradition of grounding language understanding on single images, in the form of visual question answering (Goyal et al., 2017;Hudson and Manning, 2019), visual dialogue (de Vries et al., 2017;Das et al., 2017), or visual entailment (Xie et al., 2019). Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017;Hu et al., 2019;Suhr et al., 2019;Forbes et al., 2019;Hendricks and Nematzadeh, 2021;Yan et al., 2021;Hosseinzadeh and Wang, 2021;Bogin et al., 2021;Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a;Bansal et al., 2020). While many of these benchmarks involve just two images, COVR (Bogin et al., 2021) and ISVQA (Bansal et al., 2020) provide more images, similar to our sets of 10 images.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_16",
            "content": "ISVQA and Spot-the-diff (Jhamtani and Berg-Kirkpatrick, 2018a) are most similar to our dataset, IMAGECODE. ISVQA is based on several video frames that are synthetic and cover a restricted domain, with short questions for Visual Question Answering. Spot-the-diff provides two frames from surveillance video cameras and descriptions of all their differences. IMAGECODE is unique as a) we cover a wider range of domains; b) we construct image sets that are maximally similar while being distinguishable through natural language (Section 3) and c) we limit descriptions to relevant differences. This results in (a) diverse, (b) complex and (c) pragmatically informative descriptions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_17",
            "content": "We do not claim to explicitly model pragmatics in this paper, i.e. with Rational Speech Acts (Goodman and Frank, 2016). Instead we present a dataset that is naturally suitable for pragmatic reasoning (Andreas and Klein, 2016;Cohn-Gordon et al., 2018) as a listener has to consider the context, assume a Gricean speaker and resolve ambiguities resulting from nuanced differences. The reasoning in our task and data collection is therefore also similar to ReferItGame and subsequent work (Kazemzadeh et al., 2014;Mao et al., 2016) where one crowdworker generates a referring expressing for an object in a single image and another worker picks an object based on the expression.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_18",
            "content": "Data Collection",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "361-ARR_v2_19",
            "content": "Our data collection involves two steps with a human describer and retriever. The describer is given a set of 10 highly similar images S = [I 1 , I 2 , ..., I 10 ], one of them marked as the target image I t , and has to write a description D that clearly distinguishes I t from the other distractor images. In the second step, the retriever is given the same 10 images and the description from the first step and has to identify the target image based on the description. S and D are only added to our dataset if the retrieval is successful.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_20",
            "content": "Below, we outline the main stages of data collection: first, the collection of similar, contrastive images in Section 3.1. Then, the crowdsourcing of contextual descriptions in Section 3.2 and validation of the examples via image retrieval (Section 3.3). The final IMAGECODE dataset consists of 94,020 images (partitioned into 9,402 sets) and 21,202 contextual descriptions (16,594 in the train split, 2,302 and 2,306 in the validation and test split respectively).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_21",
            "content": "Collecting Similar Images",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "361-ARR_v2_22",
            "content": "In the first stage, we collect sets of images that are highly similar but still distinguishable from each other by a human. To quantitatively measure the pairwise similarity of two images, we compute the Euclidean distance between their encodings extracted from a pre-trained CLIP model (Radford et al., 2021). 3 To study the effect of different degrees of similarity, further variegate our dataset, and enable temporal reasoning, we source our candidate images from collections of static pictures as well as videos, as detailed below. Static Pictures. We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6 (Kuznetsova et al., 2020), containing 1.74M images. For each image, we retrieve the 9 closest images from the training set based on their CLIP encodings. We then randomly sample 4,845 of these image sets. Video Frames. As sources for our video frames, we use i) Video-Storytelling (Li et al., 2019), covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT (Xu et al., 2016); and iii) YouCook (Das et al., 2013), covering cooking events. We choose these datasets as they contain publicly available and general-purpose videos (not specific to downstream tasks). We retain the original splits for train, validation, and test.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_23",
            "content": "To obtain disjoint sets of 10 similar frames, we first segment the videos into smaller scenes (also known as shots) via the scene detection functionality of ffmpeg (Tomar, 2006). Then, for each scene, we add its first frame to the set of selected images. We then iterate over every following frame and add it to the set if its pairwise Euclidean distance with each of the previously selected frames is larger than a threshold. 4 Once the set contains 10 images, we reiterate the procedure for a new set. If the scene ends and the current set contains less than 10 images, the set is discarded.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_24",
            "content": "During this process, we additionally remove frames that i) are too blurry, i.e. their BRISQUE score (Mittal et al., 2012) is larger than 0.65; or ii) contain too much text, which is detected with the OCR tool Tesseract (Smith, 2007). 5 We use all of YouCook's image sets and (due to cost constraints) randomly sample image sets from Video-Storytelling and MSR-VTT for crowdsourcing (cf. Table 1). We remark that image sets are further filtered at the final stage of annotation (Section 3.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_25",
            "content": "Crowdsourcing Contextual Descriptions",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "361-ARR_v2_26",
            "content": "After creating sets of highly-similar images in Section 3.1, we request annotators from Amazon Mechanical Turk (AMT) to write contextual descriptions for each target image in a set. Each round, a set of images is presented in random order for static pictures and respecting temporal order for video frames. This encourages annotators to take the dynamics of the event into account. We then (randomly) select 3 target images per set, and ask annotators to produce a description that discriminates them from the other images in the set. To encourage pragmatic reasoning, we do not ask for all the differences (just those sufficient for retrieval) and do not allow explicit mentions of other images (see Figure 2). We select high-quality annotators according to criteria in Appendix B and assign partly disjoint sets of annotators to train and test in order to avoid annotator bias (Geva et al., 2019). 6",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_27",
            "content": "Human Validation via Image Retrieval",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "361-ARR_v2_28",
            "content": "Finally, we validate the annotation crowdsourced in Section 3.2 by asking AMT workers to retrieve the correct target image from a set given its contextual description. For the final dataset, we retained only the examples that i) were retrieved successfully in the training set by a single worker or ii) were retrieved successfully by at least 2 out of 3 workers in the validation and test sets. As a consequence, we filtered out 26.5% of the contextual descriptions generated in Section 3.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_29",
            "content": "Data Analysis",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "361-ARR_v2_30",
            "content": "Human Accuracy and Agreement",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "361-ARR_v2_31",
            "content": "To quantify the reliability of the process outlined in Section 3, we report the inter-annotator agreement on our final dataset in Table 3. We use Krippendorff's \u03b1 as a metric (the higher the better), which accounts for incomplete data, since the number of annotators per example is not fixed. We treat the index of the target image either as a nominal variable for static images or as an ordinal variable for video frames. In both cases, we find a high degree of agreement. Moreover, in Table 3, we also report human accuracy-the percentage of times an annotator retrieved the correct target image from a contextual description (as described in Section 3.3). This provides an upper ceiling for the model performances (see Section 6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_32",
            "content": "Language Statistics",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "361-ARR_v2_33",
            "content": "In Table 4, we measure a series of statistics of the descriptions collected for IMAGECODE and compare them with other vision-and-language datasets with multiple naturalistic images (cf. Section 2), such as NLVR2 (Suhr et al., 2019) and Spot-thediff (Jhamtani and Berg-Kirkpatrick, 2018b). 8 In particular, we count the average description length, the number of distinct word types, the average dependency tree depth of each sentence, 9 and the average number of sentences per description. Based on these metrics, we find evidence that IMAGE-CODE's descriptions are longer and more syntactically complex than in the other datasets. Moreover, they include multiple sentences (11.8% of examples have 3 or more).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_34",
            "content": "Vision Statistics",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "361-ARR_v2_35",
            "content": "By calculating the average pairwise Euclidean distance between CLIP-based encodings of images in the same set, we find that video frames are more similar than static pictures -as expected -by a factor of 1.13. Moreover, we find that descriptions of video frames mention human body parts (72.1%) more often than static pictures (30.2%). On the other hand, names of colors appear in descriptions of static pictures (61.4%) more frequently than video frames (33.6%). 10 Thus, annotators resort to different strategies to discriminate between different types of image sets, focusing on the aspects that vary the most.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_36",
            "content": "Challenging Phenomena",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "361-ARR_v2_37",
            "content": "Finally, we identify 9 interesting and challenging phenomena in IMAGECODE and annotate whether they are present in 200 examples from the validation set. We provide the definition of each phenomenon, its frequency, and an illustrative example in Table 2. An example for each phenomena is given in Appendix G. For 4 of these phenomena unique to IMAGECODE, we further annotated 800 examples for the purpose of error analysis in Section 6. Inspecting these examples, we find a high number of cases where the visual context (47.0%) is required to complete the task. For instance, consider Figure 2: the description \"No bridesmaid visible at all.\" requires a retriever to resolve the co-references of the entities in 5 frames. In particular, the body parts of the bridesmaids (red boxes) visible in frames 2 and 4 would not be identifiable as such without frame 1 and 5, respectively (where they appear with matching dresses and flowers in their hands). A common example we find in the data are \"gradable\" scenarios, i.e. \"The person is looking down\" might be semantically true for more than one image but it fits best to the image where the person is looking down the most. Another group of phenomena characteristic for IMAGECODE originates from its minimally contrastive setup: annotators might focus on how an event unfolds over time (temporal context), on what is missing in a specific frame but visible in the others (negation), on what moved out of frame (visibility / occlusion), or on small regions and patches of pixels (nuances). Importantly, these phenomena are less prominent in static pictures than in video frames (cf. Table 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_38",
            "content": "Methods",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "361-ARR_v2_39",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "361-ARR_v2_40",
            "content": "In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark three state-of-the-art models that represent three main families of multimodal architectures Miech et al., 2021): i) ViL-BERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al., 2019); ii UNITER, a singlestream encoder where language and vision tokens are concatenated as inputs and processed with a single Transformer (Chen et al., 2020); iii) CLIP, a bi-encoder where language and vision streams are independent (Radford et al., 2021). It is worth noting that ViLBERT and UNITER are more expressive due to their architecture, whereas CLIP boasts a higher parameter count, is pre-trained on a larger dataset and uses a contrastive objective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_41",
            "content": "We evaluate these models under two different regimes: i) zero-shot inference, where pre-trained models are deployed on the IMAGECODE test set directly; and ii) fine-tuning, where the models are refined on the full training set before evaluation. We cast the training objective as binary classification for ViLBERT and as 10-class classification for CLIP. 11 Crucially, in both cases, positive and negative examples during training are sampled at random independently from the image set they belong to (see the first column of Figure 3). Thus, the visual context of the other images in a set is only indirectly accessible at inference time, where the image with the highest probability is predicted.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_42",
            "content": "Integrating Context into Vision-and-Language Models",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "361-ARR_v2_43",
            "content": "For the fine-tuning regime, we further investigate some modifications in the training setup and model architecture that facilitate the integration of visual and temporal context into the model. First, we use an alternative objective where all three models are trained on 10-class classification, but the 1 positive and 9 negatives are sourced from the same image set. The consequence of including positive and negative examples from the same image set in the same mini-batch is providing a wider visual context. We refer to this variant as +CONTEXTBATCH (second column of Figure 3). This setup only conveys the visual context as a weak signal, since the model has no chance to directly compare the images in the same set. Hence, we experiment with enhancing the architecture of vision-and-language models with a mechanism inspired by Bogin et al. (2021). In particular, given an encoder (CLIP, ViLBERT or UNITER), we obtain the representations of a contextual description x L \u2208 R e (where e is the model hidden size) and of the images in a set (x",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_44",
            "content": "(1) V , . . . , x (10) V ), x (i)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_45",
            "content": "V \u2208 R e from their final layer. 12 Then, we create a series of multimodal embeddings via element-wise multiplication:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_46",
            "content": "m = (x L \u2299 x (1) V , . . . , x L \u2299 x (10) V ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_47",
            "content": "Finally, we feed these to a l-layer Transformer Tf \u2236 R 10\u00d7e \u2192 R 10\u00d7e to obtain context-aware multimodal embeddings (Tf(m) 1 , . . . , Tf(m) 10 ). Since each description-image pair can now attend on the others in a set, the model can fully exploit the visual context. We obtain the score for the i-th pair through a linear classifier head W \u2208 R 1\u00d7e . The target image is predicted as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_48",
            "content": "arg max i softmax [W (Tf(m) i + m (i) )] (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_49",
            "content": "Note that we add a highway layer from the input to the output of the Transformer. We label this model variant +CONTEXTMODULE. Finally, in addition to visual context, we make models aware of the temporal context too, as shown in the fourth column of Figure 3. For videobased examples only, the multimodal embeddings of each description-image pair are summed with a learnable positional embedding t \u2208 R e that reflects the temporal order of the frames. 13 Thus, m = (x L \u2299x",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_50",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_51",
            "content": "V \u2295t (1) , . . . , x L \u2299x (10)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_52",
            "content": "V \u2295t (10) Transformer Transformer",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_53",
            "content": "Figure 3: Models with increasing levels of context integration: see Section 5 for more details. In the figure, we colour visual embeddings in red, text embeddings in blue, and positional embeddings in grey. POS is the score for the target image and NEG for the other candidates. \u229b represents dot product for CLIP and element-wise multiplication followed by a linear layer for ViLBERT/UNITER. \u2299 represents element-wise multiplication. For ease of exposition, we show 3 images instead of 10.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_54",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "361-ARR_v2_55",
            "content": "For all CLIP experiments, we use a pre-trained model with the vision backbone VIT-B/16. 14 We train the full models with a batch size of 360 examples (i.e., 36 image sets) for CLIP and 150 examples for ViLBERT/UNITER. We perform early stopping based on the validation accuracy with a maximum of 30 epochs. In the variants that adopt the base version of a model, we select a learning rate of 4 \u22c5 10 \u22126 for CLIP, 5 \u22c5 10 \u22126 for ViLBERT, 4 \u22c5 10 \u22125 for ViL-BERT+CONTEXTBATCH, 8 \u22c5 10 \u22126 for UNITER, and 7 \u22c5 10 \u22126 for UNITER++CONTEXTBATCH. We find these values via hyper-parameter search on the range [10 \u22124 , 10 \u22127 ].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_56",
            "content": "For CLIP variants that modify the model architecture, we adopt the following setup: first, we fine-tune the full model in the +CONTEXTBATCH regime as detailed above. Afterwards, we freeze the encoder parameters and train the components responsible for processing the multimodal embeddings, described in Equation (1). More details are provided in Appendix F. For ViLBERT and UNITER we finetune the whole architecture at the same time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_57",
            "content": "All descriptions in IMAGECODE exceeding the maximum length of the three models are truncated. Due to their negligible amount, this does not affect 14 https://github.com/openai/CLIP performance significantly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_58",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "361-ARR_v2_59",
            "content": "In Table 5, we report the performance of the models from Section 5 for all the test examples in IMAGE-CODE as well as for the subsets containing only video frames or static pictures (see Appendix E for validation scores). Note that the random chance baseline has an accuracy of 10%. In what follows, we compare the results across several dimensions. Zero-shot vs. fine-tuning. In the zero-shot setting, we observe that CLIP representations are surprisingly superior to UNITER/ViLBERT even though CLIP has separate streams to encode an image and its description. In the simplest fine-tuning setting (i.e., if negatives are randomly sampled independent of the image set), we find that overall there is only a small increase in performance compared to zero-shot inference. This demonstrates that in the regime where images in the same set do not appear in the same batch during training, models cannot extrapolate how to leverage the visual context at inference time. Adding context. For the fine-tuning regime, we observe instead a different trend once the visual context of the other images in a set is provided during training (+CONTEXTBATCH): CLIP and UNITER receive a significant boost in performance (i.e. +14.4% for CLIP), which is particularly accentuated for static pictures. On the other hand, ViLBERT's performance remains the same. Stacking a special module for contextualizing multimodal representations on top of the encoders (+CONTEXTMODULE), instead, yields gains for ViLBERT compared to +CON-TEXTBATCH, whereas CLIP and UNITER are unaffected (slight drop). This shows that all models can exploit visual context, but different strategies (contrastive training or dedicated modules) may be necessary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_60",
            "content": "Finally, all three models achieve the highest performance when fine-tuned with both visual and temporal context. Adding temporal positional embeddings on top of the contextual module (+TEMPORALEMBEDDINGS) yields an accuracy of 29.9 for CLIP, 25.7 for UNITER and 24.5 for ViL-BERT. Crucially, even the best-performing models lag significantly behind the (micro-averaged) human accuracy of 90.8 (cf. Table 3). Hence, despite some limited ability to integrate context, models are currently incapable of the fine-grained reasoning and pragmatic inferences needed to solve IM-AGECODE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_61",
            "content": "Pre-trained model. Across all model variants and training regimes, CLIP consistently achieves higher accuracy than ViLBERT or UNITER. This implies that a larger amount of parameters, pretraining examples or the contrastive objective are more beneficial than ViLBERT's or UNITER's more expressive model architecture. Thus, these results violate the expectations that attention between vision and language would be more suitable to jointly encode highly nuanced visual details and descriptions (Miech et al., 2021). Additionally UNITER slightly outperforms ViLBERT as its single-stream architecture might enable richer cross-modal interactions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_62",
            "content": "Video frames vs. static pictures. The highest accuracy on the subset of the data with video frames (20.9) is far lower than that for static pictures (59.4). This confirms that videos represent the main challenge in IMAGECODE, both because of the higher similarity of images in a set and of the particular factors of variation that help differentiate among them (cf. Section 4.3 and examples in Appendix G). Additionally, model performance on video frames seems to increase more consistently as more context (both visual and temporal) is provided, whereas there is no clear trend in the case of static pictures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_63",
            "content": "Error Analysis. On a broad level, we have seen that video frames are much more challenging for models. Next, to identify more fine-grained causes for the overall low performance of the vision-andlanguage models on IMAGECODE, we compute the Pearson's correlation between accuracy and a series of possible explanatory variables. In particular, we find a weak negative correlation with the number of tokens in the description (r = \u22120.11) and a weak positive correlation with the average pair-wise Euclidean distance between CLIP encodings of the images in a set (r = 0.22), which represents visual similarity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_64",
            "content": "By focusing on the 1000 annotated examples in Table 2 we observe a stark drop from overall performance on the subset of examples containing nuances, visibility/occlusion, and negation (Figure 4). This confirms insights from Kassner and Sch\u00fctze (2020) and Hosseini et al. (2021) on the difficulty of modeling negation in text-only models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_65",
            "content": "We keep data quality high through entry requirements (English speaking country, over 98% approval rate, etc.), qualification test, whitelisting workers and manually inspecting data. Most importantly our two-stage setup also allowed us to automate monitoring data quality as we could measure the description and retrieval accuracy of workers and only whitelisted those with high accuracy. We paid 0.25$ per description and 0.1$ per retrieval.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_66",
            "content": "The majority of descriptions in our test and validation split come from workers who did not work on the training set in order to avoid annotation bias. Our validation set contains 502 descriptions from workers \"seen\" from the training set and 1,800 description from \"unseen\" workers. In Table 6 we can see that models perform slightly better on seen workers across our CLIP model variants.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_67",
            "content": "Our AMT interface for the description task can be seen in Figure 6. The retriever interface looks conceptually similar, with a select-button for each image.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_68",
            "content": "The Transformer consists of 2 layers in CLIP variants and 4/5 layers in the ViLBERT/UNITER variants, both employing gelu activation. The learning rate for the fine-tuning of the Transformer and linear heads is 2 \u22c5 10 \u22126 for the CLIP +CON-TEXTMODULE, 10 \u22124 for CLIP +TEMPORALEM-BEDDINGS, 2 \u22c5 10 \u22125 for both ViLBERT variants, and 6 \u22c5 10 \u22126 for both UNITER variants. We use the Volta-framework for the standardized ViLBERT and UNITER model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_69",
            "content": "For each phenomenon we provide 1 example and a definition we used for annotation purposes. Since most examples contain more than one phenomenon, some phenomena will be effectively showcased several times. Note that we picked examples that are relatively easy to understand and spot differences in. While most examples based on video frames implicitly require some temporal knowledge, we focus on explicit textual mentions of 1) temporal markers (\"after\", \"during\", \"about to\", etc) and 2) temporal verbs (\"beginning to\", \"end to\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "361-ARR_v2_70",
            "content": "Jacob Andreas, Dan Klein, Reasoning about Pragmatics with Neural Listeners and Speakers, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Jacob Andreas",
                    "Dan Klein"
                ],
                "title": "Reasoning about Pragmatics with Neural Listeners and Speakers",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_71",
            "content": "Ankan Bansal, Yuting Zhang, Rama Chellappa, Visual question answering on image sets, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Ankan Bansal",
                    "Yuting Zhang",
                    "Rama Chellappa"
                ],
                "title": "Visual question answering on image sets",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_72",
            "content": "Ben Bogin, Shivanshu Gupta, Matt Gardner, Jonathan Berant, COVR: A test-bed for visually grounded compositional generalization with real images, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Ben Bogin",
                    "Shivanshu Gupta",
                    "Matt Gardner",
                    "Jonathan Berant"
                ],
                "title": "COVR: A test-bed for visually grounded compositional generalization with real images",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "361-ARR_v2_73",
            "content": "Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott, Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Emanuele Bugliarello",
                    "Ryan Cotterell",
                    "Naoaki Okazaki",
                    "Desmond Elliott"
                ],
                "title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_74",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: UNiversal Image-TExt Representation Learning, 2020, Computer Vision -ECCV 2020, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "UNITER: UNiversal Image-TExt Representation Learning",
                "pub_date": "2020",
                "pub_title": "Computer Vision -ECCV 2020",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "361-ARR_v2_75",
            "content": "Reuben Cohn-Gordon, Noah Goodman, Christopher Potts, Pragmatically informative image captioning with character-level inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Reuben Cohn-Gordon",
                    "Noah Goodman",
                    "Christopher Potts"
                ],
                "title": "Pragmatically informative image captioning with character-level inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_76",
            "content": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M Jos\u00e9, Devi Moura, Dhruv Parikh,  Batra, Visual dialog, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Abhishek Das",
                    "Satwik Kottur",
                    "Khushi Gupta",
                    "Avi Singh",
                    "Deshraj Yadav",
                    "M Jos\u00e9",
                    "Devi Moura",
                    "Dhruv Parikh",
                    " Batra"
                ],
                "title": "Visual dialog",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_77",
            "content": "Pradipto Das, Chenliang Xu, F Richard, Jason Doell,  Corso, A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching, 2013, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Pradipto Das",
                    "Chenliang Xu",
                    "F Richard",
                    "Jason Doell",
                    " Corso"
                ],
                "title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
                "pub_date": "2013",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_78",
            "content": "Florian Harm De Vries, Sarath Strub, Olivier Chandar, Hugo Pietquin, Aaron Larochelle,  Courville, Guesswhat?! visual object discovery through multi-modal dialogue, 2017, Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Florian Harm De Vries",
                    "Sarath Strub",
                    "Olivier Chandar",
                    "Hugo Pietquin",
                    "Aaron Larochelle",
                    " Courville"
                ],
                "title": "Guesswhat?! visual object discovery through multi-modal dialogue",
                "pub_date": "2017",
                "pub_title": "Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_79",
            "content": "UNKNOWN, None, 2001, Language, thought and compositionality. Royal Institute of Philosophy Supplements, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2001",
                "pub_title": "Language, thought and compositionality. Royal Institute of Philosophy Supplements",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_80",
            "content": "Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, Serge Belongie, Neural Naturalist: Generating Fine-Grained Image Comparisons, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Maxwell Forbes",
                    "Christine Kaeser-Chen",
                    "Piyush Sharma",
                    "Serge Belongie"
                ],
                "title": "Neural Naturalist: Generating Fine-Grained Image Comparisons",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_81",
            "content": "Mor Geva, Yoav Goldberg, Jonathan Berant, Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Mor Geva",
                    "Yoav Goldberg",
                    "Jonathan Berant"
                ],
                "title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "361-ARR_v2_82",
            "content": "D Noah, Michael C Goodman,  Frank, Pragmatic language interpretation as probabilistic inference, 2016, Trends in cognitive sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "D Noah",
                    "Michael C Goodman",
                    " Frank"
                ],
                "title": "Pragmatic language interpretation as probabilistic inference",
                "pub_date": "2016",
                "pub_title": "Trends in cognitive sciences",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_83",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_84",
            "content": "H Paul Grice, Utterer's Meaning and Intentions, 1957, The Philosophical Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    " H Paul Grice"
                ],
                "title": "Utterer's Meaning and Intentions",
                "pub_date": "1957",
                "pub_title": "The Philosophical Review",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_85",
            "content": "UNKNOWN, None, 2021, Probing Image-Language Transformers for Verb Understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Probing Image-Language Transformers for Verb Understanding",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_86",
            "content": "UNKNOWN, None, 2017, spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_87",
            "content": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, Devon Hjelm, Alessandro Sordoni, Aaron Courville, Understanding by understanding not: Modeling negation in language models, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Arian Hosseini",
                    "Siva Reddy",
                    "Dzmitry Bahdanau",
                    "Devon Hjelm",
                    "Alessandro Sordoni",
                    "Aaron Courville"
                ],
                "title": "Understanding by understanding not: Modeling negation in language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_88",
            "content": "Mehrdad Hosseinzadeh, Yang Wang, Image change captioning by learning from an auxiliary task, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Mehrdad Hosseinzadeh",
                    "Yang Wang"
                ],
                "title": "Image change captioning by learning from an auxiliary task",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_89",
            "content": "UNKNOWN, None, 2019, Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_90",
            "content": "A Drew, Christopher D Hudson,  Manning, Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "A Drew",
                    "Christopher D Hudson",
                    " Manning"
                ],
                "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_91",
            "content": "UNKNOWN, None, 2018, Learning to Describe Differences Between Pairs of Similar Images, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Learning to Describe Differences Between Pairs of Similar Images",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_92",
            "content": "Harsh Jhamtani, Taylor Berg-Kirkpatrick, Learning to describe differences between pairs of similar images, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Harsh Jhamtani",
                    "Taylor Berg-Kirkpatrick"
                ],
                "title": "Learning to describe differences between pairs of similar images",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "361-ARR_v2_93",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Nora Kassner",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_94",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Sahar Kazemzadeh",
                    "Vicente Ordonez",
                    "Mark Matten",
                    "Tamara Berg"
                ],
                "title": "Referitgame: Referring to objects in photographs of natural scenes",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_95",
            "content": "UNKNOWN, None, 2020, The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_96",
            "content": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli, Video storytelling: Textual summaries for events, 2019, IEEE Transactions on Multimedia, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Junnan Li",
                    "Yongkang Wong",
                    "Qi Zhao",
                    "Mohan Kankanhalli"
                ],
                "title": "Video storytelling: Textual summaries for events",
                "pub_date": "2019",
                "pub_title": "IEEE Transactions on Multimedia",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_97",
            "content": "Fangyu Liu, Emanuele Bugliarello, Maria Edoardo, Siva Ponti, Nigel Reddy, Desmond Collier,  Elliott, Visually grounded reasoning across languages and cultures, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Fangyu Liu",
                    "Emanuele Bugliarello",
                    "Maria Edoardo",
                    "Siva Ponti",
                    "Nigel Reddy",
                    "Desmond Collier",
                    " Elliott"
                ],
                "title": "Visually grounded reasoning across languages and cultures",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "361-ARR_v2_98",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Jiasen Lu",
                    "Dhruv Batra",
                    "Devi Parikh",
                    "Stefan Lee"
                ],
                "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_99",
            "content": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy, Generation and comprehension of unambiguous object descriptions, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Junhua Mao",
                    "Jonathan Huang",
                    "Alexander Toshev",
                    "Oana Camburu",
                    "Alan Yuille",
                    "Kevin Murphy"
                ],
                "title": "Generation and comprehension of unambiguous object descriptions",
                "pub_date": "2016",
                "pub_title": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_100",
            "content": "UNKNOWN, None, 2021, Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_101",
            "content": "Anish Mittal, Krishna Anush, Alan Moorthy,  Bovik, No-reference image quality assessment in the spatial domain, 2012, IEEE Transactions on image processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Anish Mittal",
                    "Krishna Anush",
                    "Alan Moorthy",
                    " Bovik"
                ],
                "title": "No-reference image quality assessment in the spatial domain",
                "pub_date": "2012",
                "pub_title": "IEEE Transactions on image processing",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_102",
            "content": "UNKNOWN, None, 2005, Grounding cognition: The role of perception and action in memory, language, and thinking, Cambridge University Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Grounding cognition: The role of perception and action in memory, language, and thinking",
                "pub": "Cambridge University Press"
            }
        },
        {
            "ix": "361-ARR_v2_103",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021, Proceedings of the 38th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark",
                    "Gretchen Krueger",
                    "Ilya Sutskever"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "361-ARR_v2_104",
            "content": "Ray Smith, An overview of the Tesseract OCR engine, 2007, Ninth international conference on document analysis and recognition, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Ray Smith"
                ],
                "title": "An overview of the Tesseract OCR engine",
                "pub_date": "2007",
                "pub_title": "Ninth international conference on document analysis and recognition",
                "pub": "IEEE"
            }
        },
        {
            "ix": "361-ARR_v2_105",
            "content": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, A Corpus for Reasoning about Natural Language Grounded in Photographs, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Alane Suhr",
                    "Stephanie Zhou",
                    "Ally Zhang",
                    "Iris Zhang",
                    "Huajun Bai",
                    "Yoav Artzi"
                ],
                "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "361-ARR_v2_106",
            "content": "UNKNOWN, None, , 2021. A list of color, emotion, and human body part concepts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. A list of color, emotion, and human body part concepts",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_107",
            "content": "Suramya Tomar, Converting video formats with ffmpeg, 2006, Linux Journal, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Suramya Tomar"
                ],
                "title": "Converting video formats with ffmpeg",
                "pub_date": "2006",
                "pub_title": "Linux Journal",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_108",
            "content": "Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik, Context-Aware Captions from Context-Agnostic Supervision, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Ramakrishna Vedantam",
                    "Samy Bengio",
                    "Kevin Murphy",
                    "Devi Parikh",
                    "Gal Chechik"
                ],
                "title": "Context-Aware Captions from Context-Agnostic Supervision",
                "pub_date": "2017",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "361-ARR_v2_109",
            "content": "UNKNOWN, None, 1998, Pragmatics and time. Pragmatics and Beyond New Series, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "1998",
                "pub_title": "Pragmatics and time. Pragmatics and Beyond New Series",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_110",
            "content": "UNKNOWN, None, 2019, Visual entailment: A novel task for fine-grained image understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visual entailment: A novel task for fine-grained image understanding",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_111",
            "content": "Jun Xu, Tao Mei, Ting Yao, Yong Rui, Msrvtt: A large video description dataset for bridging video and language, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Jun Xu",
                    "Tao Mei",
                    "Ting Yao",
                    "Yong Rui"
                ],
                "title": "Msrvtt: A large video description dataset for bridging video and language",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "361-ARR_v2_112",
            "content": "An Yan, Xin Wang, Tsu-Jui Fu, William Wang, L2C: Describing visual differences needs semantic understanding of individuals, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "An Yan",
                    "Xin Wang",
                    "Tsu-Jui Fu",
                    "William Wang"
                ],
                "title": "L2C: Describing visual differences needs semantic understanding of individuals",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "361-ARR_v2_0@0",
            "content": "Image Retrieval from Contextual Descriptions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_0",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@0",
            "content": "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@1",
            "content": "In order to measure to what extent current vision-and-language models master this ability, we propose a new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGECODE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 147,
            "end": 333,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@2",
            "content": "In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 335,
            "end": 485,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@3",
            "content": "As such, each description contains only the details that help distinguish between images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 487,
            "end": 575,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@4",
            "content": "Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 577,
            "end": 699,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@5",
            "content": "Images are sourced from both static pictures and video frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 701,
            "end": 762,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@6",
            "content": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on IMAGECODE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 764,
            "end": 898,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@7",
            "content": "Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 900,
            "end": 1099,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@8",
            "content": "Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 1101,
            "end": 1274,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@9",
            "content": "Our hope is that IMAGECODE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 1276,
            "end": 1424,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@10",
            "content": "We make code and dataset publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 1426,
            "end": 1469,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_2@11",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_2",
            "start": 1471,
            "end": 1471,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_4@0",
            "content": "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker's intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_4",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_4@1",
            "content": "These processes, more generally, \"The girl in blue is to the left of the girl in the middle with the purple shoes. The girl in blue is not obscured in any way.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_4",
            "start": 303,
            "end": 462,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_4@2",
            "content": "Frames 5-10 are left out for simplicity's sake.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_4",
            "start": 464,
            "end": 510,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_4@3",
            "content": "The target image, frame 3, is in green, whereas the incorrect frames are in red.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_4",
            "start": 512,
            "end": 591,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_5@0",
            "content": "fall under the umbrella term of pragmatics (Grice, 1957).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_5",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_5@1",
            "content": "Despite recent progress in multimodal systems, it remains unclear to which extent they can handle settings where context plays a major role, such as in real-world communication.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_5",
            "start": 58,
            "end": 234,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_6@0",
            "content": "To this end, we present a new challenge that requires multimodal models to leverage context to retrieve images from text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_6",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_6@1",
            "content": "In particular, given a contextual description and a set of minimally contrastive candidate images, i.e. differing only in some details, the model has to retrieve the target image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_6",
            "start": 122,
            "end": 300,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_6@2",
            "content": "In order to discriminate between similar images, human annotators naturally produce highly nuanced and grammatically complex descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_6",
            "start": 302,
            "end": 439,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_6@3",
            "content": "An example of our new challenging dataset, Image Retrieval from Contextual Descriptions (IMAGECODE), is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_6",
            "start": 441,
            "end": 562,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_7@0",
            "content": "During the data collection process, sets of similar images are selected among static pictures from Open Images (Kuznetsova et al., 2020) and (a larger portion) among video frames from diverse domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_7",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_8@0",
            "content": "Including both types of images allows for diversifying the dataset while representing different degrees of visual similarity within each set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_8",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_8@1",
            "content": "Next, we crowdsource a contextual description of a target image (presented together with the rest of the set) that contains only differences relevant for retrieval.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_8",
            "start": 142,
            "end": 305,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_8@2",
            "content": "After a filtering phase involving human retrievers, we obtain a large-scale dataset with 94,020 images and 21,202 descriptions associated with image sets of size 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_8",
            "start": 307,
            "end": 471,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_9@0",
            "content": "As a result of this annotation protocol, successfully completing the task requires models to integrate several kinds of context: i) the image set, as the descriptions often only make sense in the context of several other images and are not suitable as stand-alone captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_9",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_9@1",
            "content": "In fact, aspects of the image that are very salient and that therefore would normally be emphasized are not useful in our proposed task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_9",
            "start": 274,
            "end": 409,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_9@2",
            "content": "Instead, the focus of our descriptions are finegrained details that help discriminate between images (see Figure 1); ii) the speaker's intention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_9",
            "start": 411,
            "end": 555,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_9@3",
            "content": "Due to their high degree of image similarity, contextual descriptions may be literally true for multiple images; however, once the speaker's intention is taken into account, the correct image can be determined by virtue of pragmatics, i.e. Grice's maxim of quality 2 (see Figure 2, Figure 7); iii) temporal sequences: for video frames temporal reasoning is also required to compare different moments of an unfolding event.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_9",
            "start": 557,
            "end": 978,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_10@0",
            "content": "On our new dataset IMAGECODE, we benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, specifically ViLBERT (Lu et al., 2019) and UNITER (Chen et al., 2020) as two cross-encoder variants and CLIP as a strong biencoder (Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_10",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_10@1",
            "content": "We report several findings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_10",
            "start": 306,
            "end": 332,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_10@2",
            "content": "First, accuracy on static images is vastly superior than on video frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_10",
            "start": 334,
            "end": 406,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_10@3",
            "content": "Therefore, the degree of similarity among the candidate images has an overwhelming impact on retrieval performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_10",
            "start": 408,
            "end": 522,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_10@4",
            "content": "Second, all state-of-the-art models generally struggle with image retrieval from contextual descriptions, whereas humans consistently achieve high accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_10",
            "start": 524,
            "end": 679,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_11@0",
            "content": "Hence, we propose model variants capable of better taking context into account: i) once an imagedescription pair is encoded, we refine this representation by attending to the other images in the set;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_11",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_12@0",
            "content": "ii) we augment image encodings with temporal embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_12",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_12@1",
            "content": "Based on our results, models take advantage of this additional information fruitfully but only to a limited degree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_12",
            "start": 57,
            "end": 171,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_13@0",
            "content": "Because of its challenging nature, due to the minimally contrastive images and complex descriptions, we believe that IMAGECODE will help make visio-linguistic models more context-aware and sensitive to fine-grained details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_13",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_14@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_14",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_15@0",
            "content": "There is a long tradition of grounding language understanding on single images, in the form of visual question answering (Goyal et al., 2017;Hudson and Manning, 2019), visual dialogue (de Vries et al., 2017;Das et al., 2017), or visual entailment (Xie et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_15",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_15@1",
            "content": "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017;Hu et al., 2019;Suhr et al., 2019;Forbes et al., 2019;Hendricks and Nematzadeh, 2021;Yan et al., 2021;Hosseinzadeh and Wang, 2021;Bogin et al., 2021;Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a;Bansal et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_15",
            "start": 267,
            "end": 682,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_15@2",
            "content": "While many of these benchmarks involve just two images, COVR (Bogin et al., 2021) and ISVQA (Bansal et al., 2020) provide more images, similar to our sets of 10 images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_15",
            "start": 684,
            "end": 851,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_16@0",
            "content": "ISVQA and Spot-the-diff (Jhamtani and Berg-Kirkpatrick, 2018a) are most similar to our dataset, IMAGECODE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_16",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_16@1",
            "content": "ISVQA is based on several video frames that are synthetic and cover a restricted domain, with short questions for Visual Question Answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_16",
            "start": 107,
            "end": 246,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_16@2",
            "content": "Spot-the-diff provides two frames from surveillance video cameras and descriptions of all their differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_16",
            "start": 248,
            "end": 355,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_16@3",
            "content": "IMAGECODE is unique as a) we cover a wider range of domains; b) we construct image sets that are maximally similar while being distinguishable through natural language (Section 3) and c) we limit descriptions to relevant differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_16",
            "start": 357,
            "end": 589,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_16@4",
            "content": "This results in (a) diverse, (b) complex and (c) pragmatically informative descriptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_16",
            "start": 591,
            "end": 678,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_17@0",
            "content": "We do not claim to explicitly model pragmatics in this paper, i.e. with Rational Speech Acts (Goodman and Frank, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_17",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_17@1",
            "content": "Instead we present a dataset that is naturally suitable for pragmatic reasoning (Andreas and Klein, 2016;Cohn-Gordon et al., 2018) as a listener has to consider the context, assume a Gricean speaker and resolve ambiguities resulting from nuanced differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_17",
            "start": 120,
            "end": 377,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_17@2",
            "content": "The reasoning in our task and data collection is therefore also similar to ReferItGame and subsequent work (Kazemzadeh et al., 2014;Mao et al., 2016) where one crowdworker generates a referring expressing for an object in a single image and another worker picks an object based on the expression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_17",
            "start": 379,
            "end": 674,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_18@0",
            "content": "Data Collection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_18",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_19@0",
            "content": "Our data collection involves two steps with a human describer and retriever.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_19",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_19@1",
            "content": "The describer is given a set of 10 highly similar images S = [I 1 , I 2 , ..., I 10 ], one of them marked as the target image I t , and has to write a description D that clearly distinguishes I t from the other distractor images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_19",
            "start": 77,
            "end": 305,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_19@2",
            "content": "In the second step, the retriever is given the same 10 images and the description from the first step and has to identify the target image based on the description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_19",
            "start": 307,
            "end": 470,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_19@3",
            "content": "S and D are only added to our dataset if the retrieval is successful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_19",
            "start": 472,
            "end": 540,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_20@0",
            "content": "Below, we outline the main stages of data collection: first, the collection of similar, contrastive images in Section 3.1. Then, the crowdsourcing of contextual descriptions in Section 3.2 and validation of the examples via image retrieval (Section 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_20",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_20@1",
            "content": "The final IMAGECODE dataset consists of 94,020 images (partitioned into 9,402 sets) and 21,202 contextual descriptions (16,594 in the train split, 2,302 and 2,306 in the validation and test split respectively).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_20",
            "start": 255,
            "end": 464,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_21@0",
            "content": "Collecting Similar Images",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_21",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@0",
            "content": "In the first stage, we collect sets of images that are highly similar but still distinguishable from each other by a human.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@1",
            "content": "To quantitatively measure the pairwise similarity of two images, we compute the Euclidean distance between their encodings extracted from a pre-trained CLIP model (Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 124,
            "end": 309,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@2",
            "content": "3 To study the effect of different degrees of similarity, further variegate our dataset, and enable temporal reasoning, we source our candidate images from collections of static pictures as well as videos, as detailed below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 311,
            "end": 534,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@3",
            "content": "Static Pictures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 536,
            "end": 551,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@4",
            "content": "We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6 (Kuznetsova et al., 2020), containing 1.74M images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 553,
            "end": 708,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@5",
            "content": "For each image, we retrieve the 9 closest images from the training set based on their CLIP encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 710,
            "end": 810,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@6",
            "content": "We then randomly sample 4,845 of these image sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 812,
            "end": 861,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@7",
            "content": "Video Frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 863,
            "end": 875,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@8",
            "content": "As sources for our video frames, we use i) Video-Storytelling (Li et al., 2019), covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT (Xu et al., 2016); and iii) YouCook (Das et al., 2013), covering cooking events.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 877,
            "end": 1128,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@9",
            "content": "We choose these datasets as they contain publicly available and general-purpose videos (not specific to downstream tasks).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 1130,
            "end": 1251,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_22@10",
            "content": "We retain the original splits for train, validation, and test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_22",
            "start": 1253,
            "end": 1314,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_23@0",
            "content": "To obtain disjoint sets of 10 similar frames, we first segment the videos into smaller scenes (also known as shots) via the scene detection functionality of ffmpeg (Tomar, 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_23",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_23@1",
            "content": "Then, for each scene, we add its first frame to the set of selected images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_23",
            "start": 179,
            "end": 253,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_23@2",
            "content": "We then iterate over every following frame and add it to the set if its pairwise Euclidean distance with each of the previously selected frames is larger than a threshold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_23",
            "start": 255,
            "end": 425,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_23@3",
            "content": "4 Once the set contains 10 images, we reiterate the procedure for a new set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_23",
            "start": 427,
            "end": 502,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_23@4",
            "content": "If the scene ends and the current set contains less than 10 images, the set is discarded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_23",
            "start": 504,
            "end": 592,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_24@0",
            "content": "During this process, we additionally remove frames that i) are too blurry, i.e. their BRISQUE score (Mittal et al., 2012) is larger than 0.65; or ii) contain too much text, which is detected with the OCR tool Tesseract (Smith, 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_24",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_24@1",
            "content": "5 We use all of YouCook's image sets and (due to cost constraints) randomly sample image sets from Video-Storytelling and MSR-VTT for crowdsourcing (cf. Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_24",
            "start": 234,
            "end": 395,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_24@2",
            "content": "We remark that image sets are further filtered at the final stage of annotation (Section 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_24",
            "start": 397,
            "end": 490,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_25@0",
            "content": "Crowdsourcing Contextual Descriptions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_25",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@0",
            "content": "After creating sets of highly-similar images in Section 3.1, we request annotators from Amazon Mechanical Turk (AMT) to write contextual descriptions for each target image in a set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@1",
            "content": "Each round, a set of images is presented in random order for static pictures and respecting temporal order for video frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 182,
            "end": 305,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@2",
            "content": "This encourages annotators to take the dynamics of the event into account.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 307,
            "end": 380,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@3",
            "content": "We then (randomly) select 3 target images per set, and ask annotators to produce a description that discriminates them from the other images in the set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 382,
            "end": 533,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@4",
            "content": "To encourage pragmatic reasoning, we do not ask for all the differences (just those sufficient for retrieval) and do not allow explicit mentions of other images (see Figure 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 535,
            "end": 710,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@5",
            "content": "We select high-quality annotators according to criteria in Appendix B and assign partly disjoint sets of annotators to train and test in order to avoid annotator bias (Geva et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 712,
            "end": 898,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_26@6",
            "content": "6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_26",
            "start": 900,
            "end": 900,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_27@0",
            "content": "Human Validation via Image Retrieval",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_27",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_28@0",
            "content": "Finally, we validate the annotation crowdsourced in Section 3.2 by asking AMT workers to retrieve the correct target image from a set given its contextual description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_28",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_28@1",
            "content": "For the final dataset, we retained only the examples that i) were retrieved successfully in the training set by a single worker or ii) were retrieved successfully by at least 2 out of 3 workers in the validation and test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_28",
            "start": 168,
            "end": 393,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_28@2",
            "content": "As a consequence, we filtered out 26.5% of the contextual descriptions generated in Section 3.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_28",
            "start": 395,
            "end": 490,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_29@0",
            "content": "Data Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_29",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_30@0",
            "content": "Human Accuracy and Agreement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_30",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@0",
            "content": "To quantify the reliability of the process outlined in Section 3, we report the inter-annotator agreement on our final dataset in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@1",
            "content": "We use Krippendorff's \u03b1 as a metric (the higher the better), which accounts for incomplete data, since the number of annotators per example is not fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 139,
            "end": 291,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@2",
            "content": "We treat the index of the target image either as a nominal variable for static images or as an ordinal variable for video frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 293,
            "end": 421,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@3",
            "content": "In both cases, we find a high degree of agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 423,
            "end": 472,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@4",
            "content": "Moreover, in Table 3, we also report human accuracy-the percentage of times an annotator retrieved the correct target image from a contextual description (as described in Section 3.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 474,
            "end": 657,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_31@5",
            "content": "This provides an upper ceiling for the model performances (see Section 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_31",
            "start": 659,
            "end": 732,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_32@0",
            "content": "Language Statistics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_32",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_33@0",
            "content": "In Table 4, we measure a series of statistics of the descriptions collected for IMAGECODE and compare them with other vision-and-language datasets with multiple naturalistic images (cf. Section 2), such as NLVR2 (Suhr et al., 2019) and Spot-thediff (Jhamtani and Berg-Kirkpatrick, 2018b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_33",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_33@1",
            "content": "8 In particular, we count the average description length, the number of distinct word types, the average dependency tree depth of each sentence, 9 and the average number of sentences per description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_33",
            "start": 289,
            "end": 487,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_33@2",
            "content": "Based on these metrics, we find evidence that IMAGE-CODE's descriptions are longer and more syntactically complex than in the other datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_33",
            "start": 489,
            "end": 629,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_33@3",
            "content": "Moreover, they include multiple sentences (11.8% of examples have 3 or more).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_33",
            "start": 631,
            "end": 707,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_34@0",
            "content": "Vision Statistics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_34",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_35@0",
            "content": "By calculating the average pairwise Euclidean distance between CLIP-based encodings of images in the same set, we find that video frames are more similar than static pictures -as expected -by a factor of 1.13.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_35",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_35@1",
            "content": "Moreover, we find that descriptions of video frames mention human body parts (72.1%) more often than static pictures (30.2%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_35",
            "start": 210,
            "end": 334,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_35@2",
            "content": "On the other hand, names of colors appear in descriptions of static pictures (61.4%) more frequently than video frames (33.6%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_35",
            "start": 336,
            "end": 462,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_35@3",
            "content": "10 Thus, annotators resort to different strategies to discriminate between different types of image sets, focusing on the aspects that vary the most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_35",
            "start": 464,
            "end": 612,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_36@0",
            "content": "Challenging Phenomena",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_36",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@0",
            "content": "Finally, we identify 9 interesting and challenging phenomena in IMAGECODE and annotate whether they are present in 200 examples from the validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@1",
            "content": "We provide the definition of each phenomenon, its frequency, and an illustrative example in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 153,
            "end": 252,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@2",
            "content": "An example for each phenomena is given in Appendix G. For 4 of these phenomena unique to IMAGECODE, we further annotated 800 examples for the purpose of error analysis in Section 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 254,
            "end": 434,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@3",
            "content": "Inspecting these examples, we find a high number of cases where the visual context (47.0%) is required to complete the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 436,
            "end": 559,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@4",
            "content": "For instance, consider Figure 2: the description \"No bridesmaid visible at all.\" requires a retriever to resolve the co-references of the entities in 5 frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 561,
            "end": 719,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@5",
            "content": "In particular, the body parts of the bridesmaids (red boxes) visible in frames 2 and 4 would not be identifiable as such without frame 1 and 5, respectively (where they appear with matching dresses and flowers in their hands).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 721,
            "end": 946,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@6",
            "content": "A common example we find in the data are \"gradable\" scenarios, i.e. \"The person is looking down\" might be semantically true for more than one image but it fits best to the image where the person is looking down the most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 948,
            "end": 1167,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@7",
            "content": "Another group of phenomena characteristic for IMAGECODE originates from its minimally contrastive setup: annotators might focus on how an event unfolds over time (temporal context), on what is missing in a specific frame but visible in the others (negation), on what moved out of frame (visibility / occlusion), or on small regions and patches of pixels (nuances).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 1169,
            "end": 1532,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_37@8",
            "content": "Importantly, these phenomena are less prominent in static pictures than in video frames (cf. Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_37",
            "start": 1534,
            "end": 1635,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_38@0",
            "content": "Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_38",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_39@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_39",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_40@0",
            "content": "In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark three state-of-the-art models that represent three main families of multimodal architectures Miech et al., 2021): i) ViL-BERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al., 2019); ii UNITER, a singlestream encoder where language and vision tokens are concatenated as inputs and processed with a single Transformer (Chen et al., 2020); iii) CLIP, a bi-encoder where language and vision streams are independent (Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_40",
            "start": 0,
            "end": 653,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_40@1",
            "content": "It is worth noting that ViLBERT and UNITER are more expressive due to their architecture, whereas CLIP boasts a higher parameter count, is pre-trained on a larger dataset and uses a contrastive objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_40",
            "start": 655,
            "end": 858,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_41@0",
            "content": "We evaluate these models under two different regimes: i) zero-shot inference, where pre-trained models are deployed on the IMAGECODE test set directly; and ii) fine-tuning, where the models are refined on the full training set before evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_41",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_41@1",
            "content": "We cast the training objective as binary classification for ViLBERT and as 10-class classification for CLIP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_41",
            "start": 246,
            "end": 353,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_41@2",
            "content": "11 Crucially, in both cases, positive and negative examples during training are sampled at random independently from the image set they belong to (see the first column of Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_41",
            "start": 355,
            "end": 535,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_41@3",
            "content": "Thus, the visual context of the other images in a set is only indirectly accessible at inference time, where the image with the highest probability is predicted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_41",
            "start": 537,
            "end": 697,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_42@0",
            "content": "Integrating Context into Vision-and-Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_42",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@0",
            "content": "For the fine-tuning regime, we further investigate some modifications in the training setup and model architecture that facilitate the integration of visual and temporal context into the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@1",
            "content": "First, we use an alternative objective where all three models are trained on 10-class classification, but the 1 positive and 9 negatives are sourced from the same image set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 194,
            "end": 366,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@2",
            "content": "The consequence of including positive and negative examples from the same image set in the same mini-batch is providing a wider visual context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 368,
            "end": 510,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@3",
            "content": "We refer to this variant as +CONTEXTBATCH (second column of Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 512,
            "end": 581,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@4",
            "content": "This setup only conveys the visual context as a weak signal, since the model has no chance to directly compare the images in the same set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 583,
            "end": 720,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@5",
            "content": "Hence, we experiment with enhancing the architecture of vision-and-language models with a mechanism inspired by Bogin et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 722,
            "end": 853,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_43@6",
            "content": "In particular, given an encoder (CLIP, ViLBERT or UNITER), we obtain the representations of a contextual description x L \u2208 R e (where e is the model hidden size) and of the images in a set (x",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_43",
            "start": 855,
            "end": 1045,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_44@0",
            "content": "(1) V , . . . , x (10) V ), x (i)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_44",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_45@0",
            "content": "V \u2208 R e from their final layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_45",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_45@1",
            "content": "12 Then, we create a series of multimodal embeddings via element-wise multiplication:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_45",
            "start": 32,
            "end": 116,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_46@0",
            "content": "m = (x L \u2299 x (1) V , . . . , x L \u2299 x (10) V ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_46",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_47@0",
            "content": "Finally, we feed these to a l-layer Transformer Tf \u2236 R 10\u00d7e \u2192 R 10\u00d7e to obtain context-aware multimodal embeddings (Tf(m) 1 , . . . , Tf(m) 10 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_47",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_47@1",
            "content": "Since each description-image pair can now attend on the others in a set, the model can fully exploit the visual context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_47",
            "start": 146,
            "end": 265,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_47@2",
            "content": "We obtain the score for the i-th pair through a linear classifier head W \u2208 R 1\u00d7e .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_47",
            "start": 267,
            "end": 348,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_47@3",
            "content": "The target image is predicted as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_47",
            "start": 350,
            "end": 381,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_48@0",
            "content": "arg max i softmax [W (Tf(m) i + m (i) )] (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_48",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_49@0",
            "content": "Note that we add a highway layer from the input to the output of the Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_49",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_49@1",
            "content": "We label this model variant +CONTEXTMODULE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_49",
            "start": 82,
            "end": 124,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_49@2",
            "content": "Finally, in addition to visual context, we make models aware of the temporal context too, as shown in the fourth column of Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_49",
            "start": 126,
            "end": 257,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_49@3",
            "content": "For videobased examples only, the multimodal embeddings of each description-image pair are summed with a learnable positional embedding t \u2208 R e that reflects the temporal order of the frames.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_49",
            "start": 259,
            "end": 449,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_49@4",
            "content": "13 Thus, m = (x L \u2299x",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_49",
            "start": 451,
            "end": 470,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_50@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_50",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_51@0",
            "content": "V \u2295t (1) , . . . , x L \u2299x (10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_51",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_52@0",
            "content": "V \u2295t (10) Transformer Transformer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_52",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@0",
            "content": "Figure 3: Models with increasing levels of context integration: see Section 5 for more details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@1",
            "content": "In the figure, we colour visual embeddings in red, text embeddings in blue, and positional embeddings in grey.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 96,
            "end": 205,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@2",
            "content": "POS is the score for the target image and NEG for the other candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 207,
            "end": 277,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@3",
            "content": "\u229b represents dot product for CLIP and element-wise multiplication followed by a linear layer for ViLBERT/UNITER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 279,
            "end": 390,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@4",
            "content": "\u2299 represents element-wise multiplication.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 392,
            "end": 432,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_53@5",
            "content": "For ease of exposition, we show 3 images instead of 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_53",
            "start": 434,
            "end": 488,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_54@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_54",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_55@0",
            "content": "For all CLIP experiments, we use a pre-trained model with the vision backbone VIT-B/16.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_55",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_55@1",
            "content": "14 We train the full models with a batch size of 360 examples (i.e., 36 image sets) for CLIP and 150 examples for ViLBERT/UNITER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_55",
            "start": 88,
            "end": 216,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_55@2",
            "content": "We perform early stopping based on the validation accuracy with a maximum of 30 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_55",
            "start": 218,
            "end": 304,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_55@3",
            "content": "In the variants that adopt the base version of a model, we select a learning rate of 4 \u22c5 10 \u22126 for CLIP, 5 \u22c5 10 \u22126 for ViLBERT, 4 \u22c5 10 \u22125 for ViL-BERT+CONTEXTBATCH, 8 \u22c5 10 \u22126 for UNITER, and 7 \u22c5 10 \u22126 for UNITER++CONTEXTBATCH.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_55",
            "start": 306,
            "end": 531,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_55@4",
            "content": "We find these values via hyper-parameter search on the range [10 \u22124 , 10 \u22127 ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_55",
            "start": 533,
            "end": 610,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_56@0",
            "content": "For CLIP variants that modify the model architecture, we adopt the following setup: first, we fine-tune the full model in the +CONTEXTBATCH regime as detailed above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_56",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_56@1",
            "content": "Afterwards, we freeze the encoder parameters and train the components responsible for processing the multimodal embeddings, described in Equation (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_56",
            "start": 166,
            "end": 315,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_56@2",
            "content": "More details are provided in Appendix F. For ViLBERT and UNITER we finetune the whole architecture at the same time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_56",
            "start": 317,
            "end": 432,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_57@0",
            "content": "All descriptions in IMAGECODE exceeding the maximum length of the three models are truncated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_57",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_57@1",
            "content": "Due to their negligible amount, this does not affect 14 https://github.com/openai/CLIP performance significantly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_57",
            "start": 94,
            "end": 206,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_58@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_58",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@0",
            "content": "In Table 5, we report the performance of the models from Section 5 for all the test examples in IMAGE-CODE as well as for the subsets containing only video frames or static pictures (see Appendix E for validation scores).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@1",
            "content": "Note that the random chance baseline has an accuracy of 10%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 222,
            "end": 281,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@2",
            "content": "In what follows, we compare the results across several dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 283,
            "end": 348,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@3",
            "content": "Zero-shot vs. fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 350,
            "end": 375,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@4",
            "content": "In the zero-shot setting, we observe that CLIP representations are surprisingly superior to UNITER/ViLBERT even though CLIP has separate streams to encode an image and its description.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 377,
            "end": 560,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@5",
            "content": "In the simplest fine-tuning setting (i.e., if negatives are randomly sampled independent of the image set), we find that overall there is only a small increase in performance compared to zero-shot inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 562,
            "end": 768,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@6",
            "content": "This demonstrates that in the regime where images in the same set do not appear in the same batch during training, models cannot extrapolate how to leverage the visual context at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 770,
            "end": 963,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@7",
            "content": "Adding context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 965,
            "end": 979,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@8",
            "content": "For the fine-tuning regime, we observe instead a different trend once the visual context of the other images in a set is provided during training (+CONTEXTBATCH): CLIP and UNITER receive a significant boost in performance (i.e. +14.4% for CLIP), which is particularly accentuated for static pictures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 981,
            "end": 1280,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@9",
            "content": "On the other hand, ViLBERT's performance remains the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 1282,
            "end": 1339,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@10",
            "content": "Stacking a special module for contextualizing multimodal representations on top of the encoders (+CONTEXTMODULE), instead, yields gains for ViLBERT compared to +CON-TEXTBATCH, whereas CLIP and UNITER are unaffected (slight drop).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 1341,
            "end": 1569,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_59@11",
            "content": "This shows that all models can exploit visual context, but different strategies (contrastive training or dedicated modules) may be necessary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_59",
            "start": 1571,
            "end": 1711,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_60@0",
            "content": "Finally, all three models achieve the highest performance when fine-tuned with both visual and temporal context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_60",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_60@1",
            "content": "Adding temporal positional embeddings on top of the contextual module (+TEMPORALEMBEDDINGS) yields an accuracy of 29.9 for CLIP, 25.7 for UNITER and 24.5 for ViL-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_60",
            "start": 113,
            "end": 279,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_60@2",
            "content": "Crucially, even the best-performing models lag significantly behind the (micro-averaged) human accuracy of 90.8 (cf. Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_60",
            "start": 281,
            "end": 406,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_60@3",
            "content": "Hence, despite some limited ability to integrate context, models are currently incapable of the fine-grained reasoning and pragmatic inferences needed to solve IM-AGECODE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_60",
            "start": 408,
            "end": 578,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_61@0",
            "content": "Pre-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_61",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_61@1",
            "content": "Across all model variants and training regimes, CLIP consistently achieves higher accuracy than ViLBERT or UNITER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_61",
            "start": 19,
            "end": 132,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_61@2",
            "content": "This implies that a larger amount of parameters, pretraining examples or the contrastive objective are more beneficial than ViLBERT's or UNITER's more expressive model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_61",
            "start": 134,
            "end": 314,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_61@3",
            "content": "Thus, these results violate the expectations that attention between vision and language would be more suitable to jointly encode highly nuanced visual details and descriptions (Miech et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_61",
            "start": 316,
            "end": 512,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_61@4",
            "content": "Additionally UNITER slightly outperforms ViLBERT as its single-stream architecture might enable richer cross-modal interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_61",
            "start": 514,
            "end": 641,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_62@0",
            "content": "Video frames vs. static pictures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_62",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_62@1",
            "content": "The highest accuracy on the subset of the data with video frames (20.9) is far lower than that for static pictures (59.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_62",
            "start": 34,
            "end": 155,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_62@2",
            "content": "This confirms that videos represent the main challenge in IMAGECODE, both because of the higher similarity of images in a set and of the particular factors of variation that help differentiate among them (cf. Section 4.3 and examples in Appendix G).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_62",
            "start": 157,
            "end": 405,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_62@3",
            "content": "Additionally, model performance on video frames seems to increase more consistently as more context (both visual and temporal) is provided, whereas there is no clear trend in the case of static pictures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_62",
            "start": 407,
            "end": 609,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_63@0",
            "content": "Error Analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_63",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_63@1",
            "content": "On a broad level, we have seen that video frames are much more challenging for models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_63",
            "start": 16,
            "end": 101,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_63@2",
            "content": "Next, to identify more fine-grained causes for the overall low performance of the vision-andlanguage models on IMAGECODE, we compute the Pearson's correlation between accuracy and a series of possible explanatory variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_63",
            "start": 103,
            "end": 325,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_63@3",
            "content": "In particular, we find a weak negative correlation with the number of tokens in the description (r = \u22120.11) and a weak positive correlation with the average pair-wise Euclidean distance between CLIP encodings of the images in a set (r = 0.22), which represents visual similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_63",
            "start": 327,
            "end": 605,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_64@0",
            "content": "By focusing on the 1000 annotated examples in Table 2 we observe a stark drop from overall performance on the subset of examples containing nuances, visibility/occlusion, and negation (Figure 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_64",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_64@1",
            "content": "This confirms insights from Kassner and Sch\u00fctze (2020) and Hosseini et al. (2021) on the difficulty of modeling negation in text-only models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_64",
            "start": 196,
            "end": 336,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_65@0",
            "content": "We keep data quality high through entry requirements (English speaking country, over 98% approval rate, etc.), qualification test, whitelisting workers and manually inspecting data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_65",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_65@1",
            "content": "Most importantly our two-stage setup also allowed us to automate monitoring data quality as we could measure the description and retrieval accuracy of workers and only whitelisted those with high accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_65",
            "start": 182,
            "end": 386,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_65@2",
            "content": "We paid 0.25$ per description and 0.1$ per retrieval.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_65",
            "start": 388,
            "end": 440,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_66@0",
            "content": "The majority of descriptions in our test and validation split come from workers who did not work on the training set in order to avoid annotation bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_66",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_66@1",
            "content": "Our validation set contains 502 descriptions from workers \"seen\" from the training set and 1,800 description from \"unseen\" workers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_66",
            "start": 152,
            "end": 282,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_66@2",
            "content": "In Table 6 we can see that models perform slightly better on seen workers across our CLIP model variants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_66",
            "start": 284,
            "end": 388,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_67@0",
            "content": "Our AMT interface for the description task can be seen in Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_67",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_67@1",
            "content": "The retriever interface looks conceptually similar, with a select-button for each image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_67",
            "start": 68,
            "end": 155,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_68@0",
            "content": "The Transformer consists of 2 layers in CLIP variants and 4/5 layers in the ViLBERT/UNITER variants, both employing gelu activation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_68",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_68@1",
            "content": "The learning rate for the fine-tuning of the Transformer and linear heads is 2 \u22c5 10 \u22126 for the CLIP +CON-TEXTMODULE, 10 \u22124 for CLIP +TEMPORALEM-BEDDINGS, 2 \u22c5 10 \u22125 for both ViLBERT variants, and 6 \u22c5 10 \u22126 for both UNITER variants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_68",
            "start": 133,
            "end": 362,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_68@2",
            "content": "We use the Volta-framework for the standardized ViLBERT and UNITER model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_68",
            "start": 364,
            "end": 436,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@0",
            "content": "For each phenomenon we provide 1 example and a definition we used for annotation purposes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@1",
            "content": "Since most examples contain more than one phenomenon, some phenomena will be effectively showcased several times.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 91,
            "end": 203,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@2",
            "content": "Note that we picked examples that are relatively easy to understand and spot differences in.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 205,
            "end": 296,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@3",
            "content": "While most examples based on video frames implicitly require some temporal knowledge, we focus on explicit textual mentions of",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 298,
            "end": 423,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@4",
            "content": "1) temporal markers (\"after\", \"during\", \"about to\", etc) and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 425,
            "end": 484,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_69@5",
            "content": "2) temporal verbs (\"beginning to\", \"end to\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_69",
            "start": 486,
            "end": 530,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_70@0",
            "content": "Jacob Andreas, Dan Klein, Reasoning about Pragmatics with Neural Listeners and Speakers, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_70",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_71@0",
            "content": "Ankan Bansal, Yuting Zhang, Rama Chellappa, Visual question answering on image sets, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_71",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_72@0",
            "content": "Ben Bogin, Shivanshu Gupta, Matt Gardner, Jonathan Berant, COVR: A test-bed for visually grounded compositional generalization with real images, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_72",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_73@0",
            "content": "Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott, Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_73",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_74@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: UNiversal Image-TExt Representation Learning, 2020, Computer Vision -ECCV 2020, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_74",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_75@0",
            "content": "Reuben Cohn-Gordon, Noah Goodman, Christopher Potts, Pragmatically informative image captioning with character-level inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_75",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_76@0",
            "content": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M Jos\u00e9, Devi Moura, Dhruv Parikh,  Batra, Visual dialog, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_76",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_77@0",
            "content": "Pradipto Das, Chenliang Xu, F Richard, Jason Doell,  Corso, A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching, 2013, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_77",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_78@0",
            "content": "Florian Harm De Vries, Sarath Strub, Olivier Chandar, Hugo Pietquin, Aaron Larochelle,  Courville, Guesswhat?! visual object discovery through multi-modal dialogue, 2017, Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_78",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_79@0",
            "content": "UNKNOWN, None, 2001, Language, thought and compositionality. Royal Institute of Philosophy Supplements, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_79",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_80@0",
            "content": "Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, Serge Belongie, Neural Naturalist: Generating Fine-Grained Image Comparisons, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_80",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_81@0",
            "content": "Mor Geva, Yoav Goldberg, Jonathan Berant, Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_81",
            "start": 0,
            "end": 388,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_82@0",
            "content": "D Noah, Michael C Goodman,  Frank, Pragmatic language interpretation as probabilistic inference, 2016, Trends in cognitive sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_82",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_83@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_83",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_84@0",
            "content": "H Paul Grice, Utterer's Meaning and Intentions, 1957, The Philosophical Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_84",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2021, Probing Image-Language Transformers for Verb Understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_85",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2017, spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_86",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_87@0",
            "content": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, Devon Hjelm, Alessandro Sordoni, Aaron Courville, Understanding by understanding not: Modeling negation in language models, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_87",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_88@0",
            "content": "Mehrdad Hosseinzadeh, Yang Wang, Image change captioning by learning from an auxiliary task, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_88",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_89@0",
            "content": "UNKNOWN, None, 2019, Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_89",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_90@0",
            "content": "A Drew, Christopher D Hudson,  Manning, Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_90",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_91@0",
            "content": "UNKNOWN, None, 2018, Learning to Describe Differences Between Pairs of Similar Images, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_91",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_92@0",
            "content": "Harsh Jhamtani, Taylor Berg-Kirkpatrick, Learning to describe differences between pairs of similar images, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_92",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_93@0",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_93",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_94@0",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_94",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_95@0",
            "content": "UNKNOWN, None, 2020, The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_95",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_96@0",
            "content": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli, Video storytelling: Textual summaries for events, 2019, IEEE Transactions on Multimedia, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_96",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_97@0",
            "content": "Fangyu Liu, Emanuele Bugliarello, Maria Edoardo, Siva Ponti, Nigel Reddy, Desmond Collier,  Elliott, Visually grounded reasoning across languages and cultures, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_97",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_98@0",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_98",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_99@0",
            "content": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy, Generation and comprehension of unambiguous object descriptions, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_99",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_100@0",
            "content": "UNKNOWN, None, 2021, Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_100",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_101@0",
            "content": "Anish Mittal, Krishna Anush, Alan Moorthy,  Bovik, No-reference image quality assessment in the spatial domain, 2012, IEEE Transactions on image processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_101",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_102@0",
            "content": "UNKNOWN, None, 2005, Grounding cognition: The role of perception and action in memory, language, and thinking, Cambridge University Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_102",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_103@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021, Proceedings of the 38th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_103",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_104@0",
            "content": "Ray Smith, An overview of the Tesseract OCR engine, 2007, Ninth international conference on document analysis and recognition, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_104",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_105@0",
            "content": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, A Corpus for Reasoning about Natural Language Grounded in Photographs, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_105",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_106@0",
            "content": "UNKNOWN, None, , 2021. A list of color, emotion, and human body part concepts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_106",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_107@0",
            "content": "Suramya Tomar, Converting video formats with ffmpeg, 2006, Linux Journal, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_107",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_108@0",
            "content": "Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik, Context-Aware Captions from Context-Agnostic Supervision, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_108",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_109@0",
            "content": "UNKNOWN, None, 1998, Pragmatics and time. Pragmatics and Beyond New Series, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_109",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_110@0",
            "content": "UNKNOWN, None, 2019, Visual entailment: A novel task for fine-grained image understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_110",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_111@0",
            "content": "Jun Xu, Tao Mei, Ting Yao, Yong Rui, Msrvtt: A large video description dataset for bridging video and language, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_111",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "361-ARR_v2_112@0",
            "content": "An Yan, Xin Wang, Tsu-Jui Fu, William Wang, L2C: Describing visual differences needs semantic understanding of individuals, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "361-ARR_v2_112",
            "start": 0,
            "end": 301,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_1",
            "tgt_ix": "361-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_1",
            "tgt_ix": "361-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_4",
            "tgt_ix": "361-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_5",
            "tgt_ix": "361-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_6",
            "tgt_ix": "361-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_7",
            "tgt_ix": "361-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_8",
            "tgt_ix": "361-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_9",
            "tgt_ix": "361-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_11",
            "tgt_ix": "361-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_12",
            "tgt_ix": "361-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_13",
            "tgt_ix": "361-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_15",
            "tgt_ix": "361-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_14",
            "tgt_ix": "361-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_14",
            "tgt_ix": "361-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_14",
            "tgt_ix": "361-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_14",
            "tgt_ix": "361-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_17",
            "tgt_ix": "361-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_19",
            "tgt_ix": "361-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_20",
            "tgt_ix": "361-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_21",
            "tgt_ix": "361-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_21",
            "tgt_ix": "361-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_21",
            "tgt_ix": "361-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_21",
            "tgt_ix": "361-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_24",
            "tgt_ix": "361-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_25",
            "tgt_ix": "361-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_25",
            "tgt_ix": "361-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_27",
            "tgt_ix": "361-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_27",
            "tgt_ix": "361-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_28",
            "tgt_ix": "361-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_30",
            "tgt_ix": "361-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_30",
            "tgt_ix": "361-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_32",
            "tgt_ix": "361-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_32",
            "tgt_ix": "361-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_33",
            "tgt_ix": "361-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_34",
            "tgt_ix": "361-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_34",
            "tgt_ix": "361-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_35",
            "tgt_ix": "361-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_36",
            "tgt_ix": "361-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_36",
            "tgt_ix": "361-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_38",
            "tgt_ix": "361-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_38",
            "tgt_ix": "361-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_40",
            "tgt_ix": "361-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_39",
            "tgt_ix": "361-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_39",
            "tgt_ix": "361-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_39",
            "tgt_ix": "361-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_38",
            "tgt_ix": "361-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_41",
            "tgt_ix": "361-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_44",
            "tgt_ix": "361-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_45",
            "tgt_ix": "361-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_46",
            "tgt_ix": "361-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_47",
            "tgt_ix": "361-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_48",
            "tgt_ix": "361-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_50",
            "tgt_ix": "361-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_51",
            "tgt_ix": "361-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_52",
            "tgt_ix": "361-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_38",
            "tgt_ix": "361-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_56",
            "tgt_ix": "361-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_54",
            "tgt_ix": "361-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_54",
            "tgt_ix": "361-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_54",
            "tgt_ix": "361-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_54",
            "tgt_ix": "361-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_57",
            "tgt_ix": "361-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_60",
            "tgt_ix": "361-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_62",
            "tgt_ix": "361-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_63",
            "tgt_ix": "361-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_64",
            "tgt_ix": "361-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_65",
            "tgt_ix": "361-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_66",
            "tgt_ix": "361-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_67",
            "tgt_ix": "361-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_68",
            "tgt_ix": "361-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "361-ARR_v2_0",
            "tgt_ix": "361-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_1",
            "tgt_ix": "361-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_2",
            "tgt_ix": "361-ARR_v2_2@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_3",
            "tgt_ix": "361-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_4",
            "tgt_ix": "361-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_4",
            "tgt_ix": "361-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_4",
            "tgt_ix": "361-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_4",
            "tgt_ix": "361-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_5",
            "tgt_ix": "361-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_5",
            "tgt_ix": "361-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_6",
            "tgt_ix": "361-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_6",
            "tgt_ix": "361-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_6",
            "tgt_ix": "361-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_6",
            "tgt_ix": "361-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_7",
            "tgt_ix": "361-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_8",
            "tgt_ix": "361-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_8",
            "tgt_ix": "361-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_8",
            "tgt_ix": "361-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_9",
            "tgt_ix": "361-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_9",
            "tgt_ix": "361-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_9",
            "tgt_ix": "361-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_9",
            "tgt_ix": "361-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_10",
            "tgt_ix": "361-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_11",
            "tgt_ix": "361-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_12",
            "tgt_ix": "361-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_12",
            "tgt_ix": "361-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_13",
            "tgt_ix": "361-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_14",
            "tgt_ix": "361-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_15",
            "tgt_ix": "361-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_15",
            "tgt_ix": "361-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_15",
            "tgt_ix": "361-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_16",
            "tgt_ix": "361-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_17",
            "tgt_ix": "361-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_17",
            "tgt_ix": "361-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_17",
            "tgt_ix": "361-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_18",
            "tgt_ix": "361-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_19",
            "tgt_ix": "361-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_19",
            "tgt_ix": "361-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_19",
            "tgt_ix": "361-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_19",
            "tgt_ix": "361-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_20",
            "tgt_ix": "361-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_20",
            "tgt_ix": "361-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_21",
            "tgt_ix": "361-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_22",
            "tgt_ix": "361-ARR_v2_22@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_23",
            "tgt_ix": "361-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_24",
            "tgt_ix": "361-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_24",
            "tgt_ix": "361-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_24",
            "tgt_ix": "361-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_25",
            "tgt_ix": "361-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_26",
            "tgt_ix": "361-ARR_v2_26@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_27",
            "tgt_ix": "361-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_28",
            "tgt_ix": "361-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_28",
            "tgt_ix": "361-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_28",
            "tgt_ix": "361-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_29",
            "tgt_ix": "361-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_30",
            "tgt_ix": "361-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_31",
            "tgt_ix": "361-ARR_v2_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_32",
            "tgt_ix": "361-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_33",
            "tgt_ix": "361-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_33",
            "tgt_ix": "361-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_33",
            "tgt_ix": "361-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_33",
            "tgt_ix": "361-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_34",
            "tgt_ix": "361-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_35",
            "tgt_ix": "361-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_35",
            "tgt_ix": "361-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_35",
            "tgt_ix": "361-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_35",
            "tgt_ix": "361-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_36",
            "tgt_ix": "361-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_37",
            "tgt_ix": "361-ARR_v2_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_38",
            "tgt_ix": "361-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_39",
            "tgt_ix": "361-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_40",
            "tgt_ix": "361-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_40",
            "tgt_ix": "361-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_41",
            "tgt_ix": "361-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_41",
            "tgt_ix": "361-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_41",
            "tgt_ix": "361-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_41",
            "tgt_ix": "361-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_42",
            "tgt_ix": "361-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_43",
            "tgt_ix": "361-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_44",
            "tgt_ix": "361-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_45",
            "tgt_ix": "361-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_45",
            "tgt_ix": "361-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_46",
            "tgt_ix": "361-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_47",
            "tgt_ix": "361-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_47",
            "tgt_ix": "361-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_47",
            "tgt_ix": "361-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_47",
            "tgt_ix": "361-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_48",
            "tgt_ix": "361-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_49",
            "tgt_ix": "361-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_50",
            "tgt_ix": "361-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_51",
            "tgt_ix": "361-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_52",
            "tgt_ix": "361-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_53",
            "tgt_ix": "361-ARR_v2_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_54",
            "tgt_ix": "361-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_55",
            "tgt_ix": "361-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_56",
            "tgt_ix": "361-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_56",
            "tgt_ix": "361-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_56",
            "tgt_ix": "361-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_57",
            "tgt_ix": "361-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_57",
            "tgt_ix": "361-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_58",
            "tgt_ix": "361-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_59",
            "tgt_ix": "361-ARR_v2_59@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_60",
            "tgt_ix": "361-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_60",
            "tgt_ix": "361-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_60",
            "tgt_ix": "361-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_60",
            "tgt_ix": "361-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_61",
            "tgt_ix": "361-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_62",
            "tgt_ix": "361-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_62",
            "tgt_ix": "361-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_62",
            "tgt_ix": "361-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_62",
            "tgt_ix": "361-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_63",
            "tgt_ix": "361-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_63",
            "tgt_ix": "361-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_63",
            "tgt_ix": "361-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_63",
            "tgt_ix": "361-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_64",
            "tgt_ix": "361-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_64",
            "tgt_ix": "361-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_65",
            "tgt_ix": "361-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_65",
            "tgt_ix": "361-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_65",
            "tgt_ix": "361-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_66",
            "tgt_ix": "361-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_66",
            "tgt_ix": "361-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_66",
            "tgt_ix": "361-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_67",
            "tgt_ix": "361-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_67",
            "tgt_ix": "361-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_68",
            "tgt_ix": "361-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_68",
            "tgt_ix": "361-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_68",
            "tgt_ix": "361-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_69",
            "tgt_ix": "361-ARR_v2_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_70",
            "tgt_ix": "361-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_71",
            "tgt_ix": "361-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_72",
            "tgt_ix": "361-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_73",
            "tgt_ix": "361-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_74",
            "tgt_ix": "361-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_75",
            "tgt_ix": "361-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_76",
            "tgt_ix": "361-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_77",
            "tgt_ix": "361-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_78",
            "tgt_ix": "361-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_79",
            "tgt_ix": "361-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_80",
            "tgt_ix": "361-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_81",
            "tgt_ix": "361-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_82",
            "tgt_ix": "361-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_83",
            "tgt_ix": "361-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_84",
            "tgt_ix": "361-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_85",
            "tgt_ix": "361-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_86",
            "tgt_ix": "361-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_87",
            "tgt_ix": "361-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_88",
            "tgt_ix": "361-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_89",
            "tgt_ix": "361-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_90",
            "tgt_ix": "361-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_91",
            "tgt_ix": "361-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_92",
            "tgt_ix": "361-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_93",
            "tgt_ix": "361-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_94",
            "tgt_ix": "361-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_95",
            "tgt_ix": "361-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_96",
            "tgt_ix": "361-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_97",
            "tgt_ix": "361-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_98",
            "tgt_ix": "361-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_99",
            "tgt_ix": "361-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_100",
            "tgt_ix": "361-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_101",
            "tgt_ix": "361-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_102",
            "tgt_ix": "361-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_103",
            "tgt_ix": "361-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_104",
            "tgt_ix": "361-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_105",
            "tgt_ix": "361-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_106",
            "tgt_ix": "361-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_107",
            "tgt_ix": "361-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_108",
            "tgt_ix": "361-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_109",
            "tgt_ix": "361-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_110",
            "tgt_ix": "361-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_111",
            "tgt_ix": "361-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "361-ARR_v2_112",
            "tgt_ix": "361-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 829,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "361-ARR",
        "version": 2
    }
}