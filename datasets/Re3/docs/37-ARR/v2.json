{
    "nodes": [
        {
            "ix": "37-ARR_v2_0",
            "content": "Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_2",
            "content": "Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning. Feature transitions between utterances occur naturally. However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level. To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of utterance that needs attention. Also, we introduce a response generation strategy to help focus on emotion and keywords related to appropriate features when generating responses. Experimental results show that our approach outperforms baselines and especially, achieves significant improvements on multi-turn dialogues.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "37-ARR_v2_4",
            "content": "Humans have empathy which is the ability to understand situations others have experienced and emotions they have felt from the situations (Eisenberg and Strayer, 1987). That ability also enables to interest and console others while sharing a conversation. Thus, empathetic response generation task has been considered noteworthy. Figure 1 shows an example of a multi-turn empathetic dialogue dataset, EmpatheticDialogues (Rashkin et al., 2019) constructed to solve the task. A speaker talks about one of 32 emotion labels and a situation related to the emotion label, and a listener empathizes, responding to the speaker. Existing approaches (Rashkin et al., 2019;Lin et al., 2019;Majumder et al., 2020;Li et al., 2020;Kim et al., 2021) for the task achieve promising results but show limitations when dialogues become long because they extract features from the concatenation of all tokens in the context at the coarse-grained level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_5",
            "content": "However, at the fine-grained level, each utterance in multi-turn empathetic dialogues has features such as emotion, keywords that each denote what an interlocutor feels and primarily says, and utterance-level meaning that can be known when looking at the entire utterance. In addition, it is a natural phenomenon that features of each utterance differ from the previous, as the dialogue is prolonged. Hence, we humans instinctively recognize these feature transitions, which helps us understand how the dialogue flows and grasp the features of utterance that needs attention. Also, humans respond to others, focusing on emotion and keywords related to appropriate features. Take the example in Figure 1. In the first turn, the speaker is excited to see the speaker's sister in a long time by mentioning keywords (e.g., 'sister', 'visit', 'decade'), and the listener reacts to the excitement and asks about her by mentioning keywords (e.g., 'exciting', 'see', 'live'). However, in the second speaker utterance, the speaker becomes embarrassed because of the speaker's boyfriend's bad table manners by mentioning keywords (e.g., 'boyfriend', 'loud', 'eater').",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_6",
            "content": "We humans recognize that the features of second speaker utterance have changed compared to those of previous utterances, and usually decide to be attentive to the features of the second utterance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_7",
            "content": "Then, by focusing on information such as keywords of that utterance and emotion and keywords (e.g., 'bad', 'impression') related to the features of that utterance, humans generate empathetic, coherent, and non-generic responses like response A. However, the model which produces non-empathetic and incoherent response like response B, considers that the features of the first speaker utterance represent the context from the coarse-grained view.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_8",
            "content": "In this paper, we first propose to annotate features on each utterance at the fine-grained level ( \u00a74). Then, we introduce a novel Empathetic response generator based on Recognizing Feature Transitions (Emp-RFT), which has two essential parts: Feature Transition Recognizer and Response Generation Strategy. The first part recognizes feature transitions between utterances, utilizing comparison functions of Wang and Jiang (2017), which makes Emp-RFT understand the dialogue flow and grasp appropriate features of utterance that needs attention. The second part helps Emp-RFT focus on emotion and keywords related to appropriate features. Specifically, by fusing context with keywords, such keywords are emphasized within each utterance and get more attention when generating responses. Then, Emp-RFT detects next emotion and keywords that denote emotion and keywords of the response, which helps figure out proper emotion and keywords for generation. Lastly, inspired by Dathathri et al. (2020); , a new mechanism of Plug and Play Language Model(PPLM), contrastive PPLM using contrastive loss, is introduced, which controls Emp-RFT to actively use the keywords detected to be next keywords when generating responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_9",
            "content": "We conduct experiments on EmpatheticDialogues. Emp-RFT outperforms strong baselines, particularly, when dialogues are multi-turn.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_10",
            "content": "Our main contributions are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_11",
            "content": "(1) We introduce a novel approach that recognizes feature transitions between utterances, which results in understanding how the dialogue flows and grasping the features of utterance that the model should be attentive to. (2) We propose a response generation strategy including fusing context with keywords, next emotion and keywords detection, and contrastive PPLM. The strategy makes our model focus on emotion and keywords related to appropriate features when generating responses. (3) In the experiments, Emp-RFT outperforms baselines, especially, when dialogues are prolonged.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_12",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "37-ARR_v2_13",
            "content": "Since Rashkin et al. (2019) release EmpatheticDialogues, many approaches have been proposed to generate empathetic responses. Lin et al. (2019) propose mixture of emotional experts. Majumder et al. (2020) propose emotion grouping, emotion mimicry, and stochastic sampling. Li et al. (2020) extract emotional words through lexicon and propose an adversarial generative model. Shen et al. (2021) apply dual-learning with unpaired data for the bidirectional empathy. Gao et al. (2021) integrate emotion cause into response generation process through gated mechanism. Sabour et al. (2021); Li et al. (2022) use implicit commonsense for context modelling. Kim et al. (2021) train a model to extract words that cause the speaker's emotion and attach RSA Framework (Frank and Goodman, 2012) to any generative models to generate responses, focusing on emotion cause words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_14",
            "content": "Recently, many studies have shown remarkable improvements through recognizing transitions of features between utterances in open-domain multiturn dialogues. Qiu et al. (2020) perceive transitions of emotion states for context modelling. Zou et al. (2021) propose a module to manage keyword transitions. Zhan et al. (2021) model external knowledge transitions to select a knowledge used for generation. In multi-turn empathetic dialogues, we consider emotions, keywords, and utterancelevel meaning (Gu et al., 2021) as important features of each utterance and propose a novel approach of recognizing feature transitions between utterances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_15",
            "content": "Task Formulation",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "37-ARR_v2_16",
            "content": "Given context con = [u 1 , . . . , u n\u22121 ], where an utterance",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_17",
            "content": "u i = [u i 1 , . . . , u i |u i | ]",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_18",
            "content": "Data Preparataion",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "37-ARR_v2_19",
            "content": "In this section, we introduce feature annotation in the speaker and listener utterances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_20",
            "content": "Feature Annotation in Speaker Utterances",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "37-ARR_v2_21",
            "content": "Emotion and Keywords of Speaker Utterance (EofSU/KofSU). Speakers try to say an emotional experience that causes a certain emotion in the utterance. Thus, we leverage a model (Kim et al., 2021) which is trained to jointly detect an emotion and emotion cause words of the speaker utterance, using EmpatheticDialogues. We regard top-6 emotion cause words as keywords and remove stopwords and punctuations in keywords.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_22",
            "content": "Feature Annotation in Listener Utterances",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "37-ARR_v2_23",
            "content": "Emotion of Listener Utterance (EofLU). We finetune RoBERTa (Liu et al., 2019) to detect an emotion given a situation description in EmpatheticDialogues. Then, the model predicts an emotion of the listener utterance. Keywords of Listener Utterance (KofLU). Listeners express empathy in the utterance through three Communication Mechanisms (CMs) (Sharma et al., 2020) including emotional reaction, interpretation, and exploration. Thus, three models are leveraged, where each model is trained to detect words that cause one of three CMs, using another dialogue dataset for mental health support 1 . Then, three models predict such words in the listener utterance. Since predicted words take up slightly a lot in the listener utterance, these words are filtered out in the keyword pairs construction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_24",
            "content": "Keyword Pairs Construction. Inspired by Zou et al. (2021), keyword pairs kps are constructed not only to filter out above predicted words, but also to conduct next keyword detection. Given a dialogue 1 A dialogue has a (post, response) pair, and words which cause each CM are annotated on each dialogue. corpus, all pairs are extracted, where each pair has a head word and a tail word each from keywords in the speaker utterance and predicted words in the listener utterance in the same turn. Then, all pairs are filtered out to obtain high-frequency pairs through pointwise mutual information (PMI) 2 (Church and Hanks, 1990) which can measure the association between two words in a corpus. Filtered pairs become kps. A tail word of a kp is regarded as a keyword of the listener utterances joined to extract that keyword pair. Performances of feature annotations are summarized in Table 1 and show reliable results. However, test sets for KofLU based on Empathetic-Dialogues, don't exist. Thus, we randomly sample 100 test dialogues in EmpatheticDialogues and ask 3 human workers to annotate whether each word plays important role for empathizing in the listener utterances. By majority voting, the final verdict on each annotation is decided. We compute the inter-annotator agreement on annotation of test sets for KofLU through Fleiss' kappa (\u03ba) (Fleiss and Cohen, 1973), and result in 0.55, where 0.4 < \u03ba < 0.6 indicates moderate agreement.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_25",
            "content": "The Emp-RFT Model",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "37-ARR_v2_26",
            "content": "In this section, we detail Emp-RFT whose overall architecture is shown in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_27",
            "content": "Context Encoding",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "37-ARR_v2_28",
            "content": "Word-Level Encoder. Emp-RFT contains an encoder f \u03b8 (\u2022) which has the six-layer encoder of BART (Lewis et al., 2020) as the backbone and extracts feature vectors of each u i . Inspired by BERT (Devlin et al., 2019), we prefix each utterance with a [SEN ] token, so u i 0 = [SEN ]. Then, each token is represented as emb i j , the sum of the following four embeddings: word embedding, position embedding, role embedding and emotion embedding M e \u2208 R nemo\u00d7d 3 . Then, the encoder transforms each utterance into a list of output hidden states:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_29",
            "content": "[ \u0125i 0 , ..., \u0125i |u i | ] = f \u03b8 ([emb i 0 , ..., emb i |u i | ]),(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_30",
            "content": "where \u0125i j \u2208 R d . For each utterance, we can obtain utterance-level meaning vector \u0125i 0 derived from the token [SEN ], concatenated keyword vectors ki \u2208 R |k i |\u00d7d derived from the tokens corresponding to k i p (p is the index for keywords.), and emotion vector \u00eai = M e \u0125i 0 . Feature Transition Recognizer. Emp-RFT has a component that operates as the process illustrated in Figure 3. The component computes feature transition information between feature vectors, utilizing two comparison functions, subtraction and multiplication of Wang and Jiang (2017). Each feature vector is compared to previous two feature vectors 4 . First, emotion transition information eti i is computed:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_31",
            "content": "eti i = ReLU(W eti (f com (\u00ea i , \u00eai\u22121 , \u00eai\u22122 ))), (2) f com (\u00ea i , \u00eai\u22121 , \u00eai\u22122 ) = \uf8ee \uf8ef \uf8ef \uf8f0 (\u00ea i \u2212 \u00eai\u22121 ) \u2299 (\u00ea i \u2212 \u00eai\u22121 ) \u00eai \u2299 \u00eai\u22121 (\u00ea i \u2212 \u00eai\u22122 ) \u2299 (\u00ea i \u2212 \u00eai\u22122 ) \u00eai \u2299 \u00eai\u22122 \uf8f9 \uf8fa \uf8fa \uf8fb ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_32",
            "content": "where f com and \u2299 each denote our transition information computing function and Hadamar product, and W eti \u2208 R d\u00d74nemo . Next, utterance-level meaning transition information uti i is computed:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_33",
            "content": "uti i = ReLU(W uti (f com ( \u0125i 0 , \u0125i\u22121 0 , \u0125i\u22122 0 ))),(4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_34",
            "content": ") where W uti \u2208 R d\u00d74d . We then obtain enhanced utterance vector of each utterance by integrating utterance-level meaning vector, and emotion and utterance-level meaning transition information:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_35",
            "content": "hi = FC utt ([ \u0125i 0 ; eti i ; uti i ]),(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_36",
            "content": "where F C utt is a fully-connected layer with size of d. In addition, keyword transition information kti i is computed between concatenated keyword vectors and cross-encoded vectors c t , where t \u2208 {i \u2212 1, i \u2212 2}:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_37",
            "content": "\u2026 \u2026 h 0 i h 0 i!\" h 0 i!# $ e i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_38",
            "content": "# k i!$ # k i!\" # k i % k p i \u2026 Cross Encoding k i c i!\" c i!# kti i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_39",
            "content": "kti i = ReLU(W kti (f com ( ki , c i\u22121 , c i\u22122 )) T ), (6) c t = softmax(Q i (K t ) T ) kt ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_40",
            "content": "Q i = ki W Q , K t = kt W K ,(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_41",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_42",
            "content": "W kti \u2208 R d\u00d74d , W Q and W K \u2208 R d\u00d7d .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_43",
            "content": "We can obtain enhanced keyword vector of each keyword by integrating keyword vector, and keyword transition information:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_44",
            "content": "ki p = FC key ([ ki p ; kti i p ]),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_45",
            "content": "where F C key is a fully-connected layer with size of d. Consequently, the enhanced feature vectors guide Emp-RFT to accurately grasp the features of utterance that the model should be attentive to when given feature transition information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_46",
            "content": "Utterance-Level Encoder. Emp-RFT contains another encoder g \u03d5 (\u2022) which has the six-layer encoder of BART, and transforms enhanced utterance vectors with global position embeddings (GPE) into a context representation to capture relationships between utterances (Gu et al., 2021):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_47",
            "content": "[ \u1e271 , ..., \u1e27n\u22121 ] = g \u03d5 ([ h1 , ..., hn\u22121 ]).(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_48",
            "content": "Emp-RFT consists of hierarchical structures of encoders through word-level and utterance-level encoders. This structure makes Emp-RFT comprehend each utterance at the fine-grained level, and understand the context by integrating information based on comprehension of each utterance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_49",
            "content": "Fusing Context with Keywords. Emp-RFT fuses context with keywords as the process illustrated in Figure 4. We first dynamically build keyword graph for each context. Keywords in each context become nodes and are initialized by corresponding enhanced keyword vectors with GPE. Edges are built across the below cases: (1) between two keywords from the same utterance and (2) between a keyword from a certain utterance and another keyword from the previous two utterances. Also, a tail word in a kp whose head word is k n\u22121 p is appended as a node and connected with k n\u22121 p node. Appended nodes (AN s) are initialized through BART decoder whose parameters are frozen with GPE, and used for next keywords detection. To obtain keyword representation vi o from the keyword graph(o is the index for nodes.), nodes are updated based on multi-head graph-attention mechanism (Veli\u010dkovi\u0107 et al., 2018;Li et al., 2022). This mechanism makes Emp-RFT not only capture relationships between nodes but also manage influences of each appended node through attention architecture:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_50",
            "content": "vi o = v i o + M H mh=1 z\u2208A i o \u03b1 i,mh oz (W mh v v z ),(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_51",
            "content": "\u03b1 i,mh oz = exp((W mh q v i o ) T W mh key v z ) s\u2208A i o exp((W mh q v i o ) T W mh key v s ) ,(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_52",
            "content": "where v i o , \u2225, A i o , and \u03b1 i,mh oz each denote a node representation, the concatenation of M H attention heads, the neighbours of v i o in the adjacency matrix A, and self-attention weight and W mh v , W mh q , W mh key \u2208 R d mh \u00d7d (d mh = d/M H). Lastly, we can obtain the fused context representation H = [h 1 , ..., h n\u22121 ] by fusing the context representation with the sum of keyword representations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_53",
            "content": "h i = FC f use ([ \u1e27i ; sum([v i 1 , ..., vi |k i | ])]), (13",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_54",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_55",
            "content": "where F C f use is a fully-connected layer with size of d. Consequently, keywords are emphasized within each utterance and get greater attention when generating responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_56",
            "content": "Next Emotion and Keywords Detection. Emp-RFT detects next emotion e y and keywords k y , which helps figure out proper emotion and keywords for generation. First, based on the maxpooled fused context representation, next emotion",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_57",
            "content": "k ! i\"# Keyword Pairs(kps) \u2026 k k i i \u2026 \u2026 \u2026 k k i!\" i#$ k k i!% i#& k $ i Graph Attention Network AN AN ! AN \" AN # AN AN (k i , AN ) (k i , AN ! ) (k i , AN \" ) (k ! i , AN \" ) (k ! i , AN # ) (k k i i , AN ) (k k i i , AN # ) (k k i i ,AN AN ) H Fused Context Representation \u2026 Figure 4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_58",
            "content": ": Operation process of fusing context with keywords. AN o is the appended node. Some symbols and edges are omitted for simplicity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_59",
            "content": "distribution is predicted:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_60",
            "content": "P e = softmax(M e MP(H)),(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_61",
            "content": "where MP denotes maxpooling. We use the emotion with the highest probability (\u00ea y ) for generation. Also, Emp-RFT predicts whether the word of each AN belongs to the next keywords through the binary classification, where the true label denotes the word belongs to:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_62",
            "content": "P k = |AN s| o=1 softmax(W AN [v n o ; MP(H)]),(15)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_63",
            "content": "where W AN \u2208 R 2\u00d72d . We consider the words of AN s whose probabilities for the true label \u2265 0.8 as the keywords ( ky ) for generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_64",
            "content": "Response Generation",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "37-ARR_v2_65",
            "content": "Response Generator. Emp-RFT includes a response generator (RG) which has the six-layer decoder of BART as the backbone. Through the four embeddings with \u00eay , explained previously, we can obtain the input sequence embedding for RG. We prefix it with the sum of node representations corresponding to ky . Then, RG is fed to predict probability distribution on each next token y t based on the fused context representation:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_66",
            "content": "P (y|con, e, k, kps) = m t=1 P (y t |y <t , H). ( 16)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_67",
            "content": "Training. We apply cross-entropy loss to three objectives (eq. 14, 15, 16), and train parameters of Emp-RFT in end-to-end manner through the sum of all losses .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_68",
            "content": "Contrastive PPLM. Analysis on the generated responses of the trained Emp-RFT shows that an active reflection of ky is demanded. Thus, inspired by Dathathri et al. (2020); , we propose Contrastive PPLM with a discriminator using contrastive loss. Existing discriminators (Dathathri et al., 2020;Majumder et al., 2021) are trained to predict whether a sentence contains a certain attribute, using cross-entropy loss. Then, the gradient of the loss is passed to the generative model to generate a sentence containing such attribute during inference. However, since keywords are not attributes but objects, we train a discriminator to predict whether a response in EmpatheticDialogues is more similar to the keyword set of the response(positive sample) than the keyword sets of another responses(negative samples) in the same batch, using contrastive loss based on the similarity between objects:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_69",
            "content": "L a pplm = \u2212log exp(r T a ks a /\u03c4 ) B b=1 exp(r T a ks b /\u03c4 ) ,(17)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_70",
            "content": "where r,ks,\u03c4 and B each denote response and keyword set representations, a temperature parameter and batchsize. During inference, we repeatedly sample three random AN s except for nodes of ky , and consider the sum of such AN representations as one of negative samples and the sum of node representations corresponding to ky as a positive sample. Then, the gradient of the contrastive loss is passed to Emp-RFT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_71",
            "content": "6 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_72",
            "content": "Dataset and Baselines",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "37-ARR_v2_73",
            "content": "Dataset. Experiments were conducted on Empa-theticDialogues (Rashkin et al., 2019) which contains 24,850 multi-turn dialogues. For each dialogue, we can extract a certain number of instances corresponding to the number of turns within the dialogue. This totals to 47,611 instances, where 22,761 are multi-turn. In one turn of a dialogue, a speaker talks about one of 32 evenly distributed emotion labels and a situation related to the emotion label and a listener empathizes by responding to the speaker. Following the instructions of the dataset, we use 8:1:1 train/valid/test split.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_74",
            "content": "Baselines. We compared Emp-RFT to the following five baseline models: (1) MoEL (Lin et al., 2019) is a transformer-based generative model, which has decoders for each emotion and integrates outputs of the decoders according to predicted emotion distribution. (2) EmpDG (Li et al., 2020) uses emotional words and consists of an adversarial framework including a generator and discriminators which reflect the user feedback. (3) MIME (Majumder et al., 2020) is also a transformer-based generative model which mimics user emotion based on emotion grouping and uses stochastic sampling for varied responses. (4) MIME+Focused S1 and (5) Blender+Focused S1 (Kim et al., 2021) attach RSA Framework to MIME and Blender (Roller et al., 2021). Blender is a pretrained model with 90M parameters size, using an immense number of dialogues. It is finetuned on EmpatheticDialogues. Using distractors and Bayes' Rules, RSA Framework makes the models focus on certain parts of the post, such as emotion cause words when generating responses in the single-turn dialogues 5 . Implementation details about Emp-RFT and baselines are covered in Appendix A.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_75",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "37-ARR_v2_76",
            "content": "Automatic Evaluation. We evaluated the models, using the following three metrics: (1) Perplexity (PPL) (Vinyals and Le, 2015) measures how highly likely tokens are generated, which evaluates the overall quality of the model. (2) Distinct-n (Dist-n) (Li et al., 2016) measures how diverse the generated response is via the unique words within its n-gram. (3). We use BERTscore (F BERT ) (Zhang et al., 2019) which measures token-level semantic similarities between the generated response and the gold response based on embeddings from BERT (Devlin et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_77",
            "content": "Human Ratings. Human evaluations for the dialogues models are essential because of insufficient reliability on automatic metrics. We randomly sampled 100 test dialogues and asked 3 human workers to score models' generated responses on 1 to 5 point scale, following the four metrics (Rashkin et al., 2019): (1) Empathy measures whether the generated response understands the speaker's emotion and situation. (2) Relevance measures whether the generated response is coherent to the context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_78",
            "content": "(3) Fluency measures whether the generated response is grammatically correct and readable. (4) Since we conclude that models generating generic responses are not empathizing to the speaker, we use Diversity to measure whether the generated response is non-generic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_79",
            "content": "Human A/B Test. We further conducted a human A/B test which provides stronger intuitions and higher agreements than human ratings, because this is carried with 3 human workers selecting the better response when given two generated responses (Sabour et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_80",
            "content": "Analysis of Response Generation",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "37-ARR_v2_81",
            "content": "We abbreviate feature transition recognizer, contrastive PPLM, next emotion and keywords detection, and fusing context with keywords as FTR, CP, NEKD, and FCK, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_82",
            "content": "Automatic Evaluation Results. The overall automatic evaluation results are shown in the left part of Table 2. Emp-RFT performed exceedingly on all metrics except for PPL, which was nearly the same as Blender+Focused S1. The improvements on other metrics indicated that our approach was effective for generating generally high quality and non-generic responses which were also semantically similar with the gold response. While the utilization of pretrained models yielded significant improvements compared to models only trained on EmpatheticDialogues, Emp-RFT showed even greater performance when compared to Blender+Focused S1 endowed with more significant number of dialogues. In addition, due to utilization of FTR, Emp-RFT obtained remarkable results even on multi-turn instances, whereas, other models suffered due to their means of utilizing features for the context at the coarse-grained level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_83",
            "content": "Human Evaluation Results. In the right part of Table 2, Emp-RFT acquired the highest scores on all metrics, which demonstrated that all components of Emp-RFT helped generate responses that are empathetic, coherent to the context, and non-generic. Also, utilizing pretrained models showed significant improvements, especially on Fluency and Diversity scores. In Table 3, the generated responses from Emp-RFT were more preferred, which indicated Emp-RFT consistently outperformed other methods in various experiments. When observing at the models' performance difference between multi-turn instances and all instances, only Emp-RFT continued to perform consistently, whereas other models showed significant performance drops under multi-turn instances. From this, we concluded that Emp-RFT continuously understood the dialogue flow.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_84",
            "content": "Ablation Study. To better understand effects of each component in Emp-RFT, we conducted the ablation study. We gradually ablated each component within the response generation strategy in a hierarchical manner. (1) w/o FTR: Feature transition recognizer was disabled, which resulted in considerable drops on all metrics, especially on PPL, F BERT , Empathy, and Relevance scores on multi-turn instances, because Emp-RFT could not grasp the attention-needed features of utterance within multi-turn instances through FTR. ( 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_85",
            "content": "Analysis of Next Emotion and Keywords",
            "ntype": "title",
            "meta": {
                "section": "6.4"
            }
        },
        {
            "ix": "37-ARR_v2_86",
            "content": "We report the results in terms of NEKD in Table 4. Since all baselines have not conducted NEKD, we trained models showing promising results such as CoMAE , ConceptFlow (Zhang et al., 2020) and CG-nAR (Zou et al., 2021) with EmpatheticDialogues. (More details are covered in Appendix A.2). Then, we compared Emp-RFT to those models. Emp-RFT outperformed other models on all metrics, which proved Emp-RFT figured out which emotion and keywords were proper for generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_87",
            "content": "Emotion Label: Furious Annotated Emotion: Annoyed \u2192 Apprehensive \u2192 Confident \u2192 Hopeful u 1 : My roommate eats my food sometimes. This makes me so angry! u 2 : You should get a mini fridge and put it in your room, with a lock on it. u 3 : I think that's a great idea. I know where to get those fridges for cheap. Gold: Yea man go for it, don't procrastinate. MoEL: I am sorry you have to hear that. I hope it works out for you. EmpDG: I agree with you. I think it isn't worth before you get it back. MIME: I am sorry to hear that. I hope you don't have to deal with that. MIME+Focused S1: I agree. I have a friend who is not to be a parent. Blender+Focused S1: Roommates can be so annoying.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_88",
            "content": "\u00eay : Trusting, ky : procrastinate, safety, profit Emp-RFT: Don't procrastinate. It makes your foods safety. w/o FTR: I agree. I have a safety but pricy fridge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_89",
            "content": "Emotion Label: Proud Annotated Emotion: Trusting \u2192 Proud \u2192 Grateful\u2192 Sentimental \u2192 Disappointed \u2192 Sentimental u 1 : Hi, My girlfriend was recently let go of her job, but she put so much work in to finding a new job and got one with better pay! u 2 : That's great! It was meant to happen then. u 3 : Yes I agree, she was so happy when she found out the job was hers. u 4 : I would have been too. I've never been fired. I always quit. u 5 : She wasn't really fired, her position was no more. Schools don't have much room left for Home Ec teachers. Sad. Gold: I remember taking Home Ec. I loved that class.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_90",
            "content": "Case Study",
            "ntype": "title",
            "meta": {
                "section": "6.5"
            }
        },
        {
            "ix": "37-ARR_v2_91",
            "content": "The cases from the models are shown in Table 5. In the first case, MoEL and MIME expressed regret, which was emotionally inappropriate to the context. All baselines except for MoEL failed to grasp the proper features within the context, and therefore generated incoherent responses. Especially, Blender+Focused S1 ignored the features of u 3 . Since Emp-RFT understood the dialogue flow, it became attentive to not only the features of u 3 but also those of u 1 , u 2 , mentioning ('procrastinate', 'foods', 'safety'), which led to empathy and coherence. In the second case, all baselines couldn't understand the longer context, which resulted in improper empathy. Also, Blender+Focused S1 disregarded the features of u 5 , and therefore overlooked the speaker's sadness. Emp-RFT fully comprehended why the speaker's happiness changed to the sadness. In both cases, without FTR, the responses of Emp-RFT were non-empathetic and incoherent because of dismissing appropriate features. In the third case, we report the case in terms of the response generation strategy. Without CP, NEKD, and FCK, Emp-RFT produced a generic response. With the utilization of FCK, Emp-RFT perceived the word 'cancer' in u 3 but expressed excessive emotion by mentioning 'scary'. When Emp-RFT additionally conducted NEKD, Emp-RFT generated emotionally appropriate responses by mentioning 'sorry' and 'hard', and utilized the keyword 'lost'. Lastly, with CP, Emp-RFT generated a diverse response, actively using ky .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_92",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "37-ARR_v2_93",
            "content": "We proposed a novel approach that recognizes feature transitions between utterances, which led to understanding the dialogue flow and grasping the features of utterance that needs attention. Also, to make our model focus on emotion and keywords related to appropriate features, we introduced a response generation strategy including fusing context with keywords, next emotion and keywords detection, and contrastive PPLM. Experimental results showed that our model outperformed baselines, and especially, achieved significant improvements on multi-turn instances, which proved our approach was effective for empathetic, coherent, and nongeneric response generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_94",
            "content": "Ethical Considerations",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "37-ARR_v2_95",
            "content": "We expect that our proposed approach does not suffer from ethical problems. The dataset we use in our work is EmpatheticDialogues which is Englishbased. The dataset is constructed by crowdsourcing with Amazon Mechanical Turk, which protects private user information (Rashkin et al., 2019). In addition, the dialogue dataset is anticipated not to have responses which include discrimination, abuse, bias, etc, because the robust collection procedure of EmpatheticDialogues ensures the quality of the dataset. Thus, we expect that models trained using the dataset, do not generate inappropriate responses which harm the users. However, we inform that our model utilizes a pretrained language model, which may produce inappropriate responses. Lastly, we anticipate our model make potential users be interested and consoled by generating empathetic responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_96",
            "content": "Our model is implemented by Pytorch 10 , and based on two encoders of BART-base and a decoder of BART-base 11 . Hidden size d is 768 and the number of emotion classes n emo is 32. M H and the number of layers of graph attention network are each 4. Using Adam optimization (Kingma and Ba, 2015), our model is trained on single RTX 3090 GPU with a batch size of 4. We apply early-stopping and select a model showing the best performance through perplexity on the valid set. For contrastive PPLM, we utilize the official code of PPLM 12 . We set a temperature parameter \u03c4 and batch size to 0.5 and 64, respectively. Through represenations derived from the last token of BART decoder whose parameters are frozen, we can obtain each response representation r a and each keyword set representation ks a , where the keyword set corresponds to the response. Thus, ks a becomes a positive sample for r a , and keyword set representations for other responses in the same batch become negative samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_97",
            "content": "We utilize the repositories and follow implemetation details of CoMAE 13 , ConceptFlow 14 , and CG-nAR 15 . We train three models, using Empa-theticDialogues instead of originally used datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "37-ARR_v2_98",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "International conference on machine learning",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_99",
            "content": "UNKNOWN, None, 1990, Word association norms, mutual information, and lexicography. Computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "1990",
                "pub_title": "Word association norms, mutual information, and lexicography. Computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_100",
            "content": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, Plug and play language models: A simple approach to controlled text generation, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Sumanth Dathathri",
                    "Andrea Madotto",
                    "Janice Lan",
                    "Jane Hung",
                    "Eric Frank",
                    "Piero Molino",
                    "Jason Yosinski",
                    "Rosanne Liu"
                ],
                "title": "Plug and play language models: A simple approach to controlled text generation",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_101",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_102",
            "content": "Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, Eraser: A benchmark to evaluate rationalized nlp models, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Jay Deyoung",
                    "Sarthak Jain",
                    "Nazneen Fatema Rajani",
                    "Eric Lehman",
                    "Caiming Xiong",
                    "Richard Socher",
                    "Byron C Wallace"
                ],
                "title": "Eraser: A benchmark to evaluate rationalized nlp models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_103",
            "content": "UNKNOWN, None, 1987, Critical issues in the study of empathy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "1987",
                "pub_title": "Critical issues in the study of empathy",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_104",
            "content": "L Joseph, Jacob Fleiss,  Cohen, The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability, 1973, Educational and psychological measurement, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "L Joseph",
                    "Jacob Fleiss",
                    " Cohen"
                ],
                "title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
                "pub_date": "1973",
                "pub_title": "Educational and psychological measurement",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_105",
            "content": "C Michael,  Frank,  Noah D Goodman, Predicting pragmatic reasoning in language games, 2012, Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "C Michael",
                    " Frank",
                    " Noah D Goodman"
                ],
                "title": "Predicting pragmatic reasoning in language games",
                "pub_date": "2012",
                "pub_title": "Science",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_106",
            "content": "Jun Gao, Yuhan Liu, Haolin Deng, Wei Wang, Yu Cao, Jiachen Du, Ruifeng Xu, Improving empathetic response generation by recognizing emotion cause in conversations, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jun Gao",
                    "Yuhan Liu",
                    "Haolin Deng",
                    "Wei Wang",
                    "Yu Cao",
                    "Jiachen Du",
                    "Ruifeng Xu"
                ],
                "title": "Improving empathetic response generation by recognizing emotion cause in conversations",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_107",
            "content": "Xiaodong Gu, Jung-Woo Kang Min Yoo,  Ha, Dialogbert: Discourse-aware response generation via learning to recover and rank utterances, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Xiaodong Gu",
                    "Jung-Woo Kang Min Yoo",
                    " Ha"
                ],
                "title": "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_108",
            "content": "Hyunwoo Kim, Byeongchang Kim, Gunhee Kim, Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Hyunwoo Kim",
                    "Byeongchang Kim",
                    "Gunhee Kim"
                ],
                "title": "Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_109",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015-05-07",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_110",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_111",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jiwei Li",
                    "Michel Galley",
                    "Chris Brockett",
                    "Jianfeng Gao",
                    "William B Dolan"
                ],
                "title": "A diversity-promoting objective function for neural conversation models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_112",
            "content": "Qintong Li, Hongshen Chen, Zhaochun Ren, Pengjie Ren, Zhaopeng Tu, Zhumin Chen, Empdg: Multi-resolution interactive empathetic dialogue generation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Qintong Li",
                    "Hongshen Chen",
                    "Zhaochun Ren",
                    "Pengjie Ren",
                    "Zhaopeng Tu",
                    "Zhumin Chen"
                ],
                "title": "Empdg: Multi-resolution interactive empathetic dialogue generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_113",
            "content": "UNKNOWN, None, , Zhaochun Ren, Pengjie Ren, and Zhumin Chen. 2022. Knowledge bridging for empathetic dialogue generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Zhaochun Ren, Pengjie Ren, and Zhumin Chen. 2022. Knowledge bridging for empathetic dialogue generation",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_114",
            "content": "Zhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, Pascale Fung, Moel: Mixture of empathetic listeners, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Zhaojiang Lin",
                    "Andrea Madotto",
                    "Jamin Shin",
                    "Peng Xu",
                    "Pascale Fung"
                ],
                "title": "Moel: Mixture of empathetic listeners",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_115",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_116",
            "content": "Sudha Bodhisattwa Prasad Majumder, Michel Rao, Julian Galley,  Mcauley, Ask what's missing and what's useful: Improving clarification question generation using global knowledge, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Sudha Bodhisattwa Prasad Majumder",
                    "Michel Rao",
                    "Julian Galley",
                    " Mcauley"
                ],
                "title": "Ask what's missing and what's useful: Improving clarification question generation using global knowledge",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_117",
            "content": "Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway Ghosal, Alexander Gelbukh, Mime: Mimicking emotions for empathetic response generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Navonil Majumder",
                    "Pengfei Hong",
                    "Shanshan Peng",
                    "Jiankun Lu",
                    "Deepanway Ghosal",
                    "Alexander Gelbukh"
                ],
                "title": "Mime: Mimicking emotions for empathetic response generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_118",
            "content": "Lisong Qiu, Yingwai Shiu, Pingping Lin, Ruihua Song, Yue Liu, Dongyan Zhao, Rui Yan, What if bots feel moods?, 2020, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Lisong Qiu",
                    "Yingwai Shiu",
                    "Pingping Lin",
                    "Ruihua Song",
                    "Yue Liu",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "What if bots feel moods?",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_119",
            "content": "Eric Hannah Rashkin, Margaret Smith, Y-Lan Li,  Boureau, Towards empathetic opendomain conversation models: A new benchmark and dataset, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Eric Hannah Rashkin",
                    "Margaret Smith",
                    "Y-Lan Li",
                    " Boureau"
                ],
                "title": "Towards empathetic opendomain conversation models: A new benchmark and dataset",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_120",
            "content": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Smith, Y-Lan Boureau, Recipes for building an open-domain chatbot, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Stephen Roller",
                    "Emily Dinan",
                    "Naman Goyal",
                    "Da Ju",
                    "Mary Williamson",
                    "Yinhan Liu",
                    "Jing Xu",
                    "Myle Ott",
                    "Eric Smith",
                    "Y-Lan Boureau"
                ],
                "title": "Recipes for building an open-domain chatbot",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_121",
            "content": "UNKNOWN, None, 2021, Cem: Commonsense-aware empathetic response generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Cem: Commonsense-aware empathetic response generation",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_122",
            "content": "Ashish Sharma, Adam Miner, David Atkins, Tim Althoff, A computational approach to understanding empathy expressed in text-based mental health support, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ashish Sharma",
                    "Adam Miner",
                    "David Atkins",
                    "Tim Althoff"
                ],
                "title": "A computational approach to understanding empathy expressed in text-based mental health support",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_123",
            "content": "Lei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, Jie Zhou, Constructing emotional consensus and utilizing unpaired data for empathetic dialogue generation, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Lei Shen",
                    "Jinchao Zhang",
                    "Jiao Ou",
                    "Xiaofang Zhao",
                    "Jie Zhou"
                ],
                "title": "Constructing emotional consensus and utilizing unpaired data for empathetic dialogue generation",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_124",
            "content": "UNKNOWN, None, 2018, Graph Attention Networks. International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Graph Attention Networks. International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_125",
            "content": "UNKNOWN, None, 2015, A neural conversational model, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "A neural conversational model",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_126",
            "content": "UNKNOWN, None, 2017, A compareaggregate model for matching text sequences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A compareaggregate model for matching text sequences",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_127",
            "content": "UNKNOWN, None, , ICLR 2017: International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "ICLR 2017: International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_128",
            "content": "Hainan Haolan Zhan, Hongshen Zhang, Zhuoye Chen, Yongjun Ding, Yanyan Bao,  Lan, Augmenting knowledge-grounded conversations with sequential knowledge transition, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Hainan Haolan Zhan",
                    "Hongshen Zhang",
                    "Zhuoye Chen",
                    "Yongjun Ding",
                    "Yanyan Bao",
                    " Lan"
                ],
                "title": "Augmenting knowledge-grounded conversations with sequential knowledge transition",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_129",
            "content": "Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Grounded conversation generation as guided traverses in commonsense knowledge graphs, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Houyu Zhang",
                    "Zhenghao Liu",
                    "Chenyan Xiong",
                    "Zhiyuan Liu"
                ],
                "title": "Grounded conversation generation as guided traverses in commonsense knowledge graphs",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_130",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger,  Artzi, Bertscore: Evaluating text generation with bert, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Tianyi Zhang",
                    "Varsha Kishore",
                    "Felix Wu",
                    "Q Kilian",
                    "Yoav Weinberger",
                    " Artzi"
                ],
                "title": "Bertscore: Evaluating text generation with bert",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_131",
            "content": "Chujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, Minlie Huang, Comae: A multi-factor hierarchical framework for empathetic response generation, 2021, Findings of the Association for Computational Linguistics: ACL 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Chujie Zheng",
                    "Yong Liu",
                    "Wei Chen",
                    "Yongcai Leng",
                    "Minlie Huang"
                ],
                "title": "Comae: A multi-factor hierarchical framework for empathetic response generation",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL 2021",
                "pub": null
            }
        },
        {
            "ix": "37-ARR_v2_132",
            "content": "Yicheng Zou, Zhihua Liu, Xingwu Hu, Qi Zhang, Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Yicheng Zou",
                    "Zhihua Liu",
                    "Xingwu Hu",
                    "Qi Zhang"
                ],
                "title": "Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "37-ARR_v2_0@0",
            "content": "Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_0",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@0",
            "content": "Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@1",
            "content": "Feature transitions between utterances occur naturally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 119,
            "end": 173,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@2",
            "content": "However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 175,
            "end": 310,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@3",
            "content": "To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of utterance that needs attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 312,
            "end": 525,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@4",
            "content": "Also, we introduce a response generation strategy to help focus on emotion and keywords related to appropriate features when generating responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 527,
            "end": 672,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_2@5",
            "content": "Experimental results show that our approach outperforms baselines and especially, achieves significant improvements on multi-turn dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_2",
            "start": 674,
            "end": 813,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@0",
            "content": "Humans have empathy which is the ability to understand situations others have experienced and emotions they have felt from the situations (Eisenberg and Strayer, 1987).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@1",
            "content": "That ability also enables to interest and console others while sharing a conversation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 169,
            "end": 254,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@2",
            "content": "Thus, empathetic response generation task has been considered noteworthy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 256,
            "end": 328,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@3",
            "content": "Figure 1 shows an example of a multi-turn empathetic dialogue dataset, EmpatheticDialogues (Rashkin et al., 2019) constructed to solve the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 330,
            "end": 473,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@4",
            "content": "A speaker talks about one of 32 emotion labels and a situation related to the emotion label, and a listener empathizes, responding to the speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 475,
            "end": 620,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_4@5",
            "content": "Existing approaches (Rashkin et al., 2019;Lin et al., 2019;Majumder et al., 2020;Li et al., 2020;Kim et al., 2021) for the task achieve promising results but show limitations when dialogues become long because they extract features from the concatenation of all tokens in the context at the coarse-grained level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_4",
            "start": 622,
            "end": 933,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@0",
            "content": "However, at the fine-grained level, each utterance in multi-turn empathetic dialogues has features such as emotion, keywords that each denote what an interlocutor feels and primarily says, and utterance-level meaning that can be known when looking at the entire utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@1",
            "content": "In addition, it is a natural phenomenon that features of each utterance differ from the previous, as the dialogue is prolonged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 273,
            "end": 399,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@2",
            "content": "Hence, we humans instinctively recognize these feature transitions, which helps us understand how the dialogue flows and grasp the features of utterance that needs attention. Also, humans respond to others, focusing on emotion and keywords related to appropriate features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 401,
            "end": 672,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@3",
            "content": "Take the example in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 674,
            "end": 702,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@4",
            "content": "In the first turn, the speaker is excited to see the speaker's sister in a long time by mentioning keywords (e.g., 'sister', 'visit', 'decade'), and the listener reacts to the excitement and asks about her by mentioning keywords (e.g., 'exciting', 'see', 'live').",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 704,
            "end": 966,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_5@5",
            "content": "However, in the second speaker utterance, the speaker becomes embarrassed because of the speaker's boyfriend's bad table manners by mentioning keywords (e.g., 'boyfriend', 'loud', 'eater').",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_5",
            "start": 968,
            "end": 1156,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_6@0",
            "content": "We humans recognize that the features of second speaker utterance have changed compared to those of previous utterances, and usually decide to be attentive to the features of the second utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_6",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_7@0",
            "content": "Then, by focusing on information such as keywords of that utterance and emotion and keywords (e.g., 'bad', 'impression') related to the features of that utterance, humans generate empathetic, coherent, and non-generic responses like response A. However, the model which produces non-empathetic and incoherent response like response B, considers that the features of the first speaker utterance represent the context from the coarse-grained view.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_7",
            "start": 0,
            "end": 444,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@0",
            "content": "In this paper, we first propose to annotate features on each utterance at the fine-grained level ( \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@1",
            "content": "Then, we introduce a novel Empathetic response generator based on Recognizing Feature Transitions (Emp-RFT), which has two essential parts: Feature Transition Recognizer and Response Generation Strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 104,
            "end": 306,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@2",
            "content": "The first part recognizes feature transitions between utterances, utilizing comparison functions of Wang and Jiang (2017), which makes Emp-RFT understand the dialogue flow and grasp appropriate features of utterance that needs attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 308,
            "end": 544,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@3",
            "content": "The second part helps Emp-RFT focus on emotion and keywords related to appropriate features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 546,
            "end": 637,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@4",
            "content": "Specifically, by fusing context with keywords, such keywords are emphasized within each utterance and get more attention when generating responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 639,
            "end": 785,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@5",
            "content": "Then, Emp-RFT detects next emotion and keywords that denote emotion and keywords of the response, which helps figure out proper emotion and keywords for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 787,
            "end": 950,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_8@6",
            "content": "Lastly, inspired by Dathathri et al. (2020); , a new mechanism of Plug and Play Language Model(PPLM), contrastive PPLM using contrastive loss, is introduced, which controls Emp-RFT to actively use the keywords detected to be next keywords when generating responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_8",
            "start": 952,
            "end": 1216,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_9@0",
            "content": "We conduct experiments on EmpatheticDialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_9",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_9@1",
            "content": "Emp-RFT outperforms strong baselines, particularly, when dialogues are multi-turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_9",
            "start": 47,
            "end": 128,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_10@0",
            "content": "Our main contributions are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_10",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_11@0",
            "content": "(1) We introduce a novel approach that recognizes feature transitions between utterances, which results in understanding how the dialogue flows and grasping the features of utterance that the model should be attentive to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_11",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_11@1",
            "content": "(2) We propose a response generation strategy including fusing context with keywords, next emotion and keywords detection, and contrastive PPLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_11",
            "start": 222,
            "end": 365,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_11@2",
            "content": "The strategy makes our model focus on emotion and keywords related to appropriate features when generating responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_11",
            "start": 367,
            "end": 483,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_11@3",
            "content": "(3) In the experiments, Emp-RFT outperforms baselines, especially, when dialogues are prolonged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_11",
            "start": 485,
            "end": 580,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_12@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_12",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@0",
            "content": "Since Rashkin et al. (2019) release EmpatheticDialogues, many approaches have been proposed to generate empathetic responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@1",
            "content": "Lin et al. (2019) propose mixture of emotional experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 126,
            "end": 180,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@2",
            "content": "Majumder et al. (2020) propose emotion grouping, emotion mimicry, and stochastic sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 182,
            "end": 271,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@3",
            "content": "Li et al. (2020) extract emotional words through lexicon and propose an adversarial generative model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 273,
            "end": 373,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@4",
            "content": "Shen et al. (2021) apply dual-learning with unpaired data for the bidirectional empathy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 375,
            "end": 462,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@5",
            "content": "Gao et al. (2021) integrate emotion cause into response generation process through gated mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 464,
            "end": 562,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@6",
            "content": "Sabour et al. (2021); Li et al. (2022) use implicit commonsense for context modelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 564,
            "end": 649,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_13@7",
            "content": "Kim et al. (2021) train a model to extract words that cause the speaker's emotion and attach RSA Framework (Frank and Goodman, 2012) to any generative models to generate responses, focusing on emotion cause words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_13",
            "start": 651,
            "end": 863,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_14@0",
            "content": "Recently, many studies have shown remarkable improvements through recognizing transitions of features between utterances in open-domain multiturn dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_14",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_14@1",
            "content": "Qiu et al. (2020) perceive transitions of emotion states for context modelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_14",
            "start": 157,
            "end": 235,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_14@2",
            "content": "Zou et al. (2021) propose a module to manage keyword transitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_14",
            "start": 237,
            "end": 301,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_14@3",
            "content": "Zhan et al. (2021) model external knowledge transitions to select a knowledge used for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_14",
            "start": 303,
            "end": 400,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_14@4",
            "content": "In multi-turn empathetic dialogues, we consider emotions, keywords, and utterancelevel meaning (Gu et al., 2021) as important features of each utterance and propose a novel approach of recognizing feature transitions between utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_14",
            "start": 402,
            "end": 637,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_15@0",
            "content": "Task Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_15",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_16@0",
            "content": "Given context con = [u 1 , . . . , u n\u22121 ], where an utterance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_16",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_17@0",
            "content": "u i = [u i 1 , . . . , u i |u i | ]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_17",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_18@0",
            "content": "Data Preparataion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_18",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_19@0",
            "content": "In this section, we introduce feature annotation in the speaker and listener utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_19",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_20@0",
            "content": "Feature Annotation in Speaker Utterances",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_20",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_21@0",
            "content": "Emotion and Keywords of Speaker Utterance (EofSU/KofSU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_21",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_21@1",
            "content": "Speakers try to say an emotional experience that causes a certain emotion in the utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_21",
            "start": 57,
            "end": 147,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_21@2",
            "content": "Thus, we leverage a model (Kim et al., 2021) which is trained to jointly detect an emotion and emotion cause words of the speaker utterance, using EmpatheticDialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_21",
            "start": 149,
            "end": 315,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_21@3",
            "content": "We regard top-6 emotion cause words as keywords and remove stopwords and punctuations in keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_21",
            "start": 317,
            "end": 414,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_22@0",
            "content": "Feature Annotation in Listener Utterances",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_22",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@0",
            "content": "Emotion of Listener Utterance (EofLU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@1",
            "content": "We finetune RoBERTa (Liu et al., 2019) to detect an emotion given a situation description in EmpatheticDialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 39,
            "end": 151,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@2",
            "content": "Then, the model predicts an emotion of the listener utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 153,
            "end": 214,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@3",
            "content": "Keywords of Listener Utterance (KofLU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 216,
            "end": 254,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@4",
            "content": "Listeners express empathy in the utterance through three Communication Mechanisms (CMs) (Sharma et al., 2020) including emotional reaction, interpretation, and exploration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 256,
            "end": 427,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@5",
            "content": "Thus, three models are leveraged, where each model is trained to detect words that cause one of three CMs, using another dialogue dataset for mental health support 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 429,
            "end": 595,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@6",
            "content": "Then, three models predict such words in the listener utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 597,
            "end": 660,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_23@7",
            "content": "Since predicted words take up slightly a lot in the listener utterance, these words are filtered out in the keyword pairs construction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_23",
            "start": 662,
            "end": 796,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@0",
            "content": "Keyword Pairs Construction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@1",
            "content": "Inspired by Zou et al. (2021), keyword pairs kps are constructed not only to filter out above predicted words, but also to conduct next keyword detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 28,
            "end": 181,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@2",
            "content": "Given a dialogue 1 A dialogue has a (post, response) pair, and words which cause each CM are annotated on each dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 183,
            "end": 302,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@3",
            "content": "corpus, all pairs are extracted, where each pair has a head word and a tail word each from keywords in the speaker utterance and predicted words in the listener utterance in the same turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 304,
            "end": 491,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@4",
            "content": "Then, all pairs are filtered out to obtain high-frequency pairs through pointwise mutual information (PMI) 2 (Church and Hanks, 1990) which can measure the association between two words in a corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 493,
            "end": 690,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@5",
            "content": "Filtered pairs become kps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 692,
            "end": 717,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@6",
            "content": "A tail word of a kp is regarded as a keyword of the listener utterances joined to extract that keyword pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 719,
            "end": 826,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@7",
            "content": "Performances of feature annotations are summarized in Table 1 and show reliable results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 828,
            "end": 915,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@8",
            "content": "However, test sets for KofLU based on Empathetic-Dialogues, don't exist.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 917,
            "end": 988,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@9",
            "content": "Thus, we randomly sample 100 test dialogues in EmpatheticDialogues and ask 3 human workers to annotate whether each word plays important role for empathizing in the listener utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 990,
            "end": 1174,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@10",
            "content": "By majority voting, the final verdict on each annotation is decided.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 1176,
            "end": 1243,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_24@11",
            "content": "We compute the inter-annotator agreement on annotation of test sets for KofLU through Fleiss' kappa (\u03ba) (Fleiss and Cohen, 1973), and result in 0.55, where 0.4 < \u03ba < 0.6 indicates moderate agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_24",
            "start": 1245,
            "end": 1443,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_25@0",
            "content": "The Emp-RFT Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_25",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_26@0",
            "content": "In this section, we detail Emp-RFT whose overall architecture is shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_26",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_27@0",
            "content": "Context Encoding",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_27",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_28@0",
            "content": "Word-Level Encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_28",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_28@1",
            "content": "Emp-RFT contains an encoder f \u03b8 (\u2022) which has the six-layer encoder of BART (Lewis et al., 2020) as the backbone and extracts feature vectors of each u i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_28",
            "start": 20,
            "end": 174,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_28@2",
            "content": "Inspired by BERT (Devlin et al., 2019), we prefix each utterance with a [SEN ] token, so u i 0 = [SEN ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_28",
            "start": 176,
            "end": 279,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_28@3",
            "content": "Then, each token is represented as emb i j , the sum of the following four embeddings: word embedding, position embedding, role embedding and emotion embedding M e \u2208 R nemo\u00d7d 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_28",
            "start": 281,
            "end": 458,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_28@4",
            "content": "Then, the encoder transforms each utterance into a list of output hidden states:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_28",
            "start": 460,
            "end": 539,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_29@0",
            "content": "[ \u0125i 0 , ..., \u0125i |u i | ] = f \u03b8 ([emb i 0 , ..., emb i |u i | ]),(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_29",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@0",
            "content": "where \u0125i j \u2208 R d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@1",
            "content": "For each utterance, we can obtain utterance-level meaning vector \u0125i 0 derived from the token [SEN ], concatenated keyword vectors ki \u2208 R |k i |\u00d7d derived from the tokens corresponding to k i p (p is the index for keywords.), and emotion vector \u00eai = M e \u0125i 0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 19,
            "end": 277,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@2",
            "content": "Feature Transition Recognizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 279,
            "end": 308,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@3",
            "content": "Emp-RFT has a component that operates as the process illustrated in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 310,
            "end": 386,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@4",
            "content": "The component computes feature transition information between feature vectors, utilizing two comparison functions, subtraction and multiplication of Wang and Jiang (2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 388,
            "end": 558,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@5",
            "content": "Each feature vector is compared to previous two feature vectors 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 560,
            "end": 626,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_30@6",
            "content": "First, emotion transition information eti i is computed:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_30",
            "start": 628,
            "end": 683,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_31@0",
            "content": "eti i = ReLU(W eti (f com (\u00ea i , \u00eai\u22121 , \u00eai\u22122 ))), (2) f com (\u00ea i , \u00eai\u22121 , \u00eai\u22122 ) = \uf8ee \uf8ef \uf8ef \uf8f0 (\u00ea i \u2212 \u00eai\u22121 ) \u2299 (\u00ea i \u2212 \u00eai\u22121 ) \u00eai \u2299 \u00eai\u22121 (\u00ea i \u2212 \u00eai\u22122 ) \u2299 (\u00ea i \u2212 \u00eai\u22122 ) \u00eai \u2299 \u00eai\u22122 \uf8f9 \uf8fa \uf8fa \uf8fb ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_31",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_32@0",
            "content": "where f com and \u2299 each denote our transition information computing function and Hadamar product, and W eti \u2208 R d\u00d74nemo .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_32",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_32@1",
            "content": "Next, utterance-level meaning transition information uti i is computed:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_32",
            "start": 121,
            "end": 191,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_33@0",
            "content": "uti i = ReLU(W uti (f com ( \u0125i 0 , \u0125i\u22121 0 , \u0125i\u22122 0 ))),(4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_33",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_34@0",
            "content": ") where W uti \u2208 R d\u00d74d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_34",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_34@1",
            "content": "We then obtain enhanced utterance vector of each utterance by integrating utterance-level meaning vector, and emotion and utterance-level meaning transition information:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_34",
            "start": 25,
            "end": 193,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_35@0",
            "content": "hi = FC utt ([ \u0125i 0 ; eti i ; uti i ]),(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_35",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_36@0",
            "content": "where F C utt is a fully-connected layer with size of d. In addition, keyword transition information kti i is computed between concatenated keyword vectors and cross-encoded vectors c t , where t \u2208 {i \u2212 1, i \u2212 2}:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_36",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_37@0",
            "content": "\u2026 \u2026 h 0 i h 0 i!\" h 0 i!# $ e i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_37",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_38@0",
            "content": "# k i!$ # k i!\" # k i % k p i \u2026 Cross Encoding k i c i!\" c i!# kti i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_38",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_39@0",
            "content": "kti i = ReLU(W kti (f com ( ki , c i\u22121 , c i\u22122 )) T ), (6) c t = softmax(Q i (K t ) T ) kt ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_39",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_40@0",
            "content": "Q i = ki W Q , K t = kt W K ,(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_40",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_41@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_41",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_42@0",
            "content": "W kti \u2208 R d\u00d74d , W Q and W K \u2208 R d\u00d7d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_42",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_43@0",
            "content": "We can obtain enhanced keyword vector of each keyword by integrating keyword vector, and keyword transition information:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_43",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_44@0",
            "content": "ki p = FC key ([ ki p ; kti i p ]),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_44",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_45@0",
            "content": "where F C key is a fully-connected layer with size of d. Consequently, the enhanced feature vectors guide Emp-RFT to accurately grasp the features of utterance that the model should be attentive to when given feature transition information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_45",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_46@0",
            "content": "Utterance-Level Encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_46",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_46@1",
            "content": "Emp-RFT contains another encoder g \u03d5 (\u2022) which has the six-layer encoder of BART, and transforms enhanced utterance vectors with global position embeddings (GPE) into a context representation to capture relationships between utterances (Gu et al., 2021):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_46",
            "start": 25,
            "end": 278,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_47@0",
            "content": "[ \u1e271 , ..., \u1e27n\u22121 ] = g \u03d5 ([ h1 , ..., hn\u22121 ]).(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_47",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_48@0",
            "content": "Emp-RFT consists of hierarchical structures of encoders through word-level and utterance-level encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_48",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_48@1",
            "content": "This structure makes Emp-RFT comprehend each utterance at the fine-grained level, and understand the context by integrating information based on comprehension of each utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_48",
            "start": 105,
            "end": 281,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@0",
            "content": "Fusing Context with Keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@1",
            "content": "Emp-RFT fuses context with keywords as the process illustrated in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 30,
            "end": 104,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@2",
            "content": "We first dynamically build keyword graph for each context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 106,
            "end": 163,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@3",
            "content": "Keywords in each context become nodes and are initialized by corresponding enhanced keyword vectors with GPE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 165,
            "end": 273,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@4",
            "content": "Edges are built across the below cases: (1) between two keywords from the same utterance and (2) between a keyword from a certain utterance and another keyword from the previous two utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 275,
            "end": 467,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@5",
            "content": "Also, a tail word in a kp whose head word is k n\u22121 p is appended as a node and connected with k n\u22121 p node.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 469,
            "end": 575,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@6",
            "content": "Appended nodes (AN s) are initialized through BART decoder whose parameters are frozen with GPE, and used for next keywords detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 577,
            "end": 710,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@7",
            "content": "To obtain keyword representation vi o from the keyword graph(o is the index for nodes.), nodes are updated based on multi-head graph-attention mechanism (Veli\u010dkovi\u0107 et al., 2018;Li et al., 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 712,
            "end": 906,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_49@8",
            "content": "This mechanism makes Emp-RFT not only capture relationships between nodes but also manage influences of each appended node through attention architecture:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_49",
            "start": 908,
            "end": 1061,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_50@0",
            "content": "vi o = v i o + M H mh=1 z\u2208A i o \u03b1 i,mh oz (W mh v v z ),(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_50",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_51@0",
            "content": "\u03b1 i,mh oz = exp((W mh q v i o ) T W mh key v z ) s\u2208A i o exp((W mh q v i o ) T W mh key v s ) ,(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_51",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_52@0",
            "content": "where v i o , \u2225, A i o , and \u03b1 i,mh oz each denote a node representation, the concatenation of M H attention heads, the neighbours of v i o in the adjacency matrix A, and self-attention weight and W mh v , W mh q , W mh key \u2208 R d mh \u00d7d (d mh = d/M H).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_52",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_52@1",
            "content": "Lastly, we can obtain the fused context representation H = [h 1 , ..., h n\u22121 ] by fusing the context representation with the sum of keyword representations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_52",
            "start": 252,
            "end": 407,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_53@0",
            "content": "h i = FC f use ([ \u1e27i ; sum([v i 1 , ..., vi |k i | ])]), (13",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_53",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_54@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_54",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_55@0",
            "content": "where F C f use is a fully-connected layer with size of d. Consequently, keywords are emphasized within each utterance and get greater attention when generating responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_55",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_56@0",
            "content": "Next Emotion and Keywords Detection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_56",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_56@1",
            "content": "Emp-RFT detects next emotion e y and keywords k y , which helps figure out proper emotion and keywords for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_56",
            "start": 37,
            "end": 154,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_56@2",
            "content": "First, based on the maxpooled fused context representation, next emotion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_56",
            "start": 156,
            "end": 227,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_57@0",
            "content": "k ! i\"# Keyword Pairs(kps) \u2026 k k i i \u2026 \u2026 \u2026 k k i!\" i#$ k k i!% i#& k $ i Graph Attention Network AN AN ! AN \" AN # AN AN (k i , AN ) (k i , AN ! ) (k i , AN \" ) (k ! i , AN \" ) (k ! i , AN # ) (k k i i , AN ) (k k i i , AN # ) (k k i i ,AN AN ) H Fused Context Representation \u2026 Figure 4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_57",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_58@0",
            "content": ": Operation process of fusing context with keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_58",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_58@1",
            "content": "AN o is the appended node.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_58",
            "start": 53,
            "end": 78,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_58@2",
            "content": "Some symbols and edges are omitted for simplicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_58",
            "start": 80,
            "end": 129,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_59@0",
            "content": "distribution is predicted:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_59",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_60@0",
            "content": "P e = softmax(M e MP(H)),(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_60",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_61@0",
            "content": "where MP denotes maxpooling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_61",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_61@1",
            "content": "We use the emotion with the highest probability (\u00ea y ) for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_61",
            "start": 29,
            "end": 98,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_61@2",
            "content": "Also, Emp-RFT predicts whether the word of each AN belongs to the next keywords through the binary classification, where the true label denotes the word belongs to:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_61",
            "start": 100,
            "end": 263,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_62@0",
            "content": "P k = |AN s| o=1 softmax(W AN [v n o ; MP(H)]),(15)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_62",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_63@0",
            "content": "where W AN \u2208 R 2\u00d72d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_63",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_63@1",
            "content": "We consider the words of AN s whose probabilities for the true label \u2265 0.8 as the keywords ( ky ) for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_63",
            "start": 22,
            "end": 134,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_64@0",
            "content": "Response Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_64",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_65@0",
            "content": "Response Generator. Emp-RFT includes a response generator (RG) which has the six-layer decoder of BART as the backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_65",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_65@1",
            "content": "Through the four embeddings with \u00eay , explained previously, we can obtain the input sequence embedding for RG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_65",
            "start": 120,
            "end": 229,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_65@2",
            "content": "We prefix it with the sum of node representations corresponding to ky .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_65",
            "start": 231,
            "end": 301,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_65@3",
            "content": "Then, RG is fed to predict probability distribution on each next token y t based on the fused context representation:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_65",
            "start": 303,
            "end": 419,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_66@0",
            "content": "P (y|con, e, k, kps) = m t=1 P (y t |y <t , H).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_66",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_66@1",
            "content": "( 16)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_66",
            "start": 48,
            "end": 52,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_67@0",
            "content": "Training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_67",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_67@1",
            "content": "We apply cross-entropy loss to three objectives (eq. 14, 15, 16), and train parameters of Emp-RFT in end-to-end manner through the sum of all losses .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_67",
            "start": 10,
            "end": 159,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@0",
            "content": "Contrastive PPLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@1",
            "content": "Analysis on the generated responses of the trained Emp-RFT shows that an active reflection of ky is demanded.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 18,
            "end": 126,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@2",
            "content": "Thus, inspired by Dathathri et al. (2020); , we propose Contrastive PPLM with a discriminator using contrastive loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 128,
            "end": 244,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@3",
            "content": "Existing discriminators (Dathathri et al., 2020;Majumder et al., 2021) are trained to predict whether a sentence contains a certain attribute, using cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 246,
            "end": 413,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@4",
            "content": "Then, the gradient of the loss is passed to the generative model to generate a sentence containing such attribute during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 415,
            "end": 545,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_68@5",
            "content": "However, since keywords are not attributes but objects, we train a discriminator to predict whether a response in EmpatheticDialogues is more similar to the keyword set of the response(positive sample) than the keyword sets of another responses(negative samples) in the same batch, using contrastive loss based on the similarity between objects:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_68",
            "start": 547,
            "end": 891,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_69@0",
            "content": "L a pplm = \u2212log exp(r T a ks a /\u03c4 ) B b=1 exp(r T a ks b /\u03c4 ) ,(17)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_69",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_70@0",
            "content": "where r,ks,\u03c4 and B each denote response and keyword set representations, a temperature parameter and batchsize.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_70",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_70@1",
            "content": "During inference, we repeatedly sample three random AN s except for nodes of ky , and consider the sum of such AN representations as one of negative samples and the sum of node representations corresponding to ky as a positive sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_70",
            "start": 112,
            "end": 345,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_70@2",
            "content": "Then, the gradient of the contrastive loss is passed to Emp-RFT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_70",
            "start": 347,
            "end": 410,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_71@0",
            "content": "6 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_71",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_72@0",
            "content": "Dataset and Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_72",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@0",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@1",
            "content": "Experiments were conducted on Empa-theticDialogues (Rashkin et al., 2019) which contains 24,850 multi-turn dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 9,
            "end": 125,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@2",
            "content": "For each dialogue, we can extract a certain number of instances corresponding to the number of turns within the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 127,
            "end": 247,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@3",
            "content": "This totals to 47,611 instances, where 22,761 are multi-turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 249,
            "end": 309,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@4",
            "content": "In one turn of a dialogue, a speaker talks about one of 32 evenly distributed emotion labels and a situation related to the emotion label and a listener empathizes by responding to the speaker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 311,
            "end": 503,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_73@5",
            "content": "Following the instructions of the dataset, we use 8:1:1 train/valid/test split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_73",
            "start": 505,
            "end": 583,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@0",
            "content": "Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@1",
            "content": "We compared Emp-RFT to the following five baseline models: (1) MoEL (Lin et al., 2019) is a transformer-based generative model, which has decoders for each emotion and integrates outputs of the decoders according to predicted emotion distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 11,
            "end": 257,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@2",
            "content": "(2) EmpDG (Li et al., 2020) uses emotional words and consists of an adversarial framework including a generator and discriminators which reflect the user feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 259,
            "end": 421,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@3",
            "content": "(3) MIME (Majumder et al., 2020) is also a transformer-based generative model which mimics user emotion based on emotion grouping and uses stochastic sampling for varied responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 423,
            "end": 602,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@4",
            "content": "(4) MIME+Focused S1 and (5) Blender+Focused S1 (Kim et al., 2021) attach RSA Framework to MIME and Blender (Roller et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 604,
            "end": 732,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@5",
            "content": "Blender is a pretrained model with 90M parameters size, using an immense number of dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 734,
            "end": 826,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@6",
            "content": "It is finetuned on EmpatheticDialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 828,
            "end": 866,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@7",
            "content": "Using distractors and Bayes' Rules, RSA Framework makes the models focus on certain parts of the post, such as emotion cause words when generating responses in the single-turn dialogues 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 868,
            "end": 1056,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_74@8",
            "content": "Implementation details about Emp-RFT and baselines are covered in Appendix A.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_74",
            "start": 1058,
            "end": 1136,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_75@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_75",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_76@0",
            "content": "Automatic Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_76",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_76@1",
            "content": "We evaluated the models, using the following three metrics: (1) Perplexity (PPL) (Vinyals and Le, 2015) measures how highly likely tokens are generated, which evaluates the overall quality of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_76",
            "start": 22,
            "end": 223,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_76@2",
            "content": "(2) Distinct-n (Dist-n) (Li et al., 2016) measures how diverse the generated response is via the unique words within its n-gram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_76",
            "start": 225,
            "end": 352,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_76@3",
            "content": "(3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_76",
            "start": 354,
            "end": 357,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_76@4",
            "content": "We use BERTscore (F BERT ) (Zhang et al., 2019) which measures token-level semantic similarities between the generated response and the gold response based on embeddings from BERT (Devlin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_76",
            "start": 359,
            "end": 560,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_77@0",
            "content": "Human Ratings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_77",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_77@1",
            "content": "Human evaluations for the dialogues models are essential because of insufficient reliability on automatic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_77",
            "start": 15,
            "end": 128,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_77@2",
            "content": "We randomly sampled 100 test dialogues and asked 3 human workers to score models' generated responses on 1 to 5 point scale, following the four metrics (Rashkin et al., 2019): (1) Empathy measures whether the generated response understands the speaker's emotion and situation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_77",
            "start": 130,
            "end": 405,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_77@3",
            "content": "(2) Relevance measures whether the generated response is coherent to the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_77",
            "start": 407,
            "end": 487,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_78@0",
            "content": "(3) Fluency measures whether the generated response is grammatically correct and readable. (4) Since we conclude that models generating generic responses are not empathizing to the speaker, we use Diversity to measure whether the generated response is non-generic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_78",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_79@0",
            "content": "Human A/B Test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_79",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_79@1",
            "content": "We further conducted a human A/B test which provides stronger intuitions and higher agreements than human ratings, because this is carried with 3 human workers selecting the better response when given two generated responses (Sabour et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_79",
            "start": 16,
            "end": 262,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_80@0",
            "content": "Analysis of Response Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_80",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_81@0",
            "content": "We abbreviate feature transition recognizer, contrastive PPLM, next emotion and keywords detection, and fusing context with keywords as FTR, CP, NEKD, and FCK, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_81",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@0",
            "content": "Automatic Evaluation Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@1",
            "content": "The overall automatic evaluation results are shown in the left part of Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 30,
            "end": 108,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@2",
            "content": "Emp-RFT performed exceedingly on all metrics except for PPL, which was nearly the same as Blender+Focused S1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 110,
            "end": 218,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@3",
            "content": "The improvements on other metrics indicated that our approach was effective for generating generally high quality and non-generic responses which were also semantically similar with the gold response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 220,
            "end": 419,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@4",
            "content": "While the utilization of pretrained models yielded significant improvements compared to models only trained on EmpatheticDialogues, Emp-RFT showed even greater performance when compared to Blender+Focused S1 endowed with more significant number of dialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 421,
            "end": 678,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_82@5",
            "content": "In addition, due to utilization of FTR, Emp-RFT obtained remarkable results even on multi-turn instances, whereas, other models suffered due to their means of utilizing features for the context at the coarse-grained level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_82",
            "start": 680,
            "end": 901,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@0",
            "content": "Human Evaluation Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@1",
            "content": "In the right part of Table 2, Emp-RFT acquired the highest scores on all metrics, which demonstrated that all components of Emp-RFT helped generate responses that are empathetic, coherent to the context, and non-generic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 26,
            "end": 245,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@2",
            "content": "Also, utilizing pretrained models showed significant improvements, especially on Fluency and Diversity scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 247,
            "end": 356,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@3",
            "content": "In Table 3, the generated responses from Emp-RFT were more preferred, which indicated Emp-RFT consistently outperformed other methods in various experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 358,
            "end": 514,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@4",
            "content": "When observing at the models' performance difference between multi-turn instances and all instances, only Emp-RFT continued to perform consistently, whereas other models showed significant performance drops under multi-turn instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 516,
            "end": 749,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_83@5",
            "content": "From this, we concluded that Emp-RFT continuously understood the dialogue flow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_83",
            "start": 751,
            "end": 829,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_84@0",
            "content": "Ablation Study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_84",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_84@1",
            "content": "To better understand effects of each component in Emp-RFT, we conducted the ablation study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_84",
            "start": 16,
            "end": 106,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_84@2",
            "content": "We gradually ablated each component within the response generation strategy in a hierarchical manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_84",
            "start": 108,
            "end": 208,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_84@3",
            "content": "(1) w/o FTR: Feature transition recognizer was disabled, which resulted in considerable drops on all metrics, especially on PPL, F BERT , Empathy, and Relevance scores on multi-turn instances, because Emp-RFT could not grasp the attention-needed features of utterance within multi-turn instances through FTR. ( 2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_84",
            "start": 210,
            "end": 521,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_85@0",
            "content": "Analysis of Next Emotion and Keywords",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_85",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_86@0",
            "content": "We report the results in terms of NEKD in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_86",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_86@1",
            "content": "Since all baselines have not conducted NEKD, we trained models showing promising results such as CoMAE , ConceptFlow (Zhang et al., 2020) and CG-nAR (Zou et al., 2021) with EmpatheticDialogues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_86",
            "start": 51,
            "end": 243,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_86@2",
            "content": "(More details are covered in Appendix A.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_86",
            "start": 245,
            "end": 287,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_86@3",
            "content": "Then, we compared Emp-RFT to those models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_86",
            "start": 289,
            "end": 330,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_86@4",
            "content": "Emp-RFT outperformed other models on all metrics, which proved Emp-RFT figured out which emotion and keywords were proper for generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_86",
            "start": 332,
            "end": 468,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@0",
            "content": "Emotion Label: Furious Annotated Emotion: Annoyed \u2192 Apprehensive \u2192 Confident \u2192 Hopeful u 1 : My roommate eats my food sometimes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@1",
            "content": "This makes me so angry! u 2 : You should get a mini fridge and put it in your room, with a lock on it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 129,
            "end": 230,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@2",
            "content": "u 3 : I think that's a great idea.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 232,
            "end": 265,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@3",
            "content": "I know where to get those fridges for cheap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 267,
            "end": 310,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@4",
            "content": "Gold: Yea man go for it, don't procrastinate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 312,
            "end": 356,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@5",
            "content": "MoEL: I am sorry you have to hear that. I hope it works out for you.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 358,
            "end": 425,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@6",
            "content": "EmpDG: I agree with you. I think it isn't worth before you get it back.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 427,
            "end": 497,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@7",
            "content": "MIME: I am sorry to hear that. I hope you don't have to deal with that.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 499,
            "end": 569,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@8",
            "content": "MIME+Focused S1: I agree. I have a friend who is not to be a parent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 571,
            "end": 638,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_87@9",
            "content": "Blender+Focused S1: Roommates can be so annoying.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_87",
            "start": 640,
            "end": 688,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_88@0",
            "content": "\u00eay : Trusting, ky : procrastinate, safety, profit Emp-RFT: Don't procrastinate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_88",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_88@1",
            "content": "It makes your foods safety.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_88",
            "start": 80,
            "end": 106,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_88@2",
            "content": "w/o FTR: I agree. I have a safety but pricy fridge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_88",
            "start": 108,
            "end": 158,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@0",
            "content": "Emotion Label: Proud Annotated Emotion: Trusting \u2192 Proud \u2192 Grateful\u2192 Sentimental \u2192 Disappointed \u2192 Sentimental u 1 : Hi, My girlfriend was recently let go of her job, but she put so much work in to finding a new job and got one with better pay! u 2 : That's great! It was meant to happen then.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@1",
            "content": "u 3 : Yes I agree, she was so happy when she found out the job was hers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 293,
            "end": 364,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@2",
            "content": "u 4 : I would have been too. I've never been fired.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 366,
            "end": 416,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@3",
            "content": "I always quit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 418,
            "end": 431,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@4",
            "content": "u 5 : She wasn't really fired, her position was no more. Schools don't have much room left for Home Ec teachers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 433,
            "end": 544,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@5",
            "content": "Sad.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 546,
            "end": 549,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@6",
            "content": "Gold: I remember taking Home Ec.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 551,
            "end": 582,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_89@7",
            "content": "I loved that class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_89",
            "start": 584,
            "end": 602,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_90@0",
            "content": "Case Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_90",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@0",
            "content": "The cases from the models are shown in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@1",
            "content": "In the first case, MoEL and MIME expressed regret, which was emotionally inappropriate to the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 48,
            "end": 149,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@2",
            "content": "All baselines except for MoEL failed to grasp the proper features within the context, and therefore generated incoherent responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 151,
            "end": 281,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@3",
            "content": "Especially, Blender+Focused S1 ignored the features of u 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 283,
            "end": 342,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@4",
            "content": "Since Emp-RFT understood the dialogue flow, it became attentive to not only the features of u 3 but also those of u 1 , u 2 , mentioning ('procrastinate', 'foods', 'safety'), which led to empathy and coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 344,
            "end": 553,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@5",
            "content": "In the second case, all baselines couldn't understand the longer context, which resulted in improper empathy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 555,
            "end": 663,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@6",
            "content": "Also, Blender+Focused S1 disregarded the features of u 5 , and therefore overlooked the speaker's sadness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 665,
            "end": 770,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@7",
            "content": "Emp-RFT fully comprehended why the speaker's happiness changed to the sadness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 772,
            "end": 849,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@8",
            "content": "In both cases, without FTR, the responses of Emp-RFT were non-empathetic and incoherent because of dismissing appropriate features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 851,
            "end": 981,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@9",
            "content": "In the third case, we report the case in terms of the response generation strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 983,
            "end": 1065,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@10",
            "content": "Without CP, NEKD, and FCK, Emp-RFT produced a generic response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 1067,
            "end": 1129,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@11",
            "content": "With the utilization of FCK, Emp-RFT perceived the word 'cancer' in u 3 but expressed excessive emotion by mentioning 'scary'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 1131,
            "end": 1256,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@12",
            "content": "When Emp-RFT additionally conducted NEKD, Emp-RFT generated emotionally appropriate responses by mentioning 'sorry' and 'hard', and utilized the keyword 'lost'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 1258,
            "end": 1417,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_91@13",
            "content": "Lastly, with CP, Emp-RFT generated a diverse response, actively using ky .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_91",
            "start": 1419,
            "end": 1492,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_92@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_92",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_93@0",
            "content": "We proposed a novel approach that recognizes feature transitions between utterances, which led to understanding the dialogue flow and grasping the features of utterance that needs attention. Also, to make our model focus on emotion and keywords related to appropriate features, we introduced a response generation strategy including fusing context with keywords, next emotion and keywords detection, and contrastive PPLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_93",
            "start": 0,
            "end": 420,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_93@1",
            "content": "Experimental results showed that our model outperformed baselines, and especially, achieved significant improvements on multi-turn instances, which proved our approach was effective for empathetic, coherent, and nongeneric response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_93",
            "start": 422,
            "end": 664,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_94@0",
            "content": "Ethical Considerations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_94",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@0",
            "content": "We expect that our proposed approach does not suffer from ethical problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@1",
            "content": "The dataset we use in our work is EmpatheticDialogues which is Englishbased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 76,
            "end": 151,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@2",
            "content": "The dataset is constructed by crowdsourcing with Amazon Mechanical Turk, which protects private user information (Rashkin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 153,
            "end": 288,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@3",
            "content": "In addition, the dialogue dataset is anticipated not to have responses which include discrimination, abuse, bias, etc, because the robust collection procedure of EmpatheticDialogues ensures the quality of the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 290,
            "end": 506,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@4",
            "content": "Thus, we expect that models trained using the dataset, do not generate inappropriate responses which harm the users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 508,
            "end": 623,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@5",
            "content": "However, we inform that our model utilizes a pretrained language model, which may produce inappropriate responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 625,
            "end": 738,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_95@6",
            "content": "Lastly, we anticipate our model make potential users be interested and consoled by generating empathetic responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_95",
            "start": 740,
            "end": 854,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@0",
            "content": "Our model is implemented by Pytorch 10 , and based on two encoders of BART-base and a decoder of BART-base 11 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@1",
            "content": "Hidden size d is 768 and the number of emotion classes n emo is 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 112,
            "end": 178,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@2",
            "content": "M H and the number of layers of graph attention network are each 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 180,
            "end": 246,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@3",
            "content": "Using Adam optimization (Kingma and Ba, 2015), our model is trained on single RTX 3090 GPU with a batch size of 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 248,
            "end": 361,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@4",
            "content": "We apply early-stopping and select a model showing the best performance through perplexity on the valid set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 363,
            "end": 470,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@5",
            "content": "For contrastive PPLM, we utilize the official code of PPLM 12 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 472,
            "end": 534,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@6",
            "content": "We set a temperature parameter \u03c4 and batch size to 0.5 and 64, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 536,
            "end": 611,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@7",
            "content": "Through represenations derived from the last token of BART decoder whose parameters are frozen, we can obtain each response representation r a and each keyword set representation ks a , where the keyword set corresponds to the response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 613,
            "end": 848,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_96@8",
            "content": "Thus, ks a becomes a positive sample for r a , and keyword set representations for other responses in the same batch become negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_96",
            "start": 850,
            "end": 990,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_97@0",
            "content": "We utilize the repositories and follow implemetation details of CoMAE 13 , ConceptFlow 14 , and CG-nAR 15 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_97",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_97@1",
            "content": "We train three models, using Empa-theticDialogues instead of originally used datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_97",
            "start": 108,
            "end": 193,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_98@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_98",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_99@0",
            "content": "UNKNOWN, None, 1990, Word association norms, mutual information, and lexicography. Computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_99",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_100@0",
            "content": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, Plug and play language models: A simple approach to controlled text generation, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_100",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_101@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_101",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_102@0",
            "content": "Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, Eraser: A benchmark to evaluate rationalized nlp models, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_102",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_103@0",
            "content": "UNKNOWN, None, 1987, Critical issues in the study of empathy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_103",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_104@0",
            "content": "L Joseph, Jacob Fleiss,  Cohen, The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability, 1973, Educational and psychological measurement, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_104",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_105@0",
            "content": "C Michael,  Frank,  Noah D Goodman, Predicting pragmatic reasoning in language games, 2012, Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_105",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_106@0",
            "content": "Jun Gao, Yuhan Liu, Haolin Deng, Wei Wang, Yu Cao, Jiachen Du, Ruifeng Xu, Improving empathetic response generation by recognizing emotion cause in conversations, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_106",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_107@0",
            "content": "Xiaodong Gu, Jung-Woo Kang Min Yoo,  Ha, Dialogbert: Discourse-aware response generation via learning to recover and rank utterances, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_107",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_108@0",
            "content": "Hyunwoo Kim, Byeongchang Kim, Gunhee Kim, Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_108",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_109@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015-05-07, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_109",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_110@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_110",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_111@0",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_111",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_112@0",
            "content": "Qintong Li, Hongshen Chen, Zhaochun Ren, Pengjie Ren, Zhaopeng Tu, Zhumin Chen, Empdg: Multi-resolution interactive empathetic dialogue generation, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_112",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_113@0",
            "content": "UNKNOWN, None, , Zhaochun Ren, Pengjie Ren, and Zhumin Chen. 2022. Knowledge bridging for empathetic dialogue generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_113",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_114@0",
            "content": "Zhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, Pascale Fung, Moel: Mixture of empathetic listeners, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_114",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_115@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_115",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_116@0",
            "content": "Sudha Bodhisattwa Prasad Majumder, Michel Rao, Julian Galley,  Mcauley, Ask what's missing and what's useful: Improving clarification question generation using global knowledge, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_116",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_117@0",
            "content": "Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway Ghosal, Alexander Gelbukh, Mime: Mimicking emotions for empathetic response generation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_117",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_118@0",
            "content": "Lisong Qiu, Yingwai Shiu, Pingping Lin, Ruihua Song, Yue Liu, Dongyan Zhao, Rui Yan, What if bots feel moods?, 2020, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_118",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_119@0",
            "content": "Eric Hannah Rashkin, Margaret Smith, Y-Lan Li,  Boureau, Towards empathetic opendomain conversation models: A new benchmark and dataset, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_119",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_120@0",
            "content": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Smith, Y-Lan Boureau, Recipes for building an open-domain chatbot, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_120",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2021, Cem: Commonsense-aware empathetic response generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_121",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_122@0",
            "content": "Ashish Sharma, Adam Miner, David Atkins, Tim Althoff, A computational approach to understanding empathy expressed in text-based mental health support, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_122",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_123@0",
            "content": "Lei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, Jie Zhou, Constructing emotional consensus and utilizing unpaired data for empathetic dialogue generation, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_123",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2018, Graph Attention Networks. International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_124",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_125@0",
            "content": "UNKNOWN, None, 2015, A neural conversational model, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_125",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_126@0",
            "content": "UNKNOWN, None, 2017, A compareaggregate model for matching text sequences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_126",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_127@0",
            "content": "UNKNOWN, None, , ICLR 2017: International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_127",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_128@0",
            "content": "Hainan Haolan Zhan, Hongshen Zhang, Zhuoye Chen, Yongjun Ding, Yanyan Bao,  Lan, Augmenting knowledge-grounded conversations with sequential knowledge transition, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_128",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_129@0",
            "content": "Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Grounded conversation generation as guided traverses in commonsense knowledge graphs, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_129",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_130@0",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger,  Artzi, Bertscore: Evaluating text generation with bert, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_130",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_131@0",
            "content": "Chujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, Minlie Huang, Comae: A multi-factor hierarchical framework for empathetic response generation, 2021, Findings of the Association for Computational Linguistics: ACL 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_131",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "37-ARR_v2_132@0",
            "content": "Yicheng Zou, Zhihua Liu, Xingwu Hu, Qi Zhang, Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "37-ARR_v2_132",
            "start": 0,
            "end": 251,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_1",
            "tgt_ix": "37-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_1",
            "tgt_ix": "37-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_6",
            "tgt_ix": "37-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_7",
            "tgt_ix": "37-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_9",
            "tgt_ix": "37-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_10",
            "tgt_ix": "37-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_11",
            "tgt_ix": "37-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_12",
            "tgt_ix": "37-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_12",
            "tgt_ix": "37-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_12",
            "tgt_ix": "37-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_16",
            "tgt_ix": "37-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_15",
            "tgt_ix": "37-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_15",
            "tgt_ix": "37-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_15",
            "tgt_ix": "37-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_17",
            "tgt_ix": "37-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_18",
            "tgt_ix": "37-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_18",
            "tgt_ix": "37-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_18",
            "tgt_ix": "37-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_19",
            "tgt_ix": "37-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_20",
            "tgt_ix": "37-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_20",
            "tgt_ix": "37-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_18",
            "tgt_ix": "37-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_21",
            "tgt_ix": "37-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_22",
            "tgt_ix": "37-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_22",
            "tgt_ix": "37-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_22",
            "tgt_ix": "37-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_25",
            "tgt_ix": "37-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_25",
            "tgt_ix": "37-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_25",
            "tgt_ix": "37-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_26",
            "tgt_ix": "37-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_29",
            "tgt_ix": "37-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_31",
            "tgt_ix": "37-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_32",
            "tgt_ix": "37-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_33",
            "tgt_ix": "37-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_34",
            "tgt_ix": "37-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_35",
            "tgt_ix": "37-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_36",
            "tgt_ix": "37-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_37",
            "tgt_ix": "37-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_38",
            "tgt_ix": "37-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_39",
            "tgt_ix": "37-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_40",
            "tgt_ix": "37-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_41",
            "tgt_ix": "37-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_42",
            "tgt_ix": "37-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_43",
            "tgt_ix": "37-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_44",
            "tgt_ix": "37-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_45",
            "tgt_ix": "37-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_46",
            "tgt_ix": "37-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_47",
            "tgt_ix": "37-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_48",
            "tgt_ix": "37-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_50",
            "tgt_ix": "37-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_51",
            "tgt_ix": "37-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_52",
            "tgt_ix": "37-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_53",
            "tgt_ix": "37-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_54",
            "tgt_ix": "37-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_55",
            "tgt_ix": "37-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_56",
            "tgt_ix": "37-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_57",
            "tgt_ix": "37-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_58",
            "tgt_ix": "37-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_59",
            "tgt_ix": "37-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_60",
            "tgt_ix": "37-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_61",
            "tgt_ix": "37-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_62",
            "tgt_ix": "37-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_25",
            "tgt_ix": "37-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_63",
            "tgt_ix": "37-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_65",
            "tgt_ix": "37-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_66",
            "tgt_ix": "37-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_67",
            "tgt_ix": "37-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_69",
            "tgt_ix": "37-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_70",
            "tgt_ix": "37-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_71",
            "tgt_ix": "37-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_72",
            "tgt_ix": "37-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_72",
            "tgt_ix": "37-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_72",
            "tgt_ix": "37-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_77",
            "tgt_ix": "37-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_78",
            "tgt_ix": "37-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_79",
            "tgt_ix": "37-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_81",
            "tgt_ix": "37-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_84",
            "tgt_ix": "37-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_88",
            "tgt_ix": "37-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_90",
            "tgt_ix": "37-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_90",
            "tgt_ix": "37-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_92",
            "tgt_ix": "37-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_92",
            "tgt_ix": "37-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_93",
            "tgt_ix": "37-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_94",
            "tgt_ix": "37-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_94",
            "tgt_ix": "37-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_94",
            "tgt_ix": "37-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_94",
            "tgt_ix": "37-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "37-ARR_v2_0",
            "tgt_ix": "37-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_1",
            "tgt_ix": "37-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_2",
            "tgt_ix": "37-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_3",
            "tgt_ix": "37-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_4",
            "tgt_ix": "37-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_5",
            "tgt_ix": "37-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_6",
            "tgt_ix": "37-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_7",
            "tgt_ix": "37-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_8",
            "tgt_ix": "37-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_9",
            "tgt_ix": "37-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_9",
            "tgt_ix": "37-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_10",
            "tgt_ix": "37-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_11",
            "tgt_ix": "37-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_11",
            "tgt_ix": "37-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_11",
            "tgt_ix": "37-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_11",
            "tgt_ix": "37-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_12",
            "tgt_ix": "37-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_13",
            "tgt_ix": "37-ARR_v2_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_14",
            "tgt_ix": "37-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_15",
            "tgt_ix": "37-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_16",
            "tgt_ix": "37-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_17",
            "tgt_ix": "37-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_18",
            "tgt_ix": "37-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_19",
            "tgt_ix": "37-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_20",
            "tgt_ix": "37-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_21",
            "tgt_ix": "37-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_21",
            "tgt_ix": "37-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_21",
            "tgt_ix": "37-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_21",
            "tgt_ix": "37-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_22",
            "tgt_ix": "37-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_23",
            "tgt_ix": "37-ARR_v2_23@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_24",
            "tgt_ix": "37-ARR_v2_24@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_25",
            "tgt_ix": "37-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_26",
            "tgt_ix": "37-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_27",
            "tgt_ix": "37-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_28",
            "tgt_ix": "37-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_29",
            "tgt_ix": "37-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_30",
            "tgt_ix": "37-ARR_v2_30@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_31",
            "tgt_ix": "37-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_32",
            "tgt_ix": "37-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_32",
            "tgt_ix": "37-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_33",
            "tgt_ix": "37-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_34",
            "tgt_ix": "37-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_34",
            "tgt_ix": "37-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_35",
            "tgt_ix": "37-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_36",
            "tgt_ix": "37-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_37",
            "tgt_ix": "37-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_38",
            "tgt_ix": "37-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_39",
            "tgt_ix": "37-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_40",
            "tgt_ix": "37-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_41",
            "tgt_ix": "37-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_42",
            "tgt_ix": "37-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_43",
            "tgt_ix": "37-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_44",
            "tgt_ix": "37-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_45",
            "tgt_ix": "37-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_46",
            "tgt_ix": "37-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_46",
            "tgt_ix": "37-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_47",
            "tgt_ix": "37-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_48",
            "tgt_ix": "37-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_48",
            "tgt_ix": "37-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_49",
            "tgt_ix": "37-ARR_v2_49@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_50",
            "tgt_ix": "37-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_51",
            "tgt_ix": "37-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_52",
            "tgt_ix": "37-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_52",
            "tgt_ix": "37-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_53",
            "tgt_ix": "37-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_54",
            "tgt_ix": "37-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_55",
            "tgt_ix": "37-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_56",
            "tgt_ix": "37-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_56",
            "tgt_ix": "37-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_56",
            "tgt_ix": "37-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_57",
            "tgt_ix": "37-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_58",
            "tgt_ix": "37-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_58",
            "tgt_ix": "37-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_58",
            "tgt_ix": "37-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_59",
            "tgt_ix": "37-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_60",
            "tgt_ix": "37-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_61",
            "tgt_ix": "37-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_61",
            "tgt_ix": "37-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_61",
            "tgt_ix": "37-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_62",
            "tgt_ix": "37-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_63",
            "tgt_ix": "37-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_63",
            "tgt_ix": "37-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_64",
            "tgt_ix": "37-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_65",
            "tgt_ix": "37-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_65",
            "tgt_ix": "37-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_65",
            "tgt_ix": "37-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_65",
            "tgt_ix": "37-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_66",
            "tgt_ix": "37-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_66",
            "tgt_ix": "37-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_67",
            "tgt_ix": "37-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_67",
            "tgt_ix": "37-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_68",
            "tgt_ix": "37-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_69",
            "tgt_ix": "37-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_70",
            "tgt_ix": "37-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_70",
            "tgt_ix": "37-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_70",
            "tgt_ix": "37-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_71",
            "tgt_ix": "37-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_72",
            "tgt_ix": "37-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_73",
            "tgt_ix": "37-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_74",
            "tgt_ix": "37-ARR_v2_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_75",
            "tgt_ix": "37-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_76",
            "tgt_ix": "37-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_77",
            "tgt_ix": "37-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_77",
            "tgt_ix": "37-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_77",
            "tgt_ix": "37-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_77",
            "tgt_ix": "37-ARR_v2_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_78",
            "tgt_ix": "37-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_79",
            "tgt_ix": "37-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_79",
            "tgt_ix": "37-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_80",
            "tgt_ix": "37-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_81",
            "tgt_ix": "37-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_82",
            "tgt_ix": "37-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_83",
            "tgt_ix": "37-ARR_v2_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_84",
            "tgt_ix": "37-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_84",
            "tgt_ix": "37-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_84",
            "tgt_ix": "37-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_84",
            "tgt_ix": "37-ARR_v2_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_85",
            "tgt_ix": "37-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_86",
            "tgt_ix": "37-ARR_v2_86@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_87",
            "tgt_ix": "37-ARR_v2_87@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_88",
            "tgt_ix": "37-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_88",
            "tgt_ix": "37-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_88",
            "tgt_ix": "37-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_89",
            "tgt_ix": "37-ARR_v2_89@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_90",
            "tgt_ix": "37-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_91",
            "tgt_ix": "37-ARR_v2_91@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_92",
            "tgt_ix": "37-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_93",
            "tgt_ix": "37-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_93",
            "tgt_ix": "37-ARR_v2_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_94",
            "tgt_ix": "37-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_95",
            "tgt_ix": "37-ARR_v2_95@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_96",
            "tgt_ix": "37-ARR_v2_96@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_97",
            "tgt_ix": "37-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_97",
            "tgt_ix": "37-ARR_v2_97@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_98",
            "tgt_ix": "37-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_99",
            "tgt_ix": "37-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_100",
            "tgt_ix": "37-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_101",
            "tgt_ix": "37-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_102",
            "tgt_ix": "37-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_103",
            "tgt_ix": "37-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_104",
            "tgt_ix": "37-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_105",
            "tgt_ix": "37-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_106",
            "tgt_ix": "37-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_107",
            "tgt_ix": "37-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_108",
            "tgt_ix": "37-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_109",
            "tgt_ix": "37-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_110",
            "tgt_ix": "37-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_111",
            "tgt_ix": "37-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_112",
            "tgt_ix": "37-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_113",
            "tgt_ix": "37-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_114",
            "tgt_ix": "37-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_115",
            "tgt_ix": "37-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_116",
            "tgt_ix": "37-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_117",
            "tgt_ix": "37-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_118",
            "tgt_ix": "37-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_119",
            "tgt_ix": "37-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_120",
            "tgt_ix": "37-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_121",
            "tgt_ix": "37-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_122",
            "tgt_ix": "37-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_123",
            "tgt_ix": "37-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_124",
            "tgt_ix": "37-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_125",
            "tgt_ix": "37-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_126",
            "tgt_ix": "37-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_127",
            "tgt_ix": "37-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_128",
            "tgt_ix": "37-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_129",
            "tgt_ix": "37-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_130",
            "tgt_ix": "37-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_131",
            "tgt_ix": "37-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "37-ARR_v2_132",
            "tgt_ix": "37-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 950,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "37-ARR",
        "version": 2
    }
}