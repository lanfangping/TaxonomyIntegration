{
    "nodes": [
        {
            "ix": "30-ARR_v2_0",
            "content": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_2",
            "content": "We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "30-ARR_v2_4",
            "content": "Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks. As shown in Figure 1, the bold phrases-\"100% bad\" and \"brain cell killing\"-are underlying causes for a negative sentiment prediction that most human readers would recognise. These are defined as rationales in this paper. The underlined phrase-\"acting and plot\"has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_5",
            "content": "Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time. This issue can be severe in few-shot learning (FSL) scenarios. For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in outof-distribution (OOD) datasets. In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_6",
            "content": "There is a strand of research addressing this scenario that seeks to improve model performance by \"introducing methods and resources for training models less sensitive to spurious patterns\" (Kaushik et al., 2020). Most of this work relies on generating counterfactual augmented data (CAD), either manually (Kaushik et al., 2021) or automatically Qian et al., 2021;Yang et al., , 2020aDelaney et al., 2021). For example, Kaushik et al. (2020) proposed a humanin-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels (Kaushik et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_7",
            "content": "Generating manual counterfactuals, however, is expensive and time-consuming- Kaushik et al. (2020) report the cost of revising 2.5k instances at over $10,000. On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil- False Rationales:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_8",
            "content": "Film is good.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_9",
            "content": "Missed Rationale:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_10",
            "content": "I like it.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_11",
            "content": "Model Rationales ity compared to manual counterfactuals. To address these issues, we propose Rationales-centric Double-robustness Learning (RDL), a human-inthe-loop framework for data augmentation in a few-shot setting, which is efficient, robust, modelagnostic, and general across tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_12",
            "content": "Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure 2. Our double-robustness framework consists of two main modules. The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales. Such labelling requires less human input compared to fully manual counterfactual generation (see Section 3.1). In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same). Second, we apply a Dynamic Human-intervened Correction module, where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts. We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_13",
            "content": "Results on a sentiment analysis task, which is also used in Kaushik et al. (2020) Srivastava et al., 2020;Kaushik et al., 2021). In particular, previous work (Kaushik et al., 2020) relied on large-scale crowd-sourcing to generate useful augmented data. More recently, , and Wang and Culotta (2021) investigated the efficacy of the automatically generated counterfactuals for sentiment analysis. Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition. However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant. In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust. Moreover, our method generates semi-factuals instead of counterfactuals used in previous work. Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention. Active learning (Settles, 2009;Margatina et al., 2021), the most common example of human-in-theloop machine learning, asks human annotators only to provide high-level annotations (i.e. labels) for important examples. There is also some work exploring more explainable AI systems by exploiting feature-based information. Such methods use relatively simple models such as Na\u00efve Bayes (Stumpf * All resources are available at https://github.com/GeorgeLuImmortal/RDL-Rationalescentric-Double-robustness-Learning/ Kulesza et al., 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017;Teso and Kersting, 2019;Ghai et al., 2021;Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_14",
            "content": "Some other work uses simple neural networks such as multi-layer perceptrons (Shao et al., 2021) and shallow CNNs (Lertvittayakumjorn et al., 2020;Teso et al., 2021) because the predictions of such models can be explained in the form of features. Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights. However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability. Also, they assume access to a large amount of labelled data. In contrast, we focus on few-shot learning scenarios which are more compelling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_15",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "30-ARR_v2_16",
            "content": "The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_17",
            "content": "Static semi-factual generation is a more efficient alternative to manually generated counterfactuals (Kaushik et al., 2020). In the first phase, Rationale Marking (Section 3.1), human annotators review each document in the training set to provide rationales (i.e. phrases that support the document classification decisions shown as bold text in Figure 2). The second phase is a semi-factual generation method based on synonym replacement (Section 3.2) that produces augmented examples (blue text in Figure 2 indicates replaced words), which are added into the training set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_18",
            "content": "Dynamic human-intervened correction (Section 3.3) is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours. At the outset, sampling and sensitivity of contextual decomposition (SCD) (Jin et al., 2019) is applied to detect the rationales given by the model that is obtained in the previous step. Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model). Both false rationales and missing rationales are corrected to produce augmented examples. Finally, newly generated examples are added into the training set to re-train the deep learning model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_19",
            "content": "Rationale Marking",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "30-ARR_v2_20",
            "content": "Following Kaushik et al. (2020) and , we use the IMDb movie review dataset (Maas et al., 2011) in our experiments. It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon (Zaidan et al., 2007).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_21",
            "content": "We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions. At the outset, annotators were given instructions and examples that gently guided them to annotate rationales. Only adjectives, adverbs, nouns, and verbs were considered as rationales. Besides, rationales were required to carry complete semantic information. For example, for a phrase starting with a negation word such as \"not great\", annotators are instructed to mark the whole phrase \"not great\" as a rationale instead of just marking \"not\". We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams). Phrases consisting of numerical scores are not counted as rationales (e.g. 5 or 10 stars) since different datasets may use different rating scales, and annotating digits may hurt OOD performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_22",
            "content": "Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels. However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales. After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews. Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection. All re-annotated examples were approved only if all authors were happy with the quality of the annotations. Otherwise, the examples were re-annotated again.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_23",
            "content": "Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments). Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically. On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by Kaushik et al. (2020). Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300\u00d71707 183.68\u00d7100 \u2248 27.88 times more efficient than manually generated CAD.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_24",
            "content": "Static Semi-factual Generation",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "30-ARR_v2_25",
            "content": "We take a simple replacement strategy, which has been taken by , to generate semifactual examples. Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels. This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_26",
            "content": "Formally, given a training example x i = [t i1 , t i2 , ..., t ij ] (where t ij is the j th token of the i th document) and its ground truth label y i , we create a rationale vector r i = [a i1 , a i2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not (we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise). To generate a semi-factual example, x \u2032 i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms. The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet (Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model (Liu et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_27",
            "content": "In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x \u2032 i , with some replaced non-rationales and all the other tokens identical to x i . The label, y i , of a newly generated example is the same as the label of the original example, x i . Examples of generated data are shown in Table 1. Afterwards, the augmented examples are added into the training set used to train the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_28",
            "content": "Dynamic Human-intervened Correction",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "30-ARR_v2_29",
            "content": "Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online. Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model. SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction (Jin et al., 2019). Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales. The next phase allows human feedback to influence the learning process. To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_30",
            "content": "For false rationales (i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples. Unlike the static semi-factual generation (Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document. Examples of generated data are shown in Table 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_31",
            "content": "For missing rationales (i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data. Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted. For example, there is a positive movie review (bold font for rationales) \"Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 ....\". The model fails to identify \"fine\" and \"believable\" as rationales. Thus we extract the text \"\"Robert Urich was a fine actor, and he makes this TV movie believable .\" as a new example, and the class of this example is still positive. We extract the whole sentence rather than just the missing rationales to reserve more semantic information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_32",
            "content": "Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_33",
            "content": "Origin: The attempt at a \"lesbian scene\" was sad.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_34",
            "content": "Augment 1: The hint at a \"lesbian scene\" was sad . Augment 2: The attempt at a \"kiss scene\" was sad .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_35",
            "content": "Origin: I recommended this film a lot, specially in this difficult times for the planet . Augment 1: I recommended you film a lot, specially in this difficult times for the planet . Augment 2: I recommended this movie a lot, specially in this difficult times for the planet . Underlined spans were false rationales given by the model through SCD. Blue spans were synonyms used as replacements, and bold font were rationales identified by human annotators.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_36",
            "content": "Why Does RDL Work?",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "30-ARR_v2_37",
            "content": "Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels. More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations. For example, if a model incorrectly determines that \"Soylent Green\" is associated with positive sentiment (Table 2), the augmented examples that replace \"Soylent Green\" with other phrases such as \"Gang Orange\" break the spurious association. Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_38",
            "content": "Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios. In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_39",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "30-ARR_v2_40",
            "content": "Our intention is to improve the generalisability of models, and we use both in-distribution and OOD performance for evaluation. Our experiments are designed to address the following research questions:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_41",
            "content": "\u2022 RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance? \u2022 RQ2 Does dynamic human-intervened correction improve generalisability of models?",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_42",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "30-ARR_v2_43",
            "content": "For fair comparison with previous work (Kaushik et al., 2020;, we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset. Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1, 707, 245, and 488 reviews respectively and an enforced 50:50 class ratio.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_44",
            "content": "To measure the generalisation ability of different models, we focus on OOD performance. To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives and 2,339 negatives). These datasets were sampled to ensure a nearly 50:50 class balance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_45",
            "content": "Evaluating Static Semi-factual Generation",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "30-ARR_v2_46",
            "content": "To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static. We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_47",
            "content": "Experiment Setup",
            "ntype": "title",
            "meta": {
                "section": "5.2.1"
            }
        },
        {
            "ix": "30-ARR_v2_48",
            "content": "To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data. For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds. We report the average result of these 10 repetitions and use accuracy to measure the classification performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_49",
            "content": "Our experiments rely on an off-the-shelf cased \"RoBERTa-base\" model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model. Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs). We also explore the impact of the number of semi-factual examples on model performance. To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19 We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4. We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_50",
            "content": "Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "5.2.2"
            }
        },
        {
            "ix": "30-ARR_v2_51",
            "content": "As shown in Table 3, all static semi-factual generation (Static+n) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation. Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy. Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively. Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_52",
            "content": "The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset. The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets. In other words, a model that fits well with one dataset can cause performance decay on others. In this case, training with a smaller training set is more likely to reduce overfitting with the indistribution dataset and fit well with the SemEval dataset, which explains the big improvement. It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66\u00b10.84) than on the in-distribution dataset (93.23\u00b10.46), which could also be explained by the high similarity between the underlying distributions of these two datasets. As shown in Table 3, in most cases, DP underperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance. We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models. Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales). Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350). We call this Random Replacement (RR hereafter).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_53",
            "content": "As shown in Table 3, RR is slightly better than the baseline Static approach. This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent. However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals. These observations show that the model trained with Static+n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_54",
            "content": "Evaluating Dynamic Human-intervened Correction",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "30-ARR_v2_55",
            "content": "As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+n) marginalises when augmented data is increased.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_56",
            "content": "Using too much augmented data even hurts the Static+1150 performance. This observation is consistent with existing work on data augmentation (Wei and Zou, 2019). We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples. Human-in-the-loop can address this issue by dynamically correcting the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_57",
            "content": "To address RQ2, we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-theloop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020;. Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_58",
            "content": "Experiment Setup",
            "ntype": "title",
            "meta": {
                "section": "5.3.1"
            }
        },
        {
            "ix": "30-ARR_v2_59",
            "content": "We build up an active learning procedure as a baseline based on the model trained with Static. To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter). Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL performance).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_60",
            "content": "Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "5.3.2"
            }
        },
        {
            "ix": "30-ARR_v2_61",
            "content": "As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance. However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation. On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy. Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset. Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets. Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_62",
            "content": "We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table 4. Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset. We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset. The performance of Dynamic can be explained as a compromise of two correction methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_63",
            "content": "We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns. We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set. As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_64",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "30-ARR_v2_65",
            "content": "We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios. Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation. In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_66",
            "content": "Ethical Statement",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "30-ARR_v2_67",
            "content": "We honor the ACL Code of Ethics. No private data or non-public information was used in this work. All annotators have received labor fees corresponding to the amount of their annotated instances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "30-ARR_v2_68",
            "content": "UNKNOWN, None, 2021, Uncertainty estimation and out-of-distribution detection for counterfactual explanations: Pitfalls and solutions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Uncertainty estimation and out-of-distribution detection for counterfactual explanations: Pitfalls and solutions",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_69",
            "content": "Fuli Feng, Jizhi Zhang, Xiangnan He, Hanwang Zhang, Tat-Seng Chua, Empowering language understanding with counterfactual reasoning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Fuli Feng",
                    "Jizhi Zhang",
                    "Xiangnan He",
                    "Hanwang Zhang",
                    "Tat-Seng Chua"
                ],
                "title": "Empowering language understanding with counterfactual reasoning",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "30-ARR_v2_70",
            "content": "Q Bhavya Ghai, Yunfeng Liao, Rachel Zhang, Klaus Bellamy,  Mueller, Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers, 2021, Proc. ACM Hum.-Comput. Interact, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Q Bhavya Ghai",
                    "Yunfeng Liao",
                    "Rachel Zhang",
                    "Klaus Bellamy",
                    " Mueller"
                ],
                "title": "Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers",
                "pub_date": "2021",
                "pub_title": "Proc. ACM Hum.-Comput. Interact",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_71",
            "content": "UNKNOWN, None, 2018, Annotation artifacts in natural language inference data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Annotation artifacts in natural language inference data",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_72",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017-09-09, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Adversarial examples for evaluating reading comprehension systems",
                "pub_date": "2017-09-09",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "30-ARR_v2_73",
            "content": "Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, Xiang Ren, Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Xisen Jin",
                    "Zhongyu Wei",
                    "Junyi Du",
                    "Xiangyang Xue",
                    "Xiang Ren"
                ],
                "title": "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_74",
            "content": "Divyansh Kaushik, Eduard Hovy, Zachary Lipton, Learning the difference that makes a difference with counterfactually augmented data, 2020, International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Divyansh Kaushik",
                    "Eduard Hovy",
                    "Zachary Lipton"
                ],
                "title": "Learning the difference that makes a difference with counterfactually augmented data",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_75",
            "content": "Divyansh Kaushik, Amrith Setlur, Eduard Hovy, Zachary Lipton, Explaining the efficacy of counterfactually augmented data, 2021, International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Divyansh Kaushik",
                    "Amrith Setlur",
                    "Eduard Hovy",
                    "Zachary Lipton"
                ],
                "title": "Explaining the efficacy of counterfactually augmented data",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_76",
            "content": "Katherine Keith, David Jensen, Brendan O' Connor, Text and causal inference: A review of using text to remove confounding from causal estimates, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Katherine Keith",
                    "David Jensen",
                    "Brendan O' Connor"
                ],
                "title": "Text and causal inference: A review of using text to remove confounding from causal estimates",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_77",
            "content": "UNKNOWN, None, 2021, On generating plausible counterfactual and semi-factual explanations for deep learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "On generating plausible counterfactual and semi-factual explanations for deep learning",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_78",
            "content": "Diederik Kingma, Jimmy Ba, Adam: A method for stochastic optimization, 2014, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Diederik Kingma",
                    "Jimmy Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2014",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_79",
            "content": "Todd Kulesza, Margaret Burnett, Weng-Keen Wong, Simone Stumpf, Principles of explanatory debugging to personalize interactive machine learning, 2015, Proceedings of the 20th International Conference on Intelligent User Interfaces, IUI '15, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Todd Kulesza",
                    "Margaret Burnett",
                    "Weng-Keen Wong",
                    "Simone Stumpf"
                ],
                "title": "Principles of explanatory debugging to personalize interactive machine learning",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 20th International Conference on Intelligent User Interfaces, IUI '15",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "30-ARR_v2_80",
            "content": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Weng-Keen Wong, Yann Riche, Travis Moore, Ian Oberst, Amber Shinsel, Kevin Mcintosh, Explanatory debugging: Supporting end-user debugging of machine-learned programs, 2010, 2010 IEEE Symposium on Visual Languages and Human-Centric Computing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Todd Kulesza",
                    "Simone Stumpf",
                    "Margaret Burnett",
                    "Weng-Keen Wong",
                    "Yann Riche",
                    "Travis Moore",
                    "Ian Oberst",
                    "Amber Shinsel",
                    "Kevin Mcintosh"
                ],
                "title": "Explanatory debugging: Supporting end-user debugging of machine-learned programs",
                "pub_date": "2010",
                "pub_title": "2010 IEEE Symposium on Visual Languages and Human-Centric Computing",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_81",
            "content": "UNKNOWN, None, 2020, Find: Human-in-the-loop debugging deep text classifiers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Find: Human-in-the-loop debugging deep text classifiers",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_82",
            "content": "UNKNOWN, None, 2021, Explanation-based human debugging of nlp models: A survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Explanation-based human debugging of nlp models: A survey",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_83",
            "content": "Jiazheng Li, Linyi Yang, Barry Smyth, Ruihai Dong, Maec: A multimodal aligned earnings conference call dataset for financial risk prediction, 2020, Proceedings of the 29th ACM International Conference on Information & Knowledge Management, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Jiazheng Li",
                    "Linyi Yang",
                    "Barry Smyth",
                    "Ruihai Dong"
                ],
                "title": "Maec: A multimodal aligned earnings conference call dataset for financial risk prediction",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_84",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized bert pretraining approach. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_85",
            "content": "Jinghui Lu, Maeve Henchion, Ivan Bacher, Brian Namee, A sentence-level hierarchical bert model for document classification with limited labelled data, 2021, Discovery Science, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Jinghui Lu",
                    "Maeve Henchion",
                    "Ivan Bacher",
                    "Brian Namee"
                ],
                "title": "A sentence-level hierarchical bert model for document classification with limited labelled data",
                "pub_date": "2021",
                "pub_title": "Discovery Science",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "30-ARR_v2_86",
            "content": "UNKNOWN, None, 2020, Investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_87",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Andrew Maas",
                    "Raymond Daly",
                    "Peter Pham",
                    "Dan Huang",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Learning word vectors for sentiment analysis",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_88",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, 2021, Proceddings of the 2021 Conference on Empirical Methods in Natural Language Processing, Underline Science Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Katerina Margatina",
                    "Giorgos Vernikos"
                ],
                "title": "Active learning by acquiring contrastive examples",
                "pub_date": "2021",
                "pub_title": "Proceddings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Underline Science Inc"
            }
        },
        {
            "ix": "30-ARR_v2_89",
            "content": "Rachel Mccloy, M Ruth,  Byrne, Semifactual \"even if, 2002, thinking. Thinking & Reasoning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Rachel Mccloy",
                    "M Ruth",
                    " Byrne"
                ],
                "title": "Semifactual \"even if",
                "pub_date": "2002",
                "pub_title": "thinking. Thinking & Reasoning",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_90",
            "content": "George Miller, Wordnet: A lexical database for english, 1995, Commun. ACM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "George Miller"
                ],
                "title": "Wordnet: A lexical database for english",
                "pub_date": "1995",
                "pub_title": "Commun. ACM",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_91",
            "content": "Jianmo Ni, Jiacheng Li, Julian Mcauley, Justifying recommendations using distantly-labeled reviews and fine-grained aspects, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Jianmo Ni",
                    "Jiacheng Li",
                    "Julian Mcauley"
                ],
                "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_92",
            "content": "Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, Pengjun Xie, Counterfactual inference for text classification debiasing, 2021, Proceedings of the 59th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Chen Qian",
                    "Fuli Feng",
                    "Lijie Wen",
                    "Chunping Ma",
                    "Pengjun Xie"
                ],
                "title": "Counterfactual inference for text classification debiasing",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_93",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "30-ARR_v2_94",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, Anchors: High-precision modelagnostic explanations, 2018, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Sameer Marco Tulio Ribeiro",
                    "Carlos Singh",
                    " Guestrin"
                ],
                "title": "Anchors: High-precision modelagnostic explanations",
                "pub_date": "2018",
                "pub_title": "Proceedings of the AAAI conference on artificial intelligence",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_95",
            "content": "UNKNOWN, None, 1997, Counterfactual thinking. Psychological bulletin, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "1997",
                "pub_title": "Counterfactual thinking. Psychological bulletin",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_96",
            "content": "Sara Rosenthal, Noura Farra, Preslav Nakov, SemEval-2017 task 4: Sentiment analysis in Twitter, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Sara Rosenthal",
                    "Noura Farra",
                    "Preslav Nakov"
                ],
                "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_97",
            "content": "UNKNOWN, None, 2009, Active learning literature survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Active learning literature survey",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_98",
            "content": "Xiaoting Shao, Arseny Skryagin, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, Right for better reasons: Training differentiable models by constraining their influence functions, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Xiaoting Shao",
                    "Arseny Skryagin",
                    "Wolfgang Stammer",
                    "Patrick Schramowski",
                    "Kristian Kersting"
                ],
                "title": "Right for better reasons: Training differentiable models by constraining their influence functions",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_99",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "30-ARR_v2_100",
            "content": "Megha Srivastava, Tatsunori Hashimoto, Percy Liang, Robustness to spurious correlations via human annotations, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Megha Srivastava",
                    "Tatsunori Hashimoto",
                    "Percy Liang"
                ],
                "title": "Robustness to spurious correlations via human annotations",
                "pub_date": "2020",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "30-ARR_v2_101",
            "content": "Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations, 2021-06-19, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Wolfgang Stammer",
                    "Patrick Schramowski",
                    "Kristian Kersting"
                ],
                "title": "Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations",
                "pub_date": "2021-06-19",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_102",
            "content": "Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, Thomas Dietterich, Erin Sullivan, Jonathan Herlocker, Interacting meaningfully with machine learning systems: Three experiments, 2009, Int. J. Hum.-Comput. Stud, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Simone Stumpf",
                    "Vidya Rajaram",
                    "Lida Li",
                    "Weng-Keen Wong",
                    "Margaret Burnett",
                    "Thomas Dietterich",
                    "Erin Sullivan",
                    "Jonathan Herlocker"
                ],
                "title": "Interacting meaningfully with machine learning systems: Three experiments",
                "pub_date": "2009",
                "pub_title": "Int. J. Hum.-Comput. Stud",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_103",
            "content": "Stefano Teso, Andrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, Interactive label cleaning with example-based explanations, 2021, Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Stefano Teso",
                    "Andrea Bontempelli",
                    "Fausto Giunchiglia",
                    "Andrea Passerini"
                ],
                "title": "Interactive label cleaning with example-based explanations",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_104",
            "content": "Stefano Teso, Kristian Kersting, Explanatory interactive machine learning, 2019, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19, Association for Computing Machinery.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Stefano Teso",
                    "Kristian Kersting"
                ],
                "title": "Explanatory interactive machine learning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19",
                "pub": "Association for Computing Machinery"
            }
        },
        {
            "ix": "30-ARR_v2_105",
            "content": "Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Lifu Tu",
                    "Garima Lalwani"
                ],
                "title": "Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models",
                "pub_date": null,
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_106",
            "content": "UNKNOWN, None, 2021, Identifying and mitigating spurious correlations for improving robustness in nlp models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Identifying and mitigating spurious correlations for improving robustness in nlp models",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_107",
            "content": "Zhao Wang, Aron Culotta, Robustness to spurious correlations in text classification via automatically generated counterfactuals, 2021, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Zhao Wang",
                    "Aron Culotta"
                ],
                "title": "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
                "pub_date": "2021",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_108",
            "content": "Jason Wei, Kai Zou, EDA: Easy data augmentation techniques for boosting performance on text classification tasks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Jason Wei",
                    "Kai Zou"
                ],
                "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association Computational Linguistics"
            }
        },
        {
            "ix": "30-ARR_v2_109",
            "content": "UNKNOWN, None, , Tianlong Ma, and Liang He. 2021. A survey of human-in-the-loop for machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Tianlong Ma, and Liang He. 2021. A survey of human-in-the-loop for machine learning",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_110",
            "content": "Linyi Yang, Eoin Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, Ruihai Dong, Generating plausible counterfactual explanations for deep transformers in financial text classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Linyi Yang",
                    "Eoin Kenny",
                    "Tin Lok James Ng",
                    "Yi Yang",
                    "Barry Smyth",
                    "Ruihai Dong"
                ],
                "title": "Generating plausible counterfactual explanations for deep transformers in financial text classification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_111",
            "content": "Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, Ruihai Dong, Exploring the efficacy of automatically generated counterfactuals for sentiment analysis, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Linyi Yang",
                    "Jiazheng Li",
                    "Padraig Cunningham",
                    "Yue Zhang",
                    "Barry Smyth",
                    "Ruihai Dong"
                ],
                "title": "Exploring the efficacy of automatically generated counterfactuals for sentiment analysis",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "30-ARR_v2_112",
            "content": "Linyi Yang, Tin Lok James Ng, Barry Smyth, Riuhai Dong, Html: Hierarchical transformerbased multi-task learning for volatility prediction, 2020, Proceedings of The Web Conference 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Linyi Yang",
                    "Tin Lok James Ng",
                    "Barry Smyth",
                    "Riuhai Dong"
                ],
                "title": "Html: Hierarchical transformerbased multi-task learning for volatility prediction",
                "pub_date": "2020",
                "pub_title": "Proceedings of The Web Conference 2020",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_113",
            "content": "UNKNOWN, None, , Xisen Jin, and Xiang Ren. 2021. Refining neural networks with compositional explanations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Xisen Jin, and Xiang Ren. 2021. Refining neural networks with compositional explanations",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_114",
            "content": "Omar Zaidan, Jason Eisner, Christine Piatko, Using \"annotator rationales\" to improve machine learning for text categorization, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Omar Zaidan",
                    "Jason Eisner",
                    "Christine Piatko"
                ],
                "title": "Using \"annotator rationales\" to improve machine learning for text categorization",
                "pub_date": "2007",
                "pub_title": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
                "pub": null
            }
        },
        {
            "ix": "30-ARR_v2_115",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "30-ARR_v2_0@0",
            "content": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_0",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_2@0",
            "content": "We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_2",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_2@1",
            "content": "By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_2",
            "start": 203,
            "end": 551,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_2@2",
            "content": "Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_2",
            "start": 553,
            "end": 770,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_2@3",
            "content": "We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_2",
            "start": 772,
            "end": 878,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_4@0",
            "content": "Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_4",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_4@1",
            "content": "As shown in Figure 1, the bold phrases-\"100% bad\" and \"brain cell killing\"-are underlying causes for a negative sentiment prediction that most human readers would recognise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_4",
            "start": 206,
            "end": 378,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_4@2",
            "content": "These are defined as rationales in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_4",
            "start": 380,
            "end": 425,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_4@3",
            "content": "The underlined phrase-\"acting and plot\"has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_4",
            "start": 427,
            "end": 592,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_5@0",
            "content": "Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_5",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_5@1",
            "content": "This issue can be severe in few-shot learning (FSL) scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_5",
            "start": 188,
            "end": 249,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_5@2",
            "content": "For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in outof-distribution (OOD) datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_5",
            "start": 251,
            "end": 517,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_5@3",
            "content": "In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_5",
            "start": 519,
            "end": 773,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_6@0",
            "content": "There is a strand of research addressing this scenario that seeks to improve model performance by \"introducing methods and resources for training models less sensitive to spurious patterns\" (Kaushik et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_6",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_6@1",
            "content": "Most of this work relies on generating counterfactual augmented data (CAD), either manually (Kaushik et al., 2021) or automatically Qian et al., 2021;Yang et al., , 2020aDelaney et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_6",
            "start": 214,
            "end": 405,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_6@2",
            "content": "For example, Kaushik et al. (2020) proposed a humanin-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels (Kaushik et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_6",
            "start": 407,
            "end": 729,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_7@0",
            "content": "Generating manual counterfactuals, however, is expensive and time-consuming- Kaushik et al. (2020) report the cost of revising 2.5k instances at over $10,000.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_7",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_7@1",
            "content": "On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil- False Rationales:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_7",
            "start": 159,
            "end": 305,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_8@0",
            "content": "Film is good.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_8",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_9@0",
            "content": "Missed Rationale:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_9",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_10@0",
            "content": "I like it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_10",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_11@0",
            "content": "Model Rationales ity compared to manual counterfactuals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_11",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_11@1",
            "content": "To address these issues, we propose Rationales-centric Double-robustness Learning (RDL), a human-inthe-loop framework for data augmentation in a few-shot setting, which is efficient, robust, modelagnostic, and general across tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_11",
            "start": 57,
            "end": 287,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@0",
            "content": "Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@1",
            "content": "Our double-robustness framework consists of two main modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 146,
            "end": 206,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@2",
            "content": "The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 208,
            "end": 373,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@3",
            "content": "Such labelling requires less human input compared to fully manual counterfactual generation (see Section 3.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 375,
            "end": 484,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@4",
            "content": "In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 486,
            "end": 883,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@5",
            "content": "Second, we apply a Dynamic Human-intervened Correction module, where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 885,
            "end": 1180,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_12@6",
            "content": "We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_12",
            "start": 1182,
            "end": 1368,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@0",
            "content": "Results on a sentiment analysis task, which is also used in Kaushik et al. (2020) Srivastava et al., 2020;Kaushik et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@1",
            "content": "In particular, previous work (Kaushik et al., 2020) relied on large-scale crowd-sourcing to generate useful augmented data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 129,
            "end": 251,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@2",
            "content": "More recently, , and Wang and Culotta (2021) investigated the efficacy of the automatically generated counterfactuals for sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 253,
            "end": 393,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@3",
            "content": "Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 395,
            "end": 566,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@4",
            "content": "However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 568,
            "end": 698,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@5",
            "content": "In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 700,
            "end": 801,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@6",
            "content": "Moreover, our method generates semi-factuals instead of counterfactuals used in previous work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 803,
            "end": 896,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@7",
            "content": "Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 898,
            "end": 990,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@8",
            "content": "Active learning (Settles, 2009;Margatina et al., 2021), the most common example of human-in-theloop machine learning, asks human annotators only to provide high-level annotations (i.e. labels) for important examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 992,
            "end": 1207,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@9",
            "content": "There is also some work exploring more explainable AI systems by exploiting feature-based information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 1209,
            "end": 1310,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_13@10",
            "content": "Such methods use relatively simple models such as Na\u00efve Bayes (Stumpf * All resources are available at https://github.com/GeorgeLuImmortal/RDL-Rationalescentric-Double-robustness-Learning/ Kulesza et al., 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017;Teso and Kersting, 2019;Ghai et al., 2021;Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_13",
            "start": 1312,
            "end": 1777,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_14@0",
            "content": "Some other work uses simple neural networks such as multi-layer perceptrons (Shao et al., 2021) and shallow CNNs (Lertvittayakumjorn et al., 2020;Teso et al., 2021) because the predictions of such models can be explained in the form of features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_14",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_14@1",
            "content": "Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_14",
            "start": 246,
            "end": 542,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_14@2",
            "content": "However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_14",
            "start": 544,
            "end": 774,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_14@3",
            "content": "Also, they assume access to a large amount of labelled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_14",
            "start": 776,
            "end": 835,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_14@4",
            "content": "In contrast, we focus on few-shot learning scenarios which are more compelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_14",
            "start": 837,
            "end": 915,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_15@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_15",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_16@0",
            "content": "The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_16",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_17@0",
            "content": "Static semi-factual generation is a more efficient alternative to manually generated counterfactuals (Kaushik et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_17",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_17@1",
            "content": "In the first phase, Rationale Marking (Section 3.1), human annotators review each document in the training set to provide rationales (i.e. phrases that support the document classification decisions shown as bold text in Figure 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_17",
            "start": 125,
            "end": 354,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_17@2",
            "content": "The second phase is a semi-factual generation method based on synonym replacement (Section 3.2) that produces augmented examples (blue text in Figure 2 indicates replaced words), which are added into the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_17",
            "start": 356,
            "end": 572,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_18@0",
            "content": "Dynamic human-intervened correction (Section 3.3) is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_18",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_18@1",
            "content": "At the outset, sampling and sensitivity of contextual decomposition (SCD) (Jin et al., 2019) is applied to detect the rationales given by the model that is obtained in the previous step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_18",
            "start": 149,
            "end": 334,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_18@2",
            "content": "Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_18",
            "start": 336,
            "end": 682,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_18@3",
            "content": "Both false rationales and missing rationales are corrected to produce augmented examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_18",
            "start": 684,
            "end": 772,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_18@4",
            "content": "Finally, newly generated examples are added into the training set to re-train the deep learning model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_18",
            "start": 774,
            "end": 875,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_19@0",
            "content": "Rationale Marking",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_19",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_20@0",
            "content": "Following Kaushik et al. (2020) and , we use the IMDb movie review dataset (Maas et al., 2011) in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_20",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_20@1",
            "content": "It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon (Zaidan et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_20",
            "start": 115,
            "end": 278,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@0",
            "content": "We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@1",
            "content": "At the outset, annotators were given instructions and examples that gently guided them to annotate rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 127,
            "end": 236,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@2",
            "content": "Only adjectives, adverbs, nouns, and verbs were considered as rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 238,
            "end": 310,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@3",
            "content": "Besides, rationales were required to carry complete semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 312,
            "end": 384,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@4",
            "content": "For example, for a phrase starting with a negation word such as \"not great\", annotators are instructed to mark the whole phrase \"not great\" as a rationale instead of just marking \"not\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 386,
            "end": 570,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@5",
            "content": "We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 572,
            "end": 671,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_21@6",
            "content": "Phrases consisting of numerical scores are not counted as rationales (e.g. 5 or 10 stars) since different datasets may use different rating scales, and annotating digits may hurt OOD performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_21",
            "start": 673,
            "end": 867,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@0",
            "content": "Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@1",
            "content": "However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 125,
            "end": 359,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@2",
            "content": "After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 361,
            "end": 445,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@3",
            "content": "Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 447,
            "end": 560,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@4",
            "content": "All re-annotated examples were approved only if all authors were happy with the quality of the annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 562,
            "end": 668,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_22@5",
            "content": "Otherwise, the examples were re-annotated again.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_22",
            "start": 670,
            "end": 717,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_23@0",
            "content": "Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_23",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_23@1",
            "content": "Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_23",
            "start": 248,
            "end": 392,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_23@2",
            "content": "On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by Kaushik et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_23",
            "start": 394,
            "end": 541,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_23@3",
            "content": "Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300\u00d71707 183.68\u00d7100 \u2248 27.88 times more efficient than manually generated CAD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_23",
            "start": 543,
            "end": 806,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_24@0",
            "content": "Static Semi-factual Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_24",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_25@0",
            "content": "We take a simple replacement strategy, which has been taken by , to generate semifactual examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_25",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_25@1",
            "content": "Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_25",
            "start": 99,
            "end": 269,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_25@2",
            "content": "This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_25",
            "start": 271,
            "end": 400,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_26@0",
            "content": "Formally, given a training example x i = [t i1 , t i2 , ..., t ij ] (where t ij is the j th token of the i th document) and its ground truth label y i , we create a rationale vector r i = [a i1 , a i2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not (we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_26",
            "start": 0,
            "end": 359,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_26@1",
            "content": "To generate a semi-factual example, x \u2032 i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_26",
            "start": 361,
            "end": 523,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_26@2",
            "content": "The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet (Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model (Liu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_26",
            "start": 525,
            "end": 745,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_27@0",
            "content": "In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x \u2032 i , with some replaced non-rationales and all the other tokens identical to x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_27",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_27@1",
            "content": "The label, y i , of a newly generated example is the same as the label of the original example, x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_27",
            "start": 208,
            "end": 308,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_27@2",
            "content": "Examples of generated data are shown in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_27",
            "start": 310,
            "end": 357,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_27@3",
            "content": "Afterwards, the augmented examples are added into the training set used to train the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_27",
            "start": 359,
            "end": 449,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_28@0",
            "content": "Dynamic Human-intervened Correction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_28",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@0",
            "content": "Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@1",
            "content": "Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 150,
            "end": 243,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@2",
            "content": "SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction (Jin et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 245,
            "end": 383,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@3",
            "content": "Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 385,
            "end": 548,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@4",
            "content": "The next phase allows human feedback to influence the learning process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 550,
            "end": 620,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_29@5",
            "content": "To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_29",
            "start": 622,
            "end": 724,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_30@0",
            "content": "For false rationales (i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_30",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_30@1",
            "content": "Unlike the static semi-factual generation (Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_30",
            "start": 194,
            "end": 385,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_30@2",
            "content": "Examples of generated data are shown in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_30",
            "start": 387,
            "end": 434,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@0",
            "content": "For missing rationales (i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@1",
            "content": "Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 259,
            "end": 474,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@2",
            "content": "For example, there is a positive movie review (bold font for rationales) \"Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 ....\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 476,
            "end": 670,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@3",
            "content": "The model fails to identify \"fine\" and \"believable\" as rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 672,
            "end": 737,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@4",
            "content": "Thus we extract the text \"\"Robert Urich was a fine actor, and he makes this TV movie believable .\" as a new example, and the class of this example is still positive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 739,
            "end": 903,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_31@5",
            "content": "We extract the whole sentence rather than just the missing rationales to reserve more semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_31",
            "start": 905,
            "end": 1011,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_32@0",
            "content": "Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_32",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_33@0",
            "content": "Origin: The attempt at a \"lesbian scene\" was sad.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_33",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_34@0",
            "content": "Augment 1: The hint at a \"lesbian scene\" was sad .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_34",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_34@1",
            "content": "Augment 2: The attempt at a \"kiss scene\" was sad .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_34",
            "start": 51,
            "end": 100,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_35@0",
            "content": "Origin: I recommended this film a lot, specially in this difficult times for the planet .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_35",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_35@1",
            "content": "Augment 1: I recommended you film a lot, specially in this difficult times for the planet .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_35",
            "start": 90,
            "end": 180,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_35@2",
            "content": "Augment 2: I recommended this movie a lot, specially in this difficult times for the planet .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_35",
            "start": 182,
            "end": 274,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_35@3",
            "content": "Underlined spans were false rationales given by the model through SCD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_35",
            "start": 276,
            "end": 345,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_35@4",
            "content": "Blue spans were synonyms used as replacements, and bold font were rationales identified by human annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_35",
            "start": 347,
            "end": 454,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_36@0",
            "content": "Why Does RDL Work?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_36",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_37@0",
            "content": "Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_37",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_37@1",
            "content": "More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_37",
            "start": 228,
            "end": 387,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_37@2",
            "content": "For example, if a model incorrectly determines that \"Soylent Green\" is associated with positive sentiment (Table 2), the augmented examples that replace \"Soylent Green\" with other phrases such as \"Gang Orange\" break the spurious association.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_37",
            "start": 389,
            "end": 629,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_37@3",
            "content": "Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_37",
            "start": 631,
            "end": 821,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_38@0",
            "content": "Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_38",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_38@1",
            "content": "In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_38",
            "start": 254,
            "end": 407,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_39@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_39",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_40@0",
            "content": "Our intention is to improve the generalisability of models, and we use both in-distribution and OOD performance for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_40",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_40@1",
            "content": "Our experiments are designed to address the following research questions:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_40",
            "start": 128,
            "end": 200,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_41@0",
            "content": "\u2022 RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance? \u2022 RQ2 Does dynamic human-intervened correction improve generalisability of models?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_41",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_42@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_42",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_43@0",
            "content": "For fair comparison with previous work (Kaushik et al., 2020;, we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_43",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_43@1",
            "content": "Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1, 707, 245, and 488 reviews respectively and an enforced 50:50 class ratio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_43",
            "start": 164,
            "end": 381,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_44@0",
            "content": "To measure the generalisation ability of different models, we focus on OOD performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_44",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_44@1",
            "content": "To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives and 2,339 negatives).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_44",
            "start": 88,
            "end": 616,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_44@2",
            "content": "These datasets were sampled to ensure a nearly 50:50 class balance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_44",
            "start": 618,
            "end": 684,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_45@0",
            "content": "Evaluating Static Semi-factual Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_45",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_46@0",
            "content": "To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_46",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_46@1",
            "content": "We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_46",
            "start": 182,
            "end": 290,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_47@0",
            "content": "Experiment Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_47",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_48@0",
            "content": "To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_48",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_48@1",
            "content": "For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_48",
            "start": 154,
            "end": 268,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_48@2",
            "content": "We report the average result of these 10 repetitions and use accuracy to measure the classification performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_48",
            "start": 270,
            "end": 381,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_49@0",
            "content": "Our experiments rely on an off-the-shelf cased \"RoBERTa-base\" model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_49",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_49@1",
            "content": "Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_49",
            "start": 175,
            "end": 368,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_49@2",
            "content": "We also explore the impact of the number of semi-factual examples on model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_49",
            "start": 370,
            "end": 456,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_49@3",
            "content": "To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19 We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_49",
            "start": 458,
            "end": 666,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_49@4",
            "content": "We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_49",
            "start": 668,
            "end": 788,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_50@0",
            "content": "Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_50",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_51@0",
            "content": "As shown in Table 3, all static semi-factual generation (Static+n) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_51",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_51@1",
            "content": "Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_51",
            "start": 218,
            "end": 369,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_51@2",
            "content": "Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_51",
            "start": 371,
            "end": 522,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_51@3",
            "content": "Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_51",
            "start": 524,
            "end": 723,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@0",
            "content": "The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@1",
            "content": "The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 179,
            "end": 405,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@2",
            "content": "In other words, a model that fits well with one dataset can cause performance decay on others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 407,
            "end": 500,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@3",
            "content": "In this case, training with a smaller training set is more likely to reduce overfitting with the indistribution dataset and fit well with the SemEval dataset, which explains the big improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 502,
            "end": 695,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@4",
            "content": "It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66\u00b10.84) than on the in-distribution dataset (93.23\u00b10.46), which could also be explained by the high similarity between the underlying distributions of these two datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 697,
            "end": 994,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@5",
            "content": "As shown in Table 3, in most cases, DP underperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 996,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@6",
            "content": "We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 1183,
            "end": 1383,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@7",
            "content": "Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 1385,
            "end": 1600,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@8",
            "content": "Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 1602,
            "end": 1787,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_52@9",
            "content": "We call this Random Replacement (RR hereafter).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_52",
            "start": 1789,
            "end": 1835,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_53@0",
            "content": "As shown in Table 3, RR is slightly better than the baseline Static approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_53",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_53@1",
            "content": "This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_53",
            "start": 78,
            "end": 294,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_53@2",
            "content": "However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_53",
            "start": 296,
            "end": 473,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_53@3",
            "content": "These observations show that the model trained with Static+n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_53",
            "start": 475,
            "end": 666,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_54@0",
            "content": "Evaluating Dynamic Human-intervened Correction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_54",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_55@0",
            "content": "As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+n) marginalises when augmented data is increased.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_55",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_56@0",
            "content": "Using too much augmented data even hurts the Static+1150 performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_56",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_56@1",
            "content": "This observation is consistent with existing work on data augmentation (Wei and Zou, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_56",
            "start": 70,
            "end": 160,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_56@2",
            "content": "We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_56",
            "start": 162,
            "end": 404,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_56@3",
            "content": "Human-in-the-loop can address this issue by dynamically correcting the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_56",
            "start": 406,
            "end": 482,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_57@0",
            "content": "To address RQ2, we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-theloop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020;. Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_57",
            "start": 0,
            "end": 424,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_58@0",
            "content": "Experiment Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_58",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_59@0",
            "content": "We build up an active learning procedure as a baseline based on the model trained with Static.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_59",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_59@1",
            "content": "To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_59",
            "start": 95,
            "end": 361,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_59@2",
            "content": "Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL performance).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_59",
            "start": 363,
            "end": 590,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_60@0",
            "content": "Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_60",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@0",
            "content": "As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@1",
            "content": "However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 237,
            "end": 389,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@2",
            "content": "On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 391,
            "end": 504,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@3",
            "content": "Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 506,
            "end": 665,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@4",
            "content": "Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 667,
            "end": 811,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_61@5",
            "content": "Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_61",
            "start": 813,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_62@0",
            "content": "We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_62",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_62@1",
            "content": "Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_62",
            "start": 99,
            "end": 274,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_62@2",
            "content": "We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_62",
            "start": 276,
            "end": 515,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_62@3",
            "content": "The performance of Dynamic can be explained as a compromise of two correction methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_62",
            "start": 517,
            "end": 602,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_63@0",
            "content": "We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_63",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_63@1",
            "content": "We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_63",
            "start": 108,
            "end": 212,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_63@2",
            "content": "As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_63",
            "start": 214,
            "end": 445,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_64@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_64",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_65@0",
            "content": "We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_65",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_65@1",
            "content": "Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_65",
            "start": 132,
            "end": 342,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_65@2",
            "content": "In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_65",
            "start": 344,
            "end": 492,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_66@0",
            "content": "Ethical Statement",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_66",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_67@0",
            "content": "We honor the ACL Code of Ethics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_67",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_67@1",
            "content": "No private data or non-public information was used in this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_67",
            "start": 33,
            "end": 96,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_67@2",
            "content": "All annotators have received labor fees corresponding to the amount of their annotated instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_67",
            "start": 98,
            "end": 194,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_68@0",
            "content": "UNKNOWN, None, 2021, Uncertainty estimation and out-of-distribution detection for counterfactual explanations: Pitfalls and solutions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_68",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_69@0",
            "content": "Fuli Feng, Jizhi Zhang, Xiangnan He, Hanwang Zhang, Tat-Seng Chua, Empowering language understanding with counterfactual reasoning, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_69",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_70@0",
            "content": "Q Bhavya Ghai, Yunfeng Liao, Rachel Zhang, Klaus Bellamy,  Mueller, Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers, 2021, Proc. ACM Hum.-Comput. Interact, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_70",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_71@0",
            "content": "UNKNOWN, None, 2018, Annotation artifacts in natural language inference data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_71",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_72@0",
            "content": "Robin Jia, Percy Liang, Adversarial examples for evaluating reading comprehension systems, 2017-09-09, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_72",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_73@0",
            "content": "Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, Xiang Ren, Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_73",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_74@0",
            "content": "Divyansh Kaushik, Eduard Hovy, Zachary Lipton, Learning the difference that makes a difference with counterfactually augmented data, 2020, International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_74",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_75@0",
            "content": "Divyansh Kaushik, Amrith Setlur, Eduard Hovy, Zachary Lipton, Explaining the efficacy of counterfactually augmented data, 2021, International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_75",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_76@0",
            "content": "Katherine Keith, David Jensen, Brendan O' Connor, Text and causal inference: A review of using text to remove confounding from causal estimates, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_76",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_77@0",
            "content": "UNKNOWN, None, 2021, On generating plausible counterfactual and semi-factual explanations for deep learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_77",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_78@0",
            "content": "Diederik Kingma, Jimmy Ba, Adam: A method for stochastic optimization, 2014, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_78",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_79@0",
            "content": "Todd Kulesza, Margaret Burnett, Weng-Keen Wong, Simone Stumpf, Principles of explanatory debugging to personalize interactive machine learning, 2015, Proceedings of the 20th International Conference on Intelligent User Interfaces, IUI '15, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_79",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_80@0",
            "content": "Todd Kulesza, Simone Stumpf, Margaret Burnett, Weng-Keen Wong, Yann Riche, Travis Moore, Ian Oberst, Amber Shinsel, Kevin Mcintosh, Explanatory debugging: Supporting end-user debugging of machine-learned programs, 2010, 2010 IEEE Symposium on Visual Languages and Human-Centric Computing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_80",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_81@0",
            "content": "UNKNOWN, None, 2020, Find: Human-in-the-loop debugging deep text classifiers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_81",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_82@0",
            "content": "UNKNOWN, None, 2021, Explanation-based human debugging of nlp models: A survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_82",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_83@0",
            "content": "Jiazheng Li, Linyi Yang, Barry Smyth, Ruihai Dong, Maec: A multimodal aligned earnings conference call dataset for financial risk prediction, 2020, Proceedings of the 29th ACM International Conference on Information & Knowledge Management, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_83",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_84@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized bert pretraining approach. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_84",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_85@0",
            "content": "Jinghui Lu, Maeve Henchion, Ivan Bacher, Brian Namee, A sentence-level hierarchical bert model for document classification with limited labelled data, 2021, Discovery Science, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_85",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2020, Investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_86",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_87@0",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_87",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_88@0",
            "content": "Katerina Margatina, Giorgos Vernikos, Active learning by acquiring contrastive examples, 2021, Proceddings of the 2021 Conference on Empirical Methods in Natural Language Processing, Underline Science Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_88",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_89@0",
            "content": "Rachel Mccloy, M Ruth,  Byrne, Semifactual \"even if, 2002, thinking. Thinking & Reasoning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_89",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_90@0",
            "content": "George Miller, Wordnet: A lexical database for english, 1995, Commun. ACM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_90",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_91@0",
            "content": "Jianmo Ni, Jiacheng Li, Julian Mcauley, Justifying recommendations using distantly-labeled reviews and fine-grained aspects, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_91",
            "start": 0,
            "end": 308,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_92@0",
            "content": "Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, Pengjun Xie, Counterfactual inference for text classification debiasing, 2021, Proceedings of the 59th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_92",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_93@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_93",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_94@0",
            "content": "Sameer Marco Tulio Ribeiro, Carlos Singh,  Guestrin, Anchors: High-precision modelagnostic explanations, 2018, Proceedings of the AAAI conference on artificial intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_94",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_95@0",
            "content": "UNKNOWN, None, 1997, Counterfactual thinking. Psychological bulletin, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_95",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_96@0",
            "content": "Sara Rosenthal, Noura Farra, Preslav Nakov, SemEval-2017 task 4: Sentiment analysis in Twitter, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_96",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_97@0",
            "content": "UNKNOWN, None, 2009, Active learning literature survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_97",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_98@0",
            "content": "Xiaoting Shao, Arseny Skryagin, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, Right for better reasons: Training differentiable models by constraining their influence functions, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_98",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_99@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_99",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_100@0",
            "content": "Megha Srivastava, Tatsunori Hashimoto, Percy Liang, Robustness to spurious correlations via human annotations, 2020, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_100",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_101@0",
            "content": "Wolfgang Stammer, Patrick Schramowski, Kristian Kersting, Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations, 2021-06-19, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_101",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_102@0",
            "content": "Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, Thomas Dietterich, Erin Sullivan, Jonathan Herlocker, Interacting meaningfully with machine learning systems: Three experiments, 2009, Int. J. Hum.-Comput. Stud, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_102",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_103@0",
            "content": "Stefano Teso, Andrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, Interactive label cleaning with example-based explanations, 2021, Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_103",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_104@0",
            "content": "Stefano Teso, Kristian Kersting, Explanatory interactive machine learning, 2019, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19, Association for Computing Machinery.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_104",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_105@0",
            "content": "Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_105",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_106@0",
            "content": "UNKNOWN, None, 2021, Identifying and mitigating spurious correlations for improving robustness in nlp models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_106",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_107@0",
            "content": "Zhao Wang, Aron Culotta, Robustness to spurious correlations in text classification via automatically generated counterfactuals, 2021, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_107",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_108@0",
            "content": "Jason Wei, Kai Zou, EDA: Easy data augmentation techniques for boosting performance on text classification tasks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_108",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_109@0",
            "content": "UNKNOWN, None, , Tianlong Ma, and Liang He. 2021. A survey of human-in-the-loop for machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_109",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_110@0",
            "content": "Linyi Yang, Eoin Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, Ruihai Dong, Generating plausible counterfactual explanations for deep transformers in financial text classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_110",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_111@0",
            "content": "Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, Ruihai Dong, Exploring the efficacy of automatically generated counterfactuals for sentiment analysis, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_111",
            "start": 0,
            "end": 353,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_112@0",
            "content": "Linyi Yang, Tin Lok James Ng, Barry Smyth, Riuhai Dong, Html: Hierarchical transformerbased multi-task learning for volatility prediction, 2020, Proceedings of The Web Conference 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_112",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_113@0",
            "content": "UNKNOWN, None, , Xisen Jin, and Xiang Ren. 2021. Refining neural networks with compositional explanations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_113",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_114@0",
            "content": "Omar Zaidan, Jason Eisner, Christine Piatko, Using \"annotator rationales\" to improve machine learning for text categorization, 2007, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_114",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "30-ARR_v2_115@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "30-ARR_v2_115",
            "start": 0,
            "end": 158,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_1",
            "tgt_ix": "30-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_1",
            "tgt_ix": "30-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_2",
            "tgt_ix": "30-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_4",
            "tgt_ix": "30-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_5",
            "tgt_ix": "30-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_6",
            "tgt_ix": "30-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_7",
            "tgt_ix": "30-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_8",
            "tgt_ix": "30-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_9",
            "tgt_ix": "30-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_11",
            "tgt_ix": "30-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_10",
            "tgt_ix": "30-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_16",
            "tgt_ix": "30-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_17",
            "tgt_ix": "30-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_20",
            "tgt_ix": "30-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_23",
            "tgt_ix": "30-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_25",
            "tgt_ix": "30-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_26",
            "tgt_ix": "30-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_24",
            "tgt_ix": "30-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_24",
            "tgt_ix": "30-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_24",
            "tgt_ix": "30-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_24",
            "tgt_ix": "30-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_27",
            "tgt_ix": "30-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_30",
            "tgt_ix": "30-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_33",
            "tgt_ix": "30-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_32",
            "tgt_ix": "30-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_34",
            "tgt_ix": "30-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_37",
            "tgt_ix": "30-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_36",
            "tgt_ix": "30-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_36",
            "tgt_ix": "30-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_36",
            "tgt_ix": "30-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_38",
            "tgt_ix": "30-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_40",
            "tgt_ix": "30-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_43",
            "tgt_ix": "30-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_42",
            "tgt_ix": "30-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_42",
            "tgt_ix": "30-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_42",
            "tgt_ix": "30-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_44",
            "tgt_ix": "30-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_45",
            "tgt_ix": "30-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_45",
            "tgt_ix": "30-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_46",
            "tgt_ix": "30-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_48",
            "tgt_ix": "30-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_47",
            "tgt_ix": "30-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_47",
            "tgt_ix": "30-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_47",
            "tgt_ix": "30-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_51",
            "tgt_ix": "30-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_50",
            "tgt_ix": "30-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_50",
            "tgt_ix": "30-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_50",
            "tgt_ix": "30-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_50",
            "tgt_ix": "30-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_53",
            "tgt_ix": "30-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_55",
            "tgt_ix": "30-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_56",
            "tgt_ix": "30-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_54",
            "tgt_ix": "30-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_54",
            "tgt_ix": "30-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_54",
            "tgt_ix": "30-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_54",
            "tgt_ix": "30-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_57",
            "tgt_ix": "30-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_58",
            "tgt_ix": "30-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_58",
            "tgt_ix": "30-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_59",
            "tgt_ix": "30-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_60",
            "tgt_ix": "30-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_60",
            "tgt_ix": "30-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_60",
            "tgt_ix": "30-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_60",
            "tgt_ix": "30-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_62",
            "tgt_ix": "30-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_63",
            "tgt_ix": "30-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_64",
            "tgt_ix": "30-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_64",
            "tgt_ix": "30-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_65",
            "tgt_ix": "30-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_66",
            "tgt_ix": "30-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_66",
            "tgt_ix": "30-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "30-ARR_v2_0",
            "tgt_ix": "30-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_1",
            "tgt_ix": "30-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_2",
            "tgt_ix": "30-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_2",
            "tgt_ix": "30-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_2",
            "tgt_ix": "30-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_2",
            "tgt_ix": "30-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_3",
            "tgt_ix": "30-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_4",
            "tgt_ix": "30-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_4",
            "tgt_ix": "30-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_4",
            "tgt_ix": "30-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_4",
            "tgt_ix": "30-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_5",
            "tgt_ix": "30-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_5",
            "tgt_ix": "30-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_5",
            "tgt_ix": "30-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_5",
            "tgt_ix": "30-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_6",
            "tgt_ix": "30-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_6",
            "tgt_ix": "30-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_6",
            "tgt_ix": "30-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_7",
            "tgt_ix": "30-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_7",
            "tgt_ix": "30-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_8",
            "tgt_ix": "30-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_9",
            "tgt_ix": "30-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_10",
            "tgt_ix": "30-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_11",
            "tgt_ix": "30-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_11",
            "tgt_ix": "30-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_12",
            "tgt_ix": "30-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_13",
            "tgt_ix": "30-ARR_v2_13@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_14",
            "tgt_ix": "30-ARR_v2_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_15",
            "tgt_ix": "30-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_16",
            "tgt_ix": "30-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_17",
            "tgt_ix": "30-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_17",
            "tgt_ix": "30-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_17",
            "tgt_ix": "30-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_18",
            "tgt_ix": "30-ARR_v2_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_19",
            "tgt_ix": "30-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_20",
            "tgt_ix": "30-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_20",
            "tgt_ix": "30-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_21",
            "tgt_ix": "30-ARR_v2_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_22",
            "tgt_ix": "30-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_23",
            "tgt_ix": "30-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_23",
            "tgt_ix": "30-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_23",
            "tgt_ix": "30-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_23",
            "tgt_ix": "30-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_24",
            "tgt_ix": "30-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_25",
            "tgt_ix": "30-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_25",
            "tgt_ix": "30-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_25",
            "tgt_ix": "30-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_26",
            "tgt_ix": "30-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_26",
            "tgt_ix": "30-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_26",
            "tgt_ix": "30-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_27",
            "tgt_ix": "30-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_27",
            "tgt_ix": "30-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_27",
            "tgt_ix": "30-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_27",
            "tgt_ix": "30-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_28",
            "tgt_ix": "30-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_29",
            "tgt_ix": "30-ARR_v2_29@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_30",
            "tgt_ix": "30-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_30",
            "tgt_ix": "30-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_30",
            "tgt_ix": "30-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_31",
            "tgt_ix": "30-ARR_v2_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_32",
            "tgt_ix": "30-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_33",
            "tgt_ix": "30-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_34",
            "tgt_ix": "30-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_34",
            "tgt_ix": "30-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_35",
            "tgt_ix": "30-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_36",
            "tgt_ix": "30-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_37",
            "tgt_ix": "30-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_37",
            "tgt_ix": "30-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_37",
            "tgt_ix": "30-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_37",
            "tgt_ix": "30-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_38",
            "tgt_ix": "30-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_38",
            "tgt_ix": "30-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_39",
            "tgt_ix": "30-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_40",
            "tgt_ix": "30-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_40",
            "tgt_ix": "30-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_41",
            "tgt_ix": "30-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_42",
            "tgt_ix": "30-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_43",
            "tgt_ix": "30-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_43",
            "tgt_ix": "30-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_44",
            "tgt_ix": "30-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_44",
            "tgt_ix": "30-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_44",
            "tgt_ix": "30-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_45",
            "tgt_ix": "30-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_46",
            "tgt_ix": "30-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_46",
            "tgt_ix": "30-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_47",
            "tgt_ix": "30-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_48",
            "tgt_ix": "30-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_48",
            "tgt_ix": "30-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_48",
            "tgt_ix": "30-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_49",
            "tgt_ix": "30-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_50",
            "tgt_ix": "30-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_51",
            "tgt_ix": "30-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_51",
            "tgt_ix": "30-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_51",
            "tgt_ix": "30-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_51",
            "tgt_ix": "30-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_52",
            "tgt_ix": "30-ARR_v2_52@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_53",
            "tgt_ix": "30-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_53",
            "tgt_ix": "30-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_53",
            "tgt_ix": "30-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_53",
            "tgt_ix": "30-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_54",
            "tgt_ix": "30-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_55",
            "tgt_ix": "30-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_56",
            "tgt_ix": "30-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_56",
            "tgt_ix": "30-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_56",
            "tgt_ix": "30-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_56",
            "tgt_ix": "30-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_57",
            "tgt_ix": "30-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_58",
            "tgt_ix": "30-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_59",
            "tgt_ix": "30-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_59",
            "tgt_ix": "30-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_59",
            "tgt_ix": "30-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_60",
            "tgt_ix": "30-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_61",
            "tgt_ix": "30-ARR_v2_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_62",
            "tgt_ix": "30-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_62",
            "tgt_ix": "30-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_62",
            "tgt_ix": "30-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_62",
            "tgt_ix": "30-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_63",
            "tgt_ix": "30-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_63",
            "tgt_ix": "30-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_63",
            "tgt_ix": "30-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_64",
            "tgt_ix": "30-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_65",
            "tgt_ix": "30-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_65",
            "tgt_ix": "30-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_65",
            "tgt_ix": "30-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_66",
            "tgt_ix": "30-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_67",
            "tgt_ix": "30-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_67",
            "tgt_ix": "30-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_67",
            "tgt_ix": "30-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_68",
            "tgt_ix": "30-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_69",
            "tgt_ix": "30-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_70",
            "tgt_ix": "30-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_71",
            "tgt_ix": "30-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_72",
            "tgt_ix": "30-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_73",
            "tgt_ix": "30-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_74",
            "tgt_ix": "30-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_75",
            "tgt_ix": "30-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_76",
            "tgt_ix": "30-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_77",
            "tgt_ix": "30-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_78",
            "tgt_ix": "30-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_79",
            "tgt_ix": "30-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_80",
            "tgt_ix": "30-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_81",
            "tgt_ix": "30-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_82",
            "tgt_ix": "30-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_83",
            "tgt_ix": "30-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_84",
            "tgt_ix": "30-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_85",
            "tgt_ix": "30-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_86",
            "tgt_ix": "30-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_87",
            "tgt_ix": "30-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_88",
            "tgt_ix": "30-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_89",
            "tgt_ix": "30-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_90",
            "tgt_ix": "30-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_91",
            "tgt_ix": "30-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_92",
            "tgt_ix": "30-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_93",
            "tgt_ix": "30-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_94",
            "tgt_ix": "30-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_95",
            "tgt_ix": "30-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_96",
            "tgt_ix": "30-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_97",
            "tgt_ix": "30-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_98",
            "tgt_ix": "30-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_99",
            "tgt_ix": "30-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_100",
            "tgt_ix": "30-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_101",
            "tgt_ix": "30-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_102",
            "tgt_ix": "30-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_103",
            "tgt_ix": "30-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_104",
            "tgt_ix": "30-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_105",
            "tgt_ix": "30-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_106",
            "tgt_ix": "30-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_107",
            "tgt_ix": "30-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_108",
            "tgt_ix": "30-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_109",
            "tgt_ix": "30-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_110",
            "tgt_ix": "30-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_111",
            "tgt_ix": "30-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_112",
            "tgt_ix": "30-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_113",
            "tgt_ix": "30-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_114",
            "tgt_ix": "30-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "30-ARR_v2_115",
            "tgt_ix": "30-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 794,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "30-ARR",
        "version": 2
    }
}