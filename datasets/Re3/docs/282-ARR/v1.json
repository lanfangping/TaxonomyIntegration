{
    "nodes": [
        {
            "ix": "282-ARR_v1_0",
            "content": "Structural Supervision for Word Alignment and Machine Translation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_2",
            "content": "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, to our best knowledge, we first propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning. Particularly, we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially generate the syntactic graphs of target tokens and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "282-ARR_v1_4",
            "content": "Word alignment (Brown et al., 1993) aims to find the correspondence between tokens in a sentence pair. Neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017) works by taking an end-to-end approach to incrementally predict the target translation from a source sentence, where no explicit word alignment is required during model training or decoding. Recently, there has been an increasing interest (Zenkel et al., 2020;Chen et al., 2020Chen et al., , 2021Zhang and van Genabith, 2021) in combining the two tasks through inducing accurate Regardless of direction and type: (1) the dependencies between 'you ' and 're' and between 're' and 'naive' in English match the dependencies between 'Du' and 'bist' and between 'bist' and 'naiv' in German. (2) For English-French (Chinese) pairs, although there is no explicit dependency between 'T u '(\u4f60) and 'es'(\u5f88), we can capture the implicit dependency by tracing the dependencies between 'T u '( \u4f60 ) and 'na\u00ef ve'(\u5929\u771f) and between 'na\u00ef ve'(\u5929\u771f) and 'es'(\u5f88). word alignment in neural translation models for improving translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_5",
            "content": "Intuitively, word alignment is helpful to enforce the domain-specific terminology or improve the translations of low-frequency tokens (Song et al., 2019;Dinu et al., 2019). Also, word alignment provides supportive linguistic information on translation outputs, being useful in interactive translation with the human in the loop (Weng et al., 2019). Since the target-to-source attention in NMT models can infer rough word alignments but induce many errors with low accuracy, a number of recent works (Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020Zhang and van Genabith, 2021) focus on NMTbased alignment methods which take alignments as a by-product of NMT systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_6",
            "content": "Although NMT-based aligners have proven to be effective and achieved the State-of-the-Art alignment accuracy, they suffer from two major limitations. First, due to the autoregressive property (Sutskever et al., 2014), they (Dyer et al., 2013;Bahdanau et al., 2015;Vaswani et al., 2017;Chen et al., 2020) only leverage partial target context. The latest works (Chen et al., 2021;Zhang and van Genabith, 2021) alleviate this deficiency to exploit both sides of the target content to compute better target-to-source attention (alignment), by abandoning autoregressive decoder and sacrificing the translation ability. In addition, there are also related works (Bastings et al., 2017;Marcheggiani et al., 2018) proposing syntax-aware NMT models without word alignment task. However, they simply utilize the syntactic structure of source tokens and ignore to capture the syntactic structure of target tokens. In summary, the syntactic structure of both source and target tokens has not been thoroughly explored to guide accurate alignments, while the similarity of dependencies across diverse languages has not been utilized for producing translation outputs with high-quality and favorable generalization capabilities. Second, they (Garg et al., 2019;Zenkel et al., 2020) typically use multi-task learning architecture to jointly learn the word alignment and translation with elaborately designed loss functions. However, this is computationally expensive for training and the internal logic between the two subtasks is not well correlated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_7",
            "content": "To alleviate mentioned problems, we propose to simultaneously consider the syntactic structure of both source and target tokens. According to the similar dependencies across language pairs, the syntactic graphs of target tokens are first sequentially inferred through introduced Dynamic Graph Convolution Networks. Hierarchical Graph Random Walks are then performed based on the built syntactic graphs at both ends, as well as the initialized multi-scale and trainable \"hidden graphs\" (Nikolentzos and Vazirgiannis, 2020). We found that by correlating cross-linguistic dependencies without any additional guided loss, word alignment and translation can be more effectively integrated into a unified learning framework, efficiently correlating the internal logic between subtasks while improving the interpretability of the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_8",
            "content": "(1) We introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic graphs of target tokens to guide the word alignment learning. (2) Hierarchical Graph Random Walks are further performed to incorporate both local and global structural constraints for producing translation outputs. (3) Results on four language pairs demonstrate that our method is highly effective in such alignment-or translation-related NLP tasks, consistently outperforming baselines in alignment accuracy and translation quality. (4) We will release our codebase upon acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_9",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "282-ARR_v1_10",
            "content": "Word Alignment",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "282-ARR_v1_11",
            "content": "A naive way to extract alignments from NMT models is to choose the source token with the maximum accumulated attention weight towards the current target token (Arthur et al., 2016;Hasler et al., 2018):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_12",
            "content": "\u03b3(t) = arg max i\u2208{1,...,M } N l=1 \u03b1 l t,i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_13",
            "content": ", where i is the candidate aligned source-side position. For decoding step t in layer l, \u03b1 l t,i is the attention weight of the i-th position in the source, produced by an average of all the attention heads in Transformer. Although simple to implement, this method fails to obtain satisfactory alignment results (Li et al., 2019;Ding et al., 2019;Chen et al., 2020). In this work, we sufficiently exploit the similarity of dependencies between language pairs, training a novel multi-task learning framework to jointly learn translation and word alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_14",
            "content": "Neural Machine Translation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "282-ARR_v1_15",
            "content": "Let x = x 1 , ..., x M and y = y 1 , ..., y N be the source and target sentence respectively, neural machine translation models the probability of the target sentence conditioned on the source sentence: P (y|x; \u03b8) = N i=1 P (y i |y <i , x), where y <i is a partial translation from the first to (i-1)-th target tokens. Existing NMT models are generally equipped with encoder-decoder structure. The encoder encodes the source sentence, while the decoder generates the target sentence through a target-to-source attention mechanism and performs left-to-right autoregressive decoding. In this work, we adopt Transformer (Vaswani et al., 2017) as the baseline to build our method. Transformer is also an encoder-decoder framework while each decoder layer attends to the encoder output with multihead attention, and we refer to the original paper (Vaswani et al., 2017) for more details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_16",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "282-ARR_v1_17",
            "content": "Our work is inspired by the fact that tokens in different languages have similar dependencies under the same semantics. As shown in Figure 1, the dependencies between tokens with the same semantics in the English-German pair are highly similar, while the similarity of dependencies between English and French (Chinese) can also be implicitly captured. We regard each token as a node, and build the edges according to the corresponding dependencies between each node to form the syntactic graphs of different languages. For instance, there is a dependency between 'you' and 're', and the node 'you' is the 1-hop neighbor of 're' in the built English (syntactic) graph. While there is no explicit dependency between 'Tu' and 'es' and we have to pass through 'na\u00efve' to reach 'es' from 'Tu', so the node 'Tu' is treated as the 2-hop neighbor of 'es' in the French (syntactic) graph.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_18",
            "content": "Figure 2 shows the overall architecture of proposed multi-task learning framework. We model the joint distribution of the target tokens and the target syntactic graphs by factorizing it into the product of a series conditional distributions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_19",
            "content": "P (y, y s |x s , x) = N i=1 P (y i |y s \u2264i , x s , y <i , x) \u00d7 P (y s i |y s <i , x s , y <i , x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_20",
            "content": "where y <i , y s <i are partially generated target sentence and syntactic graph, and (x, x s ) indicates the entire information from the source side. For the tokens y, we can directly optimize translation loss. However, we do not leverage the ground-truth of the target syntactic graph to maximize the likelihood. We propose a proxy to construct the word alignment \u03b1 with graph convolution networks. \u03b1 = proxy(y s )",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_21",
            "content": "Then we optimize the word alignment loss as a surrogate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_22",
            "content": "Specifically, our approach first build the syntactic graph of source tokens, on which basis we introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic structure of observed target tokens, efficiently generating accurate alignment results which derived from the structural attention weights between both sides. To better encourage the correlation of the internal logic between word alignment and translation, Hierarchical Graph Random Walks are then performed to incorporate structural constraints for producing high-quality translation outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_23",
            "content": "Dynamic Graph Convolution Networks",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "282-ARR_v1_24",
            "content": "We can first build the source syntax graph with the output representations H e from Transformer encoder, where each node corresponds to one token representation. In particular, the adjacency matrix A s is generated from the parsed syntactic structure, where a (i,j) = 1 indicates there is a dependency between node i and j. Meanwhile, we initialize the rough adjacency matrix \u0100t containing only selfconnections for each target token. Therefore, Dynamic Graph Convolution Networks is leveraged to adaptively adjust the graph topology for obtaining refined adjacent structures. Significantly, both masking and attention mechanisms are introduced to distinguish and re-weight observed target nodes through the captured multi-hops neighbor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_25",
            "content": "For each decoding step, masking mechanism is first built for the observed set of target nodes. For each observed token (or node), we predict a soft mask M to indicate its dependency with other observed tokens. A light-weight two-layer pooling network is used to learn the mask. Step 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_26",
            "content": "M = f M (A s , Hd , H e ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_27",
            "content": "Step 3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_28",
            "content": "Step 5",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_29",
            "content": "Step 5 where Hd \u2208 R N \u00d7D denotes the D-dimension features of N observed target nodes generated from Transformer decoder. The detailed network architecture of f M can refer to the Appendix. The obtained M \u2208 R N \u00d7 N serves as an information gatekeeper, retaining the nodes that are optimal for local aggregation with a global perspective, capturing linguistic dependencies discriminatively without compromising the topology of the syntactic graph.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_30",
            "content": "\ud835\udc34 \ud835\udc61 \ud835\udc34 \ud835\udc60",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_31",
            "content": "We will then process a graph-based information aggregation (Kipf and Welling, 2016) and proceed with a linear transformation, i.e.,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_32",
            "content": "Hm = W m \u2022 \u0100t + M \u2022 Hd + b m ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_33",
            "content": "where \u2022 denotes the matrix multiplication and the formula in square bracket means information aggregation. In this way, the set of observed nodes and their edge connections at target side change dynamically in successive decoding steps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_34",
            "content": "In addition, an attention mechanism is introduced to re-weight and balance the captured multihops neighbor of each observed token. In particular, we aggregate context information by attending over the multi-hops neighbor of each node, while its updated representation is calculated by the weighted average of the connected nodes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_35",
            "content": "Hi a = ReLU \uf8eb \uf8ed y j \u2208N + (y i ) a (h) ij \u2022 (W a Hj m ) \uf8f6 \uf8f8 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_36",
            "content": "where j = 1, ..., N and N + (y i ) includes the node y i and the nodes directly connected to y i , W a is a learnable parameter. Note that the attention coefficient a ij is the normalized similarity between two nodes (Veli\u010dkovi\u0107 et al., 2017) of Ha in previous decoding step, and h-hop a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_37",
            "content": "(h) ij is the corresponding element of h-th power of matrix [a ij ].",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_38",
            "content": "The masking and attention mechanisms are iterated until the decoding process is terminated. Then we average the attention coefficients a ij over all decoding steps, and normalize them to obtain the refined adjacency matrix \u00c3t . Considering our initial intuition of the similarity for the syntactic structure at both ends, we calculate the final syntactic structure of the target sentence as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_39",
            "content": "A t = Sigmoid W s A s + \u00c3t + b s ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_40",
            "content": "where W s and b s are learnable parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_41",
            "content": "Structural Attention for Word Alignment We adopt A s and the inferred A t to update the representation of language pairs, with the target-tosource attention in (Chen et al., 2021). Finally, we choose the source token with the maximum attention weight towards the current target token.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_42",
            "content": "\u03b1 = attention (A t \u2022 H d , A s \u2022 H e ) , \u03b3(t) = arg max i\u2208{1,...,M } \u03b1 t,i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_43",
            "content": "IMPORTANT Note that even during training, we only use the ground-truth syntactic graph of the source side. The syntactic graph of target side is inferred during training and its derived attention weights subsequently participate the word alignment loss calculation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_44",
            "content": "Hierarchical Graph Random Walks",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "282-ARR_v1_45",
            "content": "In order to incorporate structural constraints for producing high-quality translation, we borrow the idea of (Nikolentzos and Vazirgiannis, 2020) to use a random walk kernel to capture the hierarchical representation of syntactic graphs. The random walk kernel can quantify the similarity of two graphs based on the number of common walks, adopted to effectively capture structures of the input graphs when compared against a number of trainable \"hidden graphs\" 1 (Nikolentzos and Vazirgiannis, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_46",
            "content": "In this work, we initialize two groups of trainable \"hidden graphs\" with differentiated scales, which compare the inputs using a random walk kernel to capture the structural representation of syntactic graphs both locally and globally. Consider the syntactic graph (denoted as G d ) in the decoder and a \"hidden graph\" G h , their direct product G \u00d7 d is a graph (Nikolentzos and Vazirgiannis, 2020) over pairs of nodes from G d and G h . Two nodes in G \u00d7 d are neighbors if and only if the corresponding nodes in G d and G h are both neighbors. It has been shown that performing a random walk on the direct product G \u00d7 d is equivalent to performing a simultaneous random walk on the two graphs G d and G h . We denote by A \u00d7 d the adjacency matrix of G \u00d7 d , and assume a uniform distribution for the starting and stopping probabilities over the nodes of G d and G h . In this way, the random walk kernel will count all pairs of matching walks on G d and G h through the adjacency matrix A \u00d7 d . We perform the P -step (P \u2208 N) random walk kernel which calculates the number of common walks of length p between two graphs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_47",
            "content": "k (p) (G d , G h ) = |V \u00d7 d | i=1 |V \u00d7 d | j=1 A \u00d7(p) d ij",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_48",
            "content": ".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_49",
            "content": "For each p \u2208 {0, 1, ..., P }, a different kernel value is calculated which can be thought of as the structural representation of graph G d . Therefore, given the two sets P = {0, 1, ..., P } and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_50",
            "content": "G h = G 1 h , G 2 h , ..., G K h where G 1 h , G 2 h , ..., G K h 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_51",
            "content": "Similar to the trainable \"kernel\" in convolution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_52",
            "content": "denote the K \"hidden graphs\", we can compute one feature for each element of the Cartesian product P \u00d7 G h , and further build a matrix R \u2208 R K\u00d7(P +1) for G d where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_53",
            "content": "R ij = k j G d , G i h .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_54",
            "content": "Finally, the matrix R is flattened as supplementary representation to incorporate structural constraints into the decoder outputs from Transformer for producing translation results. In order to capture both local and global structural information, we assign two differentiated scales (with node sizes 4 and 6) of \"hidden graphs\" to compare against the syntactic graphs at both ends. The whole process is illustrated in Figure 4. The adopted \"hidden graphs\" can learn the graph structures during training with backpropagation so that the translation outputs are highly interpretable, while the employed random walk kernel is differentiable and therefore the whole framework is end-to-end trainable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_55",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "282-ARR_v1_56",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "282-ARR_v1_57",
            "content": "We conducted our experiments on four publicly available datasets. For German-English (de-en) 2 , Romanian-English (ro-en) and French-English (fr-en) 3 , we followed the experimental setup in (Zenkel et al., 2020) and used the preprocessing scripts from (Zenkel et al., 2019). We also followed (Ding et al., 2019) the NIST corpora while the test set is from the v1testset released by TsinghuaAligner (Liu and Sun, 2015). We learned a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10K merge operations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_58",
            "content": "Settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "282-ARR_v1_59",
            "content": "We adopted parsing tools 4 to construct syntactic graphs for the language of the encoder. Both the encoder and decoder of the transformer have 4 layers of attentions with 4 attention heads each. The embedding size and hidden states are set to 512, while the feed-forward layer has 2,048 cells. The training token-level batch size is 36K. All models were trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from (Zenkel et al., 2019) 5 . We aggregated the 1and 2-hop neighbor of each target token in proposed dynamic graph convolution for alignment, and performed P = {0, 1}-steps random walk with beam size to 4 in the decoding process of translation. Alignment error rate (AER) (Och and Ney, 2000) and BLEU (Papineni et al., 2002) are used for measuring word alignment accuracy and translation quality, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_60",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "282-ARR_v1_61",
            "content": "We compare our method with two statistical baselines FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993). Besides, our proposal (structure-based) is compared to several neural baselines (content-based), and all the baselines induce alignments from attention weights of content-based representation: NAIVE-ATT (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), SD-SMOOTHGRAD (Ding et al., 2019), ADDSGD (Zenkel et al., 2019), SHIFT-ATT (Chen et al., 2020), SHIFT-AET (Chen et al., 2020), BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021). The details can refer to the Appendix or relevant references.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_62",
            "content": "Alignment Results",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "282-ARR_v1_63",
            "content": "Comparison with Baselines Table 1 compares the alignment results of our method with all the baselines. Our approach significantly outperforms both statistical and neural baselines. Specifically, it improves over GIZA++ by 2.0-7.2 AER points across different language pairs, demonstrating that building a neural aligner is better than statistical aligners and having more potential to become a universal alignment tool. When compared with neural baselines either using guided training or without guidance, we find our proposal still achieves substantial improvements over all methods. For instance, it improves over SHIFT-AET and MASK-ALIGN by 2.4 and 0.7 individually AER points on the Romanian-English pair, indicating that the incorporation of syntactic structure achieves superior alignment results compared to these that rely only on the content of inputs. Besides, we also evaluate our proposal on Chinese-English pair and compare other methods in Table 2. The experimental results are highly consistent with the observations on other language pairs, demonstrating the effectiveness of alignment based on modeling dependencies and capturing structural similarities for distant language pairs. Ablation Study Table 3 shows the ablation results on two language pairs. Our approach achieves a gain of 23.8 and 14.6 AER points with fewer parameters compared to vanilla Transformer. When considering the introduced Dynamic Graph Convolution Networks, the aggregated 1-hop neighbor can only capture the local structure, and thus the alignment accuracy is limited. In contrast, aggregating all the 1-, 2-, and 3-hop neighbor for each target node, while better capturing the global dependency, brings with it an increase of parameters and the possible introduction of noisy nodes. We finally achieve the trade-off between performance and parameter size by aggregating both the 1and 2-hop neighbor. Notably, the accuracy of alignment slightly decreases when we remove the translation task, showing the effectiveness of our multi-task learning framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_64",
            "content": "Case Study Figure 5(a) shows the attention weights from three different models for a symmetrized alignment example from de-en test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_65",
            "content": "In this example, SHIFT-ATT puts high weights wrongly on \"1968\" when predicting the target token \"tokyo\", while MASK-ALIGN fails to resolve ambiguity when predicting the target token \"in\". In contrast, our approach produces the attention weights based on structural matching of source and target tokens, which are highly consistent with the gold alignment. Furthermore, we visualize the complete syntactic structure inferred by introduced DGCN in Figure 5(b), which could explicitly reflect the dependencies between each target token.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_66",
            "content": "Translation Results",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "282-ARR_v1_67",
            "content": "Comparison with Baselines Table 4 shows the comparison of translation quality and the corresponding decoding speed. Although this work has improved the performance of word alignment, our experiments show that the benefits from the representation of syntactic structure also extend to the translation task. Compared with (Marcheggiani et al., 2018) Table 4: Comparison of BLEU scores and the averaged decoding speed tested on test sets of three language pairs. p refers that a p-step random walk is performed during the decoding process, while beam is the beam size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_68",
            "content": "introduced Hierarchical Graph Random Walks, we further conducted ablation experiments from two perspectives: the number of steps for random walk and the beam size for decoding. Table 4 shows the comparison results. It can be inferred that increasing the step length (e.g., p = 2) can improve the capability of \"hidden graphs\" to better capture the global structure. However, continuing to increase the step (e.g., p = 3) length will not always improve the performance, since it not only introduces more parameters, but also is likely to confuse the model by the complicated closed-loop structure. Moreover, increasing the beam size does not bring sustainable gains, but it inevitably decreases the speed of decoding. We also provide case studies in the Appendix, demonstrating the learned \"hidden graphs\" can capture both the local and global dependencies of target sentences, leading to more discriminative features which are further adopted to produce high-quality translations. Notably, the quality of translation significantly decreases when we remove the alignment branch, suggesting that the internal logic of both tasks are tightly correlated by exploiting the dependencies between language pairs for multi-task learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_69",
            "content": "Related Works",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "282-ARR_v1_70",
            "content": "Our work is closely related to unsupervised neural word alignment. While early unsupervised neural aligners failed to outperform their statistical counterparts such as FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), a lot of latest works (Li et al., 2019;Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020 have made significant progress by inducing unsupervised neural aligners from NMT to produce better word alignments. Significantly, BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021) leverage the both side content information of the decoder, sacrificing the ability of translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_71",
            "content": "Our work is also related to syntax-based or Transformer based neural machine translation models which have shown large advantages on a myriad of datasets. (Bastings et al., 2017) incorporated syntactic structure into the encoder of NMT model and proposed syntactic GCNs. (Marcheggiani et al., 2018) refined the above work to inject a semantic bias into sentence encoders. Transformer based NMT models (Vaswani et al., 2017;Hasler et al., 2018) attribute their superior performance to the multi-layer and multi-head self-attention architecture. (Garg et al., 2019) trained the Transformer to jointly learn word alignment and translation through multi-task learning based on existing token aligners such as GIZA++ (Och and Ney, 2003). Our work differs from prior studies in that we simultaneously incorporate the syntactic structure into both encoder and decoder to tightly correlate the internal logic of word alignment and machine translation for multi-task learning. To the best of our knowledge, this is the first work that incorporates syntactic structure based constraints into the decoder of NMT models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_72",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "282-ARR_v1_73",
            "content": "We propose a multi-task learning framework that tightly correlates the internal logic of word alignment and machine translation, by fully exploits the syntactic structure of both source and target tokens and the similarity of dependencies at both ends. Experiments show that our proposal achieves the new State-of-the-Art results among all neural methods in word alignment, while producing high-quality translations. We leave it for future work to extend our study to more downstream tasks and systems in natural language processing. A Detailed Network Architecture of f M",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_74",
            "content": "For each observed token from the target side in introduced Dynamic Graph Convolution Networks, we learn a soft M to predict its dependency with other observed tokens by a light-weight network:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_75",
            "content": "hd = M ean P ooling ( Hd + A s \u2022 H e ) \u2208 R D , M = ( Hd \u2022 W d ) \u2297 hd \u2208 R N \u00d7 N \u00d7D , M = Sigmoid M ax P ooling ( M ) \u2208 R N \u00d7 N ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_76",
            "content": "where \u2297 denotes the element-wise multiplication, and W d is a trainable matrix. BTBA (Zhang and van Genabith, 2021) predicts the current target token by paying attention to the source context and both left-side and right-side target context to produce target-to-source alignment. MASK-ALIGN (Chen et al., 2021) extracts alignments from introduced leaky attention and trains with the masked language model fashion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_77",
            "content": "Case Study In Figure 6, we present a visualization of the learned \"hidden graphs\" for de\u2192en and ro\u2192en pairs. It can be clearly observed that by introducing Hierarchical Graph Random Walks in the decoding process, the \"hidden graphs\" can capture both the local and global dependencies of target sentences, making the model obtain more discriminative features representation which are further adopted to produce high-quality translation results. Besides, we provide the translation results among different variants of our proposal in Figure 7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v1_78",
            "content": "UNKNOWN, None, 2016, Incorporating discrete translation lexicons into neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Incorporating discrete translation lexicons into neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_79",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_80",
            "content": "UNKNOWN, None, 2017, Graph convolutional encoders for syntax-aware neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Graph convolutional encoders for syntax-aware neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_81",
            "content": "Stephen A Della Peter F Brown, Vincent J Della Pietra, Robert L Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Stephen A Della Peter F Brown",
                    "Vincent J Della Pietra",
                    "Robert L Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_82",
            "content": "UNKNOWN, None, 2021, Maskalign: Self-supervised neural word alignment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Maskalign: Self-supervised neural word alignment",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_83",
            "content": "UNKNOWN, None, 2020, Accurate word alignment induction from neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Accurate word alignment induction from neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_84",
            "content": "UNKNOWN, None, 2019, Saliency-driven word alignment interpretation for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Saliency-driven word alignment interpretation for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_85",
            "content": "UNKNOWN, None, 2019, Training neural machine translation to apply terminology constraints, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Training neural machine translation to apply terminology constraints",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_86",
            "content": "Chris Dyer, Victor Chahuneau, Noah A Smith, A simple, fast, and effective reparameterization of ibm model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chris Dyer",
                    "Victor Chahuneau",
                    "Noah A Smith"
                ],
                "title": "A simple, fast, and effective reparameterization of ibm model 2",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_87",
            "content": "UNKNOWN, None, 2019, Jointly learning to align and translate with transformer models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Jointly learning to align and translate with transformer models",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_88",
            "content": "UNKNOWN, None, 2018, Neural machine translation decoding with terminology constraints, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Neural machine translation decoding with terminology constraints",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_89",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Semisupervised classification with graph convolutional networks",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_90",
            "content": "Philipp Koehn, Amittai Axelrod, Alexandra Mayne, Chris Callison-Burch, Miles Osborne, David Talbot, Edinburgh system description for the 2005 iwslt speech translation evaluation, 2005, Proceedings of the Second International Workshop on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Philipp Koehn",
                    "Amittai Axelrod",
                    "Alexandra Mayne",
                    "Chris Callison-Burch",
                    "Miles Osborne",
                    "David Talbot"
                ],
                "title": "Edinburgh system description for the 2005 iwslt speech translation evaluation",
                "pub_date": "2005",
                "pub_title": "Proceedings of the Second International Workshop on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_91",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Xintong Li",
                    "Guanlin Li",
                    "Lemao Liu",
                    "Max Meng",
                    "Shuming Shi"
                ],
                "title": "On the word alignment from neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_92",
            "content": "Yang Liu, Maosong Sun, Contrastive unsupervised word alignment with non-local features, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Yang Liu",
                    "Maosong Sun"
                ],
                "title": "Contrastive unsupervised word alignment with non-local features",
                "pub_date": "2015",
                "pub_title": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_93",
            "content": "UNKNOWN, None, 2018, Exploiting semantics in neural machine translation with graph convolutional networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Exploiting semantics in neural machine translation with graph convolutional networks",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_94",
            "content": ", Giannis Nikolentzos and Michalis Vazirgiannis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [],
                "title": "Giannis Nikolentzos and Michalis Vazirgiannis",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_95",
            "content": "Josef Franz, Hermann Och,  Ney, Improved statistical alignment models, 2000, Proceedings of the 38th annual meeting of the association for computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "Improved statistical alignment models",
                "pub_date": "2000",
                "pub_title": "Proceedings of the 38th annual meeting of the association for computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_96",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "A systematic comparison of various statistical alignment models",
                "pub_date": "2003",
                "pub_title": "Computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_97",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_98",
            "content": "UNKNOWN, None, 2016, Neural machine translation of rare words with subword units, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Neural machine translation of rare words with subword units",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_99",
            "content": "UNKNOWN, None, 2019, Code-switching for enhancing nmt with pre-specified translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Code-switching for enhancing nmt with pre-specified translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_100",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "Quoc V Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_101",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v1_102",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "282-ARR_v1_0@0",
            "content": "Structural Supervision for Word Alignment and Machine Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@0",
            "content": "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@1",
            "content": "Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 169,
            "end": 423,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@2",
            "content": "In this work, to our best knowledge, we first propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 425,
            "end": 691,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@3",
            "content": "Particularly, we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially generate the syntactic graphs of target tokens and further guide the word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 693,
            "end": 887,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@4",
            "content": "On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 889,
            "end": 1085,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_2@5",
            "content": "Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_2",
            "start": 1087,
            "end": 1358,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_4@0",
            "content": "Word alignment (Brown et al., 1993) aims to find the correspondence between tokens in a sentence pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_4",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_4@1",
            "content": "Neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017) works by taking an end-to-end approach to incrementally predict the target translation from a source sentence, where no explicit word alignment is required during model training or decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_4",
            "start": 103,
            "end": 370,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_4@2",
            "content": "Recently, there has been an increasing interest (Zenkel et al., 2020;Chen et al., 2020Chen et al., , 2021Zhang and van Genabith, 2021) in combining the two tasks through inducing accurate Regardless of direction and type: (1) the dependencies between 'you ' and 're' and between 're' and 'naive' in English match the dependencies between 'Du' and 'bist' and between 'bist' and 'naiv' in German.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_4",
            "start": 372,
            "end": 765,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_4@3",
            "content": "(2) For English-French (Chinese) pairs, although there is no explicit dependency between 'T u '(\u4f60) and 'es'(\u5f88), we can capture the implicit dependency by tracing the dependencies between 'T u '( \u4f60 ) and 'na\u00ef ve'(\u5929\u771f) and between 'na\u00ef ve'(\u5929\u771f) and 'es'(\u5f88).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_4",
            "start": 767,
            "end": 1019,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_4@4",
            "content": "word alignment in neural translation models for improving translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_4",
            "start": 1021,
            "end": 1098,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_5@0",
            "content": "Intuitively, word alignment is helpful to enforce the domain-specific terminology or improve the translations of low-frequency tokens (Song et al., 2019;Dinu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_5",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_5@1",
            "content": "Also, word alignment provides supportive linguistic information on translation outputs, being useful in interactive translation with the human in the loop (Weng et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_5",
            "start": 173,
            "end": 347,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_5@2",
            "content": "Since the target-to-source attention in NMT models can infer rough word alignments but induce many errors with low accuracy, a number of recent works (Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020Zhang and van Genabith, 2021) focus on NMTbased alignment methods which take alignments as a by-product of NMT systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_5",
            "start": 349,
            "end": 676,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@0",
            "content": "Although NMT-based aligners have proven to be effective and achieved the State-of-the-Art alignment accuracy, they suffer from two major limitations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@1",
            "content": "First, due to the autoregressive property (Sutskever et al., 2014), they (Dyer et al., 2013;Bahdanau et al., 2015;Vaswani et al., 2017;Chen et al., 2020) only leverage partial target context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 150,
            "end": 340,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@2",
            "content": "The latest works (Chen et al., 2021;Zhang and van Genabith, 2021) alleviate this deficiency to exploit both sides of the target content to compute better target-to-source attention (alignment), by abandoning autoregressive decoder and sacrificing the translation ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 342,
            "end": 612,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@3",
            "content": "In addition, there are also related works (Bastings et al., 2017;Marcheggiani et al., 2018) proposing syntax-aware NMT models without word alignment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 614,
            "end": 767,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@4",
            "content": "However, they simply utilize the syntactic structure of source tokens and ignore to capture the syntactic structure of target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 769,
            "end": 901,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@5",
            "content": "In summary, the syntactic structure of both source and target tokens has not been thoroughly explored to guide accurate alignments, while the similarity of dependencies across diverse languages has not been utilized for producing translation outputs with high-quality and favorable generalization capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 903,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@6",
            "content": "Second, they (Garg et al., 2019;Zenkel et al., 2020) typically use multi-task learning architecture to jointly learn the word alignment and translation with elaborately designed loss functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 1214,
            "end": 1406,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_6@7",
            "content": "However, this is computationally expensive for training and the internal logic between the two subtasks is not well correlated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_6",
            "start": 1408,
            "end": 1534,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_7@0",
            "content": "To alleviate mentioned problems, we propose to simultaneously consider the syntactic structure of both source and target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_7",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_7@1",
            "content": "According to the similar dependencies across language pairs, the syntactic graphs of target tokens are first sequentially inferred through introduced Dynamic Graph Convolution Networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_7",
            "start": 129,
            "end": 313,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_7@2",
            "content": "Hierarchical Graph Random Walks are then performed based on the built syntactic graphs at both ends, as well as the initialized multi-scale and trainable \"hidden graphs\" (Nikolentzos and Vazirgiannis, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_7",
            "start": 315,
            "end": 521,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_7@3",
            "content": "We found that by correlating cross-linguistic dependencies without any additional guided loss, word alignment and translation can be more effectively integrated into a unified learning framework, efficiently correlating the internal logic between subtasks while improving the interpretability of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_7",
            "start": 523,
            "end": 828,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_8@0",
            "content": "(1) We introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic graphs of target tokens to guide the word alignment learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_8",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_8@1",
            "content": "(2) Hierarchical Graph Random Walks are further performed to incorporate both local and global structural constraints for producing translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_8",
            "start": 150,
            "end": 301,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_8@2",
            "content": "(3) Results on four language pairs demonstrate that our method is highly effective in such alignment-or translation-related NLP tasks, consistently outperforming baselines in alignment accuracy and translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_8",
            "start": 303,
            "end": 520,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_8@3",
            "content": "(4) We will release our codebase upon acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_8",
            "start": 522,
            "end": 570,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_9@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_9",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_10@0",
            "content": "Word Alignment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_10",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_11@0",
            "content": "A naive way to extract alignments from NMT models is to choose the source token with the maximum accumulated attention weight towards the current target token (Arthur et al., 2016;Hasler et al., 2018):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_11",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_12@0",
            "content": "\u03b3(t) = arg max i\u2208{1,...,M } N l=1 \u03b1 l t,i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_12",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_13@0",
            "content": ", where i is the candidate aligned source-side position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_13",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_13@1",
            "content": "For decoding step t in layer l, \u03b1 l t,i is the attention weight of the i-th position in the source, produced by an average of all the attention heads in Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_13",
            "start": 57,
            "end": 221,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_13@2",
            "content": "Although simple to implement, this method fails to obtain satisfactory alignment results (Li et al., 2019;Ding et al., 2019;Chen et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_13",
            "start": 223,
            "end": 365,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_13@3",
            "content": "In this work, we sufficiently exploit the similarity of dependencies between language pairs, training a novel multi-task learning framework to jointly learn translation and word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_13",
            "start": 367,
            "end": 554,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_14@0",
            "content": "Neural Machine Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_14",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_15@0",
            "content": "Let x = x 1 , ..., x M and y = y 1 , ..., y N be the source and target sentence respectively, neural machine translation models the probability of the target sentence conditioned on the source sentence: P (y|x; \u03b8) = N i=1 P (y i |y <i , x), where y <i is a partial translation from the first to (i-1)-th target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_15",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_15@1",
            "content": "Existing NMT models are generally equipped with encoder-decoder structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_15",
            "start": 319,
            "end": 392,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_15@2",
            "content": "The encoder encodes the source sentence, while the decoder generates the target sentence through a target-to-source attention mechanism and performs left-to-right autoregressive decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_15",
            "start": 394,
            "end": 580,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_15@3",
            "content": "In this work, we adopt Transformer (Vaswani et al., 2017) as the baseline to build our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_15",
            "start": 582,
            "end": 675,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_15@4",
            "content": "Transformer is also an encoder-decoder framework while each decoder layer attends to the encoder output with multihead attention, and we refer to the original paper (Vaswani et al., 2017) for more details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_15",
            "start": 677,
            "end": 881,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_16@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_16",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_17@0",
            "content": "Our work is inspired by the fact that tokens in different languages have similar dependencies under the same semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_17",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_17@1",
            "content": "As shown in Figure 1, the dependencies between tokens with the same semantics in the English-German pair are highly similar, while the similarity of dependencies between English and French (Chinese) can also be implicitly captured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_17",
            "start": 120,
            "end": 350,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_17@2",
            "content": "We regard each token as a node, and build the edges according to the corresponding dependencies between each node to form the syntactic graphs of different languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_17",
            "start": 352,
            "end": 517,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_17@3",
            "content": "For instance, there is a dependency between 'you' and 're', and the node 'you' is the 1-hop neighbor of 're' in the built English (syntactic) graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_17",
            "start": 519,
            "end": 666,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_17@4",
            "content": "While there is no explicit dependency between 'Tu' and 'es' and we have to pass through 'na\u00efve' to reach 'es' from 'Tu', so the node 'Tu' is treated as the 2-hop neighbor of 'es' in the French (syntactic) graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_17",
            "start": 668,
            "end": 878,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_18@0",
            "content": "Figure 2 shows the overall architecture of proposed multi-task learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_18",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_18@1",
            "content": "We model the joint distribution of the target tokens and the target syntactic graphs by factorizing it into the product of a series conditional distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_18",
            "start": 83,
            "end": 240,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_19@0",
            "content": "P (y, y s |x s , x) = N i=1 P (y i |y s \u2264i , x s , y <i , x) \u00d7 P (y s i |y s <i , x s , y <i , x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_19",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_20@0",
            "content": "where y <i , y s <i are partially generated target sentence and syntactic graph, and (x, x s ) indicates the entire information from the source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_20",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_20@1",
            "content": "For the tokens y, we can directly optimize translation loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_20",
            "start": 150,
            "end": 209,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_20@2",
            "content": "However, we do not leverage the ground-truth of the target syntactic graph to maximize the likelihood.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_20",
            "start": 211,
            "end": 312,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_20@3",
            "content": "We propose a proxy to construct the word alignment \u03b1 with graph convolution networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_20",
            "start": 314,
            "end": 398,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_20@4",
            "content": "\u03b1 = proxy(y s )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_20",
            "start": 400,
            "end": 414,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_21@0",
            "content": "Then we optimize the word alignment loss as a surrogate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_21",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_22@0",
            "content": "Specifically, our approach first build the syntactic graph of source tokens, on which basis we introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic structure of observed target tokens, efficiently generating accurate alignment results which derived from the structural attention weights between both sides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_22",
            "start": 0,
            "end": 333,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_22@1",
            "content": "To better encourage the correlation of the internal logic between word alignment and translation, Hierarchical Graph Random Walks are then performed to incorporate structural constraints for producing high-quality translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_22",
            "start": 335,
            "end": 568,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_23@0",
            "content": "Dynamic Graph Convolution Networks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_23",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_24@0",
            "content": "We can first build the source syntax graph with the output representations H e from Transformer encoder, where each node corresponds to one token representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_24",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_24@1",
            "content": "In particular, the adjacency matrix A s is generated from the parsed syntactic structure, where a (i,j) = 1 indicates there is a dependency between node i and j. Meanwhile, we initialize the rough adjacency matrix \u0100t containing only selfconnections for each target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_24",
            "start": 162,
            "end": 432,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_24@2",
            "content": "Therefore, Dynamic Graph Convolution Networks is leveraged to adaptively adjust the graph topology for obtaining refined adjacent structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_24",
            "start": 434,
            "end": 574,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_24@3",
            "content": "Significantly, both masking and attention mechanisms are introduced to distinguish and re-weight observed target nodes through the captured multi-hops neighbor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_24",
            "start": 576,
            "end": 735,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_25@0",
            "content": "For each decoding step, masking mechanism is first built for the observed set of target nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_25",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_25@1",
            "content": "For each observed token (or node), we predict a soft mask M to indicate its dependency with other observed tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_25",
            "start": 95,
            "end": 208,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_25@2",
            "content": "A light-weight two-layer pooling network is used to learn the mask.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_25",
            "start": 210,
            "end": 276,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_25@3",
            "content": "Step 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_25",
            "start": 278,
            "end": 283,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_26@0",
            "content": "M = f M (A s , Hd , H e ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_26",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_27@0",
            "content": "Step 3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_27",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_28@0",
            "content": "Step 5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_28",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_29@0",
            "content": "Step 5 where Hd \u2208 R N \u00d7D denotes the D-dimension features of N observed target nodes generated from Transformer decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_29",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_29@1",
            "content": "The detailed network architecture of f M can refer to the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_29",
            "start": 121,
            "end": 187,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_29@2",
            "content": "The obtained M \u2208 R N \u00d7 N serves as an information gatekeeper, retaining the nodes that are optimal for local aggregation with a global perspective, capturing linguistic dependencies discriminatively without compromising the topology of the syntactic graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_29",
            "start": 189,
            "end": 444,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_30@0",
            "content": "\ud835\udc34 \ud835\udc61 \ud835\udc34 \ud835\udc60",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_30",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_31@0",
            "content": "We will then process a graph-based information aggregation (Kipf and Welling, 2016) and proceed with a linear transformation, i.e.,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_31",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_32@0",
            "content": "Hm = W m \u2022 \u0100t + M \u2022 Hd + b m ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_32",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_33@0",
            "content": "where \u2022 denotes the matrix multiplication and the formula in square bracket means information aggregation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_33",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_33@1",
            "content": "In this way, the set of observed nodes and their edge connections at target side change dynamically in successive decoding steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_33",
            "start": 107,
            "end": 235,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_34@0",
            "content": "In addition, an attention mechanism is introduced to re-weight and balance the captured multihops neighbor of each observed token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_34",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_34@1",
            "content": "In particular, we aggregate context information by attending over the multi-hops neighbor of each node, while its updated representation is calculated by the weighted average of the connected nodes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_34",
            "start": 131,
            "end": 328,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_35@0",
            "content": "Hi a = ReLU \uf8eb \uf8ed y j \u2208N + (y i ) a (h) ij \u2022 (W a Hj m ) \uf8f6 \uf8f8 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_35",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_36@0",
            "content": "where j = 1, ..., N and N + (y i ) includes the node y i and the nodes directly connected to y i , W a is a learnable parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_36",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_36@1",
            "content": "Note that the attention coefficient a ij is the normalized similarity between two nodes (Veli\u010dkovi\u0107 et al., 2017) of Ha in previous decoding step, and h-hop a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_36",
            "start": 129,
            "end": 286,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_37@0",
            "content": "(h) ij is the corresponding element of h-th power of matrix [a ij ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_37",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_38@0",
            "content": "The masking and attention mechanisms are iterated until the decoding process is terminated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_38",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_38@1",
            "content": "Then we average the attention coefficients a ij over all decoding steps, and normalize them to obtain the refined adjacency matrix \u00c3t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_38",
            "start": 92,
            "end": 226,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_38@2",
            "content": "Considering our initial intuition of the similarity for the syntactic structure at both ends, we calculate the final syntactic structure of the target sentence as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_38",
            "start": 228,
            "end": 398,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_39@0",
            "content": "A t = Sigmoid W s A s + \u00c3t + b s ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_39",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_40@0",
            "content": "where W s and b s are learnable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_40",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_41@0",
            "content": "Structural Attention for Word Alignment We adopt A s and the inferred A t to update the representation of language pairs, with the target-tosource attention in (Chen et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_41",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_41@1",
            "content": "Finally, we choose the source token with the maximum attention weight towards the current target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_41",
            "start": 181,
            "end": 283,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_42@0",
            "content": "\u03b1 = attention (A t \u2022 H d , A s \u2022 H e ) , \u03b3(t) = arg max i\u2208{1,...,M } \u03b1 t,i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_42",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_43@0",
            "content": "IMPORTANT Note that even during training, we only use the ground-truth syntactic graph of the source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_43",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_43@1",
            "content": "The syntactic graph of target side is inferred during training and its derived attention weights subsequently participate the word alignment loss calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_43",
            "start": 107,
            "end": 264,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_44@0",
            "content": "Hierarchical Graph Random Walks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_44",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_45@0",
            "content": "In order to incorporate structural constraints for producing high-quality translation, we borrow the idea of (Nikolentzos and Vazirgiannis, 2020) to use a random walk kernel to capture the hierarchical representation of syntactic graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_45",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_45@1",
            "content": "The random walk kernel can quantify the similarity of two graphs based on the number of common walks, adopted to effectively capture structures of the input graphs when compared against a number of trainable \"hidden graphs\" 1 (Nikolentzos and Vazirgiannis, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_45",
            "start": 238,
            "end": 500,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@0",
            "content": "In this work, we initialize two groups of trainable \"hidden graphs\" with differentiated scales, which compare the inputs using a random walk kernel to capture the structural representation of syntactic graphs both locally and globally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@1",
            "content": "Consider the syntactic graph (denoted as G d ) in the decoder and a \"hidden graph\" G h , their direct product G \u00d7 d is a graph (Nikolentzos and Vazirgiannis, 2020) over pairs of nodes from G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 236,
            "end": 437,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@2",
            "content": "Two nodes in G \u00d7 d are neighbors if and only if the corresponding nodes in G d and G h are both neighbors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 439,
            "end": 544,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@3",
            "content": "It has been shown that performing a random walk on the direct product G \u00d7 d is equivalent to performing a simultaneous random walk on the two graphs G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 546,
            "end": 707,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@4",
            "content": "We denote by A \u00d7 d the adjacency matrix of G \u00d7 d , and assume a uniform distribution for the starting and stopping probabilities over the nodes of G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 709,
            "end": 868,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@5",
            "content": "In this way, the random walk kernel will count all pairs of matching walks on G d and G h through the adjacency matrix A \u00d7 d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 870,
            "end": 995,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_46@6",
            "content": "We perform the P -step (P \u2208 N) random walk kernel which calculates the number of common walks of length p between two graphs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_46",
            "start": 997,
            "end": 1121,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_47@0",
            "content": "k (p) (G d , G h ) = |V \u00d7 d | i=1 |V \u00d7 d | j=1 A \u00d7(p) d ij",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_47",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_48@0",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_48",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_49@0",
            "content": "For each p \u2208 {0, 1, ..., P }, a different kernel value is calculated which can be thought of as the structural representation of graph G d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_49",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_49@1",
            "content": "Therefore, given the two sets P = {0, 1, ..., P } and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_49",
            "start": 141,
            "end": 193,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_50@0",
            "content": "G h = G 1 h , G 2 h , ..., G K h where G 1 h , G 2 h , ..., G K h 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_50",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_51@0",
            "content": "Similar to the trainable \"kernel\" in convolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_51",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_52@0",
            "content": "denote the K \"hidden graphs\", we can compute one feature for each element of the Cartesian product P \u00d7 G h , and further build a matrix R \u2208 R K\u00d7(P +1) for G d where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_52",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_53@0",
            "content": "R ij = k j G d , G i h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_53",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_54@0",
            "content": "Finally, the matrix R is flattened as supplementary representation to incorporate structural constraints into the decoder outputs from Transformer for producing translation results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_54",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_54@1",
            "content": "In order to capture both local and global structural information, we assign two differentiated scales (with node sizes 4 and 6) of \"hidden graphs\" to compare against the syntactic graphs at both ends.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_54",
            "start": 182,
            "end": 381,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_54@2",
            "content": "The whole process is illustrated in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_54",
            "start": 383,
            "end": 427,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_54@3",
            "content": "The adopted \"hidden graphs\" can learn the graph structures during training with backpropagation so that the translation outputs are highly interpretable, while the employed random walk kernel is differentiable and therefore the whole framework is end-to-end trainable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_54",
            "start": 429,
            "end": 696,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_55@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_55",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_56@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_56",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_57@0",
            "content": "We conducted our experiments on four publicly available datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_57",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_57@1",
            "content": "For German-English (de-en) 2 , Romanian-English (ro-en) and French-English (fr-en) 3 , we followed the experimental setup in (Zenkel et al., 2020) and used the preprocessing scripts from (Zenkel et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_57",
            "start": 66,
            "end": 274,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_57@2",
            "content": "We also followed (Ding et al., 2019) the NIST corpora while the test set is from the v1testset released by TsinghuaAligner (Liu and Sun, 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_57",
            "start": 276,
            "end": 418,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_57@3",
            "content": "We learned a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10K merge operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_57",
            "start": 420,
            "end": 531,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_58@0",
            "content": "Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_58",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@0",
            "content": "We adopted parsing tools 4 to construct syntactic graphs for the language of the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@1",
            "content": "Both the encoder and decoder of the transformer have 4 layers of attentions with 4 attention heads each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 90,
            "end": 193,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@2",
            "content": "The embedding size and hidden states are set to 512, while the feed-forward layer has 2,048 cells.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 195,
            "end": 292,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@3",
            "content": "The training token-level batch size is 36K.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 294,
            "end": 336,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@4",
            "content": "All models were trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from (Zenkel et al., 2019) 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 338,
            "end": 491,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@5",
            "content": "We aggregated the 1and 2-hop neighbor of each target token in proposed dynamic graph convolution for alignment, and performed P = {0, 1}-steps random walk with beam size to 4 in the decoding process of translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 493,
            "end": 706,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_59@6",
            "content": "Alignment error rate (AER) (Och and Ney, 2000) and BLEU (Papineni et al., 2002) are used for measuring word alignment accuracy and translation quality, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_59",
            "start": 708,
            "end": 872,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_60@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_60",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_61@0",
            "content": "We compare our method with two statistical baselines FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_61",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_61@1",
            "content": "Besides, our proposal (structure-based) is compared to several neural baselines (content-based), and all the baselines induce alignments from attention weights of content-based representation: NAIVE-ATT (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), SD-SMOOTHGRAD (Ding et al., 2019), ADDSGD (Zenkel et al., 2019), SHIFT-ATT (Chen et al., 2020), SHIFT-AET (Chen et al., 2020), BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_61",
            "start": 117,
            "end": 606,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_61@2",
            "content": "The details can refer to the Appendix or relevant references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_61",
            "start": 608,
            "end": 668,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_62@0",
            "content": "Alignment Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_62",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@0",
            "content": "Comparison with Baselines Table 1 compares the alignment results of our method with all the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@1",
            "content": "Our approach significantly outperforms both statistical and neural baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 103,
            "end": 179,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@2",
            "content": "Specifically, it improves over GIZA++ by 2.0-7.2 AER points across different language pairs, demonstrating that building a neural aligner is better than statistical aligners and having more potential to become a universal alignment tool.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 181,
            "end": 417,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@3",
            "content": "When compared with neural baselines either using guided training or without guidance, we find our proposal still achieves substantial improvements over all methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 419,
            "end": 582,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@4",
            "content": "For instance, it improves over SHIFT-AET and MASK-ALIGN by 2.4 and 0.7 individually AER points on the Romanian-English pair, indicating that the incorporation of syntactic structure achieves superior alignment results compared to these that rely only on the content of inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 584,
            "end": 859,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@5",
            "content": "Besides, we also evaluate our proposal on Chinese-English pair and compare other methods in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 861,
            "end": 960,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@6",
            "content": "The experimental results are highly consistent with the observations on other language pairs, demonstrating the effectiveness of alignment based on modeling dependencies and capturing structural similarities for distant language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 962,
            "end": 1196,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@7",
            "content": "Ablation Study Table 3 shows the ablation results on two language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1198,
            "end": 1269,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@8",
            "content": "Our approach achieves a gain of 23.8 and 14.6 AER points with fewer parameters compared to vanilla Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1271,
            "end": 1381,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@9",
            "content": "When considering the introduced Dynamic Graph Convolution Networks, the aggregated 1-hop neighbor can only capture the local structure, and thus the alignment accuracy is limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1383,
            "end": 1561,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@10",
            "content": "In contrast, aggregating all the 1-, 2-, and 3-hop neighbor for each target node, while better capturing the global dependency, brings with it an increase of parameters and the possible introduction of noisy nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1563,
            "end": 1776,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@11",
            "content": "We finally achieve the trade-off between performance and parameter size by aggregating both the 1and 2-hop neighbor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1778,
            "end": 1893,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_63@12",
            "content": "Notably, the accuracy of alignment slightly decreases when we remove the translation task, showing the effectiveness of our multi-task learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_63",
            "start": 1895,
            "end": 2048,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_64@0",
            "content": "Case Study Figure 5(a) shows the attention weights from three different models for a symmetrized alignment example from de-en test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_64",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_65@0",
            "content": "In this example, SHIFT-ATT puts high weights wrongly on \"1968\" when predicting the target token \"tokyo\", while MASK-ALIGN fails to resolve ambiguity when predicting the target token \"in\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_65",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_65@1",
            "content": "In contrast, our approach produces the attention weights based on structural matching of source and target tokens, which are highly consistent with the gold alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_65",
            "start": 188,
            "end": 354,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_65@2",
            "content": "Furthermore, we visualize the complete syntactic structure inferred by introduced DGCN in Figure 5(b), which could explicitly reflect the dependencies between each target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_65",
            "start": 356,
            "end": 532,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_66@0",
            "content": "Translation Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_66",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_67@0",
            "content": "Comparison with Baselines Table 4 shows the comparison of translation quality and the corresponding decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_67",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_67@1",
            "content": "Although this work has improved the performance of word alignment, our experiments show that the benefits from the representation of syntactic structure also extend to the translation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_67",
            "start": 116,
            "end": 304,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_67@2",
            "content": "Compared with (Marcheggiani et al., 2018) Table 4: Comparison of BLEU scores and the averaged decoding speed tested on test sets of three language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_67",
            "start": 306,
            "end": 458,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_67@3",
            "content": "p refers that a p-step random walk is performed during the decoding process, while beam is the beam size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_67",
            "start": 460,
            "end": 564,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@0",
            "content": "introduced Hierarchical Graph Random Walks, we further conducted ablation experiments from two perspectives: the number of steps for random walk and the beam size for decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@1",
            "content": "Table 4 shows the comparison results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 177,
            "end": 213,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@2",
            "content": "It can be inferred that increasing the step length (e.g., p = 2) can improve the capability of \"hidden graphs\" to better capture the global structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 215,
            "end": 364,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@3",
            "content": "However, continuing to increase the step (e.g., p = 3) length will not always improve the performance, since it not only introduces more parameters, but also is likely to confuse the model by the complicated closed-loop structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 366,
            "end": 595,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@4",
            "content": "Moreover, increasing the beam size does not bring sustainable gains, but it inevitably decreases the speed of decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 597,
            "end": 715,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@5",
            "content": "We also provide case studies in the Appendix, demonstrating the learned \"hidden graphs\" can capture both the local and global dependencies of target sentences, leading to more discriminative features which are further adopted to produce high-quality translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 717,
            "end": 979,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_68@6",
            "content": "Notably, the quality of translation significantly decreases when we remove the alignment branch, suggesting that the internal logic of both tasks are tightly correlated by exploiting the dependencies between language pairs for multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_68",
            "start": 981,
            "end": 1227,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_69@0",
            "content": "Related Works",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_69",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_70@0",
            "content": "Our work is closely related to unsupervised neural word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_70",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_70@1",
            "content": "While early unsupervised neural aligners failed to outperform their statistical counterparts such as FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), a lot of latest works (Li et al., 2019;Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020 have made significant progress by inducing unsupervised neural aligners from NMT to produce better word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_70",
            "start": 67,
            "end": 443,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_70@2",
            "content": "Significantly, BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021) leverage the both side content information of the decoder, sacrificing the ability of translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_70",
            "start": 445,
            "end": 628,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@0",
            "content": "Our work is also related to syntax-based or Transformer based neural machine translation models which have shown large advantages on a myriad of datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@1",
            "content": "(Bastings et al., 2017) incorporated syntactic structure into the encoder of NMT model and proposed syntactic GCNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 155,
            "end": 269,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@2",
            "content": "(Marcheggiani et al., 2018) refined the above work to inject a semantic bias into sentence encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 271,
            "end": 370,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@3",
            "content": "Transformer based NMT models (Vaswani et al., 2017;Hasler et al., 2018) attribute their superior performance to the multi-layer and multi-head self-attention architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 372,
            "end": 542,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@4",
            "content": "(Garg et al., 2019) trained the Transformer to jointly learn word alignment and translation through multi-task learning based on existing token aligners such as GIZA++ (Och and Ney, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 544,
            "end": 731,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@5",
            "content": "Our work differs from prior studies in that we simultaneously incorporate the syntactic structure into both encoder and decoder to tightly correlate the internal logic of word alignment and machine translation for multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 733,
            "end": 966,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_71@6",
            "content": "To the best of our knowledge, this is the first work that incorporates syntactic structure based constraints into the decoder of NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_71",
            "start": 968,
            "end": 1107,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_72@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_72",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_73@0",
            "content": "We propose a multi-task learning framework that tightly correlates the internal logic of word alignment and machine translation, by fully exploits the syntactic structure of both source and target tokens and the similarity of dependencies at both ends.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_73",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_73@1",
            "content": "Experiments show that our proposal achieves the new State-of-the-Art results among all neural methods in word alignment, while producing high-quality translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_73",
            "start": 253,
            "end": 415,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_73@2",
            "content": "We leave it for future work to extend our study to more downstream tasks and systems in natural language processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_73",
            "start": 417,
            "end": 532,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_73@3",
            "content": "A Detailed Network Architecture of f M",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_73",
            "start": 534,
            "end": 571,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_74@0",
            "content": "For each observed token from the target side in introduced Dynamic Graph Convolution Networks, we learn a soft M to predict its dependency with other observed tokens by a light-weight network:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_74",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_75@0",
            "content": "hd = M ean P ooling ( Hd + A s \u2022 H e ) \u2208 R D , M = ( Hd \u2022 W d ) \u2297 hd \u2208 R N \u00d7 N \u00d7D , M = Sigmoid M ax P ooling ( M ) \u2208 R N \u00d7 N ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_75",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_76@0",
            "content": "where \u2297 denotes the element-wise multiplication, and W d is a trainable matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_76",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_76@1",
            "content": "BTBA (Zhang and van Genabith, 2021) predicts the current target token by paying attention to the source context and both left-side and right-side target context to produce target-to-source alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_76",
            "start": 80,
            "end": 278,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_76@2",
            "content": "MASK-ALIGN (Chen et al., 2021) extracts alignments from introduced leaky attention and trains with the masked language model fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_76",
            "start": 280,
            "end": 412,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_77@0",
            "content": "Case Study In Figure 6, we present a visualization of the learned \"hidden graphs\" for de\u2192en and ro\u2192en pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_77",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_77@1",
            "content": "It can be clearly observed that by introducing Hierarchical Graph Random Walks in the decoding process, the \"hidden graphs\" can capture both the local and global dependencies of target sentences, making the model obtain more discriminative features representation which are further adopted to produce high-quality translation results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_77",
            "start": 109,
            "end": 442,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_77@2",
            "content": "Besides, we provide the translation results among different variants of our proposal in Figure 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_77",
            "start": 444,
            "end": 540,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_78@0",
            "content": "UNKNOWN, None, 2016, Incorporating discrete translation lexicons into neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_78",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_79",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2017, Graph convolutional encoders for syntax-aware neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_80",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_81@0",
            "content": "Stephen A Della Peter F Brown, Vincent J Della Pietra, Robert L Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_81",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_82@0",
            "content": "UNKNOWN, None, 2021, Maskalign: Self-supervised neural word alignment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_82",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_83@0",
            "content": "UNKNOWN, None, 2020, Accurate word alignment induction from neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_83",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_84@0",
            "content": "UNKNOWN, None, 2019, Saliency-driven word alignment interpretation for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_84",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2019, Training neural machine translation to apply terminology constraints, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_85",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_86@0",
            "content": "Chris Dyer, Victor Chahuneau, Noah A Smith, A simple, fast, and effective reparameterization of ibm model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_86",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2019, Jointly learning to align and translate with transformer models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_87",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2018, Neural machine translation decoding with terminology constraints, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_88",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_89",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_90@0",
            "content": "Philipp Koehn, Amittai Axelrod, Alexandra Mayne, Chris Callison-Burch, Miles Osborne, David Talbot, Edinburgh system description for the 2005 iwslt speech translation evaluation, 2005, Proceedings of the Second International Workshop on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_90",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_91@0",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_91",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_92@0",
            "content": "Yang Liu, Maosong Sun, Contrastive unsupervised word alignment with non-local features, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_92",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2018, Exploiting semantics in neural machine translation with graph convolutional networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_93",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_94@0",
            "content": ", Giannis Nikolentzos and Michalis Vazirgiannis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_94",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_95@0",
            "content": "Josef Franz, Hermann Och,  Ney, Improved statistical alignment models, 2000, Proceedings of the 38th annual meeting of the association for computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_95",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_96@0",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_96",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_97@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_97",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2016, Neural machine translation of rare words with subword units, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_98",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_99@0",
            "content": "UNKNOWN, None, 2019, Code-switching for enhancing nmt with pre-specified translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_99",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_100@0",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_100",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_101@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_101",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "282-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v1_102",
            "start": 0,
            "end": 23,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_1",
            "tgt_ix": "282-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_1",
            "tgt_ix": "282-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_5",
            "tgt_ix": "282-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_7",
            "tgt_ix": "282-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_8",
            "tgt_ix": "282-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_9",
            "tgt_ix": "282-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_9",
            "tgt_ix": "282-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_11",
            "tgt_ix": "282-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_12",
            "tgt_ix": "282-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_10",
            "tgt_ix": "282-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_10",
            "tgt_ix": "282-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_10",
            "tgt_ix": "282-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_10",
            "tgt_ix": "282-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_9",
            "tgt_ix": "282-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_13",
            "tgt_ix": "282-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_14",
            "tgt_ix": "282-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_14",
            "tgt_ix": "282-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_18",
            "tgt_ix": "282-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_19",
            "tgt_ix": "282-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_21",
            "tgt_ix": "282-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_22",
            "tgt_ix": "282-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_24",
            "tgt_ix": "282-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_25",
            "tgt_ix": "282-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_26",
            "tgt_ix": "282-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_27",
            "tgt_ix": "282-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_29",
            "tgt_ix": "282-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_30",
            "tgt_ix": "282-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_31",
            "tgt_ix": "282-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_32",
            "tgt_ix": "282-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_33",
            "tgt_ix": "282-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_34",
            "tgt_ix": "282-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_35",
            "tgt_ix": "282-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_36",
            "tgt_ix": "282-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_37",
            "tgt_ix": "282-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_38",
            "tgt_ix": "282-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_39",
            "tgt_ix": "282-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_40",
            "tgt_ix": "282-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_41",
            "tgt_ix": "282-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_42",
            "tgt_ix": "282-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_28",
            "tgt_ix": "282-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_43",
            "tgt_ix": "282-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_45",
            "tgt_ix": "282-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_47",
            "tgt_ix": "282-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_48",
            "tgt_ix": "282-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_49",
            "tgt_ix": "282-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_50",
            "tgt_ix": "282-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_51",
            "tgt_ix": "282-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_52",
            "tgt_ix": "282-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_53",
            "tgt_ix": "282-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_54",
            "tgt_ix": "282-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_56",
            "tgt_ix": "282-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_56",
            "tgt_ix": "282-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_57",
            "tgt_ix": "282-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_58",
            "tgt_ix": "282-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_58",
            "tgt_ix": "282-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_60",
            "tgt_ix": "282-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_60",
            "tgt_ix": "282-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_61",
            "tgt_ix": "282-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_64",
            "tgt_ix": "282-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_62",
            "tgt_ix": "282-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_62",
            "tgt_ix": "282-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_62",
            "tgt_ix": "282-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_62",
            "tgt_ix": "282-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_65",
            "tgt_ix": "282-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_67",
            "tgt_ix": "282-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_66",
            "tgt_ix": "282-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_66",
            "tgt_ix": "282-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_66",
            "tgt_ix": "282-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_70",
            "tgt_ix": "282-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_69",
            "tgt_ix": "282-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_69",
            "tgt_ix": "282-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_69",
            "tgt_ix": "282-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_73",
            "tgt_ix": "282-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_74",
            "tgt_ix": "282-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_75",
            "tgt_ix": "282-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_76",
            "tgt_ix": "282-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v1_0",
            "tgt_ix": "282-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_1",
            "tgt_ix": "282-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_2",
            "tgt_ix": "282-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_3",
            "tgt_ix": "282-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_4",
            "tgt_ix": "282-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_5",
            "tgt_ix": "282-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_5",
            "tgt_ix": "282-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_5",
            "tgt_ix": "282-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_6",
            "tgt_ix": "282-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_7",
            "tgt_ix": "282-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_7",
            "tgt_ix": "282-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_7",
            "tgt_ix": "282-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_7",
            "tgt_ix": "282-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_8",
            "tgt_ix": "282-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_8",
            "tgt_ix": "282-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_8",
            "tgt_ix": "282-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_8",
            "tgt_ix": "282-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_9",
            "tgt_ix": "282-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_10",
            "tgt_ix": "282-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_11",
            "tgt_ix": "282-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_12",
            "tgt_ix": "282-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_13",
            "tgt_ix": "282-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_13",
            "tgt_ix": "282-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_13",
            "tgt_ix": "282-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_13",
            "tgt_ix": "282-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_14",
            "tgt_ix": "282-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_15",
            "tgt_ix": "282-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_16",
            "tgt_ix": "282-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_17",
            "tgt_ix": "282-ARR_v1_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_18",
            "tgt_ix": "282-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_18",
            "tgt_ix": "282-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_19",
            "tgt_ix": "282-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_20",
            "tgt_ix": "282-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_21",
            "tgt_ix": "282-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_22",
            "tgt_ix": "282-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_22",
            "tgt_ix": "282-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_23",
            "tgt_ix": "282-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_24",
            "tgt_ix": "282-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_24",
            "tgt_ix": "282-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_24",
            "tgt_ix": "282-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_24",
            "tgt_ix": "282-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_25",
            "tgt_ix": "282-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_25",
            "tgt_ix": "282-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_25",
            "tgt_ix": "282-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_25",
            "tgt_ix": "282-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_26",
            "tgt_ix": "282-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_27",
            "tgt_ix": "282-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_28",
            "tgt_ix": "282-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_29",
            "tgt_ix": "282-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_29",
            "tgt_ix": "282-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_29",
            "tgt_ix": "282-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_30",
            "tgt_ix": "282-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_31",
            "tgt_ix": "282-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_32",
            "tgt_ix": "282-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_33",
            "tgt_ix": "282-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_33",
            "tgt_ix": "282-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_34",
            "tgt_ix": "282-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_34",
            "tgt_ix": "282-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_35",
            "tgt_ix": "282-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_36",
            "tgt_ix": "282-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_36",
            "tgt_ix": "282-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_37",
            "tgt_ix": "282-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_38",
            "tgt_ix": "282-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_38",
            "tgt_ix": "282-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_38",
            "tgt_ix": "282-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_39",
            "tgt_ix": "282-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_40",
            "tgt_ix": "282-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_41",
            "tgt_ix": "282-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_41",
            "tgt_ix": "282-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_42",
            "tgt_ix": "282-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_43",
            "tgt_ix": "282-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_43",
            "tgt_ix": "282-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_44",
            "tgt_ix": "282-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_45",
            "tgt_ix": "282-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_45",
            "tgt_ix": "282-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_46",
            "tgt_ix": "282-ARR_v1_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_47",
            "tgt_ix": "282-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_48",
            "tgt_ix": "282-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_49",
            "tgt_ix": "282-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_49",
            "tgt_ix": "282-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_50",
            "tgt_ix": "282-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_51",
            "tgt_ix": "282-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_52",
            "tgt_ix": "282-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_53",
            "tgt_ix": "282-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_54",
            "tgt_ix": "282-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_54",
            "tgt_ix": "282-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_54",
            "tgt_ix": "282-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_54",
            "tgt_ix": "282-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_55",
            "tgt_ix": "282-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_56",
            "tgt_ix": "282-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_57",
            "tgt_ix": "282-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_57",
            "tgt_ix": "282-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_57",
            "tgt_ix": "282-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_57",
            "tgt_ix": "282-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_58",
            "tgt_ix": "282-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_59",
            "tgt_ix": "282-ARR_v1_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_60",
            "tgt_ix": "282-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_61",
            "tgt_ix": "282-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_61",
            "tgt_ix": "282-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_61",
            "tgt_ix": "282-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_62",
            "tgt_ix": "282-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_63",
            "tgt_ix": "282-ARR_v1_63@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_64",
            "tgt_ix": "282-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_65",
            "tgt_ix": "282-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_65",
            "tgt_ix": "282-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_65",
            "tgt_ix": "282-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_66",
            "tgt_ix": "282-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_67",
            "tgt_ix": "282-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_67",
            "tgt_ix": "282-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_67",
            "tgt_ix": "282-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_67",
            "tgt_ix": "282-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_68",
            "tgt_ix": "282-ARR_v1_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_69",
            "tgt_ix": "282-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_70",
            "tgt_ix": "282-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_70",
            "tgt_ix": "282-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_70",
            "tgt_ix": "282-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_71",
            "tgt_ix": "282-ARR_v1_71@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_72",
            "tgt_ix": "282-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_73",
            "tgt_ix": "282-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_73",
            "tgt_ix": "282-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_73",
            "tgt_ix": "282-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_73",
            "tgt_ix": "282-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_74",
            "tgt_ix": "282-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_75",
            "tgt_ix": "282-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_76",
            "tgt_ix": "282-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_76",
            "tgt_ix": "282-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_76",
            "tgt_ix": "282-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_77",
            "tgt_ix": "282-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_77",
            "tgt_ix": "282-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_77",
            "tgt_ix": "282-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_78",
            "tgt_ix": "282-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_79",
            "tgt_ix": "282-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_80",
            "tgt_ix": "282-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_81",
            "tgt_ix": "282-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_82",
            "tgt_ix": "282-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_83",
            "tgt_ix": "282-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_84",
            "tgt_ix": "282-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_85",
            "tgt_ix": "282-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_86",
            "tgt_ix": "282-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_87",
            "tgt_ix": "282-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_88",
            "tgt_ix": "282-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_89",
            "tgt_ix": "282-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_90",
            "tgt_ix": "282-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_91",
            "tgt_ix": "282-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_92",
            "tgt_ix": "282-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_93",
            "tgt_ix": "282-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_94",
            "tgt_ix": "282-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_95",
            "tgt_ix": "282-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_96",
            "tgt_ix": "282-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_97",
            "tgt_ix": "282-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_98",
            "tgt_ix": "282-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_99",
            "tgt_ix": "282-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_100",
            "tgt_ix": "282-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_101",
            "tgt_ix": "282-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v1_102",
            "tgt_ix": "282-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1158,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "282-ARR",
        "version": 1
    }
}