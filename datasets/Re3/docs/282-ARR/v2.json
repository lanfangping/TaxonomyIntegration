{
    "nodes": [
        {
            "ix": "282-ARR_v2_0",
            "content": "Structural Supervision for Word Alignment and Machine Translation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_2",
            "content": "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multitask learning. Particularly, we won't leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "282-ARR_v2_4",
            "content": "Word alignment (Brown et al., 1993) aims to find the correspondence between tokens in a sentence pair. Neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017) works by taking an end-to-end approach to incrementally predict the target translation from a source sentence, where no explicit word alignment is required during model training or decoding. Recently, there has been an Regardless of direction and type: (1) the dependencies between 'you ' and 're' and between 're' and 'naive' in English match the dependencies between 'Du' and 'bist' and between 'bist' and 'naiv' in German. (2) For English-French (Chinese) pairs, although there is no explicit dependency between 'T u '(\u4f60) and 'es'(\u5f88), we can capture the implicit dependency by tracing the dependencies between 'T u '( \u4f60 ) and 'na\u00ef ve'(\u5929\u771f) and between 'na\u00ef ve'(\u5929\u771f) and 'es'(\u5f88). increasing interest (Zenkel et al., 2020;Chen et al., 2020Chen et al., , 2021Zhang and van Genabith, 2021) in combining the two tasks through inducing accurate word alignment in neural translation models for improving translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_5",
            "content": "Intuitively, word alignment is helpful to enforce the domain-specific terminology or improve the translations of low-frequency tokens (Song et al., 2019;Dinu et al., 2019). Also, word alignment provides supportive linguistic information on translation outputs, being useful in interactive translation with the human in the loop (Weng et al., 2019). Since the target-to-source attention in NMT models can infer rough word alignments but induce many errors with low accuracy, a number of recent works (Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020Zhang and van Genabith, 2021) focus on NMTbased alignment methods which take alignments as a by-product of NMT systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_6",
            "content": "Although NMT-based aligners have proven to be effective and achieved the State-of-the-Art alignment accuracy, they suffer from two major limitations. First, due to the autoregressive property (Sutskever et al., 2014), they (Dyer et al., 2013;Bahdanau et al., 2015;Vaswani et al., 2017;Chen et al., 2020) only leverage partial target context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_7",
            "content": "The latest works (Chen et al., 2021;Zhang and van Genabith, 2021) alleviate this deficiency to exploit both sides of the target content to compute better target-to-source attention (alignment), by abandoning autoregressive decoder and sacrificing the translation ability. In addition, there are also related works (Bastings et al., 2017;Marcheggiani et al., 2018) proposing syntax-aware NMT models without word alignment task. However, they simply utilize the syntactic structure of source tokens and ignore to capture the syntactic structure of target tokens. In summary, the syntactic structure of both source and target tokens has not been thoroughly explored to guide accurate alignments, while the similarity of dependencies across diverse languages has not been utilized for producing translation outputs with high-quality and favorable generalization capabilities. Second, they (Garg et al., 2019;Zenkel et al., 2020) typically use multi-task learning architecture to jointly learn the word alignment and translation with elaborately designed loss functions. However, this is computationally expensive for training and the internal logic between the two subtasks is not well correlated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_8",
            "content": "To alleviate mentioned problems, we propose to simultaneously consider the syntactic structure of both source and target tokens. According to the similar dependencies across language pairs, the syntactic graphs of target tokens are first sequentially inferred through introduced Dynamic Graph Convolution Networks. Hierarchical Graph Random Walks are then performed based on the built syntactic graphs at both ends, as well as the initialized multi-scale and trainable \"hidden graphs\" (Nikolentzos and Vazirgiannis, 2020). We found that by correlating cross-linguistic dependencies without any additional guided loss, word alignment and translation can be more effectively integrated into a unified learning framework, efficiently correlating the internal logic between subtasks while improving the interpretability of the model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_9",
            "content": "(1) We introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic graphs of target tokens and further guide the word alignment learning. (2) Hierarchical Graph Random Walks are further performed to incorporate both local and global structural constraints for producing translation outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_10",
            "content": "(3) Results on four language pairs demonstrate that our method is highly effective in such alignmentor translation-related NLP tasks, consistently out-performing baselines in alignment accuracy and translation quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_11",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "282-ARR_v2_12",
            "content": "Word Alignment",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "282-ARR_v2_13",
            "content": "A naive way to extract alignments from NMT models is to choose the source token with the maximum accumulated attention weight towards the current target token (Arthur et al., 2016;Hasler et al., 2018):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_14",
            "content": "\u03b3(t) = arg max i\u2208{1,...,M } N l=1 \u03b1 l t,i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_15",
            "content": ", where i is the candidate aligned source-side position. For decoding step t in layer l, \u03b1 l t,i is the attention weight of the i-th position in the source, produced by an average of all the attention heads in Transformer (Vaswani et al., 2017). Although simple to implement, this method fails to obtain satisfactory alignment results Ding et al., 2019;Chen et al., 2020). In this work, we sufficiently exploit the similarity of dependencies between language pairs, training a novel multi-task learning framework to jointly learn translation and word alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_16",
            "content": "Neural Machine Translation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "282-ARR_v2_17",
            "content": "Let x = x 1 , ..., x M and y = y 1 , ..., y N be the source and target sentence respectively, neural machine translation models the probability of the target sentence conditioned on the source sentence: P (y|x; \u03b8) = N i=1 P (y i |y <i , x), where y <i is a partial translation from the first to (i-1)-th target tokens. Existing NMT models are generally equipped with the encoder-decoder structure. The encoder encodes the source sentence, while the decoder generates the target sentence through a target-tosource attention mechanism and performs left-toright autoregressive decoding. In this work, we adopt Transformer (Vaswani et al., 2017) as the baseline to build our method, which is also an encoder-decoder framework while each decoder layer attends to the encoder output with multi-head attention.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_18",
            "content": "Approach",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "282-ARR_v2_19",
            "content": "Our work is inspired by the fact that tokens in different languages have similar dependencies under the same semantics. As shown in Figure 1, the dependencies between tokens with the same semantics in the English-German pair are highly similar, while the similarity of dependencies between English and French (Chinese) can also be implicitly captured. We regard each token as a node, and build the edges according to the corresponding dependencies between each node to form the syntactic graphs of different languages. For instance, there is a dependency between 'you' and 're', and the node 'you' is the 1-hop neighbor of 're' in the built English (syntactic) graph. While there is no explicit dependency between 'Tu' and 'es' and we have to pass through 'na\u00efve' to reach 'es' from 'Tu', so the node 'Tu' is treated as the 2-hop neighbor of 'es' in the French (syntactic) graph.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_20",
            "content": "Multi-task Learning",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "282-ARR_v2_21",
            "content": "Figure 2 shows the overall architecture of proposed multi-task learning framework. We model the joint distribution of the target tokens and the target syntactic graphs by factorizing it into the product of a series conditional distributions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_22",
            "content": "P (y, y s |x s , x) = N i=1 P (y i |y s \u2264i , x s , y <i , x) \u00d7 P (y s i |y s <i , x s , y <i , x),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_23",
            "content": "where y <i , y s <i are partially generated target sentence and syntactic graph, and (x, x s ) indicates the entire information from the source side. For the tokens y, we can directly optimize translation loss. However, since we mainly focus on the word alignment dataset, we do not leverage the groundtruth of the target syntactic graph to maximize the likelihood. In order to use the supervised signal of word alignment, we propose a proxy to construct the word alignment \u03b1 with graph convolution networks:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_24",
            "content": "\u03b1 = proxy(y s ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_25",
            "content": "where the proxy construction will be elaborated in the next section. Then we optimize the word alignment loss as a surrogate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_26",
            "content": "In summary, we learn three tasks simultaneously, machine translation and word alignment via supervised signals while inferring syntactic graph of the target side as a byproduct in an unsupervised way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_27",
            "content": "Specifically, our approach first build the syntactic graph of source tokens, on which basis we introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic structure of observed target tokens, efficiently generating accurate alignment results which derived from the structural attention weights between both sides. To better encourage the correlation of the internal logic between word alignment and translation, Hierarchical Graph Random Walks are then performed to incorporate structural constraints for producing high-quality translation outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_28",
            "content": "Dynamic Graph Convolution Networks",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "282-ARR_v2_29",
            "content": "We can first build the source syntax graph with the output representations H e from Transformer encoder, where each node corresponds to one token representation. In particular, the adjacency matrix A s is generated from the parsed syntactic structure, where a (i,j) = 1 indicates there is a dependency between node i and j. Meanwhile, we initialize the rough adjacency matrix \u0100t containing only self-connections for each target token. Afterwards, Dynamic Graph Convolution Networks are leveraged to adaptively adjust the graph topology for obtaining refined adjacent structures. Significantly, both masking and attention mechanisms are introduced to distinguish and re-weight observed target nodes through the captured multi-hops neighbor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_30",
            "content": "For each decoding step, masking mechanism is first built for the observed set of target nodes. For each observed token (or node), we predict a soft mask M to indicate its dependency with other observed tokens. It treats any of the observed tokens as the central node alternately, to reward its significant dependencies from multi-hops neighbor and penalize leftovers. A light-weight two-layer pooling network is used to learn the mask which could be formulated as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_31",
            "content": "M = f M (A s , Hd , H e ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_32",
            "content": "where Hd \u2208 R N \u00d7D denotes the D-dimension features of N observed target nodes generated from Transformer decoder. The detailed network architecture of f M can refer to the Appendix. The obtained M \u2208 R N \u00d7 N serves as an information gatekeeper, retaining the nodes that are optimal for local aggregation with a global perspective, capturing linguistic dependencies discriminatively without compromising the topology of the syntactic graph. We will then process a graph-based information aggregation (Kipf and Welling, 2016) and proceed with a linear transformation, i.e.,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_33",
            "content": "Hm = W m \u2022 \u0100t + M \u2022 Hd + b m ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_34",
            "content": "where \u2022 denotes the matrix multiplication and the formula in square bracket means information aggregation. In this way, the set of observed nodes and their edge connections at target side change dynamically in successive decoding steps.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_35",
            "content": "In addition, an attention mechanism is introduced to re-weight and balance the captured multihops neighbor of each observed token. In particular, we aggregate context information by attending over the multi-hops neighbor of each node, while its updated representation is calculated by the weighted average of the connected nodes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_36",
            "content": "Hi a = ReLU \uf8eb \uf8ed y j \u2208N + (y i ) a (h) ij \u2022 (W a Hj m ) \uf8f6 \uf8f8 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_37",
            "content": "where j = 1, ..., N and N + (y i ) includes the node y i and the nodes directly connected to y i , W a is a Step 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_38",
            "content": "Step 3",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_39",
            "content": "Step 5",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_40",
            "content": "Step 5 learnable parameter. Note that the attention coefficient a ij is the normalized similarity between two nodes (Veli\u010dkovi\u0107 et al., 2017) of Ha in previous decoding step, and h-hop a",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_41",
            "content": "\ud835\udc34 \ud835\udc61 \ud835\udc34 \ud835\udc60",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_42",
            "content": "(h) ij is the corresponding element of h-th power of matrix [a ij ].",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_43",
            "content": "Figure 3 illustrates the detailed process of introduced DGCN. The masking and attention mechanisms are iterated until the decoding process is terminated. Then we average the attention coefficients a ij over all decoding steps, and normalize them to obtain the refined adjacency matrix \u00c3t . Considering our initial intuition of the similarity for the syntactic structure at both ends, we calculate the final syntactic structure of the target sentence as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_44",
            "content": "A t = Sigmoid W s A s + \u00c3t + b s ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_45",
            "content": "where W s and b s are learnable parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_46",
            "content": "Structural Attention for Word Alignment We adopt A s and the inferred A t to update the representation of language pairs, with the target-to-source attention in (Chen et al., 2021). The learned representation simultaneously contains the content and structure information of the context for accurate word alignment. Finally, we choose the source token with the maximum attention weight towards the current target token:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_47",
            "content": "\u03b1 = attention (A t \u2022 H d , A s \u2022 H e ) , \u03b3(t) = arg max i\u2208{1,...,M } \u03b1 t,i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_48",
            "content": "IMPORTANT Note that even during training, we only use the ground-truth syntactic graph of source side. The syntactic graph of target side is inferred during training and its derived attention weights subsequently participate the loss calculation of word alignment task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_49",
            "content": "Hierarchical Graph Random Walks",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "282-ARR_v2_50",
            "content": "In order to incorporate structural constraints for producing high-quality translation, we borrow the idea of (Nikolentzos and Vazirgiannis, 2020) to use a random walk kernel to capture the hierarchical representation of syntactic graphs. The random walk kernel can quantify the similarity of two graphs based on the number of common walks, adopted to effectively capture structures of the input graphs when compared against a number of trainable \"hidden graphs\" 1 . The adopted \"hidden graphs\" can learn the graph structures during training with backpropagation so that the translation outputs are highly interpretable, while the employed random walk kernel is differentiable and therefore the whole framework is end-to-end trainable. The whole process is illustrated in Figure 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_51",
            "content": "1 Similar to the trainable \"kernel\" in convolution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_52",
            "content": "In this work, we initialize two groups of trainable \"hidden graphs\" with differentiated scales, which compare the inputs using a random walk kernel to capture the structural representation of syntactic graphs both locally and globally. Consider the syntactic graph (denoted as G d ) in the decoder and a \"hidden graph\" G h , their direct product G \u00d7 d is a graph over pairs of nodes from G d and G h . We refer to the original paper (Nikolentzos and Vazirgiannis, 2020) for more details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_53",
            "content": "It has been shown that performing a random walk on the direct product G \u00d7 d is equivalent to performing a simultaneous random walk on the two graphs G d and G h . We denote by A \u00d7 d the adjacency matrix of G \u00d7 d , and assume a uniform distribution for the starting and stopping probabilities over the nodes of G d and G h . In this way, the random walk kernel will count all pairs of matching walks on G d and G h through the adjacency matrix A \u00d7 d . We then perform the P -step (P \u2208 N) random walk kernel which calculates the number of common walks of length p between two graphs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_54",
            "content": "k (p) (G d , G h ) = |V \u00d7 d | i=1 |V \u00d7 d | j=1 A \u00d7(p) d ij",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_55",
            "content": ".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_56",
            "content": "For each p \u2208 {0, 1, ..., P }, a different kernel value is calculated which can be thought of as the structural representation of graph G d . Therefore, given the two sets P = {0, 1, ..., P } and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_57",
            "content": "G h = G 1 h , G 2 h , ..., G K h where G 1 h , G 2 h , ..., G K h",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_58",
            "content": "denote the K \"hidden graphs\", we can compute one feature for each element of the Cartesian product P \u00d7 G h , and further build a matrix R \u2208 R K\u00d7(P +1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_59",
            "content": "Nat\u00fcrlich gibt es da einen Unterschied .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_60",
            "content": "Of course there is a difference . for",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_61",
            "content": "G d where R ij = k j G d , G i h .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_62",
            "content": "Finally, the matrix R is flattened as supplementary representation to incorporate structural constraints into the decoder outputs from Transformer for guiding translation outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_63",
            "content": "In order to capture both local and global structural information, we assign two differentiated scales (with node sizes 4 and 6) of \"hidden graphs\" to compare against the syntactic graphs at both ends. In the meantime, the syntactic information from both encoder and decoder are considered to access the robust and high-quality translation system. We also provide case studies of the experiments in Figure 5, demonstrating the learned \"hidden graphs\" can capture both the local and global dependencies of target sentences, leading to more discriminative features which are further adopted to guide the translation outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_64",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "282-ARR_v2_65",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "282-ARR_v2_66",
            "content": "We conducted our experiments on four publicly available datasets. For German-English (de-en) 2 , Romanian-English (ro-en) and French-English (fr-en) 3 , we followed the experimental setup in (Zenkel et al., 2020) and used the preprocessing scripts from (Zenkel et al., 2019). We also followed 2 https://www-i6.informatik.rwthaachen.de/goldAlignment/ 3 http://web.eecs.umich.edu/~mihalcea/wpt/index.html (Ding et al., 2019) to set the last 1K sentences of the training data before preprocessing as validation set. The Chinese-English training set is from the NIST corpora while the test set is from the v1testset released by TsinghuaAligner (Liu and Sun, 2015). We learned a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10K merge operations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_67",
            "content": "Settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "282-ARR_v2_68",
            "content": "We adopted parsing tools 4 to construct syntactic graphs for the language of the encoder. Both the encoder and the decoder of Transformer have 4 layers of attentions with 4 attention heads each. The embedding size and hidden states are set to 512, while the feed-forward layer has 2,048 cells. The training token-level batch size is 36K. All models were trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from (Zenkel et al., 2019) 5 . We aggregated the 1and 2-hop neighbor of each target token in proposed dynamic graph convolutions for alignment, and performed P = {0, 1}-steps random walk with beam size to 4 in the decoding process of translation. Alignment error rate (AER) (Och and Ney, 2000) and BLEU (Papineni et al., 2002) are used for measuring word alignment accuracy and translation quality, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_69",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "282-ARR_v2_70",
            "content": "We compare our method with two statistical baselines FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993). Besides, our proposal (structure-based) is compared to several neural baselines (content-based), and all the baselines induce alignments from attention weights of content-based representation: NAIVE-ATT (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), SD-SMOOTHGRAD (Ding et al., 2019), ADDSGD (Zenkel et al., 2019), SHIFT-ATT (Chen et al., 2020), SHIFT-AET (Chen et al., 2020), BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021). NAIVE-ATT (Garg et al., 2019) SD-SMOOTHGRAD (Ding et al., 2019) induces alignments from token saliency. ADDSGD (Zenkel et al., 2019) explicitly adds an extra attention layer on top of Transformer to predict the to-be-aligned target token. SHIFT-ATT (Chen et al., 2020) induces alignments when the to-be-aligned target token is the decoder input instead of the output. SHIFT-AET (Chen et al., 2020) extracts alignments from an additional module with supervision from symmetrized SHIFT-ATT alignments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_71",
            "content": "BTBA (Zhang and van Genabith, 2021) predicts the current target token by paying attention to the source context and both left-side and right-side target context to produce target-to-source alignment. MASK-ALIGN (Chen et al., 2021) extracts alignments from introduced leaky attention and trains with the masked language model fashion.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_72",
            "content": "Alignment Results",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "282-ARR_v2_73",
            "content": "Comparison with Baselines Table 1 compares the alignment results of our method with all the baselines. Our approach significantly outperforms both statistical and neural baselines. Specifically, it improves over GIZA++ by 2.0-7.2 AER points across different language pairs, demonstrating that building a neural aligner is better than statistical aligners. When compared with neural baselines either using guided training or without guidance, we find our proposal still achieves substantial improvements over all methods. For instance, it improves over SHIFT-AET and MASK-ALIGN by 2.4 and 0.7 individually AER points on the Romanian-English pair, indicating that the incorporation of syntactic structure achieves superior alignment results compared to these that rely only on the content of inputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_74",
            "content": "Besides, we also evaluate our proposal on Chinese-English pair and compare other methods in Table 2. The experimental results are highly consistent with the observations on other language pairs, demonstrating the effectiveness of alignment based on modeling dependencies and capturing structural similarities for distant language pairs. Ablation Study Table 3 shows the ablation results on two language pairs. Our approach achieves a gain of 23.8 and 14.6 AER points with fewer parameters compared to vanilla Transformer. When considering the introduced Dynamic Graph Convolution Networks, the aggregated 1-hop neighbor can only capture the local structure, and thus the alignment accuracy is limited. In contrast, aggregating all the 1-, 2-, and 3-hop neighbor for each target node, while better capturing the global dependency, brings with it an increase of parameters and the possible introduction of noisy nodes. We finally achieve the trade-off between performance and parameter size by aggregating both the 1and 2-hop neighbor. Notably, the accuracy of alignment slightly decreases when we remove the translation task, showing the effectiveness of our multi-task learning framework. Case Study Figure 6(a) shows the attention weights from three different models for a symmetrized alignment example from de-en test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_75",
            "content": "In this example, SHIFT-ATT puts high weights wrongly on \"1968\" when predicting the target token \"tokyo\", while MASK-ALIGN fails to resolve ambiguity when predicting the target token \"in\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_76",
            "content": "In contrast, our approach produces the attention weights based on structural matching of source and target tokens, which are highly consistent with the gold alignment. Furthermore, we visualize the complete syntactic structure inferred by introduced DGCN in Figure 6(b), which could explicitly reflect the dependencies between each target token.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_77",
            "content": "Translation Results",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "282-ARR_v2_78",
            "content": "Comparison with Baselines Table 4 shows the comparison of translation quality and the corresponding decoding speed. Although this work has improved the performance of word alignment, our experiments show that the benefits from the representation of syntactic structure also extend to the translation task. Compared with (Marcheggiani et al., 2018) that only utilize syntactic structure at the encoder side, we substantially improve the performance by incorporating syntactic structure at the decoder side.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_79",
            "content": "Ablation Study To investigate the effectiveness of introduced Hierarchical Graph Random Walks, we further conducted ablation experiments from two perspectives: the number of steps for random walk and the beam size for decoding. Table 4 shows the comparison results. It can be inferred that increasing the step length (e.g., p = 2) can improve the capability of \"hidden graphs\" to better capture the global structure. However, continuing to increase the step (e.g., p = 3) length will not always improve the performance, since it not only introduces more parameters, but also is likely to confuse the model by the complicated closed-loop structure which is prevalent in the graph network. Moreover, increasing the beam size does not bring sustainable gains, but it inevitably decreases the speed of decoding. Notably, the quality of translation significantly decreases when we remove the alignment branch, suggesting that the internal logic of both tasks are tightly correlated by exploiting the dependencies between language pairs for multi-task learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_80",
            "content": "5 Related Works Table 4: Comparison of BLEU scores and the averaged decoding speed tested on test sets of three language pairs. p refers that a p-step random walk is performed during the decoding process, while beam is the beam size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_81",
            "content": "2013) and GIZA++ (Och and Ney, 2003), a lot of latest works Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020 have made significant progress by inducing unsupervised neural aligners from NMT to produce better word alignments. Significantly, BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021) leverage the both side content information of the decoder, sacrificing the ability of translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_82",
            "content": "Our work is also related to syntax-based or Transformer based neural machine translation models which have shown large advantages on a myriad of datasets. (Bastings et al., 2017) incorporated syntactic structure into the encoder of NMT model and proposed syntactic GCNs. (Marcheggiani et al., 2018) refined the above work to inject a semantic bias into sentence encoders. Transformer based NMT models (Vaswani et al., 2017;Hasler et al., 2018) attribute their superior performance to the multi-layer and multi-head self-attention architecture. (Garg et al., 2019) trained the Transformer to jointly learn word alignment and translation through multi-task learning based on existing token aligners such as GIZA++ (Och and Ney, 2003). Our work differs from prior studies in that we simultaneously incorporate the syntactic structure into both encoder and decoder to tightly correlate the internal logic of word alignment and machine translation for multi-task learning. To the best of our knowledge, this is the first work that incorporates syntactic structure based constraints into the decoder of NMT models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_83",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "282-ARR_v2_84",
            "content": "We propose a multi-task learning framework that tightly correlates the internal logic of word alignment and machine translation, by fully exploits the syntactic structure of both source and target tokens and the similarity of dependencies at both ends. Experiments show that our proposal achieves the new State-of-the-Art results among all neural methods in word alignment, while producing high-quality translations. We leave it for future work to extend our study to more downstream tasks and systems in natural language processing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_85",
            "content": "A Detailed Network Architecture of f M For each observed token from the target side, we learn a soft mask M to predict its dependency with other observed tokens by a light-weight network:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "282-ARR_v2_86",
            "content": "Philip Arthur, Graham Neubig, Satoshi Nakamura, Incorporating discrete translation lexicons into neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Philip Arthur",
                    "Graham Neubig",
                    "Satoshi Nakamura"
                ],
                "title": "Incorporating discrete translation lexicons into neural machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_87",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_88",
            "content": "Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima'an, Graph convolutional encoders for syntax-aware neural machine translation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jasmijn Bastings",
                    "Ivan Titov",
                    "Wilker Aziz",
                    "Diego Marcheggiani",
                    "Khalil Sima'an"
                ],
                "title": "Graph convolutional encoders for syntax-aware neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_89",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "F Peter",
                    "Stephen Brown",
                    "Vincent Pietra",
                    "Robert Della Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_90",
            "content": "Chi Chen, Maosong Sun, Yang Liu, Maskalign: Self-supervised neural word alignment, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Chi Chen",
                    "Maosong Sun",
                    "Yang Liu"
                ],
                "title": "Maskalign: Self-supervised neural word alignment",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "282-ARR_v2_91",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yun Chen",
                    "Yang Liu",
                    "Guanhua Chen",
                    "Xin Jiang",
                    "Qun Liu"
                ],
                "title": "Accurate word alignment induction from neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_92",
            "content": "Shuoyang Ding, Hainan Xu, Philipp Koehn, Saliency-driven word alignment interpretation for neural machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Shuoyang Ding",
                    "Hainan Xu",
                    "Philipp Koehn"
                ],
                "title": "Saliency-driven word alignment interpretation for neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_93",
            "content": "Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan, Training neural machine translation to apply terminology constraints, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Georgiana Dinu",
                    "Prashant Mathur",
                    "Marcello Federico",
                    "Yaser Al-Onaizan"
                ],
                "title": "Training neural machine translation to apply terminology constraints",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_94",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chris Dyer",
                    "Victor Chahuneau",
                    "Noah Smith"
                ],
                "title": "A simple, fast, and effective reparameterization of IBM model 2",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_95",
            "content": "Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik, Jointly learning to align and translate with transformer models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Sarthak Garg",
                    "Stephan Peitz",
                    "Udhyakumar Nallasamy",
                    "Matthias Paulik"
                ],
                "title": "Jointly learning to align and translate with transformer models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_96",
            "content": "Eva Hasler, Adri\u00e0 De Gispert, Gonzalo Iglesias, Bill Byrne, Neural machine translation decoding with terminology constraints, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Eva Hasler",
                    "Adri\u00e0 De Gispert",
                    "Gonzalo Iglesias",
                    "Bill Byrne"
                ],
                "title": "Neural machine translation decoding with terminology constraints",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_97",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Semisupervised classification with graph convolutional networks",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_98",
            "content": "Philipp Koehn, Amittai Axelrod, Alexandra Mayne, Chris Callison-Burch, Miles Osborne, David Talbot, Edinburgh system description for the 2005 IWSLT speech translation evaluation, 2005, Proceedings of the Second International Workshop on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Philipp Koehn",
                    "Amittai Axelrod",
                    "Alexandra Mayne",
                    "Chris Callison-Burch",
                    "Miles Osborne",
                    "David Talbot"
                ],
                "title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation",
                "pub_date": "2005",
                "pub_title": "Proceedings of the Second International Workshop on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_99",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Xintong Li",
                    "Guanlin Li",
                    "Lemao Liu",
                    "Max Meng",
                    "Shuming Shi"
                ],
                "title": "On the word alignment from neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_100",
            "content": "Yang Liu, Maosong Sun, Contrastive unsupervised word alignment with non-local features, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Yang Liu",
                    "Maosong Sun"
                ],
                "title": "Contrastive unsupervised word alignment with non-local features",
                "pub_date": "2015",
                "pub_title": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_101",
            "content": "Diego Marcheggiani, Jasmijn Bastings, Ivan Titov, Exploiting semantics in neural machine translation with graph convolutional networks, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Diego Marcheggiani",
                    "Jasmijn Bastings",
                    "Ivan Titov"
                ],
                "title": "Exploiting semantics in neural machine translation with graph convolutional networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_102",
            "content": ", Giannis Nikolentzos and Michalis Vazirgiannis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [],
                "title": "Giannis Nikolentzos and Michalis Vazirgiannis",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_103",
            "content": "Josef Franz, Hermann Och,  Ney, Improved statistical alignment models, 2000, Proceedings of the 38th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "Improved statistical alignment models",
                "pub_date": "2000",
                "pub_title": "Proceedings of the 38th",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_104",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Hong Kong. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Hong Kong. Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_105",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "A systematic comparison of various statistical alignment models",
                "pub_date": "2003",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_106",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_107",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "282-ARR_v2_108",
            "content": "Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, Min Zhang, Code-switching for enhancing NMT with pre-specified translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Kai Song",
                    "Yue Zhang",
                    "Heng Yu",
                    "Weihua Luo",
                    "Kun Wang",
                    "Min Zhang"
                ],
                "title": "Code-switching for enhancing NMT with pre-specified translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_109",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_110",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "Quoc V Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_111",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_112",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_113",
            "content": "UNKNOWN, None, 2019, Correct-andmemorize: Learning to translate from interactive revisions, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Correct-andmemorize: Learning to translate from interactive revisions",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_114",
            "content": "UNKNOWN, None, 2019, Adding interpretable attention to neural translation models improves word alignment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Adding interpretable attention to neural translation models improves word alignment",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_115",
            "content": "Thomas Zenkel, Joern Wuebker, John Denero, End-to-end neural word alignment outperforms GIZA++, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Thomas Zenkel",
                    "Joern Wuebker",
                    "John Denero"
                ],
                "title": "End-to-end neural word alignment outperforms GIZA++",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "282-ARR_v2_116",
            "content": "Jingyi Zhang, Josef Van Genabith, A bidirectional transformer based alignment model for unsupervised word alignment, 2021, Proceedings of the 59th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Jingyi Zhang",
                    "Josef Van Genabith"
                ],
                "title": "A bidirectional transformer based alignment model for unsupervised word alignment",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th",
                "pub": null
            }
        },
        {
            "ix": "282-ARR_v2_117",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "282-ARR_v2_0@0",
            "content": "Structural Supervision for Word Alignment and Machine Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@0",
            "content": "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@1",
            "content": "Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 169,
            "end": 423,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@2",
            "content": "In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multitask learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 425,
            "end": 661,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@3",
            "content": "Particularly, we won't leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 663,
            "end": 983,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@4",
            "content": "On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 985,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_2@5",
            "content": "Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_2",
            "start": 1183,
            "end": 1454,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_4@0",
            "content": "Word alignment (Brown et al., 1993) aims to find the correspondence between tokens in a sentence pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_4",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_4@1",
            "content": "Neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017) works by taking an end-to-end approach to incrementally predict the target translation from a source sentence, where no explicit word alignment is required during model training or decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_4",
            "start": 103,
            "end": 370,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_4@2",
            "content": "Recently, there has been an Regardless of direction and type: (1) the dependencies between 'you ' and 're' and between 're' and 'naive' in English match the dependencies between 'Du' and 'bist' and between 'bist' and 'naiv' in German.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_4",
            "start": 372,
            "end": 605,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_4@3",
            "content": "(2) For English-French (Chinese) pairs, although there is no explicit dependency between 'T u '(\u4f60) and 'es'(\u5f88), we can capture the implicit dependency by tracing the dependencies between 'T u '( \u4f60 ) and 'na\u00ef ve'(\u5929\u771f) and between 'na\u00ef ve'(\u5929\u771f) and 'es'(\u5f88).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_4",
            "start": 607,
            "end": 859,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_4@4",
            "content": "increasing interest (Zenkel et al., 2020;Chen et al., 2020Chen et al., , 2021Zhang and van Genabith, 2021) in combining the two tasks through inducing accurate word alignment in neural translation models for improving translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_4",
            "start": 861,
            "end": 1098,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_5@0",
            "content": "Intuitively, word alignment is helpful to enforce the domain-specific terminology or improve the translations of low-frequency tokens (Song et al., 2019;Dinu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_5",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_5@1",
            "content": "Also, word alignment provides supportive linguistic information on translation outputs, being useful in interactive translation with the human in the loop (Weng et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_5",
            "start": 173,
            "end": 347,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_5@2",
            "content": "Since the target-to-source attention in NMT models can infer rough word alignments but induce many errors with low accuracy, a number of recent works (Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020Zhang and van Genabith, 2021) focus on NMTbased alignment methods which take alignments as a by-product of NMT systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_5",
            "start": 349,
            "end": 676,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_6@0",
            "content": "Although NMT-based aligners have proven to be effective and achieved the State-of-the-Art alignment accuracy, they suffer from two major limitations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_6",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_6@1",
            "content": "First, due to the autoregressive property (Sutskever et al., 2014), they (Dyer et al., 2013;Bahdanau et al., 2015;Vaswani et al., 2017;Chen et al., 2020) only leverage partial target context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_6",
            "start": 150,
            "end": 340,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@0",
            "content": "The latest works (Chen et al., 2021;Zhang and van Genabith, 2021) alleviate this deficiency to exploit both sides of the target content to compute better target-to-source attention (alignment), by abandoning autoregressive decoder and sacrificing the translation ability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@1",
            "content": "In addition, there are also related works (Bastings et al., 2017;Marcheggiani et al., 2018) proposing syntax-aware NMT models without word alignment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 272,
            "end": 425,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@2",
            "content": "However, they simply utilize the syntactic structure of source tokens and ignore to capture the syntactic structure of target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 427,
            "end": 559,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@3",
            "content": "In summary, the syntactic structure of both source and target tokens has not been thoroughly explored to guide accurate alignments, while the similarity of dependencies across diverse languages has not been utilized for producing translation outputs with high-quality and favorable generalization capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 561,
            "end": 870,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@4",
            "content": "Second, they (Garg et al., 2019;Zenkel et al., 2020) typically use multi-task learning architecture to jointly learn the word alignment and translation with elaborately designed loss functions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 872,
            "end": 1064,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_7@5",
            "content": "However, this is computationally expensive for training and the internal logic between the two subtasks is not well correlated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_7",
            "start": 1066,
            "end": 1192,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_8@0",
            "content": "To alleviate mentioned problems, we propose to simultaneously consider the syntactic structure of both source and target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_8",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_8@1",
            "content": "According to the similar dependencies across language pairs, the syntactic graphs of target tokens are first sequentially inferred through introduced Dynamic Graph Convolution Networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_8",
            "start": 129,
            "end": 313,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_8@2",
            "content": "Hierarchical Graph Random Walks are then performed based on the built syntactic graphs at both ends, as well as the initialized multi-scale and trainable \"hidden graphs\" (Nikolentzos and Vazirgiannis, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_8",
            "start": 315,
            "end": 521,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_8@3",
            "content": "We found that by correlating cross-linguistic dependencies without any additional guided loss, word alignment and translation can be more effectively integrated into a unified learning framework, efficiently correlating the internal logic between subtasks while improving the interpretability of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_8",
            "start": 523,
            "end": 828,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_9@0",
            "content": "(1) We introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic graphs of target tokens and further guide the word alignment learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_9",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_9@1",
            "content": "(2) Hierarchical Graph Random Walks are further performed to incorporate both local and global structural constraints for producing translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_9",
            "start": 159,
            "end": 310,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_10@0",
            "content": "(3) Results on four language pairs demonstrate that our method is highly effective in such alignmentor translation-related NLP tasks, consistently out-performing baselines in alignment accuracy and translation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_10",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_11@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_11",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_12@0",
            "content": "Word Alignment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_12",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_13@0",
            "content": "A naive way to extract alignments from NMT models is to choose the source token with the maximum accumulated attention weight towards the current target token (Arthur et al., 2016;Hasler et al., 2018):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_13",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_14@0",
            "content": "\u03b3(t) = arg max i\u2208{1,...,M } N l=1 \u03b1 l t,i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_14",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_15@0",
            "content": ", where i is the candidate aligned source-side position.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_15",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_15@1",
            "content": "For decoding step t in layer l, \u03b1 l t,i is the attention weight of the i-th position in the source, produced by an average of all the attention heads in Transformer (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_15",
            "start": 57,
            "end": 244,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_15@2",
            "content": "Although simple to implement, this method fails to obtain satisfactory alignment results Ding et al., 2019;Chen et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_15",
            "start": 246,
            "end": 371,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_15@3",
            "content": "In this work, we sufficiently exploit the similarity of dependencies between language pairs, training a novel multi-task learning framework to jointly learn translation and word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_15",
            "start": 373,
            "end": 560,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_16@0",
            "content": "Neural Machine Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_16",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_17@0",
            "content": "Let x = x 1 , ..., x M and y = y 1 , ..., y N be the source and target sentence respectively, neural machine translation models the probability of the target sentence conditioned on the source sentence: P (y|x; \u03b8) = N i=1 P (y i |y <i , x), where y <i is a partial translation from the first to (i-1)-th target tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_17",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_17@1",
            "content": "Existing NMT models are generally equipped with the encoder-decoder structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_17",
            "start": 319,
            "end": 396,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_17@2",
            "content": "The encoder encodes the source sentence, while the decoder generates the target sentence through a target-tosource attention mechanism and performs left-toright autoregressive decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_17",
            "start": 398,
            "end": 582,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_17@3",
            "content": "In this work, we adopt Transformer (Vaswani et al., 2017) as the baseline to build our method, which is also an encoder-decoder framework while each decoder layer attends to the encoder output with multi-head attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_17",
            "start": 584,
            "end": 802,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_18@0",
            "content": "Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_18",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_19@0",
            "content": "Our work is inspired by the fact that tokens in different languages have similar dependencies under the same semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_19",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_19@1",
            "content": "As shown in Figure 1, the dependencies between tokens with the same semantics in the English-German pair are highly similar, while the similarity of dependencies between English and French (Chinese) can also be implicitly captured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_19",
            "start": 120,
            "end": 350,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_19@2",
            "content": "We regard each token as a node, and build the edges according to the corresponding dependencies between each node to form the syntactic graphs of different languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_19",
            "start": 352,
            "end": 517,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_19@3",
            "content": "For instance, there is a dependency between 'you' and 're', and the node 'you' is the 1-hop neighbor of 're' in the built English (syntactic) graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_19",
            "start": 519,
            "end": 666,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_19@4",
            "content": "While there is no explicit dependency between 'Tu' and 'es' and we have to pass through 'na\u00efve' to reach 'es' from 'Tu', so the node 'Tu' is treated as the 2-hop neighbor of 'es' in the French (syntactic) graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_19",
            "start": 668,
            "end": 878,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_20@0",
            "content": "Multi-task Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_20",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_21@0",
            "content": "Figure 2 shows the overall architecture of proposed multi-task learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_21",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_21@1",
            "content": "We model the joint distribution of the target tokens and the target syntactic graphs by factorizing it into the product of a series conditional distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_21",
            "start": 83,
            "end": 240,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_22@0",
            "content": "P (y, y s |x s , x) = N i=1 P (y i |y s \u2264i , x s , y <i , x) \u00d7 P (y s i |y s <i , x s , y <i , x),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_22",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_23@0",
            "content": "where y <i , y s <i are partially generated target sentence and syntactic graph, and (x, x s ) indicates the entire information from the source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_23",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_23@1",
            "content": "For the tokens y, we can directly optimize translation loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_23",
            "start": 150,
            "end": 209,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_23@2",
            "content": "However, since we mainly focus on the word alignment dataset, we do not leverage the groundtruth of the target syntactic graph to maximize the likelihood.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_23",
            "start": 211,
            "end": 364,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_23@3",
            "content": "In order to use the supervised signal of word alignment, we propose a proxy to construct the word alignment \u03b1 with graph convolution networks:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_23",
            "start": 366,
            "end": 507,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_24@0",
            "content": "\u03b1 = proxy(y s ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_24",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_25@0",
            "content": "where the proxy construction will be elaborated in the next section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_25",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_25@1",
            "content": "Then we optimize the word alignment loss as a surrogate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_25",
            "start": 69,
            "end": 124,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_26@0",
            "content": "In summary, we learn three tasks simultaneously, machine translation and word alignment via supervised signals while inferring syntactic graph of the target side as a byproduct in an unsupervised way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_26",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_27@0",
            "content": "Specifically, our approach first build the syntactic graph of source tokens, on which basis we introduce Dynamic Graph Convolution Networks to sequentially infer the syntactic structure of observed target tokens, efficiently generating accurate alignment results which derived from the structural attention weights between both sides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_27",
            "start": 0,
            "end": 333,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_27@1",
            "content": "To better encourage the correlation of the internal logic between word alignment and translation, Hierarchical Graph Random Walks are then performed to incorporate structural constraints for producing high-quality translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_27",
            "start": 335,
            "end": 568,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_28@0",
            "content": "Dynamic Graph Convolution Networks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_28",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_29@0",
            "content": "We can first build the source syntax graph with the output representations H e from Transformer encoder, where each node corresponds to one token representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_29",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_29@1",
            "content": "In particular, the adjacency matrix A s is generated from the parsed syntactic structure, where a (i,j) = 1 indicates there is a dependency between node i and j. Meanwhile, we initialize the rough adjacency matrix \u0100t containing only self-connections for each target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_29",
            "start": 162,
            "end": 433,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_29@2",
            "content": "Afterwards, Dynamic Graph Convolution Networks are leveraged to adaptively adjust the graph topology for obtaining refined adjacent structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_29",
            "start": 435,
            "end": 577,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_29@3",
            "content": "Significantly, both masking and attention mechanisms are introduced to distinguish and re-weight observed target nodes through the captured multi-hops neighbor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_29",
            "start": 579,
            "end": 738,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_30@0",
            "content": "For each decoding step, masking mechanism is first built for the observed set of target nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_30",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_30@1",
            "content": "For each observed token (or node), we predict a soft mask M to indicate its dependency with other observed tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_30",
            "start": 95,
            "end": 208,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_30@2",
            "content": "It treats any of the observed tokens as the central node alternately, to reward its significant dependencies from multi-hops neighbor and penalize leftovers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_30",
            "start": 210,
            "end": 366,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_30@3",
            "content": "A light-weight two-layer pooling network is used to learn the mask which could be formulated as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_30",
            "start": 368,
            "end": 463,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_31@0",
            "content": "M = f M (A s , Hd , H e ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_31",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_32@0",
            "content": "where Hd \u2208 R N \u00d7D denotes the D-dimension features of N observed target nodes generated from Transformer decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_32",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_32@1",
            "content": "The detailed network architecture of f M can refer to the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_32",
            "start": 114,
            "end": 180,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_32@2",
            "content": "The obtained M \u2208 R N \u00d7 N serves as an information gatekeeper, retaining the nodes that are optimal for local aggregation with a global perspective, capturing linguistic dependencies discriminatively without compromising the topology of the syntactic graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_32",
            "start": 182,
            "end": 437,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_32@3",
            "content": "We will then process a graph-based information aggregation (Kipf and Welling, 2016) and proceed with a linear transformation, i.e.,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_32",
            "start": 439,
            "end": 569,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_33@0",
            "content": "Hm = W m \u2022 \u0100t + M \u2022 Hd + b m ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_33",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_34@0",
            "content": "where \u2022 denotes the matrix multiplication and the formula in square bracket means information aggregation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_34",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_34@1",
            "content": "In this way, the set of observed nodes and their edge connections at target side change dynamically in successive decoding steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_34",
            "start": 107,
            "end": 235,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_35@0",
            "content": "In addition, an attention mechanism is introduced to re-weight and balance the captured multihops neighbor of each observed token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_35",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_35@1",
            "content": "In particular, we aggregate context information by attending over the multi-hops neighbor of each node, while its updated representation is calculated by the weighted average of the connected nodes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_35",
            "start": 131,
            "end": 328,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_36@0",
            "content": "Hi a = ReLU \uf8eb \uf8ed y j \u2208N + (y i ) a (h) ij \u2022 (W a Hj m ) \uf8f6 \uf8f8 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_36",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_37@0",
            "content": "where j = 1, ..., N and N + (y i ) includes the node y i and the nodes directly connected to y i , W a is a Step 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_37",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_38@0",
            "content": "Step 3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_38",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_39@0",
            "content": "Step 5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_39",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_40@0",
            "content": "Step 5 learnable parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_40",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_40@1",
            "content": "Note that the attention coefficient a ij is the normalized similarity between two nodes (Veli\u010dkovi\u0107 et al., 2017) of Ha in previous decoding step, and h-hop a",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_40",
            "start": 28,
            "end": 185,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_41@0",
            "content": "\ud835\udc34 \ud835\udc61 \ud835\udc34 \ud835\udc60",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_41",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_42@0",
            "content": "(h) ij is the corresponding element of h-th power of matrix [a ij ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_42",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_43@0",
            "content": "Figure 3 illustrates the detailed process of introduced DGCN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_43",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_43@1",
            "content": "The masking and attention mechanisms are iterated until the decoding process is terminated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_43",
            "start": 62,
            "end": 152,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_43@2",
            "content": "Then we average the attention coefficients a ij over all decoding steps, and normalize them to obtain the refined adjacency matrix \u00c3t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_43",
            "start": 154,
            "end": 288,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_43@3",
            "content": "Considering our initial intuition of the similarity for the syntactic structure at both ends, we calculate the final syntactic structure of the target sentence as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_43",
            "start": 290,
            "end": 460,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_44@0",
            "content": "A t = Sigmoid W s A s + \u00c3t + b s ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_44",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_45@0",
            "content": "where W s and b s are learnable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_45",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_46@0",
            "content": "Structural Attention for Word Alignment We adopt A s and the inferred A t to update the representation of language pairs, with the target-to-source attention in (Chen et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_46",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_46@1",
            "content": "The learned representation simultaneously contains the content and structure information of the context for accurate word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_46",
            "start": 182,
            "end": 313,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_46@2",
            "content": "Finally, we choose the source token with the maximum attention weight towards the current target token:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_46",
            "start": 315,
            "end": 417,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_47@0",
            "content": "\u03b1 = attention (A t \u2022 H d , A s \u2022 H e ) , \u03b3(t) = arg max i\u2208{1,...,M } \u03b1 t,i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_47",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_48@0",
            "content": "IMPORTANT Note that even during training, we only use the ground-truth syntactic graph of source side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_48",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_48@1",
            "content": "The syntactic graph of target side is inferred during training and its derived attention weights subsequently participate the loss calculation of word alignment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_48",
            "start": 103,
            "end": 268,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_49@0",
            "content": "Hierarchical Graph Random Walks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_49",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_50@0",
            "content": "In order to incorporate structural constraints for producing high-quality translation, we borrow the idea of (Nikolentzos and Vazirgiannis, 2020) to use a random walk kernel to capture the hierarchical representation of syntactic graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_50",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_50@1",
            "content": "The random walk kernel can quantify the similarity of two graphs based on the number of common walks, adopted to effectively capture structures of the input graphs when compared against a number of trainable \"hidden graphs\" 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_50",
            "start": 238,
            "end": 464,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_50@2",
            "content": "The adopted \"hidden graphs\" can learn the graph structures during training with backpropagation so that the translation outputs are highly interpretable, while the employed random walk kernel is differentiable and therefore the whole framework is end-to-end trainable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_50",
            "start": 466,
            "end": 733,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_50@3",
            "content": "The whole process is illustrated in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_50",
            "start": 735,
            "end": 779,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_51@0",
            "content": "1 Similar to the trainable \"kernel\" in convolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_51",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_52@0",
            "content": "In this work, we initialize two groups of trainable \"hidden graphs\" with differentiated scales, which compare the inputs using a random walk kernel to capture the structural representation of syntactic graphs both locally and globally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_52",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_52@1",
            "content": "Consider the syntactic graph (denoted as G d ) in the decoder and a \"hidden graph\" G h , their direct product G \u00d7 d is a graph over pairs of nodes from G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_52",
            "start": 236,
            "end": 400,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_52@2",
            "content": "We refer to the original paper (Nikolentzos and Vazirgiannis, 2020) for more details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_52",
            "start": 402,
            "end": 486,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_53@0",
            "content": "It has been shown that performing a random walk on the direct product G \u00d7 d is equivalent to performing a simultaneous random walk on the two graphs G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_53",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_53@1",
            "content": "We denote by A \u00d7 d the adjacency matrix of G \u00d7 d , and assume a uniform distribution for the starting and stopping probabilities over the nodes of G d and G h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_53",
            "start": 163,
            "end": 322,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_53@2",
            "content": "In this way, the random walk kernel will count all pairs of matching walks on G d and G h through the adjacency matrix A \u00d7 d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_53",
            "start": 324,
            "end": 449,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_53@3",
            "content": "We then perform the P -step (P \u2208 N) random walk kernel which calculates the number of common walks of length p between two graphs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_53",
            "start": 451,
            "end": 580,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_54@0",
            "content": "k (p) (G d , G h ) = |V \u00d7 d | i=1 |V \u00d7 d | j=1 A \u00d7(p) d ij",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_54",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_55@0",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_55",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_56@0",
            "content": "For each p \u2208 {0, 1, ..., P }, a different kernel value is calculated which can be thought of as the structural representation of graph G d .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_56",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_56@1",
            "content": "Therefore, given the two sets P = {0, 1, ..., P } and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_56",
            "start": 141,
            "end": 193,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_57@0",
            "content": "G h = G 1 h , G 2 h , ..., G K h where G 1 h , G 2 h , ..., G K h",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_57",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_58@0",
            "content": "denote the K \"hidden graphs\", we can compute one feature for each element of the Cartesian product P \u00d7 G h , and further build a matrix R \u2208 R K\u00d7(P +1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_58",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_59@0",
            "content": "Nat\u00fcrlich gibt es da einen Unterschied .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_59",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_60@0",
            "content": "Of course there is a difference .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_60",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_60@1",
            "content": "for",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_60",
            "start": 34,
            "end": 36,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_61@0",
            "content": "G d where R ij = k j G d , G i h .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_61",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_62@0",
            "content": "Finally, the matrix R is flattened as supplementary representation to incorporate structural constraints into the decoder outputs from Transformer for guiding translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_62",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_63@0",
            "content": "In order to capture both local and global structural information, we assign two differentiated scales (with node sizes 4 and 6) of \"hidden graphs\" to compare against the syntactic graphs at both ends.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_63",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_63@1",
            "content": "In the meantime, the syntactic information from both encoder and decoder are considered to access the robust and high-quality translation system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_63",
            "start": 201,
            "end": 345,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_63@2",
            "content": "We also provide case studies of the experiments in Figure 5, demonstrating the learned \"hidden graphs\" can capture both the local and global dependencies of target sentences, leading to more discriminative features which are further adopted to guide the translation outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_63",
            "start": 347,
            "end": 620,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_64@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_64",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_65@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_65",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_66@0",
            "content": "We conducted our experiments on four publicly available datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_66",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_66@1",
            "content": "For German-English (de-en) 2 , Romanian-English (ro-en) and French-English (fr-en) 3 , we followed the experimental setup in (Zenkel et al., 2020) and used the preprocessing scripts from (Zenkel et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_66",
            "start": 66,
            "end": 274,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_66@2",
            "content": "We also followed 2 https://www-i6.informatik.rwthaachen.de/goldAlignment/ 3 http://web.eecs.umich.edu/~mihalcea/wpt/index.html (Ding et al., 2019) to set the last 1K sentences of the training data before preprocessing as validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_66",
            "start": 276,
            "end": 511,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_66@3",
            "content": "The Chinese-English training set is from the NIST corpora while the test set is from the v1testset released by TsinghuaAligner (Liu and Sun, 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_66",
            "start": 513,
            "end": 659,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_66@4",
            "content": "We learned a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10K merge operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_66",
            "start": 661,
            "end": 772,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_67@0",
            "content": "Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_67",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@0",
            "content": "We adopted parsing tools 4 to construct syntactic graphs for the language of the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@1",
            "content": "Both the encoder and the decoder of Transformer have 4 layers of attentions with 4 attention heads each.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 90,
            "end": 193,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@2",
            "content": "The embedding size and hidden states are set to 512, while the feed-forward layer has 2,048 cells.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 195,
            "end": 292,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@3",
            "content": "The training token-level batch size is 36K.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 294,
            "end": 336,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@4",
            "content": "All models were trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from (Zenkel et al., 2019) 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 338,
            "end": 491,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@5",
            "content": "We aggregated the 1and 2-hop neighbor of each target token in proposed dynamic graph convolutions for alignment, and performed P = {0, 1}-steps random walk with beam size to 4 in the decoding process of translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 493,
            "end": 707,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_68@6",
            "content": "Alignment error rate (AER) (Och and Ney, 2000) and BLEU (Papineni et al., 2002) are used for measuring word alignment accuracy and translation quality, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_68",
            "start": 709,
            "end": 873,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_69@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_69",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@0",
            "content": "We compare our method with two statistical baselines FAST-ALIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@1",
            "content": "Besides, our proposal (structure-based) is compared to several neural baselines (content-based), and all the baselines induce alignments from attention weights of content-based representation: NAIVE-ATT (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), NAIVE-ATT-LA (Garg et al., 2019), SD-SMOOTHGRAD (Ding et al., 2019), ADDSGD (Zenkel et al., 2019), SHIFT-ATT (Chen et al., 2020), SHIFT-AET (Chen et al., 2020), BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 117,
            "end": 606,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@2",
            "content": "NAIVE-ATT (Garg et al., 2019) SD-SMOOTHGRAD (Ding et al., 2019) induces alignments from token saliency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 608,
            "end": 710,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@3",
            "content": "ADDSGD (Zenkel et al., 2019) explicitly adds an extra attention layer on top of Transformer to predict the to-be-aligned target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 712,
            "end": 845,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@4",
            "content": "SHIFT-ATT (Chen et al., 2020) induces alignments when the to-be-aligned target token is the decoder input instead of the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 847,
            "end": 974,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_70@5",
            "content": "SHIFT-AET (Chen et al., 2020) extracts alignments from an additional module with supervision from symmetrized SHIFT-ATT alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_70",
            "start": 976,
            "end": 1106,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_71@0",
            "content": "BTBA (Zhang and van Genabith, 2021) predicts the current target token by paying attention to the source context and both left-side and right-side target context to produce target-to-source alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_71",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_71@1",
            "content": "MASK-ALIGN (Chen et al., 2021) extracts alignments from introduced leaky attention and trains with the masked language model fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_71",
            "start": 200,
            "end": 332,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_72@0",
            "content": "Alignment Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_72",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_73@0",
            "content": "Comparison with Baselines Table 1 compares the alignment results of our method with all the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_73",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_73@1",
            "content": "Our approach significantly outperforms both statistical and neural baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_73",
            "start": 103,
            "end": 179,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_73@2",
            "content": "Specifically, it improves over GIZA++ by 2.0-7.2 AER points across different language pairs, demonstrating that building a neural aligner is better than statistical aligners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_73",
            "start": 181,
            "end": 354,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_73@3",
            "content": "When compared with neural baselines either using guided training or without guidance, we find our proposal still achieves substantial improvements over all methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_73",
            "start": 356,
            "end": 519,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_73@4",
            "content": "For instance, it improves over SHIFT-AET and MASK-ALIGN by 2.4 and 0.7 individually AER points on the Romanian-English pair, indicating that the incorporation of syntactic structure achieves superior alignment results compared to these that rely only on the content of inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_73",
            "start": 521,
            "end": 796,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@0",
            "content": "Besides, we also evaluate our proposal on Chinese-English pair and compare other methods in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@1",
            "content": "The experimental results are highly consistent with the observations on other language pairs, demonstrating the effectiveness of alignment based on modeling dependencies and capturing structural similarities for distant language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 101,
            "end": 335,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@2",
            "content": "Ablation Study Table 3 shows the ablation results on two language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 337,
            "end": 408,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@3",
            "content": "Our approach achieves a gain of 23.8 and 14.6 AER points with fewer parameters compared to vanilla Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 410,
            "end": 520,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@4",
            "content": "When considering the introduced Dynamic Graph Convolution Networks, the aggregated 1-hop neighbor can only capture the local structure, and thus the alignment accuracy is limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 522,
            "end": 700,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@5",
            "content": "In contrast, aggregating all the 1-, 2-, and 3-hop neighbor for each target node, while better capturing the global dependency, brings with it an increase of parameters and the possible introduction of noisy nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 702,
            "end": 915,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@6",
            "content": "We finally achieve the trade-off between performance and parameter size by aggregating both the 1and 2-hop neighbor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 917,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@7",
            "content": "Notably, the accuracy of alignment slightly decreases when we remove the translation task, showing the effectiveness of our multi-task learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 1034,
            "end": 1187,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_74@8",
            "content": "Case Study Figure 6(a) shows the attention weights from three different models for a symmetrized alignment example from de-en test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_74",
            "start": 1189,
            "end": 1323,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_75@0",
            "content": "In this example, SHIFT-ATT puts high weights wrongly on \"1968\" when predicting the target token \"tokyo\", while MASK-ALIGN fails to resolve ambiguity when predicting the target token \"in\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_75",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_76@0",
            "content": "In contrast, our approach produces the attention weights based on structural matching of source and target tokens, which are highly consistent with the gold alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_76",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_76@1",
            "content": "Furthermore, we visualize the complete syntactic structure inferred by introduced DGCN in Figure 6(b), which could explicitly reflect the dependencies between each target token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_76",
            "start": 168,
            "end": 344,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_77@0",
            "content": "Translation Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_77",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_78@0",
            "content": "Comparison with Baselines Table 4 shows the comparison of translation quality and the corresponding decoding speed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_78",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_78@1",
            "content": "Although this work has improved the performance of word alignment, our experiments show that the benefits from the representation of syntactic structure also extend to the translation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_78",
            "start": 116,
            "end": 304,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_78@2",
            "content": "Compared with (Marcheggiani et al., 2018) that only utilize syntactic structure at the encoder side, we substantially improve the performance by incorporating syntactic structure at the decoder side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_78",
            "start": 306,
            "end": 504,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@0",
            "content": "Ablation Study To investigate the effectiveness of introduced Hierarchical Graph Random Walks, we further conducted ablation experiments from two perspectives: the number of steps for random walk and the beam size for decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@1",
            "content": "Table 4 shows the comparison results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 228,
            "end": 264,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@2",
            "content": "It can be inferred that increasing the step length (e.g., p = 2) can improve the capability of \"hidden graphs\" to better capture the global structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 266,
            "end": 415,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@3",
            "content": "However, continuing to increase the step (e.g., p = 3) length will not always improve the performance, since it not only introduces more parameters, but also is likely to confuse the model by the complicated closed-loop structure which is prevalent in the graph network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 417,
            "end": 686,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@4",
            "content": "Moreover, increasing the beam size does not bring sustainable gains, but it inevitably decreases the speed of decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 688,
            "end": 806,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_79@5",
            "content": "Notably, the quality of translation significantly decreases when we remove the alignment branch, suggesting that the internal logic of both tasks are tightly correlated by exploiting the dependencies between language pairs for multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_79",
            "start": 808,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_80@0",
            "content": "5 Related Works Table 4: Comparison of BLEU scores and the averaged decoding speed tested on test sets of three language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_80",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_80@1",
            "content": "p refers that a p-step random walk is performed during the decoding process, while beam is the beam size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_80",
            "start": 128,
            "end": 232,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_81@0",
            "content": "2013) and GIZA++ (Och and Ney, 2003), a lot of latest works Garg et al., 2019;Zenkel et al., 2019Zenkel et al., , 2020 have made significant progress by inducing unsupervised neural aligners from NMT to produce better word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_81",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_81@1",
            "content": "Significantly, BTBA (Zhang and van Genabith, 2021) and MASK-ALIGN (Chen et al., 2021) leverage the both side content information of the decoder, sacrificing the ability of translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_81",
            "start": 235,
            "end": 418,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@0",
            "content": "Our work is also related to syntax-based or Transformer based neural machine translation models which have shown large advantages on a myriad of datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@1",
            "content": "(Bastings et al., 2017) incorporated syntactic structure into the encoder of NMT model and proposed syntactic GCNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 155,
            "end": 269,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@2",
            "content": "(Marcheggiani et al., 2018) refined the above work to inject a semantic bias into sentence encoders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 271,
            "end": 370,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@3",
            "content": "Transformer based NMT models (Vaswani et al., 2017;Hasler et al., 2018) attribute their superior performance to the multi-layer and multi-head self-attention architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 372,
            "end": 542,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@4",
            "content": "(Garg et al., 2019) trained the Transformer to jointly learn word alignment and translation through multi-task learning based on existing token aligners such as GIZA++ (Och and Ney, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 544,
            "end": 731,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@5",
            "content": "Our work differs from prior studies in that we simultaneously incorporate the syntactic structure into both encoder and decoder to tightly correlate the internal logic of word alignment and machine translation for multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 733,
            "end": 966,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_82@6",
            "content": "To the best of our knowledge, this is the first work that incorporates syntactic structure based constraints into the decoder of NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_82",
            "start": 968,
            "end": 1107,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_83@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_83",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_84@0",
            "content": "We propose a multi-task learning framework that tightly correlates the internal logic of word alignment and machine translation, by fully exploits the syntactic structure of both source and target tokens and the similarity of dependencies at both ends.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_84",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_84@1",
            "content": "Experiments show that our proposal achieves the new State-of-the-Art results among all neural methods in word alignment, while producing high-quality translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_84",
            "start": 253,
            "end": 415,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_84@2",
            "content": "We leave it for future work to extend our study to more downstream tasks and systems in natural language processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_84",
            "start": 417,
            "end": 532,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_85@0",
            "content": "A Detailed Network Architecture of f M For each observed token from the target side, we learn a soft mask M to predict its dependency with other observed tokens by a light-weight network:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_85",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_86@0",
            "content": "Philip Arthur, Graham Neubig, Satoshi Nakamura, Incorporating discrete translation lexicons into neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_86",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_87@0",
            "content": "UNKNOWN, None, 2015, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_87",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_88@0",
            "content": "Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima'an, Graph convolutional encoders for syntax-aware neural machine translation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_88",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_89@0",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_89",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_90@0",
            "content": "Chi Chen, Maosong Sun, Yang Liu, Maskalign: Self-supervised neural word alignment, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_90",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_91@0",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_91",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_92@0",
            "content": "Shuoyang Ding, Hainan Xu, Philipp Koehn, Saliency-driven word alignment interpretation for neural machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_92",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_93@0",
            "content": "Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan, Training neural machine translation to apply terminology constraints, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_93",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_94@0",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_94",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_95@0",
            "content": "Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik, Jointly learning to align and translate with transformer models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_95",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_96@0",
            "content": "Eva Hasler, Adri\u00e0 De Gispert, Gonzalo Iglesias, Bill Byrne, Neural machine translation decoding with terminology constraints, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_96",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_97@0",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_97",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_98@0",
            "content": "Philipp Koehn, Amittai Axelrod, Alexandra Mayne, Chris Callison-Burch, Miles Osborne, David Talbot, Edinburgh system description for the 2005 IWSLT speech translation evaluation, 2005, Proceedings of the Second International Workshop on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_98",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_99@0",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_99",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_100@0",
            "content": "Yang Liu, Maosong Sun, Contrastive unsupervised word alignment with non-local features, 2015, Twenty-Ninth AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_100",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_101@0",
            "content": "Diego Marcheggiani, Jasmijn Bastings, Ivan Titov, Exploiting semantics in neural machine translation with graph convolutional networks, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_101",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_102@0",
            "content": ", Giannis Nikolentzos and Michalis Vazirgiannis, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_102",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_103@0",
            "content": "Josef Franz, Hermann Och,  Ney, Improved statistical alignment models, 2000, Proceedings of the 38th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_103",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_104@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Hong Kong. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_104",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_105@0",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_105",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_106@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_106",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_107@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_107",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_108@0",
            "content": "Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, Min Zhang, Code-switching for enhancing NMT with pre-specified translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_108",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_109@0",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_109",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_110@0",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_110",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_111@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_111",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_112",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_113@0",
            "content": "UNKNOWN, None, 2019, Correct-andmemorize: Learning to translate from interactive revisions, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_113",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2019, Adding interpretable attention to neural translation models improves word alignment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_114",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_115@0",
            "content": "Thomas Zenkel, Joern Wuebker, John Denero, End-to-end neural word alignment outperforms GIZA++, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_115",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_116@0",
            "content": "Jingyi Zhang, Josef Van Genabith, A bidirectional transformer based alignment model for unsupervised word alignment, 2021, Proceedings of the 59th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_116",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "282-ARR_v2_117@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "282-ARR_v2_117",
            "start": 0,
            "end": 206,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_1",
            "tgt_ix": "282-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_1",
            "tgt_ix": "282-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_5",
            "tgt_ix": "282-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_6",
            "tgt_ix": "282-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_9",
            "tgt_ix": "282-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_8",
            "tgt_ix": "282-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_10",
            "tgt_ix": "282-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_11",
            "tgt_ix": "282-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_11",
            "tgt_ix": "282-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_13",
            "tgt_ix": "282-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_14",
            "tgt_ix": "282-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_12",
            "tgt_ix": "282-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_12",
            "tgt_ix": "282-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_12",
            "tgt_ix": "282-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_12",
            "tgt_ix": "282-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_11",
            "tgt_ix": "282-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_15",
            "tgt_ix": "282-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_16",
            "tgt_ix": "282-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_16",
            "tgt_ix": "282-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_17",
            "tgt_ix": "282-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_21",
            "tgt_ix": "282-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_22",
            "tgt_ix": "282-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_23",
            "tgt_ix": "282-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_24",
            "tgt_ix": "282-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_25",
            "tgt_ix": "282-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_26",
            "tgt_ix": "282-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_27",
            "tgt_ix": "282-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_29",
            "tgt_ix": "282-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_30",
            "tgt_ix": "282-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_31",
            "tgt_ix": "282-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_32",
            "tgt_ix": "282-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_33",
            "tgt_ix": "282-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_34",
            "tgt_ix": "282-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_35",
            "tgt_ix": "282-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_36",
            "tgt_ix": "282-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_37",
            "tgt_ix": "282-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_38",
            "tgt_ix": "282-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_40",
            "tgt_ix": "282-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_41",
            "tgt_ix": "282-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_42",
            "tgt_ix": "282-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_43",
            "tgt_ix": "282-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_44",
            "tgt_ix": "282-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_45",
            "tgt_ix": "282-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_46",
            "tgt_ix": "282-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_47",
            "tgt_ix": "282-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_39",
            "tgt_ix": "282-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_48",
            "tgt_ix": "282-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_50",
            "tgt_ix": "282-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_51",
            "tgt_ix": "282-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_52",
            "tgt_ix": "282-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_53",
            "tgt_ix": "282-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_54",
            "tgt_ix": "282-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_55",
            "tgt_ix": "282-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_56",
            "tgt_ix": "282-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_57",
            "tgt_ix": "282-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_58",
            "tgt_ix": "282-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_59",
            "tgt_ix": "282-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_61",
            "tgt_ix": "282-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_62",
            "tgt_ix": "282-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_60",
            "tgt_ix": "282-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_63",
            "tgt_ix": "282-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_65",
            "tgt_ix": "282-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_65",
            "tgt_ix": "282-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_67",
            "tgt_ix": "282-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_67",
            "tgt_ix": "282-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_69",
            "tgt_ix": "282-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_69",
            "tgt_ix": "282-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_69",
            "tgt_ix": "282-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_71",
            "tgt_ix": "282-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_75",
            "tgt_ix": "282-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_76",
            "tgt_ix": "282-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_78",
            "tgt_ix": "282-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_80",
            "tgt_ix": "282-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_81",
            "tgt_ix": "282-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_84",
            "tgt_ix": "282-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_83",
            "tgt_ix": "282-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_83",
            "tgt_ix": "282-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_83",
            "tgt_ix": "282-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "282-ARR_v2_0",
            "tgt_ix": "282-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_1",
            "tgt_ix": "282-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_2",
            "tgt_ix": "282-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_3",
            "tgt_ix": "282-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_4",
            "tgt_ix": "282-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_5",
            "tgt_ix": "282-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_5",
            "tgt_ix": "282-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_5",
            "tgt_ix": "282-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_6",
            "tgt_ix": "282-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_6",
            "tgt_ix": "282-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_7",
            "tgt_ix": "282-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_8",
            "tgt_ix": "282-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_8",
            "tgt_ix": "282-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_8",
            "tgt_ix": "282-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_8",
            "tgt_ix": "282-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_9",
            "tgt_ix": "282-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_9",
            "tgt_ix": "282-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_10",
            "tgt_ix": "282-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_11",
            "tgt_ix": "282-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_12",
            "tgt_ix": "282-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_13",
            "tgt_ix": "282-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_14",
            "tgt_ix": "282-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_15",
            "tgt_ix": "282-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_15",
            "tgt_ix": "282-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_15",
            "tgt_ix": "282-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_15",
            "tgt_ix": "282-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_16",
            "tgt_ix": "282-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_17",
            "tgt_ix": "282-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_17",
            "tgt_ix": "282-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_17",
            "tgt_ix": "282-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_17",
            "tgt_ix": "282-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_18",
            "tgt_ix": "282-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_19",
            "tgt_ix": "282-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_20",
            "tgt_ix": "282-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_21",
            "tgt_ix": "282-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_21",
            "tgt_ix": "282-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_22",
            "tgt_ix": "282-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_23",
            "tgt_ix": "282-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_23",
            "tgt_ix": "282-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_23",
            "tgt_ix": "282-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_23",
            "tgt_ix": "282-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_24",
            "tgt_ix": "282-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_25",
            "tgt_ix": "282-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_25",
            "tgt_ix": "282-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_26",
            "tgt_ix": "282-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_27",
            "tgt_ix": "282-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_27",
            "tgt_ix": "282-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_28",
            "tgt_ix": "282-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_29",
            "tgt_ix": "282-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_29",
            "tgt_ix": "282-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_29",
            "tgt_ix": "282-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_29",
            "tgt_ix": "282-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_30",
            "tgt_ix": "282-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_30",
            "tgt_ix": "282-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_30",
            "tgt_ix": "282-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_30",
            "tgt_ix": "282-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_31",
            "tgt_ix": "282-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_32",
            "tgt_ix": "282-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_32",
            "tgt_ix": "282-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_32",
            "tgt_ix": "282-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_32",
            "tgt_ix": "282-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_33",
            "tgt_ix": "282-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_34",
            "tgt_ix": "282-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_34",
            "tgt_ix": "282-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_35",
            "tgt_ix": "282-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_35",
            "tgt_ix": "282-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_36",
            "tgt_ix": "282-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_37",
            "tgt_ix": "282-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_38",
            "tgt_ix": "282-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_39",
            "tgt_ix": "282-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_40",
            "tgt_ix": "282-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_40",
            "tgt_ix": "282-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_41",
            "tgt_ix": "282-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_42",
            "tgt_ix": "282-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_43",
            "tgt_ix": "282-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_43",
            "tgt_ix": "282-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_43",
            "tgt_ix": "282-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_43",
            "tgt_ix": "282-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_44",
            "tgt_ix": "282-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_45",
            "tgt_ix": "282-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_46",
            "tgt_ix": "282-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_46",
            "tgt_ix": "282-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_46",
            "tgt_ix": "282-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_47",
            "tgt_ix": "282-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_48",
            "tgt_ix": "282-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_48",
            "tgt_ix": "282-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_49",
            "tgt_ix": "282-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_50",
            "tgt_ix": "282-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_50",
            "tgt_ix": "282-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_50",
            "tgt_ix": "282-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_50",
            "tgt_ix": "282-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_51",
            "tgt_ix": "282-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_52",
            "tgt_ix": "282-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_52",
            "tgt_ix": "282-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_52",
            "tgt_ix": "282-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_53",
            "tgt_ix": "282-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_53",
            "tgt_ix": "282-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_53",
            "tgt_ix": "282-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_53",
            "tgt_ix": "282-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_54",
            "tgt_ix": "282-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_55",
            "tgt_ix": "282-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_56",
            "tgt_ix": "282-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_56",
            "tgt_ix": "282-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_57",
            "tgt_ix": "282-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_58",
            "tgt_ix": "282-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_59",
            "tgt_ix": "282-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_60",
            "tgt_ix": "282-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_60",
            "tgt_ix": "282-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_61",
            "tgt_ix": "282-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_62",
            "tgt_ix": "282-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_63",
            "tgt_ix": "282-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_63",
            "tgt_ix": "282-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_63",
            "tgt_ix": "282-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_64",
            "tgt_ix": "282-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_65",
            "tgt_ix": "282-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_66",
            "tgt_ix": "282-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_67",
            "tgt_ix": "282-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_68",
            "tgt_ix": "282-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_69",
            "tgt_ix": "282-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_70",
            "tgt_ix": "282-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_71",
            "tgt_ix": "282-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_71",
            "tgt_ix": "282-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_72",
            "tgt_ix": "282-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_73",
            "tgt_ix": "282-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_74",
            "tgt_ix": "282-ARR_v2_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_75",
            "tgt_ix": "282-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_76",
            "tgt_ix": "282-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_76",
            "tgt_ix": "282-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_77",
            "tgt_ix": "282-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_78",
            "tgt_ix": "282-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_78",
            "tgt_ix": "282-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_78",
            "tgt_ix": "282-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_79",
            "tgt_ix": "282-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_80",
            "tgt_ix": "282-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_80",
            "tgt_ix": "282-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_81",
            "tgt_ix": "282-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_81",
            "tgt_ix": "282-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_82",
            "tgt_ix": "282-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_83",
            "tgt_ix": "282-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_84",
            "tgt_ix": "282-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_84",
            "tgt_ix": "282-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_84",
            "tgt_ix": "282-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_85",
            "tgt_ix": "282-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_86",
            "tgt_ix": "282-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_87",
            "tgt_ix": "282-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_88",
            "tgt_ix": "282-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_89",
            "tgt_ix": "282-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_90",
            "tgt_ix": "282-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_91",
            "tgt_ix": "282-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_92",
            "tgt_ix": "282-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_93",
            "tgt_ix": "282-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_94",
            "tgt_ix": "282-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_95",
            "tgt_ix": "282-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_96",
            "tgt_ix": "282-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_97",
            "tgt_ix": "282-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_98",
            "tgt_ix": "282-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_99",
            "tgt_ix": "282-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_100",
            "tgt_ix": "282-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_101",
            "tgt_ix": "282-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_102",
            "tgt_ix": "282-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_103",
            "tgt_ix": "282-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_104",
            "tgt_ix": "282-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_105",
            "tgt_ix": "282-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_106",
            "tgt_ix": "282-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_107",
            "tgt_ix": "282-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_108",
            "tgt_ix": "282-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_109",
            "tgt_ix": "282-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_110",
            "tgt_ix": "282-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_111",
            "tgt_ix": "282-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_112",
            "tgt_ix": "282-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_113",
            "tgt_ix": "282-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_114",
            "tgt_ix": "282-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_115",
            "tgt_ix": "282-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_116",
            "tgt_ix": "282-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "282-ARR_v2_117",
            "tgt_ix": "282-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 773,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "282-ARR",
        "version": 2
    }
}