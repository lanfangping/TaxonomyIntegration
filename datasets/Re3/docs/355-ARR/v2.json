{
    "nodes": [
        {
            "ix": "355-ARR_v2_0",
            "content": "Weakly Supervised Word Segmentation for Computational Language Documentation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_2",
            "content": "Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data. This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation. Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality. In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner. This work opens the way for interactive annotation tools for documentary linguists.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "355-ARR_v2_4",
            "content": "Recent years have witnessed a blooming of research aimed at applying language technologies (LTs) to \"under-resourced languages\". 1 Such studies have been mostly motivated on three main grounds (not necessarily mutually exclusive): (a) to develop tools that could speed up the work of field linguists collecting and annotating recordings for these languages; (b) to provide linguistic communities with LTs that are necessary in an increasingly digitalised world, e.g. to interact with smartphones or computers in their own language and communicate with speakers of other languages; (c) to challenge existing machine-learning techniques in very low resource settings, where hardly any resource (dictionary, corpus, grammar) is available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_5",
            "content": "Those objectives are thoroughly discussed in a recent position paper (Bird, 2020) who notices, among other things, that objective (c) (training language processing tools with zero resource) is questionable in the context of language documentation works which can often rely on some pre-existing knowledge, such as a word list, or information from related languages. Accordingly, this paper explores ways to make the best of prior resources and improve the effectiveness of unsupervised language analysis techniques for the purpose of linguistic documentation. Our main objective is to develop tools that will effectively assist field linguists in their documentary tasks (objective (a)). We focus on segmentation tasks, which aim to automatically identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008;Doyle and Levy, 2013;Eskander et al., 2016;Godard et al., 2018b;Eskander et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_6",
            "content": "Following these authors, we experiment with Bayesian non-parametric segmentation models, derived in our case from and subsequent work, which we recap in Section 2. Our first contribution is in Section 3 which studies multiple semi-supervised learning regimes aimed to take advantage of pre-existing linguistic material such as incomplete segmentations and word lists.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_7",
            "content": "In Sections 4 and 5, we experimentally assess the pros and cons of these weakly supervised approaches in batch and online learning, for two extremely low-resource languages currently in the process of being documented: Mboshi, a Bantu language used in former studies (Godard et al., 2018a); and Japhug, a language from the Sino-Tibetan family spoken in the Western part of China thoroughly documented by Jacques (2021). These two languages were selected because they illustrate actual documentation processes, for which high-quality linguistic resources have been derived from fieldwork, at the end of a long and difficult procedure (Aiton, 2021). A complementary analysis follows, where we use the Japhug corpus to take a closer look at the units identified automatically, contrasting morpheme-based and word-based supervision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_8",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "355-ARR_v2_9",
            "content": "Going from audio recordings to fully annotated transcripts implies two successive segmentation steps: the first segments words and happens during the production of phonemic or orthographic transcripts; the second further splits words into morphs, which are then annotated with syntactic information and glosses. We mostly focus on the former task, assuming a two-step process: first, the computation of a phonemic transcript that we assume is given; then the segmentation into words for which we consider two settings: batch and online learning. The word and morpheme segmentation tasks are closely related and rely on similar tools: using the Japhug corpus, which contains both levels of segmentations, we also study the implications of using lists of words vs morphemes as weak supervision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_10",
            "content": "In its baseline form, the word segmentation process is fully unsupervised, and the only training material is a set of transcribed sentences (see Fig. 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_11",
            "content": "We rely on Bayesian non-parametric approaches to word segmentation (see (Cohen, 2016) for a thorough exposition), and our baselines are the unigram version of the dpseg model and a variant where the underlying Dirichlet Process is replaced by a Pitman-Yor Process as in (Neubig, 2014). We selected unigram models for their simplicity, which (a) makes them amenable to the processing of very small sets of sentences; (b) makes the online learning setting tractable. While using higher-order models or more sophisticated models of the same family (Teh, 2006b;Mochihashi et al., 2009) may improve the performance (see for an experimental comparison), we believe that in our low-resource conditions, these variations would be small 2 and would not change our main conclusions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_12",
            "content": "Word segmentation models fundamentally rely on probabilistic models for word sequences defining P (w = w 1 . . . w T ); word sequences can also be viewed as segmented sequences of characters y = y 1 . . . y L , so that the same model can be used for the joint probability of (y, b), with b = b 1 . . . b L representing the vector of boundary locations where value b t = 1 (resp. b t = 0) denotes a boundary (resp. no boundary) after symbol y t . In an unsupervised setting, these boundaries are hidden and are latent variables in the model. Such models lend themselves well to Gibbs sampling, which repeatedly produces samples of each boundary given all the other boundaries in the corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_13",
            "content": "In dpseg, the underlying sequence model is a unigram model: P (w 1 . . . w T ) = T t=1 P (w t ). The probability of individual words corresponds to a Dirichlet Process with parameters \u03b1, the concentration parameter, and P 0 , the base distribution, and yields the following formulation for the conditional probability of w t given the past words w <t :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_14",
            "content": "P (w t = w|w <t ) = n w (w <t ) + \u03b1P 0 (w) t + \u03b1 \u2212 1 ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_15",
            "content": "where n w (w <t ) counts the number of times w has occurred in the past. With lower values of \u03b1, the most frequent words tend to be generated more (hence, concentration), while with higher values, the words are more smoothly distributed. P 0 , the base distribution, assigns scores to arbitrary character strings; use a length model and a uniform character model. For word w made of characters y 1 , ..., y m , P 0 is computed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_16",
            "content": "P 0 (w) = p # (1 \u2212 p # ) m\u22121 length model m j=1 P (y j ) character model (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_17",
            "content": "where p # is the probability to end the word. For this model, Gibbs sampling compares at each position t two sequences of words w t=0 (no boundary at position t) and w t=1 (a boundary is inserted). As these sequences only differ minimally, terms such as P (b t = 0|y, b \u2212t ) are readily derived (see e.g. ). Gibbs sampling is performed for a number of iterations that are sufficient to reach convergence, and we use the last iteration to uncover the resulting segmentation. To speed up mixing, also use annealing, so that a larger search space is explored.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_18",
            "content": "An extension of dpseg, denoted pypseg, uses a Pitman-Yor Process (PYP) instead of the Dirichlet Process and generalises equation (1) with an additional discount parameter, which enables to better control the generation of new words. PYPs are introduced in (Teh, 2006b;Mochihashi et al., 2009); a fast implementation is in (Neubig, 2014). For our experiments, both models have y = b 1 \u00e1 2 a 3 \u00e1 4 m 5 i 6 k 7 \u00fa 8 n 9 d 10 \u00e1 11 p 12 o 13 o 14 y 15 \u00e1 16 k 17 a 18 l 19 a 20 b = 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 w 6=0 b\u00e1a \u00e1mik\u00fand\u00e1 poo y\u00e1 kala b = 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 w 6=1 b\u00e1a \u00e1mi k\u00fand\u00e1 poo y\u00e1 kala",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_19",
            "content": "Figure 1: The sentence segmentation task illustrated with a sentence from the Mboshi corpus: 'b\u00e1a \u00e1mik\u00fand\u00e1 poo y\u00e1 kala' ('they found the old village'). The two possible segmentations only differ in one boundary at position t = 6, one (w 6=0 ) where '\u00e1mik\u00fand\u00e1' is one single unit and one (w 6=1 ) where it is split in two.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_20",
            "content": "been re-implemented in Python. This implementation is available at https://github.com/ shuokabe/pyseg.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_21",
            "content": "Supervising word segmentation",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "355-ARR_v2_22",
            "content": "In this section, we discuss realistic sources of weak supervision for segmentation tasks and how they can be included in Bayesian models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_23",
            "content": "Finding supervision information",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "355-ARR_v2_24",
            "content": "Segmentation boundaries Segmentation data, corresponding to the location of boundary (and non-boundary) information, can be obtained in different ways. For instance, when audio recordings are available, prosodic cues such as short silences or specific intonative patterns can serve to identify plausible locations for word endings. Longer pauses generally denote the end of an utterance, which we assume are already given. This would yield a sparse partial annotation, where supervision data is randomly scattered across the corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_25",
            "content": "Another realistic situation where we have access to a partial annotation is when a small subset is already segmented. In this case, the partial annotation is dense and concentrated in a few sentences, a semi-supervised setting also studied in (Sirts and Goldwater, 2013). We thus consider two questions: (a) which is more effective between dense and sparse annotations? (b) how effective is supervision in an incremental learning regime, where automatic (dense) annotations are progressively corrected and used to update the model? Word lists Word lists constitute another valuable and common source of information. They may contain morphs, morphemes, lexemes or fully inflected forms, with various levels of information (part-ofspeech, gloss, translation, etc.). In this study, we consider that lists of surface forms are available and evaluate their usefulness, depending on their size and on the way they were collected. A related question is about the relative interest of word and morph lists, which we study in Section 5.3. The use of more sophisticated forms of lexical information regarding word structure, PoS, is out of the scope of this paper and is left for future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_26",
            "content": "Having a collection of fully segmented utterances, as discussed above, is another way to generate word lists. So these two sources of information must be viewed as complementary ways to supervise the task at hand: boundary marks at the token level, word list at the type level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_27",
            "content": "Forms of Weak Supervision",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "355-ARR_v2_28",
            "content": "Segmentation boundaries Observed segmentation boundaries can be used to facilitate the training process. Two experimental conditions, both affecting the Gibbs sampler (gs), have been considered:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_29",
            "content": "\u2022 gs.sparse: a fraction (\u03bb%) of the actual boundaries are observed, which corresponds to a sparse annotation scenario. \u2022 gs.dense: for \u03bb% of sentences, all boundary and non-boundary variables are given.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_30",
            "content": "In both cases, we modify the sampling process and make sure that the value of observed variables is not sampled, as in (Sirts and Goldwater, 2013).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_31",
            "content": "Using a word list Assuming now that a word list D is available, we consider the following approaches to reinforce the likelihood of units in D in the output segmentation:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_32",
            "content": "\u2022 d.count: D is used to initialise the 'internal' model dictionary, and words in D are created with a fixed pseudo-count of value \u03bb. Formally, \u2200w \u2208 D, the counting function n w () of Equation (1) will add \u03bb to their actual count. \u2022 d.mix: D is combined with the base distribution, resulting in the following mixture P 0 :",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_33",
            "content": "P 0 (w) = \u03bb |D| 1 {w\u2208D} + (1 \u2212 \u03bb)P 0 (w),(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_34",
            "content": "where \u03bb \u2208 [0, 1], |D| is the size of D, and 1 {w\u2208D} is the indicator function testing membership in D. As for d.count, P 0 increases the probability of words in D, but in a looser way, due to the term \u03b1P 0 in Equation (1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_35",
            "content": "\u2022 d.ngram: the baseline dpseg version uses a uniform character model for P 0 (Equation ( 2)); here, we use D to train a character n-gram language model (LM), with n = 2 and add-k smoothing in our experiments. \u2022 d.mix+ngram: this method combines d.mix and d.ngram: P 0 is replaced with the mixture P 0 of Equation ( 3) and the character model is an n-gram LM. This can be viewed as a proxy to the complete nested Dirichlet Process of Mochihashi et al. ( 2009), with D implementing a cache mechanism for known words.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_36",
            "content": "We have also used weaker forms of supervision aimed at learning a better length model, with hardly any improvement with respect to the baseline; these results are not reported below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_37",
            "content": "Incremental training",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "355-ARR_v2_38",
            "content": "In addition to the static use of supervision information described above, we also considered a more dynamic training regime, where dense annotations are provided in a sequential manner through interaction with an expert linguist, enabling incremental learning. To measure the effectiveness of this approach, we contrast three scenarios in Section 5.2:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_39",
            "content": "\u2022 the baseline is the post-edition of a fully unsupervised model without further training; \u2022 the post-edition of a fully unsupervised model, with additional Gibbs sampling iterations every batch utterances for iter iterations. This aims at propagating forward the supervision information obtained from past annotations. This method is referred to as o.regular. \u2022 on top of this, we also used the past annotated sentences to reestimate the base distribution of the underlying process as in d.ngram. The corresponding results are labelled o.2level in Figure 2.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_40",
            "content": "4 Experimental settings",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_41",
            "content": "Linguistic material",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "355-ARR_v2_42",
            "content": "Two languages have been considered in this paper: Mboshi and Japhug.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_43",
            "content": "Mboshi is a tonal Bantu language spoken in the Republic of Congo (Bantu C25). The data has been collected as part of the BULB project . It has seven vowels and 25 consonant phonemes with five prenasalised consonants (made of two to three consonants), a common feature in Bantu languages (Embanga Aborobongui, 2013;Kouarata, 2014). Although the language is usually not written, linguists have transcribed it with graphemes in a way that approximates the phonetic content. To mark the distinction between long and short vowels, they were either duplicated (VV) or not (V). One challenge for Mboshi word segmentation is its complex phonological rules, notably, vowel elision patterns whereby a vowel disappears before another one (also a common Bantu feature) (Rialland et al., 2015). This kind of phenomenon makes it harder to find the boundaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_44",
            "content": "From a morphological point of view, words are composed of roots and affixes. Another characteristic Bantu feature is its deletion rule for class-prefix consonants in nouns. Templates for verb structure are also quite rigid, with affixes following a strict ordering (Godard et al., 2018a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_45",
            "content": "Our corpus is a manual alphabetic transcription of audio recordings. 3 It contains 5,312 sentences segmented in words, one sentence per line.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_46",
            "content": "Japhug is a Sino-Tibetan language from the Gyalrong family spoken in the Sichuan province in China. Japhug has eight vowels and 50 consonant phonemes, which can combine to create a large number (more than 400) of consonant clusters. The rich cluster feature is one important characteristic of Japhug, which actually has one of the largest inventory of consonant clusters in the Trans-Himalayan language family. The structure of these clusters can be analysed by looking at patterns of partial reduplication of syllable initial consonants. There are no tones in this language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_47",
            "content": "Japhug also has a rich morphology, both for verbs and nouns. Remarkably, in verb forms, up to six or seven prefixes can be chained to express features such as tense, aspect, modality, while suffixation is used to express inflectional phenomena.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_48",
            "content": "Even though these processes are quite regular, they contribute to generating a large number of possible word forms. Recordings, annotated corpora, and dictionaries for Japhug are available from the Pangloss collection. 4 An extensive description of the language is given in (Jacques, 2021). 5 Our training material has been extracted from the L A T E X source files of this book, by collecting all Japhug examples. These can easily be retrieved by searching the \\gll command introducing Japhug sentences. Not only are the resulting sentences well-curated, but they are also segmented at two levels: words and morphemes. This will lead to a specific experiment presented in Section 5. For the latter, we use the word-based and morphemebased segmentations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_49",
            "content": "Table 1 displays the general statistics for the two languages. N utt , N type , and N token represent the number of utterances, of word types, and of word tokens, respectively. WL represents the average token length, while TL is the average type length. The sentences used for semi-supervision correspond to the first 200 sentences of each dataset, which is a realistic amount of data. Likewise, lexical supervision corresponds to the list of words observed in the same 200 sentences, and respectively contain 517 words for Mboshi, 664 words and 493 morphemes for Japhug.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_50",
            "content": "Model settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "355-ARR_v2_51",
            "content": "In our experimental setting, we made sure to also resample the hyperparameter(s) after each iteration, following mostly (Teh, 2006a;Mochihashi et al., 2009): the concentration parameter \u03b1 has a Gamma posterior distribution, and the discount parameter d a Beta distribution. The initial values of the hyperparameters were set as in Goldwater et al.'s work on the unigram dpseg: concentration parameter: \u03b1 = 20, p # = 0.5, discount parameter for pypseg: d = 0.5. The Gibbs sampler always runs for 20,000 iterations and simulated annealing is implemented as in with 10 increments of temperature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_52",
            "content": "All the results are obtained by collecting the predicted boundaries at the end of the last sampling iteration of one single run.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_53",
            "content": "Evaluation metrics",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "355-ARR_v2_54",
            "content": "Following , evaluation relies on 'PRF' metrics: precision, recall, and F-score, defined as follows: precision P = T P T P +F P , recall R = T P T P +F N , and F-score F = 2 * precision * recall precision+recall , where TP are the true positives (match in the reference and segmented texts), FP are the false positives, and FN are the false negatives. These metrics are computed at three levels: 6",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_55",
            "content": "\u2022 boundary level (BP, BR, BF): compare the reference boundary vectors with the predictions; \u2022 token level (WP, WR, WF): compare word in the reference and segmented sentences: a correct match requires two correct boundaries; \u2022 type level (LP, LR, LF): compare the set of unique words in the reference and segmented utterances.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_56",
            "content": "To have an overall view of the output text, we also report the average type and token lengths (TL and WL) as well as their counts (N type and N token ), as in Table 1. Numbers are computed on the entire text (including the supervised part).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_57",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "355-ARR_v2_58",
            "content": "This section presents the results for the models presented above. We also report the performance of SentencePiece, another word segmentation tool based on a unigram language model (Kudo, 2018): 7 To boost this baseline, the vocabulary size has been set to the reference number of N type (cf. Table 1). Supplementary material additionally contains results for Morfessor baselines (Creutz and Lagus, 2002), with the corresponding weak supervision. As a reminder, our supervision here consists of the first 200 sentences in the text, either directly given as observed boundaries or used to generate the initial word list.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_59",
            "content": "Using weak supervision",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "355-ARR_v2_60",
            "content": "dpseg",
            "ntype": "title",
            "meta": {
                "section": "5.1.1"
            }
        },
        {
            "ix": "355-ARR_v2_61",
            "content": "Table 2 displays our experimental results for the 5K Mboshi corpus for SentencePiece (SP), dpseg and pypseg with various amounts of supervision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_62",
            "content": "First, the unsupervised dpseg model has better results than SP on all three levels by a significant margin. SP, on the other hand, produces more types as it 'knows' the actual number of types to generate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_63",
            "content": "Regarding segmentation boundaries, the gs.sparse model has disappointing results, with scores lower than the baseline. On the other hand, the dense supervision manages to improve the baseline scores by around 2.5 points for BF, 4.5 points for WF, and 7.5 points for LF. This is an encouraging result, since, with less than 5% of the whole text, the model has improved in a noticeable way, especially at type level, which seems to be difficult for fully unsupervised learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_64",
            "content": "When supervising with a word list, all models but d.2gram outperform the baseline. Yet, the d.count and d.mix methods have lower scores than the gs.dense: this was expected for BF and WF-where directly supervising boundaries is likely to be more useful than an indirect one, but less so for LF. Regarding the d.2gram model, its poor BF and WF scores are more than compensated by an increase of around 12 points in LF, showing the impact of a better type model. Finally, by combining the d.mix and d.2gram strategies, d.mix+2gram obtains the overall best results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_65",
            "content": "pypseg",
            "ntype": "title",
            "meta": {
                "section": "5.1.2"
            }
        },
        {
            "ix": "355-ARR_v2_66",
            "content": "Results are in the right part of Table 2, where the baseline is the fully unsupervised pypseg. It slightly outperforms dpseg by less than 1 point in terms of F-scores. In our setting, although PYP increases the number of discovered types, it does not improve the performance in any significant manner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_67",
            "content": "This trend is confirmed for weakly supervised models: 8 the gs.dense model is the only one benefiting from a small improvement in all Fscores. d.count underperforms both the baseline and its dpseg version. With worsened BF and WF scores compared to the baseline, d.mix+2gram with pypseg is worse than with dpseg. Overall, the former seems to benefit less from annotations than the latter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_68",
            "content": "The performance of the bigram character model is noteworthy both with dpseg and pypseg. This improvement alone (i.e. d.2gram) is responsible not only for a large increase in LF, but also for an average type length that gets much closer to its true value (6.39 in the reference, 6.60 with dpseg and d.mix+2gram).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_69",
            "content": "Results for Japhug",
            "ntype": "title",
            "meta": {
                "section": "5.1.3"
            }
        },
        {
            "ix": "355-ARR_v2_70",
            "content": "Table 3 displays a selection of results for Japhug (segmented in words). As previously observed, supervision noticeably improves the results for both models, with pypseg outperforming dpseg by a small margin on all metrics. 9 Note also that SP is much worse than Bayesian models, only reaching the same F-score as dpseg for the LF metric.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_71",
            "content": "The best results are obtained with lexical supervision and the d.mix+2gram model for dpseg: it combines the type boost in P 0 from d.mix and the improved base model from d.2gram. 32.9k 34.2k 36.6k 33.8k 34.3k 33.8k 25.1k 25.0k 33.9k 33.5k 33.7k 24.8k model (green) can be attributed to the use of the bigram character model. It gives this model an initial edge over o.regular that remains significant for the first 3,000 sentences. Here again, the benefits of improving the base distribution (character-based model) as much as possible in the early training iterations clearly appear.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_72",
            "content": "Incremental learning",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "355-ARR_v2_73",
            "content": "Supervising words and morphemes",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "355-ARR_v2_74",
            "content": "This section addresses a recurring issue in word segmentation model related to the linguistic nature of the units learnt by the model and the consequences of choosing one or the other reference in training. The Japhug corpus contains both annotation levels and is a perfect test bed for this study. We have thus used a segmentation model (dpseg) with and without weak supervision (using the d.mix+2gram variant) at the level of words or morphemes, and the results are also evaluated against the two references (a segmentation in words or in morphemes). Results are in Table 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_75",
            "content": "In the unsupervised setting, segmentation metrics are markedly better with morpheme-based ref- With word supervision, we observe a shift in behaviour that is consistent with the provided annotations: better word-level metrics with word-based annotations, and accordingly, a decrease of performance for morpheme-based scores. With morpheme supervision, results are more contrasted: an improvement for word segmentation (because some words are also morphemes) that is not matched for morpheme boundaries. Looking at the detailed results (see appendix A.1, Table 7), one can see that this is due to an undersegmentation, which yields a poor recall at the boundary and token levels. Here, the main remaining benefit of supervision is an increase in the LF score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_76",
            "content": "These preliminary results suggest that considering only one type of boundary is a too naive view of the segmentation process and does not allow us to fully benefit from annotated data. They call for models that would carefully distinguish boundaries within words and between words, with appropriate supervision for each of these levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_77",
            "content": "Error analysis",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "355-ARR_v2_78",
            "content": "It is noteworthy that dictionary supervision almost deterministically ensures that the input word types will occur in the segmented output. For instance, 96% of the words in the Mboshi supervision dictionary are found in the output of the d.mix+2gram method, whereas we only find 44% with fully unsupervised learning. Similar trends are observed for Japhug. Some remaining errors are, however, observed: in the example of Figure 3, the word 'bana' belongs to the supervision dictionary but remains attached to the following word 'ba'. Additional examples are in appendix A.2. This may be because both words 'bana' and 'ba' often occur together, a cooccurrence that can not be captured by our unigram model . reference bana ba adi otEE imbva unsupervised banaba adio tEE imbva supervised banaba adi otEEimbva",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_79",
            "content": "Related work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "355-ARR_v2_80",
            "content": "Unsupervised segmentation is a generic NLP task that can be performed at multiple levels of analysis: a document segmented in sections, a speech segmented in utterances, an utterance segmented in words, a word segmented in morphemes, syllables or phonemes. It has been studied in multiple ways, and we report here recent work related to word discovery for language documentation, noting that the same methods also apply to the unsupervised segmentation of continuous speech into 'words' (de Marcken, 1996) which has given rise to a vast literature on language acquisition. Recently, this task has become central in preprocessing pipelines, with new implementations of simple models (Sennrich et al., 2016;Kudo and Richardson, 2018). Linear segmentation models in the Bayesian realm can be traced back to (Goldwater et al., 2006. They were extended with nesting in (Mochihashi et al., 2009), where the base distribution of the Dirichlet Process is a char-based nonparametric model; and in (Uchiumi et al., 2015;L\u00f6ser and Allauzen, 2016), who consider hidden state variables in the word generation process. This extension enables, for instance, to jointly learn segmentation and PoS tagging or to introduce some morphotactics in the model. Other sources of weak supervisions along these lines concern the use of higher-order n-grams and of prosodic cues (Doyle and Levy, 2013). Finally, (B\u00f6rschinger and Johnson, 2012) (with particle filtering techniques) and (Neubig, 2014) (with block sampling) study ways to speed up inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_81",
            "content": "The unsupervised techniques exposed in Section 2 only depend on the design of a probabilistic word generation process. This means that they are also readily applicable when this process is conditioned to some input, for instance, when a translation is available as an additional information source. This setup is notably studied in (Neubig et al., 2011;Stahlberg et al., 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017;Godard et al., 2018c).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_82",
            "content": "A somewhat richer trend of works aimed at informing word segmentation relies on the model of adaptor grammars (AG) of Johnson et al. (2007), applied to the segmentation task as early as (Johnson, 2008). AGs generalise finite-state models such as dpseg and pypseg by modelling trees and subtrees, rather than mere strings. Their use necessitates a context-free description of the language, which enables to integrate information regarding word and syllable structures. Even generic descriptions can be useful, but finding the most appropriate and effective one is challenging Eskander et al., 2016). This formalism has also been used to introduce syntactic information , prosodic information (B\u00f6rschinger and Johnson, 2014), and partial annotations (Sirts and Goldwater, 2013). Recent software packages for AGs are presented in (Bernard et al., 2020) and (Eskander et al., 2020). Using AGs comes, however, with a high computational price, as the Gibbs sampling process typically requires repeated parses of the corpus, even though cheaper estimation techniques may also be considered (Cohen et al., 2010). As our goal is to integrate learning techniques in interactive annotation tools, AGs were not deemed appropriate, and we explored simpler alternatives.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_83",
            "content": "Similar arguments apply to the use of neural networks, which have attracted a growing interest even for very low-resource languages, combining supervised segmentation methods (Moeng et al., 2021;Liu et al., 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017;Kann et al., 2018;Lane and Bird, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_84",
            "content": "Conclusion and outlook",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "355-ARR_v2_85",
            "content": "In this work, we have studied various ways to use weak supervision for automatic word segmentation. In language documentation scenarios, such supervision is often available, taking the form of a partial annotation or word lists. Bayesian non-parametric models lend themselves well to this setting, and our experiments have shown that two variants of a simple unigram model were getting a substantial boost from weak supervision, a result that has been obtained with two languages currently being documented. The most effective approach seems to start with a small set of fully segmented data, which helps learning in two ways: as a training signal for segmentation and as lexical prior for the base distribution. Based on this observation, we have further evaluated the longer-term benefits of an incremental training regime and also contrasted the improvement obtained using a word-based vs a morpheme-based vocabulary list.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_86",
            "content": "Our future work will continue to explore the interplay between word and morpheme segmentations, as both are required in actual documentation settings, possibly extending our analyses on additional languages. We will also consider supervising the annotation process with lists of non-inflected forms, which requires to jointly learn inflectional patterns and segmentation. Finally, our main objective remains to integrate these techniques into an annotation platform and evaluate how much they help speed up the annotation process, hence the need to control the run-time of our algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_87",
            "content": "A.1 Full results",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_88",
            "content": "Table 5 displays the complete results of Table 2 with both precision and recall for the three evaluation levels. SentencePiece (SP) tends to have more balanced scores for precision and recall, whereas dpseg displays a wider gap between the two metrics, especially at type level. The 'Morf' column displays the performance of Morfessor 2.0 (Creutz and Lagus, 2002;Smit et al., 2014). 10 These results have been obtained with the morph-length parameter set to the observed average token length (4.19). This setting led to better F-scores than using the gold number of types for num-morph-types or the default Morfessor model. The Morfessor model outperforms SentencePiece significantly for both boundary (BF) and token (WF) F-scores, while it lags behind for the type-based metrics. Compared to the unsupervised dpseg, Morfessor is worse on all accounts by a wide margin.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_89",
            "content": "Table 6, in turn, displays the complete results of Table 3, again with both precision and recall for the three evaluation levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_90",
            "content": "The 'Morf' column in Table 6 also represents the Morfessor results, with a morph-length parameter of 4.73. Here again, Morfessor outperforms Senten-cePiece on the boundary and token-level F-scores (to a smaller extent) but not at type level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_91",
            "content": "Finally, Table 7 displays the complete results for the word and morpheme experiment (Table 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_92",
            "content": "reference obengi amipasa koo sa k\u0159 unsupervised obengia mipasa koo sak\u0159 supervised obengi amipasa koo sak\u0159 Figure 4 shows an example sentence derived from the Mboshi data. The word 'obengi' is present in the supervision dictionary. In the unsupervised model (unsupervised line), the word was wrongly segmented, affecting the second word, 'amipasa'. In the supervised model with d.mix+2gram, the word is correctly segmented as 'obengi', and the second word is also correct, although not in the supervision dictionary. Figure 5 presents two of the 200 sentences used for supervision in Mboshi. This means that all the words in the example are in the supervision dictionary, which can explain why words such as 'owoi', 'atyeeli', or 'lekonyi' are correctly segmented in the weakly supervised setting. Yet, some errors remain (e.g. 'adimo' instead of 'adi mo') mainly because of the cooccurrence effect. reference atyeeli adi mo lekonyi unsupervised at yee li adi mole konyi supervised atyeeli adimo lekonyi reference n\u0159 owoi dzue la baa unsupervised n\u0159 o wo i dzuela baa supervised n\u0159 owoi dzue la baa",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "355-ARR_v2_93",
            "content": "Gilles Adda, Sebastian St\u00fcker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Blachon, H\u00e9l\u00e8ne Bonneau-Maynard, Pierre Godard, Fatima Hamlaoui, Dmitri Idiatov, Guy-No\u00ebl Kouarata, Lori Lamel, Breaking the Unwritten Language Barrier: The Bulb Project, 2016, Proceedings of SLTU (Spoken Language Technologies for Under-Resourced Languages), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Gilles Adda",
                    "Sebastian St\u00fcker",
                    "Martine Adda-Decker",
                    "Odette Ambouroue",
                    "Laurent Besacier",
                    "David Blachon",
                    "H\u00e9l\u00e8ne Bonneau-Maynard",
                    "Pierre Godard",
                    "Fatima Hamlaoui",
                    "Dmitri Idiatov",
                    "Guy-No\u00ebl Kouarata",
                    "Lori Lamel"
                ],
                "title": "Breaking the Unwritten Language Barrier: The Bulb Project",
                "pub_date": "2016",
                "pub_title": "Proceedings of SLTU (Spoken Language Technologies for Under-Resourced Languages)",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_94",
            "content": "Grant Aiton, Translating fieldwork into datasets: The development of a corpus for the quantitative investigation of grammatical phenomena in Eibela, 2021, Proceedings of the Workshop on Computational Methods for Endangered Languages, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Grant Aiton"
                ],
                "title": "Translating fieldwork into datasets: The development of a corpus for the quantitative investigation of grammatical phenomena in Eibela",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Workshop on Computational Methods for Endangered Languages",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_95",
            "content": "Antonios Anastasopoulos, David Chiang, A case study on using speech-to-translation alignments for language documentation, 2017, Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Antonios Anastasopoulos",
                    "David Chiang"
                ],
                "title": "A case study on using speech-to-translation alignments for language documentation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_96",
            "content": "Mathieu Bernard, Roland Thiolliere, Amanda Saksida, Georgia Loukatou, Elin Larsen, Mark Johnson, Laia Fibla, Emmanuel Dupoux, Robert Daland, Xuan Cao, Alejandrina Cristia, Wordseg: Standardizing unsupervised word form segmentation from text, 2020, Behavior Research Methods, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Mathieu Bernard",
                    "Roland Thiolliere",
                    "Amanda Saksida",
                    "Georgia Loukatou",
                    "Elin Larsen",
                    "Mark Johnson",
                    "Laia Fibla",
                    "Emmanuel Dupoux",
                    "Robert Daland",
                    "Xuan Cao",
                    "Alejandrina Cristia"
                ],
                "title": "Wordseg: Standardizing unsupervised word form segmentation from text",
                "pub_date": "2020",
                "pub_title": "Behavior Research Methods",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_97",
            "content": "Steven Bird, Decolonising Speech and Language Technology, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Steven Bird"
                ],
                "title": "Decolonising Speech and Language Technology",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_98",
            "content": "Benjamin B\u00f6rschinger, Mark Johnson, Using rejuvenation to improve particle filtering for bayesian word segmentation, 2012, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Benjamin B\u00f6rschinger",
                    "Mark Johnson"
                ],
                "title": "Using rejuvenation to improve particle filtering for bayesian word segmentation",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "355-ARR_v2_99",
            "content": "Benjamin B\u00f6rschinger, Mark Johnson, Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Benjamin B\u00f6rschinger",
                    "Mark Johnson"
                ],
                "title": "Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars",
                "pub_date": "2014",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_100",
            "content": "Shay Cohen, Bayesian Analysis in Natural Language Processing, 2016, Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Shay Cohen"
                ],
                "title": "Bayesian Analysis in Natural Language Processing",
                "pub_date": "2016",
                "pub_title": "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_101",
            "content": "B Shay, David Cohen, Noah Blei,  Smith, Variational inference for Adaptor Grammars, 2010, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "B Shay",
                    "David Cohen",
                    "Noah Blei",
                    " Smith"
                ],
                "title": "Variational inference for Adaptor Grammars",
                "pub_date": "2010",
                "pub_title": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_102",
            "content": "Mathias Creutz, Krista Lagus, Unsupervised discovery of morphemes, 2002, Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Mathias Creutz",
                    "Krista Lagus"
                ],
                "title": "Unsupervised discovery of morphemes",
                "pub_date": "2002",
                "pub_title": "Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_103",
            "content": "UNKNOWN, None, 1996, Unsupervised Language Acquisition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "1996",
                "pub_title": "Unsupervised Language Acquisition",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_104",
            "content": "Gabriel Doyle, Roger Levy, Combining multiple information types in Bayesian word segmentation, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Gabriel Doyle",
                    "Roger Levy"
                ],
                "title": "Combining multiple information types in Bayesian word segmentation",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_105",
            "content": "UNKNOWN, None, 2013, Processus segmentaux et tonals en Mbondzi -(vari\u00e9t\u00e9 de la langue embosi C25) -. Theses, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Processus segmentaux et tonals en Mbondzi -(vari\u00e9t\u00e9 de la langue embosi C25) -. Theses",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_106",
            "content": "Ramy Eskander, Francesca Callejas, Elizabeth Nichols, Judith Klavans, Smaranda Muresan, Mor-phAGram, evaluation and framework for unsupervised morphological segmentation, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Ramy Eskander",
                    "Francesca Callejas",
                    "Elizabeth Nichols",
                    "Judith Klavans",
                    "Smaranda Muresan"
                ],
                "title": "Mor-phAGram, evaluation and framework for unsupervised morphological segmentation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_107",
            "content": "Ramy Eskander, Judith Klavans, Smaranda Muresan, Unsupervised morphological segmentation for low-resource polysynthetic languages, 2019, Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Ramy Eskander",
                    "Judith Klavans",
                    "Smaranda Muresan"
                ],
                "title": "Unsupervised morphological segmentation for low-resource polysynthetic languages",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_108",
            "content": "Ramy Eskander, Owen Rambow, Tianchun Yang, Extending the use of Adaptor Grammars for unsupervised morphological segmentation of unseen languages, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Ramy Eskander",
                    "Owen Rambow",
                    "Tianchun Yang"
                ],
                "title": "Extending the use of Adaptor Grammars for unsupervised morphological segmentation of unseen languages",
                "pub_date": "2016",
                "pub_title": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_109",
            "content": "Pierre Godard, Gilles Adda, Martine Adda-Decker, Alexandre Allauzen, Laurent Besacier, H\u00e9l\u00e8ne Bonneau-Maynard, Guy-No\u00ebl Kouarata, Kevin L\u00f6ser, Annie Rialland, Fran\u00e7ois Yvon, Preliminary Experiments on Unsupervised Word Discovery in Mboshi, 2016, Proceedings of Interspeech 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Pierre Godard",
                    "Gilles Adda",
                    "Martine Adda-Decker",
                    "Alexandre Allauzen",
                    "Laurent Besacier",
                    "H\u00e9l\u00e8ne Bonneau-Maynard",
                    "Guy-No\u00ebl Kouarata",
                    "Kevin L\u00f6ser",
                    "Annie Rialland",
                    "Fran\u00e7ois Yvon"
                ],
                "title": "Preliminary Experiments on Unsupervised Word Discovery in Mboshi",
                "pub_date": "2016",
                "pub_title": "Proceedings of Interspeech 2016",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_110",
            "content": "Pierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, Jamison Cooper-Leavitt, Guy-Noel Kouarata, Lori Lamel, H\u00e9l\u00e8ne Maynard, Markus Mueller, Annie Rialland, Sebastian Stueker, Fran\u00e7ois Yvon, and Marcely Zanon-Boito. 2018a. A very low resource language speech corpus for computational language documentation experiments, , Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Pierre Godard",
                    "Gilles Adda",
                    "Martine Adda-Decker",
                    "Juan Benjumea",
                    "Laurent Besacier",
                    "Jamison Cooper-Leavitt",
                    "Guy-Noel Kouarata",
                    "Lori Lamel",
                    "H\u00e9l\u00e8ne Maynard",
                    "Markus Mueller",
                    "Annie Rialland"
                ],
                "title": "Sebastian Stueker, Fran\u00e7ois Yvon, and Marcely Zanon-Boito. 2018a. A very low resource language speech corpus for computational language documentation experiments",
                "pub_date": null,
                "pub_title": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_111",
            "content": "Pierre Godard, Laurent Besacier, Fran\u00e7ois Yvon, Martine Adda-Decker, Gilles Adda, H\u00e9l\u00e8ne Maynard, Annie Rialland, Adaptor Grammars for the linguist: Word segmentation experiments for very low-resource languages, 2018, Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Pierre Godard",
                    "Laurent Besacier",
                    "Fran\u00e7ois Yvon",
                    "Martine Adda-Decker",
                    "Gilles Adda",
                    "H\u00e9l\u00e8ne Maynard",
                    "Annie Rialland"
                ],
                "title": "Adaptor Grammars for the linguist: Word segmentation experiments for very low-resource languages",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_112",
            "content": "Pierre Godard,  Marcely Zanon, Lucas Boito, Alexandre Ondel, Fran\u00e7ois Berard,  Yvon, Unsupervised word segmentation from speech with attention, 2018, Proc. Interspeech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Pierre Godard",
                    " Marcely Zanon",
                    "Lucas Boito",
                    "Alexandre Ondel",
                    "Fran\u00e7ois Berard",
                    " Yvon"
                ],
                "title": "Unsupervised word segmentation from speech with attention",
                "pub_date": "2018",
                "pub_title": "Proc. Interspeech",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_113",
            "content": "Sharon Goldwater, Thomas Griffiths, Mark Johnson, Contextual dependencies in unsupervised word segmentation, 2006, Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Sharon Goldwater",
                    "Thomas Griffiths",
                    "Mark Johnson"
                ],
                "title": "Contextual dependencies in unsupervised word segmentation",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_114",
            "content": "UNKNOWN, None, 2009, A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "A Bayesian framework for word segmentation: Exploring the effects of context. Cognition",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_115",
            "content": "UNKNOWN, None, 2021, A grammar of Japhug. Number 1 in Comprehensive Grammar Library, Language Science Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "A grammar of Japhug. Number 1 in Comprehensive Grammar Library",
                "pub": "Language Science Press"
            }
        },
        {
            "ix": "355-ARR_v2_116",
            "content": "Mark Johnson, Unsupervised word segmentation for Sesotho using adaptor grammars, 2008, Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Mark Johnson"
                ],
                "title": "Unsupervised word segmentation for Sesotho using adaptor grammars",
                "pub_date": "2008",
                "pub_title": "Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_117",
            "content": "Mark Johnson, Anne Christophe, Emmanuel Dupoux, Katherine Demuth, Modelling function words improves unsupervised word segmentation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Mark Johnson",
                    "Anne Christophe",
                    "Emmanuel Dupoux",
                    "Katherine Demuth"
                ],
                "title": "Modelling function words improves unsupervised word segmentation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_118",
            "content": "Mark Johnson, Sharon Goldwater, Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars, 2009, Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Mark Johnson",
                    "Sharon Goldwater"
                ],
                "title": "Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars",
                "pub_date": "2009",
                "pub_title": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_119",
            "content": "Mark Johnson, Thomas Griffiths, Sharon Goldwater, Adaptor Grammars: a Framework for Specifying Compositional Nonparametric Bayesian Models, 2007, Advances in Neural Information Processing Systems, MIT Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Mark Johnson",
                    "Thomas Griffiths",
                    "Sharon Goldwater"
                ],
                "title": "Adaptor Grammars: a Framework for Specifying Compositional Nonparametric Bayesian Models",
                "pub_date": "2007",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "MIT Press"
            }
        },
        {
            "ix": "355-ARR_v2_120",
            "content": "Katharina Kann, Jesus Manuel Mager, Ivan Hois, Hinrich Meza-Ruiz,  Sch\u00fctze, Fortification of neural morphological segmentation models for polysynthetic minimal-resource languages, 2018-01, Proceedings of the 2018 Conference Tumi Moeng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Katharina Kann",
                    "Jesus Manuel Mager",
                    "Ivan Hois",
                    "Hinrich Meza-Ruiz",
                    " Sch\u00fctze"
                ],
                "title": "Fortification of neural morphological segmentation models for polysynthetic minimal-resource languages",
                "pub_date": "2018-01",
                "pub_title": "Proceedings of the 2018 Conference Tumi Moeng",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_121",
            "content": "UNKNOWN, None, 2014, Simple, correct parallelization for blocked Gibbs sampling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Simple, correct parallelization for blocked Gibbs sampling",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_122",
            "content": "Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, Tatsuya Kawahara, An unsupervised model for joint phrase alignment and extraction, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Graham Neubig",
                    "Taro Watanabe",
                    "Eiichiro Sumita",
                    "Shinsuke Mori",
                    "Tatsuya Kawahara"
                ],
                "title": "An unsupervised model for joint phrase alignment and extraction",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_123",
            "content": "UNKNOWN, None, 2015, Dropping of the class-prefix consonant, vowel elision and automatic phonological mining in Embosi, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Dropping of the class-prefix consonant, vowel elision and automatic phonological mining in Embosi",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_124",
            "content": "UNKNOWN, None, , Proceedings of the 44th Annual Conference on African Linguistics, Somerville. Cascadilla.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Proceedings of the 44th Annual Conference on African Linguistics",
                "pub": "Somerville. Cascadilla"
            }
        },
        {
            "ix": "355-ARR_v2_125",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "355-ARR_v2_126",
            "content": "Miikka Silfverberg, Adam Wiemerslage, Ling Liu, Lingshuang Jack Mao, Data augmentation for morphological reinflection, 2017, Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Miikka Silfverberg",
                    "Adam Wiemerslage",
                    "Ling Liu",
                    "Lingshuang Jack Mao"
                ],
                "title": "Data augmentation for morphological reinflection",
                "pub_date": "2017",
                "pub_title": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_127",
            "content": "Kairit Sirts, Sharon Goldwater, Minimallysupervised morphological segmentation using Adaptor Grammars, 2013, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Kairit Sirts",
                    "Sharon Goldwater"
                ],
                "title": "Minimallysupervised morphological segmentation using Adaptor Grammars",
                "pub_date": "2013",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_128",
            "content": "Peter Smit, Sami Virpioja, Stig-Arne Gr\u00f6nroos, Mikko Kurimo, Morfessor 2.0: Toolkit for statistical morphological segmentation, 2014, Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Peter Smit",
                    "Sami Virpioja",
                    "Stig-Arne Gr\u00f6nroos",
                    "Mikko Kurimo"
                ],
                "title": "Morfessor 2.0: Toolkit for statistical morphological segmentation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "355-ARR_v2_129",
            "content": "Felix Stahlberg, Tim Schlippe, Stephan Vogel, Tanja Schultz, Word segmentation through cross-lingual word-to-phoneme alignment, 2012, Spoken Language Technology Workshop (SLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Felix Stahlberg",
                    "Tim Schlippe",
                    "Stephan Vogel",
                    "Tanja Schultz"
                ],
                "title": "Word segmentation through cross-lingual word-to-phoneme alignment",
                "pub_date": "2012",
                "pub_title": "Spoken Language Technology Workshop (SLT)",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_130",
            "content": ", None, , IEEE, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [],
                "title": null,
                "pub_date": null,
                "pub_title": "IEEE",
                "pub": "IEEE"
            }
        },
        {
            "ix": "355-ARR_v2_131",
            "content": "UNKNOWN, None, 2006, A Bayesian interpretation of interpolated Kneser-Ney, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2006",
                "pub_title": "A Bayesian interpretation of interpolated Kneser-Ney",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_132",
            "content": "Yee Whye Teh, A hierarchical Bayesian language model based on Pitman-Yor processes, 2006, Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Yee Whye Teh"
                ],
                "title": "A hierarchical Bayesian language model based on Pitman-Yor processes",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "355-ARR_v2_133",
            "content": ", Complete results on the 5K Mboshi text for various models and weak supervision settings (20K iterations, 200 supervision sentences, \u03bb = 0.25), , SP stands for SentencePiece, Morf for Morfessor, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [],
                "title": "Complete results on the 5K Mboshi text for various models and weak supervision settings (20K iterations, 200 supervision sentences, \u03bb = 0.25)",
                "pub_date": null,
                "pub_title": "SP stands for SentencePiece, Morf for Morfessor",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "355-ARR_v2_0@0",
            "content": "Weakly Supervised Word Segmentation for Computational Language Documentation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_0",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@0",
            "content": "Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@1",
            "content": "However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 167,
            "end": 375,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@2",
            "content": "This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 377,
            "end": 499,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@3",
            "content": "Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 501,
            "end": 687,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@4",
            "content": "In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 689,
            "end": 812,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_2@5",
            "content": "This work opens the way for interactive annotation tools for documentary linguists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_2",
            "start": 814,
            "end": 896,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_4@0",
            "content": "Recent years have witnessed a blooming of research aimed at applying language technologies (LTs) to \"under-resourced languages\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_4",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_4@1",
            "content": "1 Such studies have been mostly motivated on three main grounds (not necessarily mutually exclusive): (a) to develop tools that could speed up the work of field linguists collecting and annotating recordings for these languages; (b) to provide linguistic communities with LTs that are necessary in an increasingly digitalised world, e.g. to interact with smartphones or computers in their own language and communicate with speakers of other languages; (c) to challenge existing machine-learning techniques in very low resource settings, where hardly any resource (dictionary, corpus, grammar) is available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_4",
            "start": 129,
            "end": 734,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_5@0",
            "content": "Those objectives are thoroughly discussed in a recent position paper (Bird, 2020) who notices, among other things, that objective (c) (training language processing tools with zero resource) is questionable in the context of language documentation works which can often rely on some pre-existing knowledge, such as a word list, or information from related languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_5",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_5@1",
            "content": "Accordingly, this paper explores ways to make the best of prior resources and improve the effectiveness of unsupervised language analysis techniques for the purpose of linguistic documentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_5",
            "start": 366,
            "end": 558,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_5@2",
            "content": "Our main objective is to develop tools that will effectively assist field linguists in their documentary tasks (objective (a)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_5",
            "start": 560,
            "end": 686,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_5@3",
            "content": "We focus on segmentation tasks, which aim to automatically identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008;Doyle and Levy, 2013;Eskander et al., 2016;Godard et al., 2018b;Eskander et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_5",
            "start": 688,
            "end": 924,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_6@0",
            "content": "Following these authors, we experiment with Bayesian non-parametric segmentation models, derived in our case from and subsequent work, which we recap in Section 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_6",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_6@1",
            "content": "Our first contribution is in Section 3 which studies multiple semi-supervised learning regimes aimed to take advantage of pre-existing linguistic material such as incomplete segmentations and word lists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_6",
            "start": 164,
            "end": 366,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_7@0",
            "content": "In Sections 4 and 5, we experimentally assess the pros and cons of these weakly supervised approaches in batch and online learning, for two extremely low-resource languages currently in the process of being documented: Mboshi, a Bantu language used in former studies (Godard et al., 2018a); and Japhug, a language from the Sino-Tibetan family spoken in the Western part of China thoroughly documented by Jacques (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_7",
            "start": 0,
            "end": 418,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_7@1",
            "content": "These two languages were selected because they illustrate actual documentation processes, for which high-quality linguistic resources have been derived from fieldwork, at the end of a long and difficult procedure (Aiton, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_7",
            "start": 420,
            "end": 646,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_7@2",
            "content": "A complementary analysis follows, where we use the Japhug corpus to take a closer look at the units identified automatically, contrasting morpheme-based and word-based supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_7",
            "start": 648,
            "end": 827,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_8@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_8",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_9@0",
            "content": "Going from audio recordings to fully annotated transcripts implies two successive segmentation steps: the first segments words and happens during the production of phonemic or orthographic transcripts; the second further splits words into morphs, which are then annotated with syntactic information and glosses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_9",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_9@1",
            "content": "We mostly focus on the former task, assuming a two-step process: first, the computation of a phonemic transcript that we assume is given; then the segmentation into words for which we consider two settings: batch and online learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_9",
            "start": 312,
            "end": 544,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_9@2",
            "content": "The word and morpheme segmentation tasks are closely related and rely on similar tools: using the Japhug corpus, which contains both levels of segmentations, we also study the implications of using lists of words vs morphemes as weak supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_9",
            "start": 546,
            "end": 791,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_10@0",
            "content": "In its baseline form, the word segmentation process is fully unsupervised, and the only training material is a set of transcribed sentences (see Fig. 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_10",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_11@0",
            "content": "We rely on Bayesian non-parametric approaches to word segmentation (see (Cohen, 2016) for a thorough exposition), and our baselines are the unigram version of the dpseg model and a variant where the underlying Dirichlet Process is replaced by a Pitman-Yor Process as in (Neubig, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_11",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_11@1",
            "content": "We selected unigram models for their simplicity, which (a) makes them amenable to the processing of very small sets of sentences; (b) makes the online learning setting tractable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_11",
            "start": 286,
            "end": 463,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_11@2",
            "content": "While using higher-order models or more sophisticated models of the same family (Teh, 2006b;Mochihashi et al., 2009) may improve the performance (see for an experimental comparison), we believe that in our low-resource conditions, these variations would be small 2 and would not change our main conclusions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_11",
            "start": 465,
            "end": 771,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_12@0",
            "content": "Word segmentation models fundamentally rely on probabilistic models for word sequences defining P (w = w 1 . . . w T ); word sequences can also be viewed as segmented sequences of characters y = y 1 . . . y L , so that the same model can be used for the joint probability of (y, b), with b = b 1 . . . b L representing the vector of boundary locations where value b t = 1 (resp. b t = 0) denotes a boundary (resp. no boundary) after symbol y t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_12",
            "start": 0,
            "end": 444,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_12@1",
            "content": "In an unsupervised setting, these boundaries are hidden and are latent variables in the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_12",
            "start": 446,
            "end": 539,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_12@2",
            "content": "Such models lend themselves well to Gibbs sampling, which repeatedly produces samples of each boundary given all the other boundaries in the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_12",
            "start": 541,
            "end": 688,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_13@0",
            "content": "In dpseg, the underlying sequence model is a unigram model: P (w 1 . . . w T ) = T t=1 P (w t ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_13",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_13@1",
            "content": "The probability of individual words corresponds to a Dirichlet Process with parameters \u03b1, the concentration parameter, and P 0 , the base distribution, and yields the following formulation for the conditional probability of w t given the past words w <t :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_13",
            "start": 97,
            "end": 351,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_14@0",
            "content": "P (w t = w|w <t ) = n w (w <t ) + \u03b1P 0 (w) t + \u03b1 \u2212 1 ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_14",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_15@0",
            "content": "where n w (w <t ) counts the number of times w has occurred in the past.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_15",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_15@1",
            "content": "With lower values of \u03b1, the most frequent words tend to be generated more (hence, concentration), while with higher values, the words are more smoothly distributed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_15",
            "start": 73,
            "end": 236,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_15@2",
            "content": "P 0 , the base distribution, assigns scores to arbitrary character strings; use a length model and a uniform character model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_15",
            "start": 238,
            "end": 362,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_15@3",
            "content": "For word w made of characters y 1 , ..., y m , P 0 is computed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_15",
            "start": 364,
            "end": 429,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_16@0",
            "content": "P 0 (w) = p # (1 \u2212 p # ) m\u22121 length model m j=1 P (y j ) character model (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_16",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_17@0",
            "content": "where p # is the probability to end the word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_17",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_17@1",
            "content": "For this model, Gibbs sampling compares at each position t two sequences of words w t=0 (no boundary at position t) and w t=1 (a boundary is inserted).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_17",
            "start": 46,
            "end": 196,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_17@2",
            "content": "As these sequences only differ minimally, terms such as P (b t = 0|y, b \u2212t ) are readily derived (see e.g. ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_17",
            "start": 198,
            "end": 306,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_17@3",
            "content": "Gibbs sampling is performed for a number of iterations that are sufficient to reach convergence, and we use the last iteration to uncover the resulting segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_17",
            "start": 308,
            "end": 472,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_17@4",
            "content": "To speed up mixing, also use annealing, so that a larger search space is explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_17",
            "start": 474,
            "end": 555,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_18@0",
            "content": "An extension of dpseg, denoted pypseg, uses a Pitman-Yor Process (PYP) instead of the Dirichlet Process and generalises equation (1) with an additional discount parameter, which enables to better control the generation of new words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_18",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_18@1",
            "content": "PYPs are introduced in (Teh, 2006b;Mochihashi et al., 2009); a fast implementation is in (Neubig, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_18",
            "start": 233,
            "end": 336,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_18@2",
            "content": "For our experiments, both models have y = b 1 \u00e1 2 a 3 \u00e1 4 m 5 i 6 k 7 \u00fa 8 n 9 d 10 \u00e1 11 p 12 o 13 o 14 y 15 \u00e1 16 k 17 a 18 l 19 a 20 b = 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 w 6=0 b\u00e1a \u00e1mik\u00fand\u00e1 poo y\u00e1 kala b = 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 w 6=1 b\u00e1a \u00e1mi k\u00fand\u00e1 poo y\u00e1 kala",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_18",
            "start": 338,
            "end": 620,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_19@0",
            "content": "Figure 1: The sentence segmentation task illustrated with a sentence from the Mboshi corpus: 'b\u00e1a \u00e1mik\u00fand\u00e1 poo y\u00e1 kala' ('they found the old village').",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_19",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_19@1",
            "content": "The two possible segmentations only differ in one boundary at position t = 6, one (w 6=0 ) where '\u00e1mik\u00fand\u00e1' is one single unit and one (w 6=1 ) where it is split in two.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_19",
            "start": 152,
            "end": 320,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_20@0",
            "content": "been re-implemented in Python.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_20",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_20@1",
            "content": "This implementation is available at https://github.com/ shuokabe/pyseg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_20",
            "start": 31,
            "end": 101,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_21@0",
            "content": "Supervising word segmentation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_21",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_22@0",
            "content": "In this section, we discuss realistic sources of weak supervision for segmentation tasks and how they can be included in Bayesian models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_22",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_23@0",
            "content": "Finding supervision information",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_23",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_24@0",
            "content": "Segmentation boundaries Segmentation data, corresponding to the location of boundary (and non-boundary) information, can be obtained in different ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_24",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_24@1",
            "content": "For instance, when audio recordings are available, prosodic cues such as short silences or specific intonative patterns can serve to identify plausible locations for word endings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_24",
            "start": 152,
            "end": 330,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_24@2",
            "content": "Longer pauses generally denote the end of an utterance, which we assume are already given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_24",
            "start": 332,
            "end": 421,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_24@3",
            "content": "This would yield a sparse partial annotation, where supervision data is randomly scattered across the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_24",
            "start": 423,
            "end": 531,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@0",
            "content": "Another realistic situation where we have access to a partial annotation is when a small subset is already segmented.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@1",
            "content": "In this case, the partial annotation is dense and concentrated in a few sentences, a semi-supervised setting also studied in (Sirts and Goldwater, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 118,
            "end": 270,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@2",
            "content": "We thus consider two questions: (a) which is more effective between dense and sparse annotations? (b) how effective is supervision in an incremental learning regime, where automatic (dense) annotations are progressively corrected and used to update the model?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 272,
            "end": 530,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@3",
            "content": "Word lists Word lists constitute another valuable and common source of information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 532,
            "end": 614,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@4",
            "content": "They may contain morphs, morphemes, lexemes or fully inflected forms, with various levels of information (part-ofspeech, gloss, translation, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 616,
            "end": 762,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@5",
            "content": "In this study, we consider that lists of surface forms are available and evaluate their usefulness, depending on their size and on the way they were collected.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 764,
            "end": 922,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@6",
            "content": "A related question is about the relative interest of word and morph lists, which we study in Section 5.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 924,
            "end": 1028,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_25@7",
            "content": "The use of more sophisticated forms of lexical information regarding word structure, PoS, is out of the scope of this paper and is left for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_25",
            "start": 1030,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_26@0",
            "content": "Having a collection of fully segmented utterances, as discussed above, is another way to generate word lists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_26",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_26@1",
            "content": "So these two sources of information must be viewed as complementary ways to supervise the task at hand: boundary marks at the token level, word list at the type level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_26",
            "start": 110,
            "end": 276,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_27@0",
            "content": "Forms of Weak Supervision",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_27",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_28@0",
            "content": "Segmentation boundaries Observed segmentation boundaries can be used to facilitate the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_28",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_28@1",
            "content": "Two experimental conditions, both affecting the Gibbs sampler (gs), have been considered:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_28",
            "start": 105,
            "end": 193,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_29@0",
            "content": "\u2022 gs.sparse: a fraction (\u03bb%) of the actual boundaries are observed, which corresponds to a sparse annotation scenario. \u2022 gs.dense: for \u03bb% of sentences, all boundary and non-boundary variables are given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_29",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_30@0",
            "content": "In both cases, we modify the sampling process and make sure that the value of observed variables is not sampled, as in (Sirts and Goldwater, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_30",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_31@0",
            "content": "Using a word list Assuming now that a word list D is available, we consider the following approaches to reinforce the likelihood of units in D in the output segmentation:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_31",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_32@0",
            "content": "\u2022 d.count: D is used to initialise the 'internal' model dictionary, and words in D are created with a fixed pseudo-count of value \u03bb. Formally, \u2200w \u2208 D, the counting function n w () of Equation (1) will add \u03bb to their actual count. \u2022 d.mix: D is combined with the base distribution, resulting in the following mixture P 0 :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_32",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_33@0",
            "content": "P 0 (w) = \u03bb |D| 1 {w\u2208D} + (1 \u2212 \u03bb)P 0 (w),(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_33",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_34@0",
            "content": "where \u03bb \u2208 [0, 1], |D| is the size of D, and 1 {w\u2208D} is the indicator function testing membership in D. As for d.count, P 0 increases the probability of words in D, but in a looser way, due to the term \u03b1P 0 in Equation (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_34",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_35@0",
            "content": "\u2022 d.ngram: the baseline dpseg version uses a uniform character model for P 0 (Equation ( 2)); here, we use D to train a character n-gram language model (LM), with n = 2 and add-k smoothing in our experiments. \u2022 d.mix+ngram: this method combines d.mix and d.ngram: P 0 is replaced with the mixture P 0 of Equation ( 3) and the character model is an n-gram LM. This can be viewed as a proxy to the complete nested Dirichlet Process of Mochihashi et al. ( 2009), with D implementing a cache mechanism for known words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_35",
            "start": 0,
            "end": 513,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_36@0",
            "content": "We have also used weaker forms of supervision aimed at learning a better length model, with hardly any improvement with respect to the baseline; these results are not reported below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_36",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_37@0",
            "content": "Incremental training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_37",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_38@0",
            "content": "In addition to the static use of supervision information described above, we also considered a more dynamic training regime, where dense annotations are provided in a sequential manner through interaction with an expert linguist, enabling incremental learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_38",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_38@1",
            "content": "To measure the effectiveness of this approach, we contrast three scenarios in Section 5.2:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_38",
            "start": 261,
            "end": 350,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_39@0",
            "content": "\u2022 the baseline is the post-edition of a fully unsupervised model without further training; \u2022 the post-edition of a fully unsupervised model, with additional Gibbs sampling iterations every batch utterances for iter iterations. This aims at propagating forward the supervision information obtained from past annotations. This method is referred to as o.regular. \u2022 on top of this, we also used the past annotated sentences to reestimate the base distribution of the underlying process as in d.ngram. The corresponding results are labelled o.2level in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_39",
            "start": 0,
            "end": 557,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_40@0",
            "content": "4 Experimental settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_40",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_41@0",
            "content": "Linguistic material",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_41",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_42@0",
            "content": "Two languages have been considered in this paper: Mboshi and Japhug.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_42",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@0",
            "content": "Mboshi is a tonal Bantu language spoken in the Republic of Congo (Bantu C25).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@1",
            "content": "The data has been collected as part of the BULB project .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 78,
            "end": 134,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@2",
            "content": "It has seven vowels and 25 consonant phonemes with five prenasalised consonants (made of two to three consonants), a common feature in Bantu languages (Embanga Aborobongui, 2013;Kouarata, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 136,
            "end": 329,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@3",
            "content": "Although the language is usually not written, linguists have transcribed it with graphemes in a way that approximates the phonetic content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 331,
            "end": 469,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@4",
            "content": "To mark the distinction between long and short vowels, they were either duplicated (VV) or not (V).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 471,
            "end": 569,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@5",
            "content": "One challenge for Mboshi word segmentation is its complex phonological rules, notably, vowel elision patterns whereby a vowel disappears before another one (also a common Bantu feature) (Rialland et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 571,
            "end": 780,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_43@6",
            "content": "This kind of phenomenon makes it harder to find the boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_43",
            "start": 782,
            "end": 844,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_44@0",
            "content": "From a morphological point of view, words are composed of roots and affixes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_44",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_44@1",
            "content": "Another characteristic Bantu feature is its deletion rule for class-prefix consonants in nouns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_44",
            "start": 77,
            "end": 171,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_44@2",
            "content": "Templates for verb structure are also quite rigid, with affixes following a strict ordering (Godard et al., 2018a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_44",
            "start": 173,
            "end": 287,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_45@0",
            "content": "Our corpus is a manual alphabetic transcription of audio recordings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_45",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_45@1",
            "content": "3 It contains 5,312 sentences segmented in words, one sentence per line.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_45",
            "start": 69,
            "end": 140,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_46@0",
            "content": "Japhug is a Sino-Tibetan language from the Gyalrong family spoken in the Sichuan province in China.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_46",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_46@1",
            "content": "Japhug has eight vowels and 50 consonant phonemes, which can combine to create a large number (more than 400) of consonant clusters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_46",
            "start": 100,
            "end": 231,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_46@2",
            "content": "The rich cluster feature is one important characteristic of Japhug, which actually has one of the largest inventory of consonant clusters in the Trans-Himalayan language family.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_46",
            "start": 233,
            "end": 409,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_46@3",
            "content": "The structure of these clusters can be analysed by looking at patterns of partial reduplication of syllable initial consonants.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_46",
            "start": 411,
            "end": 537,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_46@4",
            "content": "There are no tones in this language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_46",
            "start": 539,
            "end": 574,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_47@0",
            "content": "Japhug also has a rich morphology, both for verbs and nouns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_47",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_47@1",
            "content": "Remarkably, in verb forms, up to six or seven prefixes can be chained to express features such as tense, aspect, modality, while suffixation is used to express inflectional phenomena.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_47",
            "start": 61,
            "end": 243,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@0",
            "content": "Even though these processes are quite regular, they contribute to generating a large number of possible word forms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@1",
            "content": "Recordings, annotated corpora, and dictionaries for Japhug are available from the Pangloss collection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 116,
            "end": 217,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@2",
            "content": "4 An extensive description of the language is given in (Jacques, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 219,
            "end": 289,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@3",
            "content": "5 Our training material has been extracted from the L A T E X source files of this book, by collecting all Japhug examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 291,
            "end": 413,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@4",
            "content": "These can easily be retrieved by searching the \\gll command introducing Japhug sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 415,
            "end": 503,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@5",
            "content": "Not only are the resulting sentences well-curated, but they are also segmented at two levels: words and morphemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 505,
            "end": 618,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@6",
            "content": "This will lead to a specific experiment presented in Section 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 620,
            "end": 682,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_48@7",
            "content": "For the latter, we use the word-based and morphemebased segmentations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_48",
            "start": 684,
            "end": 753,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_49@0",
            "content": "Table 1 displays the general statistics for the two languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_49",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_49@1",
            "content": "N utt , N type , and N token represent the number of utterances, of word types, and of word tokens, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_49",
            "start": 63,
            "end": 175,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_49@2",
            "content": "WL represents the average token length, while TL is the average type length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_49",
            "start": 177,
            "end": 252,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_49@3",
            "content": "The sentences used for semi-supervision correspond to the first 200 sentences of each dataset, which is a realistic amount of data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_49",
            "start": 254,
            "end": 384,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_49@4",
            "content": "Likewise, lexical supervision corresponds to the list of words observed in the same 200 sentences, and respectively contain 517 words for Mboshi, 664 words and 493 morphemes for Japhug.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_49",
            "start": 386,
            "end": 570,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_50@0",
            "content": "Model settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_50",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_51@0",
            "content": "In our experimental setting, we made sure to also resample the hyperparameter(s) after each iteration, following mostly (Teh, 2006a;Mochihashi et al., 2009): the concentration parameter \u03b1 has a Gamma posterior distribution, and the discount parameter d a Beta distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_51",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_51@1",
            "content": "The initial values of the hyperparameters were set as in Goldwater et al.'s work on the unigram dpseg: concentration parameter: \u03b1 = 20, p # = 0.5, discount parameter for pypseg: d = 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_51",
            "start": 274,
            "end": 459,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_51@2",
            "content": "The Gibbs sampler always runs for 20,000 iterations and simulated annealing is implemented as in with 10 increments of temperature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_51",
            "start": 461,
            "end": 591,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_52@0",
            "content": "All the results are obtained by collecting the predicted boundaries at the end of the last sampling iteration of one single run.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_52",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_53@0",
            "content": "Evaluation metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_53",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_54@0",
            "content": "Following , evaluation relies on 'PRF' metrics: precision, recall, and F-score, defined as follows: precision P = T P T P +F P , recall R = T P T P +F N , and F-score F = 2 * precision * recall precision+recall , where TP are the true positives (match in the reference and segmented texts), FP are the false positives, and FN are the false negatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_54",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_54@1",
            "content": "These metrics are computed at three levels: 6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_54",
            "start": 351,
            "end": 395,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_55@0",
            "content": "\u2022 boundary level (BP, BR, BF): compare the reference boundary vectors with the predictions; \u2022 token level (WP, WR, WF): compare word in the reference and segmented sentences: a correct match requires two correct boundaries; \u2022 type level (LP, LR, LF): compare the set of unique words in the reference and segmented utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_55",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_56@0",
            "content": "To have an overall view of the output text, we also report the average type and token lengths (TL and WL) as well as their counts (N type and N token ), as in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_56",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_56@1",
            "content": "Numbers are computed on the entire text (including the supervised part).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_56",
            "start": 168,
            "end": 239,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_57@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_57",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_58@0",
            "content": "This section presents the results for the models presented above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_58",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_58@1",
            "content": "We also report the performance of SentencePiece, another word segmentation tool based on a unigram language model (Kudo, 2018): 7 To boost this baseline, the vocabulary size has been set to the reference number of N type (cf. Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_58",
            "start": 66,
            "end": 300,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_58@2",
            "content": "Supplementary material additionally contains results for Morfessor baselines (Creutz and Lagus, 2002), with the corresponding weak supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_58",
            "start": 302,
            "end": 444,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_58@3",
            "content": "As a reminder, our supervision here consists of the first 200 sentences in the text, either directly given as observed boundaries or used to generate the initial word list.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_58",
            "start": 446,
            "end": 617,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_59@0",
            "content": "Using weak supervision",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_59",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_60@0",
            "content": "dpseg",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_60",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_61@0",
            "content": "Table 2 displays our experimental results for the 5K Mboshi corpus for SentencePiece (SP), dpseg and pypseg with various amounts of supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_61",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_62@0",
            "content": "First, the unsupervised dpseg model has better results than SP on all three levels by a significant margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_62",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_62@1",
            "content": "SP, on the other hand, produces more types as it 'knows' the actual number of types to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_62",
            "start": 108,
            "end": 203,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_63@0",
            "content": "Regarding segmentation boundaries, the gs.sparse model has disappointing results, with scores lower than the baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_63",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_63@1",
            "content": "On the other hand, the dense supervision manages to improve the baseline scores by around 2.5 points for BF, 4.5 points for WF, and 7.5 points for LF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_63",
            "start": 119,
            "end": 268,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_63@2",
            "content": "This is an encouraging result, since, with less than 5% of the whole text, the model has improved in a noticeable way, especially at type level, which seems to be difficult for fully unsupervised learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_63",
            "start": 270,
            "end": 474,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_64@0",
            "content": "When supervising with a word list, all models but d.2gram outperform the baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_64",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_64@1",
            "content": "Yet, the d.count and d.mix methods have lower scores than the gs.dense: this was expected for BF and WF-where directly supervising boundaries is likely to be more useful than an indirect one, but less so for LF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_64",
            "start": 83,
            "end": 293,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_64@2",
            "content": "Regarding the d.2gram model, its poor BF and WF scores are more than compensated by an increase of around 12 points in LF, showing the impact of a better type model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_64",
            "start": 295,
            "end": 459,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_64@3",
            "content": "Finally, by combining the d.mix and d.2gram strategies, d.mix+2gram obtains the overall best results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_64",
            "start": 461,
            "end": 561,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_65@0",
            "content": "pypseg",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_65",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_66@0",
            "content": "Results are in the right part of Table 2, where the baseline is the fully unsupervised pypseg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_66",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_66@1",
            "content": "It slightly outperforms dpseg by less than 1 point in terms of F-scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_66",
            "start": 95,
            "end": 166,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_66@2",
            "content": "In our setting, although PYP increases the number of discovered types, it does not improve the performance in any significant manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_66",
            "start": 168,
            "end": 300,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_67@0",
            "content": "This trend is confirmed for weakly supervised models: 8 the gs.dense model is the only one benefiting from a small improvement in all Fscores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_67",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_67@1",
            "content": "d.count underperforms both the baseline and its dpseg version.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_67",
            "start": 143,
            "end": 204,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_67@2",
            "content": "With worsened BF and WF scores compared to the baseline, d.mix+2gram with pypseg is worse than with dpseg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_67",
            "start": 206,
            "end": 311,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_67@3",
            "content": "Overall, the former seems to benefit less from annotations than the latter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_67",
            "start": 313,
            "end": 387,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_68@0",
            "content": "The performance of the bigram character model is noteworthy both with dpseg and pypseg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_68",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_68@1",
            "content": "This improvement alone (i.e. d.2gram) is responsible not only for a large increase in LF, but also for an average type length that gets much closer to its true value (6.39 in the reference, 6.60 with dpseg and d.mix+2gram).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_68",
            "start": 88,
            "end": 310,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_69@0",
            "content": "Results for Japhug",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_69",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_70@0",
            "content": "Table 3 displays a selection of results for Japhug (segmented in words).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_70",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_70@1",
            "content": "As previously observed, supervision noticeably improves the results for both models, with pypseg outperforming dpseg by a small margin on all metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_70",
            "start": 73,
            "end": 222,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_70@2",
            "content": "9 Note also that SP is much worse than Bayesian models, only reaching the same F-score as dpseg for the LF metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_70",
            "start": 224,
            "end": 337,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_71@0",
            "content": "The best results are obtained with lexical supervision and the d.mix+2gram model for dpseg: it combines the type boost in P 0 from d.mix and the improved base model from d.2gram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_71",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_71@1",
            "content": "32.9k 34.2k 36.6k 33.8k 34.3k 33.8k 25.1k 25.0k 33.9k 33.5k 33.7k 24.8k model (green) can be attributed to the use of the bigram character model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_71",
            "start": 179,
            "end": 323,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_71@2",
            "content": "It gives this model an initial edge over o.regular that remains significant for the first 3,000 sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_71",
            "start": 325,
            "end": 430,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_71@3",
            "content": "Here again, the benefits of improving the base distribution (character-based model) as much as possible in the early training iterations clearly appear.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_71",
            "start": 432,
            "end": 583,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_72@0",
            "content": "Incremental learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_72",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_73@0",
            "content": "Supervising words and morphemes",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_73",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_74@0",
            "content": "This section addresses a recurring issue in word segmentation model related to the linguistic nature of the units learnt by the model and the consequences of choosing one or the other reference in training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_74",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_74@1",
            "content": "The Japhug corpus contains both annotation levels and is a perfect test bed for this study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_74",
            "start": 207,
            "end": 297,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_74@2",
            "content": "We have thus used a segmentation model (dpseg) with and without weak supervision (using the d.mix+2gram variant) at the level of words or morphemes, and the results are also evaluated against the two references (a segmentation in words or in morphemes).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_74",
            "start": 299,
            "end": 551,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_74@3",
            "content": "Results are in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_74",
            "start": 553,
            "end": 575,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_75@0",
            "content": "In the unsupervised setting, segmentation metrics are markedly better with morpheme-based ref- With word supervision, we observe a shift in behaviour that is consistent with the provided annotations: better word-level metrics with word-based annotations, and accordingly, a decrease of performance for morpheme-based scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_75",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_75@1",
            "content": "With morpheme supervision, results are more contrasted: an improvement for word segmentation (because some words are also morphemes) that is not matched for morpheme boundaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_75",
            "start": 325,
            "end": 501,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_75@2",
            "content": "Looking at the detailed results (see appendix A.1, Table 7), one can see that this is due to an undersegmentation, which yields a poor recall at the boundary and token levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_75",
            "start": 503,
            "end": 677,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_75@3",
            "content": "Here, the main remaining benefit of supervision is an increase in the LF score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_75",
            "start": 679,
            "end": 757,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_76@0",
            "content": "These preliminary results suggest that considering only one type of boundary is a too naive view of the segmentation process and does not allow us to fully benefit from annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_76",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_76@1",
            "content": "They call for models that would carefully distinguish boundaries within words and between words, with appropriate supervision for each of these levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_76",
            "start": 185,
            "end": 335,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_77@0",
            "content": "Error analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_77",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@0",
            "content": "It is noteworthy that dictionary supervision almost deterministically ensures that the input word types will occur in the segmented output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@1",
            "content": "For instance, 96% of the words in the Mboshi supervision dictionary are found in the output of the d.mix+2gram method, whereas we only find 44% with fully unsupervised learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 140,
            "end": 316,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@2",
            "content": "Similar trends are observed for Japhug.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 318,
            "end": 356,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@3",
            "content": "Some remaining errors are, however, observed: in the example of Figure 3, the word 'bana' belongs to the supervision dictionary but remains attached to the following word 'ba'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 358,
            "end": 533,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@4",
            "content": "Additional examples are in appendix A.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 535,
            "end": 574,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@5",
            "content": "This may be because both words 'bana' and 'ba' often occur together, a cooccurrence that can not be captured by our unigram model .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 576,
            "end": 706,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_78@6",
            "content": "reference bana ba adi otEE imbva unsupervised banaba adio tEE imbva supervised banaba adi otEEimbva",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_78",
            "start": 708,
            "end": 806,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_79@0",
            "content": "Related work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_79",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@0",
            "content": "Unsupervised segmentation is a generic NLP task that can be performed at multiple levels of analysis: a document segmented in sections, a speech segmented in utterances, an utterance segmented in words, a word segmented in morphemes, syllables or phonemes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@1",
            "content": "It has been studied in multiple ways, and we report here recent work related to word discovery for language documentation, noting that the same methods also apply to the unsupervised segmentation of continuous speech into 'words' (de Marcken, 1996) which has given rise to a vast literature on language acquisition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 257,
            "end": 571,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@2",
            "content": "Recently, this task has become central in preprocessing pipelines, with new implementations of simple models (Sennrich et al., 2016;Kudo and Richardson, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 573,
            "end": 731,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@3",
            "content": "Linear segmentation models in the Bayesian realm can be traced back to (Goldwater et al., 2006.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 733,
            "end": 827,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@4",
            "content": "They were extended with nesting in (Mochihashi et al., 2009), where the base distribution of the Dirichlet Process is a char-based nonparametric model; and in (Uchiumi et al., 2015;L\u00f6ser and Allauzen, 2016), who consider hidden state variables in the word generation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 829,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@5",
            "content": "This extension enables, for instance, to jointly learn segmentation and PoS tagging or to introduce some morphotactics in the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 1105,
            "end": 1236,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@6",
            "content": "Other sources of weak supervisions along these lines concern the use of higher-order n-grams and of prosodic cues (Doyle and Levy, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 1238,
            "end": 1374,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_80@7",
            "content": "Finally, (B\u00f6rschinger and Johnson, 2012) (with particle filtering techniques) and (Neubig, 2014) (with block sampling) study ways to speed up inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_80",
            "start": 1376,
            "end": 1527,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_81@0",
            "content": "The unsupervised techniques exposed in Section 2 only depend on the design of a probabilistic word generation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_81",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_81@1",
            "content": "This means that they are also readily applicable when this process is conditioned to some input, for instance, when a translation is available as an additional information source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_81",
            "start": 119,
            "end": 297,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_81@2",
            "content": "This setup is notably studied in (Neubig et al., 2011;Stahlberg et al., 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017;Godard et al., 2018c).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_81",
            "start": 299,
            "end": 488,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@0",
            "content": "A somewhat richer trend of works aimed at informing word segmentation relies on the model of adaptor grammars (AG) of Johnson et al. (2007), applied to the segmentation task as early as (Johnson, 2008).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@1",
            "content": "AGs generalise finite-state models such as dpseg and pypseg by modelling trees and subtrees, rather than mere strings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 203,
            "end": 320,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@2",
            "content": "Their use necessitates a context-free description of the language, which enables to integrate information regarding word and syllable structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 322,
            "end": 466,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@3",
            "content": "Even generic descriptions can be useful, but finding the most appropriate and effective one is challenging Eskander et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 468,
            "end": 597,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@4",
            "content": "This formalism has also been used to introduce syntactic information , prosodic information (B\u00f6rschinger and Johnson, 2014), and partial annotations (Sirts and Goldwater, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 599,
            "end": 775,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@5",
            "content": "Recent software packages for AGs are presented in (Bernard et al., 2020) and (Eskander et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 777,
            "end": 877,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@6",
            "content": "Using AGs comes, however, with a high computational price, as the Gibbs sampling process typically requires repeated parses of the corpus, even though cheaper estimation techniques may also be considered (Cohen et al., 2010).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 879,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_82@7",
            "content": "As our goal is to integrate learning techniques in interactive annotation tools, AGs were not deemed appropriate, and we explored simpler alternatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_82",
            "start": 1105,
            "end": 1255,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_83@0",
            "content": "Similar arguments apply to the use of neural networks, which have attracted a growing interest even for very low-resource languages, combining supervised segmentation methods (Moeng et al., 2021;Liu et al., 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017;Kann et al., 2018;Lane and Bird, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_83",
            "start": 0,
            "end": 337,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_84@0",
            "content": "Conclusion and outlook",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_84",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_85@0",
            "content": "In this work, we have studied various ways to use weak supervision for automatic word segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_85",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_85@1",
            "content": "In language documentation scenarios, such supervision is often available, taking the form of a partial annotation or word lists.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_85",
            "start": 100,
            "end": 227,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_85@2",
            "content": "Bayesian non-parametric models lend themselves well to this setting, and our experiments have shown that two variants of a simple unigram model were getting a substantial boost from weak supervision, a result that has been obtained with two languages currently being documented.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_85",
            "start": 229,
            "end": 506,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_85@3",
            "content": "The most effective approach seems to start with a small set of fully segmented data, which helps learning in two ways: as a training signal for segmentation and as lexical prior for the base distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_85",
            "start": 508,
            "end": 711,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_85@4",
            "content": "Based on this observation, we have further evaluated the longer-term benefits of an incremental training regime and also contrasted the improvement obtained using a word-based vs a morpheme-based vocabulary list.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_85",
            "start": 713,
            "end": 924,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_86@0",
            "content": "Our future work will continue to explore the interplay between word and morpheme segmentations, as both are required in actual documentation settings, possibly extending our analyses on additional languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_86",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_86@1",
            "content": "We will also consider supervising the annotation process with lists of non-inflected forms, which requires to jointly learn inflectional patterns and segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_86",
            "start": 208,
            "end": 370,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_86@2",
            "content": "Finally, our main objective remains to integrate these techniques into an annotation platform and evaluate how much they help speed up the annotation process, hence the need to control the run-time of our algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_86",
            "start": 372,
            "end": 587,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_87@0",
            "content": "A.1 Full results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_87",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@0",
            "content": "Table 5 displays the complete results of Table 2 with both precision and recall for the three evaluation levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@1",
            "content": "SentencePiece (SP) tends to have more balanced scores for precision and recall, whereas dpseg displays a wider gap between the two metrics, especially at type level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 113,
            "end": 277,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@2",
            "content": "The 'Morf' column displays the performance of Morfessor 2.0 (Creutz and Lagus, 2002;Smit et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 279,
            "end": 381,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@3",
            "content": "10 These results have been obtained with the morph-length parameter set to the observed average token length (4.19).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 383,
            "end": 498,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@4",
            "content": "This setting led to better F-scores than using the gold number of types for num-morph-types or the default Morfessor model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 500,
            "end": 622,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@5",
            "content": "The Morfessor model outperforms SentencePiece significantly for both boundary (BF) and token (WF) F-scores, while it lags behind for the type-based metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 624,
            "end": 779,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_88@6",
            "content": "Compared to the unsupervised dpseg, Morfessor is worse on all accounts by a wide margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_88",
            "start": 781,
            "end": 868,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_89@0",
            "content": "Table 6, in turn, displays the complete results of Table 3, again with both precision and recall for the three evaluation levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_89",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_90@0",
            "content": "The 'Morf' column in Table 6 also represents the Morfessor results, with a morph-length parameter of 4.73.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_90",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_90@1",
            "content": "Here again, Morfessor outperforms Senten-cePiece on the boundary and token-level F-scores (to a smaller extent) but not at type level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_90",
            "start": 107,
            "end": 240,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_91@0",
            "content": "Finally, Table 7 displays the complete results for the word and morpheme experiment (Table 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_91",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@0",
            "content": "reference obengi amipasa koo sa k\u0159 unsupervised obengia mipasa koo sak\u0159 supervised obengi amipasa koo sak\u0159 Figure 4 shows an example sentence derived from the Mboshi data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@1",
            "content": "The word 'obengi' is present in the supervision dictionary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 172,
            "end": 230,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@2",
            "content": "In the unsupervised model (unsupervised line), the word was wrongly segmented, affecting the second word, 'amipasa'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 232,
            "end": 347,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@3",
            "content": "In the supervised model with d.mix+2gram, the word is correctly segmented as 'obengi', and the second word is also correct, although not in the supervision dictionary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 349,
            "end": 515,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@4",
            "content": "Figure 5 presents two of the 200 sentences used for supervision in Mboshi.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 517,
            "end": 590,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@5",
            "content": "This means that all the words in the example are in the supervision dictionary, which can explain why words such as 'owoi', 'atyeeli', or 'lekonyi' are correctly segmented in the weakly supervised setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 592,
            "end": 796,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@6",
            "content": "Yet, some errors remain (e.g. 'adimo' instead of 'adi mo') mainly because of the cooccurrence effect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 798,
            "end": 898,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_92@7",
            "content": "reference atyeeli adi mo lekonyi unsupervised at yee li adi mole konyi supervised atyeeli adimo lekonyi reference n\u0159 owoi dzue la baa unsupervised n\u0159 o wo i dzuela baa supervised n\u0159 owoi dzue la baa",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_92",
            "start": 900,
            "end": 1097,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_93@0",
            "content": "Gilles Adda, Sebastian St\u00fcker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Blachon, H\u00e9l\u00e8ne Bonneau-Maynard, Pierre Godard, Fatima Hamlaoui, Dmitri Idiatov, Guy-No\u00ebl Kouarata, Lori Lamel, Breaking the Unwritten Language Barrier: The Bulb Project, 2016, Proceedings of SLTU (Spoken Language Technologies for Under-Resourced Languages), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_93",
            "start": 0,
            "end": 353,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_94@0",
            "content": "Grant Aiton, Translating fieldwork into datasets: The development of a corpus for the quantitative investigation of grammatical phenomena in Eibela, 2021, Proceedings of the Workshop on Computational Methods for Endangered Languages, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_94",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_95@0",
            "content": "Antonios Anastasopoulos, David Chiang, A case study on using speech-to-translation alignments for language documentation, 2017, Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_95",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_96@0",
            "content": "Mathieu Bernard, Roland Thiolliere, Amanda Saksida, Georgia Loukatou, Elin Larsen, Mark Johnson, Laia Fibla, Emmanuel Dupoux, Robert Daland, Xuan Cao, Alejandrina Cristia, Wordseg: Standardizing unsupervised word form segmentation from text, 2020, Behavior Research Methods, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_96",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_97@0",
            "content": "Steven Bird, Decolonising Speech and Language Technology, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_97",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_98@0",
            "content": "Benjamin B\u00f6rschinger, Mark Johnson, Using rejuvenation to improve particle filtering for bayesian word segmentation, 2012, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_98",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_99@0",
            "content": "Benjamin B\u00f6rschinger, Mark Johnson, Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_99",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_100@0",
            "content": "Shay Cohen, Bayesian Analysis in Natural Language Processing, 2016, Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_100",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_101@0",
            "content": "B Shay, David Cohen, Noah Blei,  Smith, Variational inference for Adaptor Grammars, 2010, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_101",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_102@0",
            "content": "Mathias Creutz, Krista Lagus, Unsupervised discovery of morphemes, 2002, Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_102",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_103@0",
            "content": "UNKNOWN, None, 1996, Unsupervised Language Acquisition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_103",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_104@0",
            "content": "Gabriel Doyle, Roger Levy, Combining multiple information types in Bayesian word segmentation, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_104",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_105@0",
            "content": "UNKNOWN, None, 2013, Processus segmentaux et tonals en Mbondzi -(vari\u00e9t\u00e9 de la langue embosi C25) -. Theses, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_105",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_106@0",
            "content": "Ramy Eskander, Francesca Callejas, Elizabeth Nichols, Judith Klavans, Smaranda Muresan, Mor-phAGram, evaluation and framework for unsupervised morphological segmentation, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_106",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_107@0",
            "content": "Ramy Eskander, Judith Klavans, Smaranda Muresan, Unsupervised morphological segmentation for low-resource polysynthetic languages, 2019, Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_107",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_108@0",
            "content": "Ramy Eskander, Owen Rambow, Tianchun Yang, Extending the use of Adaptor Grammars for unsupervised morphological segmentation of unseen languages, 2016, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_108",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_109@0",
            "content": "Pierre Godard, Gilles Adda, Martine Adda-Decker, Alexandre Allauzen, Laurent Besacier, H\u00e9l\u00e8ne Bonneau-Maynard, Guy-No\u00ebl Kouarata, Kevin L\u00f6ser, Annie Rialland, Fran\u00e7ois Yvon, Preliminary Experiments on Unsupervised Word Discovery in Mboshi, 2016, Proceedings of Interspeech 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_109",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_110@0",
            "content": "Pierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, Jamison Cooper-Leavitt, Guy-Noel Kouarata, Lori Lamel, H\u00e9l\u00e8ne Maynard, Markus Mueller, Annie Rialland, Sebastian Stueker, Fran\u00e7ois Yvon, and Marcely Zanon-Boito. 2018a. A very low resource language speech corpus for computational language documentation experiments, , Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_110",
            "start": 0,
            "end": 453,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_111@0",
            "content": "Pierre Godard, Laurent Besacier, Fran\u00e7ois Yvon, Martine Adda-Decker, Gilles Adda, H\u00e9l\u00e8ne Maynard, Annie Rialland, Adaptor Grammars for the linguist: Word segmentation experiments for very low-resource languages, 2018, Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_111",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_112@0",
            "content": "Pierre Godard,  Marcely Zanon, Lucas Boito, Alexandre Ondel, Fran\u00e7ois Berard,  Yvon, Unsupervised word segmentation from speech with attention, 2018, Proc. Interspeech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_112",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_113@0",
            "content": "Sharon Goldwater, Thomas Griffiths, Mark Johnson, Contextual dependencies in unsupervised word segmentation, 2006, Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_113",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2009, A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_114",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_115@0",
            "content": "UNKNOWN, None, 2021, A grammar of Japhug. Number 1 in Comprehensive Grammar Library, Language Science Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_115",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_116@0",
            "content": "Mark Johnson, Unsupervised word segmentation for Sesotho using adaptor grammars, 2008, Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_116",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_117@0",
            "content": "Mark Johnson, Anne Christophe, Emmanuel Dupoux, Katherine Demuth, Modelling function words improves unsupervised word segmentation, 2014, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_117",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_118@0",
            "content": "Mark Johnson, Sharon Goldwater, Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars, 2009, Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_118",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_119@0",
            "content": "Mark Johnson, Thomas Griffiths, Sharon Goldwater, Adaptor Grammars: a Framework for Specifying Compositional Nonparametric Bayesian Models, 2007, Advances in Neural Information Processing Systems, MIT Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_119",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_120@0",
            "content": "Katharina Kann, Jesus Manuel Mager, Ivan Hois, Hinrich Meza-Ruiz,  Sch\u00fctze, Fortification of neural morphological segmentation models for polysynthetic minimal-resource languages, 2018-01, Proceedings of the 2018 Conference Tumi Moeng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_120",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_121@0",
            "content": "UNKNOWN, None, 2014, Simple, correct parallelization for blocked Gibbs sampling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_121",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_122@0",
            "content": "Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, Tatsuya Kawahara, An unsupervised model for joint phrase alignment and extraction, 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_122",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_123@0",
            "content": "UNKNOWN, None, 2015, Dropping of the class-prefix consonant, vowel elision and automatic phonological mining in Embosi, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_123",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_124@0",
            "content": "UNKNOWN, None, , Proceedings of the 44th Annual Conference on African Linguistics, Somerville. Cascadilla.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_124",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_125@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_125",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_126@0",
            "content": "Miikka Silfverberg, Adam Wiemerslage, Ling Liu, Lingshuang Jack Mao, Data augmentation for morphological reinflection, 2017, Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_126",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_127@0",
            "content": "Kairit Sirts, Sharon Goldwater, Minimallysupervised morphological segmentation using Adaptor Grammars, 2013, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_127",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_128@0",
            "content": "Peter Smit, Sami Virpioja, Stig-Arne Gr\u00f6nroos, Mikko Kurimo, Morfessor 2.0: Toolkit for statistical morphological segmentation, 2014, Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_128",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_129@0",
            "content": "Felix Stahlberg, Tim Schlippe, Stephan Vogel, Tanja Schultz, Word segmentation through cross-lingual word-to-phoneme alignment, 2012, Spoken Language Technology Workshop (SLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_129",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_130@0",
            "content": ", None, , IEEE, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_130",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_131@0",
            "content": "UNKNOWN, None, 2006, A Bayesian interpretation of interpolated Kneser-Ney, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_131",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_132@0",
            "content": "Yee Whye Teh, A hierarchical Bayesian language model based on Pitman-Yor processes, 2006, Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_132",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "355-ARR_v2_133@0",
            "content": ", Complete results on the 5K Mboshi text for various models and weak supervision settings (20K iterations, 200 supervision sentences, \u03bb = 0.25), , SP stands for SentencePiece, Morf for Morfessor, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "355-ARR_v2_133",
            "start": 0,
            "end": 196,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_1",
            "tgt_ix": "355-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_1",
            "tgt_ix": "355-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_4",
            "tgt_ix": "355-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_5",
            "tgt_ix": "355-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_6",
            "tgt_ix": "355-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_7",
            "tgt_ix": "355-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_9",
            "tgt_ix": "355-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_10",
            "tgt_ix": "355-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_11",
            "tgt_ix": "355-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_12",
            "tgt_ix": "355-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_13",
            "tgt_ix": "355-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_14",
            "tgt_ix": "355-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_15",
            "tgt_ix": "355-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_16",
            "tgt_ix": "355-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_18",
            "tgt_ix": "355-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_19",
            "tgt_ix": "355-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_20",
            "tgt_ix": "355-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_22",
            "tgt_ix": "355-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_24",
            "tgt_ix": "355-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_23",
            "tgt_ix": "355-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_23",
            "tgt_ix": "355-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_23",
            "tgt_ix": "355-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_23",
            "tgt_ix": "355-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_26",
            "tgt_ix": "355-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_28",
            "tgt_ix": "355-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_30",
            "tgt_ix": "355-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_31",
            "tgt_ix": "355-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_33",
            "tgt_ix": "355-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_34",
            "tgt_ix": "355-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_36",
            "tgt_ix": "355-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_38",
            "tgt_ix": "355-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_37",
            "tgt_ix": "355-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_37",
            "tgt_ix": "355-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_37",
            "tgt_ix": "355-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_37",
            "tgt_ix": "355-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_40",
            "tgt_ix": "355-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_42",
            "tgt_ix": "355-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_44",
            "tgt_ix": "355-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_45",
            "tgt_ix": "355-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_47",
            "tgt_ix": "355-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_51",
            "tgt_ix": "355-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_50",
            "tgt_ix": "355-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_50",
            "tgt_ix": "355-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_50",
            "tgt_ix": "355-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_52",
            "tgt_ix": "355-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_54",
            "tgt_ix": "355-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_53",
            "tgt_ix": "355-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_53",
            "tgt_ix": "355-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_53",
            "tgt_ix": "355-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_53",
            "tgt_ix": "355-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_56",
            "tgt_ix": "355-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_58",
            "tgt_ix": "355-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_59",
            "tgt_ix": "355-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_61",
            "tgt_ix": "355-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_62",
            "tgt_ix": "355-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_63",
            "tgt_ix": "355-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_64",
            "tgt_ix": "355-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_66",
            "tgt_ix": "355-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_67",
            "tgt_ix": "355-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_65",
            "tgt_ix": "355-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_65",
            "tgt_ix": "355-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_65",
            "tgt_ix": "355-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_65",
            "tgt_ix": "355-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_68",
            "tgt_ix": "355-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_70",
            "tgt_ix": "355-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_69",
            "tgt_ix": "355-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_69",
            "tgt_ix": "355-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_69",
            "tgt_ix": "355-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_71",
            "tgt_ix": "355-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_72",
            "tgt_ix": "355-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_74",
            "tgt_ix": "355-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_75",
            "tgt_ix": "355-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_73",
            "tgt_ix": "355-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_73",
            "tgt_ix": "355-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_73",
            "tgt_ix": "355-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_73",
            "tgt_ix": "355-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_76",
            "tgt_ix": "355-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_77",
            "tgt_ix": "355-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_77",
            "tgt_ix": "355-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_81",
            "tgt_ix": "355-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_83",
            "tgt_ix": "355-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_87",
            "tgt_ix": "355-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_89",
            "tgt_ix": "355-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_90",
            "tgt_ix": "355-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_86",
            "tgt_ix": "355-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_91",
            "tgt_ix": "355-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "355-ARR_v2_0",
            "tgt_ix": "355-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_1",
            "tgt_ix": "355-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_2",
            "tgt_ix": "355-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_3",
            "tgt_ix": "355-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_4",
            "tgt_ix": "355-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_4",
            "tgt_ix": "355-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_5",
            "tgt_ix": "355-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_5",
            "tgt_ix": "355-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_5",
            "tgt_ix": "355-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_5",
            "tgt_ix": "355-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_6",
            "tgt_ix": "355-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_6",
            "tgt_ix": "355-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_7",
            "tgt_ix": "355-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_7",
            "tgt_ix": "355-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_7",
            "tgt_ix": "355-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_8",
            "tgt_ix": "355-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_9",
            "tgt_ix": "355-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_9",
            "tgt_ix": "355-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_9",
            "tgt_ix": "355-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_10",
            "tgt_ix": "355-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_11",
            "tgt_ix": "355-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_11",
            "tgt_ix": "355-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_11",
            "tgt_ix": "355-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_12",
            "tgt_ix": "355-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_12",
            "tgt_ix": "355-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_12",
            "tgt_ix": "355-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_13",
            "tgt_ix": "355-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_13",
            "tgt_ix": "355-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_14",
            "tgt_ix": "355-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_15",
            "tgt_ix": "355-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_15",
            "tgt_ix": "355-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_15",
            "tgt_ix": "355-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_15",
            "tgt_ix": "355-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_16",
            "tgt_ix": "355-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_17",
            "tgt_ix": "355-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_18",
            "tgt_ix": "355-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_18",
            "tgt_ix": "355-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_18",
            "tgt_ix": "355-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_19",
            "tgt_ix": "355-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_19",
            "tgt_ix": "355-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_20",
            "tgt_ix": "355-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_20",
            "tgt_ix": "355-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_21",
            "tgt_ix": "355-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_22",
            "tgt_ix": "355-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_23",
            "tgt_ix": "355-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_24",
            "tgt_ix": "355-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_24",
            "tgt_ix": "355-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_24",
            "tgt_ix": "355-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_24",
            "tgt_ix": "355-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_25",
            "tgt_ix": "355-ARR_v2_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_26",
            "tgt_ix": "355-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_26",
            "tgt_ix": "355-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_27",
            "tgt_ix": "355-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_28",
            "tgt_ix": "355-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_28",
            "tgt_ix": "355-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_29",
            "tgt_ix": "355-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_30",
            "tgt_ix": "355-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_31",
            "tgt_ix": "355-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_32",
            "tgt_ix": "355-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_33",
            "tgt_ix": "355-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_34",
            "tgt_ix": "355-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_35",
            "tgt_ix": "355-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_36",
            "tgt_ix": "355-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_37",
            "tgt_ix": "355-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_38",
            "tgt_ix": "355-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_38",
            "tgt_ix": "355-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_39",
            "tgt_ix": "355-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_40",
            "tgt_ix": "355-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_41",
            "tgt_ix": "355-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_42",
            "tgt_ix": "355-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_43",
            "tgt_ix": "355-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_44",
            "tgt_ix": "355-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_44",
            "tgt_ix": "355-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_44",
            "tgt_ix": "355-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_45",
            "tgt_ix": "355-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_45",
            "tgt_ix": "355-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_46",
            "tgt_ix": "355-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_47",
            "tgt_ix": "355-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_47",
            "tgt_ix": "355-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_48",
            "tgt_ix": "355-ARR_v2_48@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_49",
            "tgt_ix": "355-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_50",
            "tgt_ix": "355-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_51",
            "tgt_ix": "355-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_51",
            "tgt_ix": "355-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_51",
            "tgt_ix": "355-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_52",
            "tgt_ix": "355-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_53",
            "tgt_ix": "355-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_54",
            "tgt_ix": "355-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_54",
            "tgt_ix": "355-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_55",
            "tgt_ix": "355-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_56",
            "tgt_ix": "355-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_56",
            "tgt_ix": "355-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_57",
            "tgt_ix": "355-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_58",
            "tgt_ix": "355-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_58",
            "tgt_ix": "355-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_58",
            "tgt_ix": "355-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_58",
            "tgt_ix": "355-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_59",
            "tgt_ix": "355-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_60",
            "tgt_ix": "355-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_61",
            "tgt_ix": "355-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_62",
            "tgt_ix": "355-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_62",
            "tgt_ix": "355-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_63",
            "tgt_ix": "355-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_63",
            "tgt_ix": "355-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_63",
            "tgt_ix": "355-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_64",
            "tgt_ix": "355-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_64",
            "tgt_ix": "355-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_64",
            "tgt_ix": "355-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_64",
            "tgt_ix": "355-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_65",
            "tgt_ix": "355-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_66",
            "tgt_ix": "355-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_66",
            "tgt_ix": "355-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_66",
            "tgt_ix": "355-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_67",
            "tgt_ix": "355-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_67",
            "tgt_ix": "355-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_67",
            "tgt_ix": "355-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_67",
            "tgt_ix": "355-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_68",
            "tgt_ix": "355-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_68",
            "tgt_ix": "355-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_69",
            "tgt_ix": "355-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_70",
            "tgt_ix": "355-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_70",
            "tgt_ix": "355-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_70",
            "tgt_ix": "355-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_71",
            "tgt_ix": "355-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_71",
            "tgt_ix": "355-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_71",
            "tgt_ix": "355-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_71",
            "tgt_ix": "355-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_72",
            "tgt_ix": "355-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_73",
            "tgt_ix": "355-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_74",
            "tgt_ix": "355-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_74",
            "tgt_ix": "355-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_74",
            "tgt_ix": "355-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_74",
            "tgt_ix": "355-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_75",
            "tgt_ix": "355-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_75",
            "tgt_ix": "355-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_75",
            "tgt_ix": "355-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_75",
            "tgt_ix": "355-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_76",
            "tgt_ix": "355-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_76",
            "tgt_ix": "355-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_77",
            "tgt_ix": "355-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_78",
            "tgt_ix": "355-ARR_v2_78@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_79",
            "tgt_ix": "355-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_80",
            "tgt_ix": "355-ARR_v2_80@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_81",
            "tgt_ix": "355-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_81",
            "tgt_ix": "355-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_81",
            "tgt_ix": "355-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_82",
            "tgt_ix": "355-ARR_v2_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_83",
            "tgt_ix": "355-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_84",
            "tgt_ix": "355-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_85",
            "tgt_ix": "355-ARR_v2_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_86",
            "tgt_ix": "355-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_86",
            "tgt_ix": "355-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_86",
            "tgt_ix": "355-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_87",
            "tgt_ix": "355-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_88",
            "tgt_ix": "355-ARR_v2_88@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_89",
            "tgt_ix": "355-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_90",
            "tgt_ix": "355-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_90",
            "tgt_ix": "355-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_91",
            "tgt_ix": "355-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_92",
            "tgt_ix": "355-ARR_v2_92@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_93",
            "tgt_ix": "355-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_94",
            "tgt_ix": "355-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_95",
            "tgt_ix": "355-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_96",
            "tgt_ix": "355-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_97",
            "tgt_ix": "355-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_98",
            "tgt_ix": "355-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_99",
            "tgt_ix": "355-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_100",
            "tgt_ix": "355-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_101",
            "tgt_ix": "355-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_102",
            "tgt_ix": "355-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_103",
            "tgt_ix": "355-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_104",
            "tgt_ix": "355-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_105",
            "tgt_ix": "355-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_106",
            "tgt_ix": "355-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_107",
            "tgt_ix": "355-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_108",
            "tgt_ix": "355-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_109",
            "tgt_ix": "355-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_110",
            "tgt_ix": "355-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_111",
            "tgt_ix": "355-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_112",
            "tgt_ix": "355-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_113",
            "tgt_ix": "355-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_114",
            "tgt_ix": "355-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_115",
            "tgt_ix": "355-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_116",
            "tgt_ix": "355-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_117",
            "tgt_ix": "355-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_118",
            "tgt_ix": "355-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_119",
            "tgt_ix": "355-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_120",
            "tgt_ix": "355-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_121",
            "tgt_ix": "355-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_122",
            "tgt_ix": "355-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_123",
            "tgt_ix": "355-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_124",
            "tgt_ix": "355-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_125",
            "tgt_ix": "355-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_126",
            "tgt_ix": "355-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_127",
            "tgt_ix": "355-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_128",
            "tgt_ix": "355-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_129",
            "tgt_ix": "355-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_130",
            "tgt_ix": "355-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_131",
            "tgt_ix": "355-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_132",
            "tgt_ix": "355-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "355-ARR_v2_133",
            "tgt_ix": "355-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 909,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "355-ARR",
        "version": 2
    }
}