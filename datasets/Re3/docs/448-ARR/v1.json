{
    "nodes": [
        {
            "ix": "448-ARR_v1_0",
            "content": "Ranking-Constrained Learning with Rationales for Text Classification",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_2",
            "content": "We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationaleaugmented baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "448-ARR_v1_4",
            "content": "Text classification has been used for numerous applications including sentiment analysis (Hemmatian and Sohrabi, 2019), information retrieval (Aggarwal and Zhai, 2012), and language identification (Jauhiainen et al., 2019). When presented with a large number of labeled documents, common text classification models demonstrate impressive results. In practical settings, however, labeled data is often scarce. Labeling documents is a tedious task that requires time and effort, thus curating a large labeled corpus can be expensive and even unrealistic.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_5",
            "content": "There is a wide range of use cases for businesses and industry that require curating a labeled dataset for the current task before the need to move on to the next task arises. For example, consider legal case document classification where documents need to be labeled as relevant/not-relevant to the current case at hand. The next legal case requires labeling the documents as relevant/not-relevant for that particular case, and so on. Similarly, several fast-response tasks such as immediate analysis of news and social media posts for a breaking news, for a recently released product, for a policy announcement, etc., require fast curation of a small and yet informative labeled dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_6",
            "content": "An effective approach to make the best use of the human's time and maximize classifier performance with a small labeled dataset is to elicit rich feedback, in the form of rationales for classification, during the labeling process (Zaidan et al., 2007Donahue and Grauman, 2011;Sharma and Bilgic, 2018). For sentiment classification, for example, the annotators might highlight certain segments of the text that convinced them to label the review as positive. Unlike humans, a classifier will not know which segments of the document are responsible for its label during training, until it has been presented with many training samples. Since the human annotators read the document to decide its label in the first place, they have already spent the time to find the justifications for their labeling decision; hence, previous studies have shown that the extra time needed to highlight a piece of the text as a rationale for its label is not high and is often worth more (for improving the classifier) than spending that time to label an additional document. Zaidan et al. (2007) showed that rationale annotation has low overhead, roughly twice the time required for annotating only the labels. Sharma and Bilgic (2018) showed that annotating a single document with rationales can be worth as many as 20 documents that are simply annotated with labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_7",
            "content": "Prior work on learning with rationales focused on one-hot encoding of the text in combination with logistic regression and support vector machines (Zaidan et al., 2007;Sharma and Bilgic, 2018), deep learning with multi-task learning (Melamud et al., 2019), and rationale-augmented attentionbased models (Bahdanau et al., 2014), which still required a large set of labeled documents. We propose a general approach that is applicable to both one-hot encoding as well as deep learning embedding representations and that is highly effective under limited labeling settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_8",
            "content": "The rationale supervision can be understood as an expectation that a document should have a higher probability of belonging to its class than the same document from which the rationale(s) are removed. Motivated by this intuition, we formulate a hybrid loss function to combine classification loss with ranking constraints for rationale supervision, which serves as an effective way of directing the model's focus to rationales during training. Our contributions in this paper include: \u2022 We formulate a general and effective learningwith-rationales method for text classification. \u2022 We study its empirical effectiveness on three human-annotated text classification datasets (sentiment analysis, aviation safety, and scientific articles). \u2022 We compare our method to several baselines, and empirical findings show that it achieves the stateof-the-art results. For example, our proposed method is able to achieve 80% accuracy on the IMDb movie review dataset (Zaidan et al., 2007) with as few as 23 documents, whereas a finetuned BERT model that does not use rationales required 73 documents, and the most competitive rationale-augmented baseline required 63 documents to achieve the same level of accuracy. \u2022 We annotate a new text classification dataset with rationales and make it publicly available. The rest of the paper is organized as follows. We first discuss related work and how our work differs from previous work in Section 2. We formalize our learning with rationales approach in Section 3 and detail the experimental methodology in Section 4, followed by a discussion of the results in Section 5. We discuss the limitations and future work in Section 6 and then conclude.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_9",
            "content": "2 Related Work Zaidan et al. (2007) presented one of the first approaches to learning with rationales for text classification. They proposed to utilize humanprovided rationales by converting the rationales into constraints for training support vector machines. They later extended the framework to a rationale-constrained probabilistic model . Sharma and Bilgic (2018) proposed a general method to incorporate rationales into the training of any classifier by weighting the rationale features higher than the non-rationale features. However, their method relied on using a bag-of-words representation of the documents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_10",
            "content": "As deep learning achieved the state-of-the-art performance on text classification (e.g., (Sun et al., 2019;Devlin et al., 2018;Zhang et al., 2015;Yang et al., 2016)), recent work proposed methods specifically for training deep learning models using rationale supervision. Some methods utilized the rationales to generate rationale-augmented representations of the text while others utilized the rationales for richer supervision of the model. For instance, Zhang et al. (2016) proposed a Rationale-Augmented CNN (RA-CNN) that jointly learns from the labels of the documents as well as the labels at the sentence level, by using a two-step approach. However, their approach still requires sufficient amounts of data for training a model at the sentence level to learn a valid rationale-augmented representation of a document. Errica et al. (2020) proposed a representation learning approach to leverage rationales by learning to focus on relevant input tokens in the embedding space. Bao et al. (2018) proposed a framework to derive machine attentions from human-provided rationales. Sastry and Milios (2020) defined a new attribution score for words by computing the partial derivative of the output with respect to the input in the word embedding space, and used misattribution error as an additional supervision in the loss function. Our method has two major differences from these work: i) our approach can use but does not require an attention mechanism to focus on the rationales and ii) our approach does not require learning a separate representation for the rationales.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_11",
            "content": "The work most closely related to ours is the model proposed by Melamud et al. (2019), which jointly learns to predict the labels for text as well as the labels for each token of every input sentence by determining whether the token is part of the rationales or not. Our approach differs from theirs as our ranking loss is calculated by using only the model's predictions, rather than introducing auxiliary learning tasks. Moreover, the approach we propose is more general: it can be used for any model that can utilize a logistic loss, ranging from a logistic regression model coupled with a one-hot encoding of words to a Long Short-Term Memory (LSTM) model coupled with word embeddings. In their same paper, Melamud et al. (2019) proposed another method that utilizes rationales by constructing rationale prototypes and rationale-biased text vectors. However, these vectors are computed using a rationale-bias function to directly estimate the similarity between words and annotated rationales without incorporating any learning, and thus this method works well only for few-shot learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_12",
            "content": "Learning with Rationales",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "448-ARR_v1_13",
            "content": "Let D = {x 1 , x 2 , \u2022 \u2022 \u2022 , x n } be a set of documents.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_14",
            "content": "A small subset of the documents, L \u2282 D, are annotated with labels, x i , y i where the value of y i belongs to a label space, C = {c 1 , c 2 , \u2022 \u2022 \u2022 , c k }. y i is unknown for a much larger set of unlabeled documents, U = D \\ L, represented as x i , ? . Each document, x i , contains a number of sentences, {s i1 , s i2 , ..., s im }, each of which is represented as a sequence of words:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_15",
            "content": "s ij = {q 1 ij , q 2 ij , \u2022 \u2022 \u2022 , q l ij }.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_16",
            "content": "In the learning with rationales framework, a subset of the words is marked by the human annotator as rationales (i.e., justifications for the document's assigned label). Let r i = q l ij be the set of all words that are marked as rationales within a document, x i . It is possible that none of the words are marked as rationales, and hence, r i = \u2205 for such documents. In the learning-with-rationales setting, L is modified to contain x i , r i , y i and U represents x i , \u2205, ? . The objective is to train a model, f , that utilizes the documents x i , their labels y i , and their rationales r i during training, and uses only the documents x i at prediction time, as rationales are naturally not available for the test documents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_17",
            "content": "Our Approach -LwR-RC",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "448-ARR_v1_18",
            "content": "We first describe our proposed approach, Learning with Rationales -Ranking-Constrained (LwR-RC), and then illustrate how it can be specialized for training deep learning models. To illustrate the motivation behind our approach, consider an example document, D, that contains three sentences: \"s1: The movie came out last year. s2: The plot was decent. s3: Acting was superb.\", which is labeled as 'positive' by the annotator. Assume for the sake of example, the annotator highlights only s3 as the rationale. Let M be a masked document that is same as the original document D, but from which the sentences containing the rationale phrases are removed. In this case, M would be missing s3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_19",
            "content": "We postulate that the model should be more sure about the positive label of document D than the label of document M, since D contains the essential evidence, 'Acting was superb', for the 'positive' label, whereas M lacks that evidence. Similarly, let R be the document that contains only the rationale sentence s3. We postulate that the model should be more sure about the label 'positive' of R than the label of M, since R provides strong evidence for the label, whereas M lacks that evidence. 1 Traditional learning without rationales approaches optimize a loss function to compute the model's error on its predictions, e.g., a binary crossentropy classification loss, L clf , is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_20",
            "content": "L clf = \u2212 1 |L| i (y i \u2022 log(p(y i |x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x i )))(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_21",
            "content": "In order to leverage the annotated rationales, we formalize our postulations by providing the model with two additional objectives during training. The first objective is to train the model to be more confident about the label of a document (D) than the label of the same document in which the rationales are masked (M). The second objective is to train the model to be more confident about the label of document that contains only the rationales (R) than the label of the same document in which the rationales are masked (M). We achieve these objectives by using a ranking-constrained classification approach, as described next. Let x i , r i , y i \u2208 L be a training document. First, we construct an artificial document x i by masking out all the sentences that contain rationales r i . We construct another artificial document x r i consisting of only the sentences that contain rationales r i . The ranking-constrained classification approach incorporates the rationales into learning by modeling two expectations: (i) the model should be more sure of assigning the correct label y i to x i than assigning y i to x i , because x i represents a document from which the rationales have been removed, and we refer to this objective as 'Document versus Masked document' (DvM), where D represents x i and M represents x i , and (ii) the model should be more sure of assigning the correct label y i to x r i than assigning y i to x i , and we refer to this objective as 'Rationale versus Masked document' (RvM), where R represents x r i and M represents x i . Another possible objective can be 'Rationale versus Document' (RvD), however, we excluded RvD objective from our approach for the following reason. Consider the following cases for a binary (positive/negative) classification task:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_22",
            "content": "\u2022 Case 1: D = R+M is positive; R is positive; M is",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_23",
            "content": "neutral or it contains a small amount of leftover positive. In this case, RvD requires R > R+M, which forces M to be negative, whereas RvM requires R > M, which does not necessarily require M to be negative. Thus, RvD is guaranteed to be the wrong approach. RvM forces R > M, but gives the model the flexibility to decide whether M is a small positive, neutral, or negative. \u2022 Case 2: D = R+M is positive; R is positive; M is negative. In this case, RvD requires R > R+M, which forces M to be negative, whereas RvM simply requires R > M. In this case, RvD is the correct choice, but RvM cannot be called the guaranteed wrong choice. \u2022 Remaining cases: The cases where D and R are negative are similar. As the cases above show, RvM is more flexible: RvM simply nudges the model in the correct direction and leaves the judgement about M to the data. RvD, on the other hand, is a more forceful approach; it forces the model to always make a judgement about M, which is the incorrect judgement in case 1. Thus, we include only the RvM and DvM objectives in our proposed approach.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_24",
            "content": "Formally, let",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_25",
            "content": "y i \u2208 {0, 1}: f (x i ) = p(y i = 1 | x i ) = sigmoid(W z z i ) for some parameter matrix W z",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_26",
            "content": ", where z i is the vector representation of x i . For modeling the DvM objective, let \u00b5 i = W z z i and \u00b5 i = W z z i where z i is the vector representation of x i . If the correct label is y i = 1, we would like \u00b5 i > 0 and \u00b5 i > \u00b5 i . If the correct label is y i = 0, we would like \u00b5 i < 0 and \u00b5 i < \u00b5 i . We convert this constraint into a logistic loss, as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_27",
            "content": "L i DvM = log(1 + exp(\u2212(\u00b5 i \u2212 \u00b5 i ))), y i = 1 log(1 + exp(\u2212(\u00b5 i \u2212 \u00b5 i ))), y i = 0 (2) Summing L i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_28",
            "content": "DvM over all the training instances and reorganizing the terms, we get:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_29",
            "content": "L DvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x i , x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x i , x i )))(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_30",
            "content": "where,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_31",
            "content": "p(y i |x i , x i ) = 1 1 + e \u2212(\u00b5 i \u2212\u00b5 i ) (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_32",
            "content": "We define the ranking loss similarly for the RvM component, using documents R and M and their respective scores \u00b5 r i = W z z r i and \u00b5 i = W z z i , where z r i is the vector representation of x r i . The ranking loss L RvM is then defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_33",
            "content": "L RvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x r i , x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x r i , x i )))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_34",
            "content": "(5) where,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_35",
            "content": "p(y i |x r i , x i ) = 1 1 + e \u2212(\u00b5 r i \u2212\u00b5 i ) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_36",
            "content": "We combine the classification loss L clf with the ranking losses, L DvM and L RvM , resulting in the main objective function for our approach:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_37",
            "content": "L = (1\u2212\u03bb 1 \u2212\u03bb 2 )L clf +\u03bb 1 L DvM +\u03bb 2 L RvM (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_38",
            "content": "where, 0 \u2264 \u03bb 1 \u2264 1, 0 \u2264 \u03bb 2 \u2264 1, and \u03bb 1 + \u03bb 2 \u2264 1. \u03bb 1 and \u03bb 2 are two hyper-parameters that control the importance of the classification loss and the ranking losses relative to one another. We study the effect of these hyper-parameters on the model's performance and provide insights into their relative importance in Section 5.2. We next describe how LwR-RC can be implemented through a neural network architecture, which can be specialized to a logistic regression or to a deep learning model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_39",
            "content": "LwR-RC with Deep Learning",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "448-ARR_v1_40",
            "content": "Figure 1 shows the deep learning architecture illustrating how the LwR-RC approach can minimize the loss function of Equation ( 7). For every sentence {s i1 , s i2 , ..., s im } within a document x i , we use an embedding model to create sentence embedding vectors {t i1 , t i2 , ..., t im }, and pass them through an average pooling layer to create a single vector, z i , representing a document. Similarly, the same sentence embedding vectors are passed through two different pooling layers to create two masked averages, z i and z r i , representing the document without rationales and the document containing only the rationales, respectively. There are several strategies for aggregating many sentence vectors into a single document vector; we use the average pooling strategy for the experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_41",
            "content": "The LwR-RC approach can be used to train any model that uses cross-entropy loss functions, including logistic regression and deep neural networks. It can also work with several representations, including one-hot encoding of the words, word2vec (Mikolov et al., 2013), and doc2vec (Le and Mikolov, 2014), as well as more recent language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). For example, if we remove the embedding layer and the hidden layers, and represent the sentences using one-hot encoding of the words, we would get a simple logistic regression classifier. If we use BERT for encoding the sentences in the embedding layer, then we can either use BERT embeddings directly or fine-tune the BERT model on downstream classification tasks by optimizing the ranking-constrained loss function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_42",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "448-ARR_v1_43",
            "content": "In this section, we describe the three datasets, several baselines, and the experimental settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_44",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "448-ARR_v1_45",
            "content": "We used two publicly available datasets: a sentiment classification dataset and an aviation safety dataset. Both datasets were annotated with labels and rationales. Additionally, we introduce a new scientific article classification dataset that we annotated with labels and rationales.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_46",
            "content": "IMDb is a movie review dataset annotated by Zaidan et al. (2007). It consists of 1,800 documents. We used 600 reviews as the training set, 600 reviews as the validation set, and 600 reviews as the test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_47",
            "content": "ASRS is an Aviation Safety Reporting System dataset. We used the same balanced binary classification dataset created by Melamud et al. (2019), consisting of reports labeled with either 'Proficiency' or 'Physical Environment.' The original split had 386 documents for training and 392 documents for testing. We split the test set into two and use 196 documents for validation set and 196 documents for test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_48",
            "content": "AIvsCR contains scientific articles that we collected from arXiv and annotated with rationales. This dataset contains 2,394 documents from Artificial Intelligence (cs.AI) and Cryptography and Security (cs.CR) categories. Two annotators independently annotated 394 documents with rationales for the ground truth label, and we computed the inter-annotator agreement for the rationales in the same manner as Zaidan et al. (2007). We used 394 human-annotated documents as the training set, 1,000 documents as the validation set, and 1,000 documents as the test set. Note that the validation and test sets do not need rationales; they only need the documents and their labels for evaluation. We make this dataset publicly available, and provide a complete description of this dataset in the appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_49",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "448-ARR_v1_50",
            "content": "For training LwR-RC, we fine-tuned a pre-trained 'bert-base-uncased' version of the BERT (Devlin et al., 2018) model on downstream classification task using our ranking-constrained loss function. We input each sentence within a document to BERT and used the '[CLS]' logits from the last hidden layer as the sentence embeddings. To fit the model into GPU (NVIDIA Quadro RTX 5000) memory, we truncated each input sentence to at most 48 tokens (including two special tokens '[CLS]' and '[SEP]'), and each document to at most 64 sentences. We used only one hidden layer with 100 nodes in the hidden layers section of Figure 1, and used tanh as the activation function. The total number of model parameters for LwR-RC is 109,559,241. The running time of training LwR-RC is similar to training a fine-tuned BERT model without using rationales; LwR-RC needs to make two more forward passes to compute \u00b5 i and \u00b5 r i for x i and x r i , respectively. We present average learning curves over 5 different runs to assess how the models would perform under varying labeling regiments, and plot error bars showing the standard error. Each learning curve starts with a bootstrap of 5 randomly selected documents from each label. Each step of the learning curve corresponds to labeling 20 additional documents. For a fair comparison between various learning strategies, all learning strategies (our approach and the baselines) are fed the same sequence of documents. After the bootstrap phase, we run 10 more steps, and hence the budget of learning curves runs up to 10 + 20 \u00d7 10 = 210 documents. Tuning Hyper-parameters. For a fair comparison between our method and the baselines, at each iteration of learning, we performed grid search to optimize the tunable hyper-parameters of each method using the held-out validation set. For LwR-RC, we experimented with different pairs of hyperparameters, \u03bb 1 and \u03bb 2 , whose values were selected from the set {0, 0.125, 0.25, 0.5}. We fine-tuned BERT model for LwR-RC for 10 epochs, and selected the best model across different epochs using the held-out validation set. We next discuss the details of the baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_51",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "448-ARR_v1_52",
            "content": "We compare our approach with one Learning without Rationales (Lw/oR) baseline and four Learning with Rationales (LwR) baselines. Learning without Rationales. The Lw/oR-BERT baseline fine-tunes the BERT model for downstream classification tasks, and optimizes the model by only minimizing the classification loss function, L clf , according to Equation (1). It is worth noting that traditional Lw/oR approaches that fine-tune BERT model on classification tasks have shown impressive performances, and therefore, Lw/oR-BERT is a strong baseline. For example, Sun et al. (2019) achieved the state-of-the-art performances on eight text classification tasks by fine-tuning the BERT model, outperforming both CNN and LSTM based models as well as using just pre-trained BERT embeddings. We observed similar trends in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_53",
            "content": "Learning with Rationales Baselines. We conducted experiments using four learning-withrationales baselines from the literature. 1) Rationale-Augmented SVM (RA-SVM): This approach is Zaidan et al. (2007)'s model that translates the importance of rationales into additional constraints for training support vector machines. This method requires three hyper-parameters: regularization C for the original samples, regularization C contrast for the contrast samples, and margin \u00b5 between the original and contrast samples. We optimized these hyper-parameters using grid search, and selected the values of both C and C contrast from the set {0.01, 0.1, 1, 10, 100} and the value of \u00b5 from the set {0.01, 0.1, 1, 10}. 2) Rationale-Augmented LR (RA-LR): This approach is Sharma and Bilgic (2018)'s approach that emphasizes the rationales and de-emphasizes non-rationales in the vectorized feature matrix representation of the documents. It has three hyper-parameters, weight r for the rationale terms, weight o for the non-rationale terms, and regularization C. We selected the value of r from the set {1, 10, 100}, the value of o from the set {0.01, 0.1, 1}, and the value of C from the set {0.01, 0.1, 1, 10, 100} to optimize the hyper-parameters using grid search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_54",
            "content": "3) RB-BOW-PROTO and 4) RB-WAVG-BERT: These are two models proposed by Melamud et al. (2019) that achieved the state-of-the-art performance in their experiments compared to Rationale-Augmented CNN (Zhang et al., 2016), Rationale-Augmented SVM (Sharma and Bilgic, 2018), and ULMFiT (Howard and Ruder, 2018). RB-BOW-PROTO uses a pre-trained word2vec embedding to construct rationale-biased text vectors for each class as prototypes, and then uses nearest-neighbor classification, instead of training a model to finetune the embeddings. This method has one hyperparameter, \u03b1, that controls the impact of rationale biases on the rationale-bias function. We selected the value of \u03b1 from the set {1, 3, 6, 12} to optimize it using grid search. The second approach, RB-WAVG-BERT, which is more closely related to our work, fine-tunes BERT model to jointly learn the labels on documents and the labels on tokens. We fine-tuned this model for 10 epochs and selected the best model across different epochs, using the learning rate of 5e-6, as suggested by the paper. Melamud et al. (2019) found that RB-BOW-PROTO performed better under extremely-limited labeling settings, and that RB-WAVG-BERT performed better when the training size was larger; hence, we included both approaches as baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_55",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "448-ARR_v1_56",
            "content": "We first present results comparing LwR-RC with the baselines, and then discuss the effects of the two ranking-constrained losses on the performance of LwR-RC. embeddings. Zaidan et al. (2007) and Sharma and Bilgic (2018) showed that RA-SVM and RA-LR outperformed several Lw/oR approaches, and hence these two are strong LwR baselines. Still, a finetuned BERT model that does not use rationales is able to outperform two strong baselines that used rationales but did not utilize the BERT embeddings. This result highlights the added benefit of the \"existing knowledge\" that pretrained embeddings provide. BERT Baselines. RB-WAVG-BERT, the baseline that fine-tuned BERT model and utilized rationales, outperforms Lw/oR-BERT, the baseline that did not use rationales, showing the benefits of utilizing rationales with recent deep learning models. However, the improvements provided by RB-WAVG-BERT become noticeable only after the model has seen enough data (e.g., more than 50 documents), which was also noted by Melamud et al. (2019). LwR-RC vs. the Best Baseline. We next turn our attention to a fairer comparison: LwR-RC versus RB-WAVG-BERT; both used and fine-tuned BERT embeddings and both utilized rationales. LwR-RC provides statistically significant improvements 2 over RB-WAVG-BERT, with a p-value of less than 0.05, especially when the annotation budget is small, and it performs comparably at larger budgets. For IMDb, LwR-RC provides up to 22.3% improvements in accuracy over RB-WAVG-BERT; for ASRS, LwR-RC provides up to 21.7% improvements in accuracy over RB-WAVG-BERT. For AIvsCR dataset, Lw/oR-BERT can quickly reach 90% accuracy even without utilizing rationales, and thus the improvements provided by LwR-RC on this dataset for most training budgets are not as large as the improvements on the other two datasets; however, LwR-RC can still provide up to 8.67% improvements in accuracy over RB-WAVG-BERT. Regarding RB-BOW-PROTO, as Melamud et al. (2019) also observed, it performs well only under extremely-limited budget settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_57",
            "content": "Comparison with the Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "448-ARR_v1_58",
            "content": "Corresponding to the learning curves presented in Figure 2, Table 1 shows the number of annotated documents needed for training LwR-RC as well as the two fine-tuned BERT baselines, Lw/oR-BERT and RB-WAVG-BERT, to achieve a target accuracy (ranging from 65% to 90%). As Table 1 shows, LwR-RC usually needs 2 and sometimes 3 times fewer number of annotated documents compared to Lw/oR-BERT and RB-WAVG-BERT to achieve the same level of accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_59",
            "content": "The Effects of the Loss Functions",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "448-ARR_v1_60",
            "content": "We further investigate the effects of the two ranking-constrained losses. Specifically, we want to understand how LwR-RC behaves with the two ranking-constrained losses: LwR-RC DvM that uses only L DvM (setting \u03bb 1 to 0.25 and \u03bb 2 to 0 in Equation ( 7)), and LwR-RC RvM that uses only L RvM (setting \u03bb 1 to 0 and \u03bb 2 to 0.25 in Equation ( 7)). Figure 3 presents the learning curves for these settings. For the IMDb dataset, LwR-RC RvM achieves a slightly higher accuracy than LwR-RC DvM after 100 training documents. For ASRS dataset, LwR-RC DvM performs the best, and for AIvsCR dataset, LwR-RC RvM performs the best.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_61",
            "content": "To investigate it further, we provide average statistics for the number of sentences, the number of rationale sentences, and the percentage of rationale sentences within the documents for each dataset in Table 2. We observe that LwR-RC RvM performs better when the percentage of rationale sentences in documents is high, e.g., IMDb and AIvsCR datasets, and LwR-RC DvM performs better when the percentage of rationale sentences is low in the documents, e.g., ASRS dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_62",
            "content": "We hypothesize that different ranking constraints may be affected differently by a number of factors, including the budget for training documents, the diversity of rationales, the number of rationales provided for each document, how thorough the annotator was in providing rationales, and the domain, to name a few. Table 2 provides only a glimpse of such a study. An exhaustive study is needed for making a definitive conclusion about how various document and rationale statistics affect different ranking-constrained losses, which is beyond the scope of this study. However, the tuning strategy that picks the best \u03bb parameters for LwR-RC at each iteration of learning using a validation set, and hence chooses the appropriate balance between the two loss functions, works well in practice, as was shown in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_63",
            "content": "Limitations and Future Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "448-ARR_v1_64",
            "content": "We presented experimental results for binary classification tasks in this paper. To the best of our knowledge, prior learning-with-rationales frameworks also focused on binary classification tasks in their experiments. Extending the framework to multi-class settings is a promising future direction. Such an extension would require adapting the loss functions to multi-class settings and creating multi-class classification datasets with rationales. Extending the framework to multi-label settings where a document can be assigned more than one label, however, is more challenging, both for formulating the problem as well as annotating the datasets with rationales, because rationales need to be assigned to their respective labels, which might be more than one in a single document.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_65",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "448-ARR_v1_66",
            "content": "We presented a novel approach to incorporate rationales as ranking-constraints into the training of classification models with cross-entropy loss. The proposed approach is general enough that it can be used for simple models, such as logistic regression with one-hot encoding of documents, as well as deep learning models combined with text embeddings. We conducted empirical evaluations comparing the proposed approach to several baselines and observed that the proposed approach outperformed the baselines in most settings, and was comparable to them at the remaining settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_67",
            "content": "In this section, we supplement the results presented in the paper with the following: \u2022 In the paper, we focused on experimental results with a budget of up to 210 training documents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_68",
            "content": "Here, we supplement the main results in the paper with a larger budget of up to 310 documents. \u2022 We present the improvements in accuracy provided by LwR-RC over the two fine-tuned BERT baselines, Lw/oR-BERT and RB-WAVG-BERT, for all three datasets at varying budgets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_69",
            "content": "LwR-RC to Lw/oR-BERT and RB-WAVG-BERT. \u2022 In the paper, we provided the formulation of LwR-RC for binary classification for the ease of exposition. Here, we extend the formulation of LwR-RC to multi-class classification. \u2022 We provide a complete description of the AIvsCR dataset that we collected and annotated with rationales for the ground truth labels. \u2022 Additionally, we provide the AIvsCR dataset and the other two datasets (IMDb and ASRS), as well as the source code for all the experiments in our paper with this submission as separate .zip files.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_70",
            "content": "In the paper, we focused on experimental results with a budget of up to 210 training documents (Figure 2). We supplement the results in Figure 2 with a larger budget of up to 310 training documents in Figure 4. As can be seen in Figure 4, the trends of all the results in the paper remain the same even with larger budgets. For IMDb and AIvsCR datasets, LwR-RC still performs better or comparably to the most competitive baseline, RB-WAVG-BERT; for ASRS dataset, LwR-RC still outperforms all the baselines. However, as the number of labeled documents grows, we expect our models and the baselines to converge to a similar accuracy, as the models no longer need the human-provided rationales and can learn statistically \"what is important\" from a large collection of documents that are simply annotated with labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_71",
            "content": "We present the improvements in accuracy provided by LwR-RC compared to the baselines for the three datasets across different training budgets. Specifically, we compare LwR-RC with the two fine-tuned BERT based approaches, Lw/oR-BERT and RB-WAVG-BERT. As shown in Table 3, LwR-RC provides significant improvements in accuracy over the two baselines across most training budgets: for IMDb, the improvements are up to 23.68%; for ASRS, the improvements are up to 28.31%; for AIvsCR, the improvements are up to 8.67%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_72",
            "content": "In this section, we provide a summary of pairwise one-tailed t-tests comparing LwR-RC with the two most competitive baselines, Lw/oR-BERT and RB-WAVG-BERT, for all three datasets at varying budget regiments. Table 4 shows the p-values of onetailed paired t-tests with the alternative hypothesis \"the performance of LwR-RC is better than the baseline approach\". As this result shows, LwR-RC statistically significantly outperforms both Lw/oR-BERT and RB-WAVG-BERT at most budget regiments with a p-value of less than 0.05.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_73",
            "content": "In our paper, we focused on binary classification. LwR-RC, can be extended to multi-class classification with a few modifications. For multi-class classification, let",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_74",
            "content": "y i \u2208 {c 1 , c 2 , \u2022 \u2022 \u2022 , c k }: f (x i ) = p(y i = c | x i ) = sof tmax(W z z i )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_75",
            "content": "for some parameter vector/matrix W z , where c is the correct label for instance x i and z i is the vector representation of x i . Assuming that y i is encoded as onehot representation, the classification loss function, L clf , will then change from binary cross-entropy to categorical cross-entropy:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_76",
            "content": "L clf = \u2212 1 |L| i (y i \u2022 log(p(y i |x i )))(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_77",
            "content": "For modeling the DvM objective of LwR-RC, let \u00b5 i = W z z i and \u00b5 i = W z z i , where z i is the vector representation of x i . Then, for the correct label c, we would like \u00b5 c i > 0 and \u00b5 c i > \u00b5 c i , which results in the following objective function:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_78",
            "content": "L DvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x i , x i )) (9) where, p(y i |x i , x i ) = sof tmax(\u2212(\u00b5 i \u2212 \u00b5 i ))(10",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_79",
            "content": ") We define the ranking loss similarly for the RvM component, this time using the R and M documents and their respective scores \u00b5 r i = W z z r i and \u00b5 i = W z z i , where z r i is the vector representation of x r i . The ranking loss L RvM is then defined as: Figure 4: Comparison between our approach, LwR-RC, and the five baselines using the best hyper-parameter setting for each method. where, p(y i |x r i , x i ) = sof tmax(\u2212(\u00b5 r i \u2212 \u00b5 i )) (12) We combine the classification loss L clf with the ranking losses, L DvM and L RvM , resulting in the main objective function for our approach for multi-class classification: RC to the two fine-tuned BERT baselines for all three datasets at varying budget regiments. We report the pvalues for one-tailed paired t-tests with the alternative hypothesis \"the performance of our approach is better than the baseline approach\". The results where LwR-RC performs statistically significantly better than the baselines (p-value of less than 0.05) are boldfaced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_80",
            "content": "L RvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x r i , x i ))(",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_81",
            "content": "L = (1 \u2212 \u03bb 1 \u2212 \u03bb 2 )L clf + \u03bb 1 L DvM + \u03bb 2 L RvM (13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_82",
            "content": "In our study, we experimented with three humanannotated datasets, IMDb, ASRS, and AIvsCR. We collected and annotated the AIvsCR dataset. To construct this dataset, we first collected 6,000 articles equally from two categories, cs.AI and cs.CR, from arXiv.org using a custom search query in the arXiv API. We provide the code, including the custom search queries, that we used to collect the data from arXiv.org with the supplementary material.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_83",
            "content": "For annotating the AIvsCR dataset, two annotators, A1 and A2, were provided with the same instructions as Zaidan et al. (2007) described in their paper: highlight the rationales at your best but do not mark everything.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_84",
            "content": "A1 A2 # rationales per document 3.8 8.4 # rationale words per document 17.4 31.3 % rationales overlapping with A1 100 30.5 % rationales overlapping with A2 64.0 100 Table 5: Average statistics for AIvsCR dataset and the two annotators, A1 and A2. The table presents the number of rationales and the number of rationale words per document provided by the two annotators, as well as the inter-annotator agreement for their rationale annotation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_85",
            "content": "We calculated the inter-annotator agreement for the rationales, where the rationales provided by the two annotators for the same document are considered as overlapping if they have at least one word in common, following the same manner of Zaidan et al. (2007). The relevant statistics are shown in Table 5. To make the best use of each annotator's effort, for every document, we kept the overlapping words, phrases, and sentences between the two annotators' highlighted rationales as the final rationales, as illustrated in the following example: \u2022 A1: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision. \u2022 A2: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision. \u2022 Final: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "448-ARR_v1_86",
            "content": "C Charu, Chengxiang Aggarwal,  Zhai, A survey of text classification algorithms, 2012, Mining text data, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "C Charu",
                    "Chengxiang Aggarwal",
                    " Zhai"
                ],
                "title": "A survey of text classification algorithms",
                "pub_date": "2012",
                "pub_title": "Mining text data",
                "pub": "Springer"
            }
        },
        {
            "ix": "448-ARR_v1_87",
            "content": "UNKNOWN, None, 2014, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_88",
            "content": "UNKNOWN, None, 2018, Deriving machine attention from human rationales, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Deriving machine attention from human rationales",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_89",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_90",
            "content": "Jeff Donahue, Kristen Grauman, Annotator rationales for visual recognition, 2011, 2011 International Conference on Computer Vision, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Jeff Donahue",
                    "Kristen Grauman"
                ],
                "title": "Annotator rationales for visual recognition",
                "pub_date": "2011",
                "pub_title": "2011 International Conference on Computer Vision",
                "pub": "IEEE"
            }
        },
        {
            "ix": "448-ARR_v1_91",
            "content": "UNKNOWN, None, 2020, Concept matching for low-resource classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Concept matching for low-resource classification",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_92",
            "content": "Fatemeh Hemmatian, Mohammad Sohrabi, A survey on classification techniques for opinion mining and sentiment analysis, 2019, Artificial Intelligence Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Fatemeh Hemmatian",
                    "Mohammad Sohrabi"
                ],
                "title": "A survey on classification techniques for opinion mining and sentiment analysis",
                "pub_date": "2019",
                "pub_title": "Artificial Intelligence Review",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_93",
            "content": "UNKNOWN, None, 2018, Universal language model fine-tuning for text classification, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Universal language model fine-tuning for text classification",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_94",
            "content": "Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, Krister Lind\u00e9n, Automatic language identification in texts: A survey, 2019, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Tommi Jauhiainen",
                    "Marco Lui",
                    "Marcos Zampieri",
                    "Timothy Baldwin",
                    "Krister Lind\u00e9n"
                ],
                "title": "Automatic language identification in texts: A survey",
                "pub_date": "2019",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_95",
            "content": "Quoc Le, Tomas Mikolov, Distributed representations of sentences and documents, 2014, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Quoc Le",
                    "Tomas Mikolov"
                ],
                "title": "Distributed representations of sentences and documents",
                "pub_date": "2014",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "448-ARR_v1_96",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_97",
            "content": "Oren Melamud, Mihaela Bornea, Ken Barker, Combining unsupervised pre-training and annotator rationales to improve low-shot text classification, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Oren Melamud",
                    "Mihaela Bornea",
                    "Ken Barker"
                ],
                "title": "Combining unsupervised pre-training and annotator rationales to improve low-shot text classification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_98",
            "content": "UNKNOWN, None, 2013, Efficient estimation of word representations in vector space, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Efficient estimation of word representations in vector space",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_99",
            "content": "Shama Chandramouli, Evangelos Sastry,  Milios, Active neural learners for text with dual supervision, 2020, Neural Computing and Applications, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Shama Chandramouli",
                    "Evangelos Sastry",
                    " Milios"
                ],
                "title": "Active neural learners for text with dual supervision",
                "pub_date": "2020",
                "pub_title": "Neural Computing and Applications",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_100",
            "content": "UNKNOWN, None, 2018, Learning with rationales for document classification. Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Learning with rationales for document classification. Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_101",
            "content": "Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang, How to fine-tune bert for text classification?, 2019, Chinese Computational Linguistics, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Chi Sun",
                    "Xipeng Qiu",
                    "Yige Xu",
                    "Xuanjing Huang"
                ],
                "title": "How to fine-tune bert for text classification?",
                "pub_date": "2019",
                "pub_title": "Chinese Computational Linguistics",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "448-ARR_v1_102",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Zhilin Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "R Russ",
                    "Quoc V Salakhutdinov",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "448-ARR_v1_103",
            "content": "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy, Hierarchical attention networks for document classification, 2016, Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Zichao Yang",
                    "Diyi Yang",
                    "Chris Dyer",
                    "Xiaodong He",
                    "Alex Smola",
                    "Eduard Hovy"
                ],
                "title": "Hierarchical attention networks for document classification",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_104",
            "content": "Omar Zaidan, Jason Eisner, Modeling annotators: A generative approach to learning from annotator rationales, 2008, Proceedings of the 2008 conference on Empirical methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Omar Zaidan",
                    "Jason Eisner"
                ],
                "title": "Modeling annotators: A generative approach to learning from annotator rationales",
                "pub_date": "2008",
                "pub_title": "Proceedings of the 2008 conference on Empirical methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_105",
            "content": "Omar Zaidan, Jason Eisner, Christine Piatko, Using \"annotator rationales\" to improve machine learning for text categorization, 2007, Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Omar Zaidan",
                    "Jason Eisner",
                    "Christine Piatko"
                ],
                "title": "Using \"annotator rationales\" to improve machine learning for text categorization",
                "pub_date": "2007",
                "pub_title": "Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_106",
            "content": "Jason Omar F Zaidan, Christine Eisner,  Piatko, Machine learning with annotator rationales to reduce annotation cost, 2008, Proceedings of the NIPS* 2008 workshop on cost sensitive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Jason Omar F Zaidan",
                    "Christine Eisner",
                    " Piatko"
                ],
                "title": "Machine learning with annotator rationales to reduce annotation cost",
                "pub_date": "2008",
                "pub_title": "Proceedings of the NIPS* 2008 workshop on cost sensitive learning",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_107",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Lecun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "448-ARR_v1_108",
            "content": "Ye Zhang, Iain Marshall, Byron C Wallace, Rationale-augmented convolutional neural networks for text classification, 2016, Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, NIH Public Access.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Ye Zhang",
                    "Iain Marshall",
                    "Byron C Wallace"
                ],
                "title": "Rationale-augmented convolutional neural networks for text classification",
                "pub_date": "2016",
                "pub_title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing",
                "pub": "NIH Public Access"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "448-ARR_v1_0@0",
            "content": "Ranking-Constrained Learning with Rationales for Text Classification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_0",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_2@0",
            "content": "We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_2",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_2@1",
            "content": "We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_2",
            "start": 186,
            "end": 322,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_2@2",
            "content": "We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationaleaugmented baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_2",
            "start": 324,
            "end": 607,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_4@0",
            "content": "Text classification has been used for numerous applications including sentiment analysis (Hemmatian and Sohrabi, 2019), information retrieval (Aggarwal and Zhai, 2012), and language identification (Jauhiainen et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_4",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_4@1",
            "content": "When presented with a large number of labeled documents, common text classification models demonstrate impressive results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_4",
            "start": 224,
            "end": 345,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_4@2",
            "content": "In practical settings, however, labeled data is often scarce.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_4",
            "start": 347,
            "end": 407,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_4@3",
            "content": "Labeling documents is a tedious task that requires time and effort, thus curating a large labeled corpus can be expensive and even unrealistic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_4",
            "start": 409,
            "end": 551,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_5@0",
            "content": "There is a wide range of use cases for businesses and industry that require curating a labeled dataset for the current task before the need to move on to the next task arises.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_5",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_5@1",
            "content": "For example, consider legal case document classification where documents need to be labeled as relevant/not-relevant to the current case at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_5",
            "start": 176,
            "end": 320,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_5@2",
            "content": "The next legal case requires labeling the documents as relevant/not-relevant for that particular case, and so on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_5",
            "start": 322,
            "end": 434,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_5@3",
            "content": "Similarly, several fast-response tasks such as immediate analysis of news and social media posts for a breaking news, for a recently released product, for a policy announcement, etc., require fast curation of a small and yet informative labeled dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_5",
            "start": 436,
            "end": 688,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@0",
            "content": "An effective approach to make the best use of the human's time and maximize classifier performance with a small labeled dataset is to elicit rich feedback, in the form of rationales for classification, during the labeling process (Zaidan et al., 2007Donahue and Grauman, 2011;Sharma and Bilgic, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@1",
            "content": "For sentiment classification, for example, the annotators might highlight certain segments of the text that convinced them to label the review as positive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 302,
            "end": 456,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@2",
            "content": "Unlike humans, a classifier will not know which segments of the document are responsible for its label during training, until it has been presented with many training samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 458,
            "end": 632,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@3",
            "content": "Since the human annotators read the document to decide its label in the first place, they have already spent the time to find the justifications for their labeling decision; hence, previous studies have shown that the extra time needed to highlight a piece of the text as a rationale for its label is not high and is often worth more (for improving the classifier) than spending that time to label an additional document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 634,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@4",
            "content": "Zaidan et al. (2007) showed that rationale annotation has low overhead, roughly twice the time required for annotating only the labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 1056,
            "end": 1190,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_6@5",
            "content": "Sharma and Bilgic (2018) showed that annotating a single document with rationales can be worth as many as 20 documents that are simply annotated with labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_6",
            "start": 1192,
            "end": 1348,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_7@0",
            "content": "Prior work on learning with rationales focused on one-hot encoding of the text in combination with logistic regression and support vector machines (Zaidan et al., 2007;Sharma and Bilgic, 2018), deep learning with multi-task learning (Melamud et al., 2019), and rationale-augmented attentionbased models (Bahdanau et al., 2014), which still required a large set of labeled documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_7",
            "start": 0,
            "end": 381,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_7@1",
            "content": "We propose a general approach that is applicable to both one-hot encoding as well as deep learning embedding representations and that is highly effective under limited labeling settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_7",
            "start": 383,
            "end": 568,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@0",
            "content": "The rationale supervision can be understood as an expectation that a document should have a higher probability of belonging to its class than the same document from which the rationale(s) are removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@1",
            "content": "Motivated by this intuition, we formulate a hybrid loss function to combine classification loss with ranking constraints for rationale supervision, which serves as an effective way of directing the model's focus to rationales during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 201,
            "end": 442,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@2",
            "content": "Our contributions in this paper include: \u2022 We formulate a general and effective learningwith-rationales method for text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 444,
            "end": 578,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@3",
            "content": "\u2022 We study its empirical effectiveness on three human-annotated text classification datasets (sentiment analysis, aviation safety, and scientific articles).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 580,
            "end": 735,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@4",
            "content": "\u2022 We compare our method to several baselines, and empirical findings show that it achieves the stateof-the-art results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 737,
            "end": 855,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@5",
            "content": "For example, our proposed method is able to achieve 80% accuracy on the IMDb movie review dataset (Zaidan et al., 2007) with as few as 23 documents, whereas a finetuned BERT model that does not use rationales required 73 documents, and the most competitive rationale-augmented baseline required 63 documents to achieve the same level of accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 857,
            "end": 1202,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@6",
            "content": "\u2022 We annotate a new text classification dataset with rationales and make it publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 1204,
            "end": 1298,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@7",
            "content": "The rest of the paper is organized as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 1300,
            "end": 1345,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@8",
            "content": "We first discuss related work and how our work differs from previous work in Section 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 1347,
            "end": 1433,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@9",
            "content": "We formalize our learning with rationales approach in Section 3 and detail the experimental methodology in Section 4, followed by a discussion of the results in Section 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 1435,
            "end": 1605,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_8@10",
            "content": "We discuss the limitations and future work in Section 6 and then conclude.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_8",
            "start": 1607,
            "end": 1680,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_9@0",
            "content": "2 Related Work Zaidan et al. (2007) presented one of the first approaches to learning with rationales for text classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_9",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_9@1",
            "content": "They proposed to utilize humanprovided rationales by converting the rationales into constraints for training support vector machines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_9",
            "start": 127,
            "end": 259,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_9@2",
            "content": "They later extended the framework to a rationale-constrained probabilistic model .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_9",
            "start": 261,
            "end": 342,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_9@3",
            "content": "Sharma and Bilgic (2018) proposed a general method to incorporate rationales into the training of any classifier by weighting the rationale features higher than the non-rationale features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_9",
            "start": 344,
            "end": 531,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_9@4",
            "content": "However, their method relied on using a bag-of-words representation of the documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_9",
            "start": 533,
            "end": 617,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@0",
            "content": "As deep learning achieved the state-of-the-art performance on text classification (e.g., (Sun et al., 2019;Devlin et al., 2018;Zhang et al., 2015;Yang et al., 2016)), recent work proposed methods specifically for training deep learning models using rationale supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@1",
            "content": "Some methods utilized the rationales to generate rationale-augmented representations of the text while others utilized the rationales for richer supervision of the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 272,
            "end": 441,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@2",
            "content": "For instance, Zhang et al. (2016) proposed a Rationale-Augmented CNN (RA-CNN) that jointly learns from the labels of the documents as well as the labels at the sentence level, by using a two-step approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 443,
            "end": 647,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@3",
            "content": "However, their approach still requires sufficient amounts of data for training a model at the sentence level to learn a valid rationale-augmented representation of a document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 649,
            "end": 823,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@4",
            "content": "Errica et al. (2020) proposed a representation learning approach to leverage rationales by learning to focus on relevant input tokens in the embedding space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 825,
            "end": 981,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@5",
            "content": "Bao et al. (2018) proposed a framework to derive machine attentions from human-provided rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 983,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@6",
            "content": "Sastry and Milios (2020) defined a new attribution score for words by computing the partial derivative of the output with respect to the input in the word embedding space, and used misattribution error as an additional supervision in the loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 1083,
            "end": 1334,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_10@7",
            "content": "Our method has two major differences from these work: i) our approach can use but does not require an attention mechanism to focus on the rationales and ii) our approach does not require learning a separate representation for the rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_10",
            "start": 1336,
            "end": 1576,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_11@0",
            "content": "The work most closely related to ours is the model proposed by Melamud et al. (2019), which jointly learns to predict the labels for text as well as the labels for each token of every input sentence by determining whether the token is part of the rationales or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_11",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_11@1",
            "content": "Our approach differs from theirs as our ranking loss is calculated by using only the model's predictions, rather than introducing auxiliary learning tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_11",
            "start": 266,
            "end": 420,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_11@2",
            "content": "Moreover, the approach we propose is more general: it can be used for any model that can utilize a logistic loss, ranging from a logistic regression model coupled with a one-hot encoding of words to a Long Short-Term Memory (LSTM) model coupled with word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_11",
            "start": 422,
            "end": 687,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_11@3",
            "content": "In their same paper, Melamud et al. (2019) proposed another method that utilizes rationales by constructing rationale prototypes and rationale-biased text vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_11",
            "start": 689,
            "end": 851,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_11@4",
            "content": "However, these vectors are computed using a rationale-bias function to directly estimate the similarity between words and annotated rationales without incorporating any learning, and thus this method works well only for few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_11",
            "start": 853,
            "end": 1090,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_12@0",
            "content": "Learning with Rationales",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_12",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_13@0",
            "content": "Let D = {x 1 , x 2 , \u2022 \u2022 \u2022 , x n } be a set of documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_13",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_14@0",
            "content": "A small subset of the documents, L \u2282 D, are annotated with labels, x i , y i where the value of y i belongs to a label space, C = {c 1 , c 2 , \u2022 \u2022 \u2022 , c k }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_14",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_14@1",
            "content": "y i is unknown for a much larger set of unlabeled documents, U = D \\ L, represented as x i , ? .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_14",
            "start": 158,
            "end": 253,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_14@2",
            "content": "Each document, x i , contains a number of sentences, {s i1 , s i2 , ..., s im }, each of which is represented as a sequence of words:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_14",
            "start": 255,
            "end": 387,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_15@0",
            "content": "s ij = {q 1 ij , q 2 ij , \u2022 \u2022 \u2022 , q l ij }.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_15",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_16@0",
            "content": "In the learning with rationales framework, a subset of the words is marked by the human annotator as rationales (i.e., justifications for the document's assigned label).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_16",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_16@1",
            "content": "Let r i = q l ij be the set of all words that are marked as rationales within a document, x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_16",
            "start": 170,
            "end": 264,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_16@2",
            "content": "It is possible that none of the words are marked as rationales, and hence, r i = \u2205 for such documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_16",
            "start": 266,
            "end": 367,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_16@3",
            "content": "In the learning-with-rationales setting, L is modified to contain x i , r i , y i and U represents x i , \u2205, ? .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_16",
            "start": 369,
            "end": 479,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_16@4",
            "content": "The objective is to train a model, f , that utilizes the documents x i , their labels y i , and their rationales r i during training, and uses only the documents x i at prediction time, as rationales are naturally not available for the test documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_16",
            "start": 481,
            "end": 731,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_17@0",
            "content": "Our Approach -LwR-RC",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_17",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_18@0",
            "content": "We first describe our proposed approach, Learning with Rationales -Ranking-Constrained (LwR-RC), and then illustrate how it can be specialized for training deep learning models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_18",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_18@1",
            "content": "To illustrate the motivation behind our approach, consider an example document, D, that contains three sentences: \"s1: The movie came out last year. s2: The plot was decent. s3: Acting was superb.\", which is labeled as 'positive' by the annotator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_18",
            "start": 178,
            "end": 424,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_18@2",
            "content": "Assume for the sake of example, the annotator highlights only s3 as the rationale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_18",
            "start": 426,
            "end": 507,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_18@3",
            "content": "Let M be a masked document that is same as the original document D, but from which the sentences containing the rationale phrases are removed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_18",
            "start": 509,
            "end": 650,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_18@4",
            "content": "In this case, M would be missing s3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_18",
            "start": 652,
            "end": 687,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_19@0",
            "content": "We postulate that the model should be more sure about the positive label of document D than the label of document M, since D contains the essential evidence, 'Acting was superb', for the 'positive' label, whereas M lacks that evidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_19",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_19@1",
            "content": "Similarly, let R be the document that contains only the rationale sentence s3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_19",
            "start": 236,
            "end": 313,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_19@2",
            "content": "We postulate that the model should be more sure about the label 'positive' of R than the label of M, since R provides strong evidence for the label, whereas M lacks that evidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_19",
            "start": 315,
            "end": 493,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_19@3",
            "content": "1 Traditional learning without rationales approaches optimize a loss function to compute the model's error on its predictions, e.g., a binary crossentropy classification loss, L clf , is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_19",
            "start": 495,
            "end": 692,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_20@0",
            "content": "L clf = \u2212 1 |L| i (y i \u2022 log(p(y i |x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x i )))(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_20",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@0",
            "content": "In order to leverage the annotated rationales, we formalize our postulations by providing the model with two additional objectives during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@1",
            "content": "The first objective is to train the model to be more confident about the label of a document (D) than the label of the same document in which the rationales are masked (M).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 148,
            "end": 319,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@2",
            "content": "The second objective is to train the model to be more confident about the label of document that contains only the rationales (R) than the label of the same document in which the rationales are masked (M).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 321,
            "end": 525,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@3",
            "content": "We achieve these objectives by using a ranking-constrained classification approach, as described next.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 527,
            "end": 628,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@4",
            "content": "Let x i , r i , y i \u2208 L be a training document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 630,
            "end": 676,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@5",
            "content": "First, we construct an artificial document x i by masking out all the sentences that contain rationales r i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 678,
            "end": 786,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@6",
            "content": "We construct another artificial document x r i consisting of only the sentences that contain rationales r i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 788,
            "end": 896,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@7",
            "content": "The ranking-constrained classification approach incorporates the rationales into learning by modeling two expectations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 898,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@8",
            "content": "(i) the model should be more sure of assigning the correct label y i to x i than assigning y i to x i , because x i represents a document from which the rationales have been removed, and we refer to this objective as 'Document versus Masked document' (DvM), where D represents x i and M represents x i , and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 1018,
            "end": 1324,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@9",
            "content": "(ii) the model should be more sure of assigning the correct label y i to x r i than assigning y i to x i , and we refer to this objective as 'Rationale versus Masked document' (RvM), where R represents x r i and M represents x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 1326,
            "end": 1555,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@10",
            "content": "Another possible objective can be 'Rationale versus Document' (RvD), however, we excluded RvD objective from our approach for the following reason.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 1557,
            "end": 1703,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_21@11",
            "content": "Consider the following cases for a binary (positive/negative) classification task:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_21",
            "start": 1705,
            "end": 1786,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_22@0",
            "content": "\u2022 Case 1: D = R+M is positive; R is positive; M is",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_22",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@0",
            "content": "neutral or it contains a small amount of leftover positive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@1",
            "content": "In this case, RvD requires R > R+M, which forces M to be negative, whereas RvM requires R > M, which does not necessarily require M to be negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 60,
            "end": 206,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@2",
            "content": "Thus, RvD is guaranteed to be the wrong approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 208,
            "end": 256,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@3",
            "content": "RvM forces R > M, but gives the model the flexibility to decide whether M is a small positive, neutral, or negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 258,
            "end": 373,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@4",
            "content": "\u2022 Case 2: D = R+M is positive; R is positive; M is negative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 375,
            "end": 434,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@5",
            "content": "In this case, RvD requires R > R+M, which forces M to be negative, whereas RvM simply requires R > M. In this case, RvD is the correct choice, but RvM cannot be called the guaranteed wrong choice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 436,
            "end": 631,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@6",
            "content": "\u2022 Remaining cases: The cases where D and R are negative are similar.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 633,
            "end": 700,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@7",
            "content": "As the cases above show, RvM is more flexible: RvM simply nudges the model in the correct direction and leaves the judgement about M to the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 702,
            "end": 846,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@8",
            "content": "RvD, on the other hand, is a more forceful approach; it forces the model to always make a judgement about M, which is the incorrect judgement in case 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 848,
            "end": 999,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_23@9",
            "content": "Thus, we include only the RvM and DvM objectives in our proposed approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_23",
            "start": 1001,
            "end": 1074,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_24@0",
            "content": "Formally, let",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_24",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_25@0",
            "content": "y i \u2208 {0, 1}: f (x i ) = p(y i = 1 | x i ) = sigmoid(W z z i ) for some parameter matrix W z",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_25",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_26@0",
            "content": ", where z i is the vector representation of x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_26",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_26@1",
            "content": "For modeling the DvM objective, let \u00b5 i = W z z i and \u00b5 i = W z z i where z i is the vector representation of x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_26",
            "start": 50,
            "end": 164,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_26@2",
            "content": "If the correct label is y i = 1, we would like \u00b5 i > 0 and \u00b5 i > \u00b5 i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_26",
            "start": 166,
            "end": 235,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_26@3",
            "content": "If the correct label is y i = 0, we would like \u00b5 i < 0 and \u00b5 i < \u00b5 i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_26",
            "start": 237,
            "end": 306,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_26@4",
            "content": "We convert this constraint into a logistic loss, as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_26",
            "start": 308,
            "end": 367,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_27@0",
            "content": "L i DvM = log(1 + exp(\u2212(\u00b5 i \u2212 \u00b5 i ))), y i = 1 log(1 + exp(\u2212(\u00b5 i \u2212 \u00b5 i ))), y i = 0 (2) Summing L i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_27",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_28@0",
            "content": "DvM over all the training instances and reorganizing the terms, we get:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_28",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_29@0",
            "content": "L DvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x i , x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x i , x i )))(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_29",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_30@0",
            "content": "where,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_30",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_31@0",
            "content": "p(y i |x i , x i ) = 1 1 + e \u2212(\u00b5 i \u2212\u00b5 i ) (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_31",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_32@0",
            "content": "We define the ranking loss similarly for the RvM component, using documents R and M and their respective scores \u00b5 r i = W z z r i and \u00b5 i = W z z i , where z r i is the vector representation of x r i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_32",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_32@1",
            "content": "The ranking loss L RvM is then defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_32",
            "start": 202,
            "end": 243,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_33@0",
            "content": "L RvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x r i , x i )) +(1 \u2212 y i ) \u2022 log(1 \u2212 p(y i |x r i , x i )))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_33",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_34@0",
            "content": "(5) where,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_34",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_35@0",
            "content": "p(y i |x r i , x i ) = 1 1 + e \u2212(\u00b5 r i \u2212\u00b5 i ) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_35",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_36@0",
            "content": "We combine the classification loss L clf with the ranking losses, L DvM and L RvM , resulting in the main objective function for our approach:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_36",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_37@0",
            "content": "L = (1\u2212\u03bb 1 \u2212\u03bb 2 )L clf +\u03bb 1 L DvM +\u03bb 2 L RvM (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_37",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_38@0",
            "content": "where, 0 \u2264 \u03bb 1 \u2264 1, 0 \u2264 \u03bb 2 \u2264 1, and \u03bb 1 + \u03bb 2 \u2264 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_38",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_38@1",
            "content": "\u03bb 1 and \u03bb 2 are two hyper-parameters that control the importance of the classification loss and the ranking losses relative to one another.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_38",
            "start": 52,
            "end": 190,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_38@2",
            "content": "We study the effect of these hyper-parameters on the model's performance and provide insights into their relative importance in Section 5.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_38",
            "start": 192,
            "end": 331,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_38@3",
            "content": "We next describe how LwR-RC can be implemented through a neural network architecture, which can be specialized to a logistic regression or to a deep learning model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_38",
            "start": 333,
            "end": 496,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_39@0",
            "content": "LwR-RC with Deep Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_39",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_40@0",
            "content": "Figure 1 shows the deep learning architecture illustrating how the LwR-RC approach can minimize the loss function of Equation ( 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_40",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_40@1",
            "content": "For every sentence {s i1 , s i2 , ..., s im } within a document x i , we use an embedding model to create sentence embedding vectors {t i1 , t i2 , ..., t im }, and pass them through an average pooling layer to create a single vector, z i , representing a document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_40",
            "start": 132,
            "end": 396,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_40@2",
            "content": "Similarly, the same sentence embedding vectors are passed through two different pooling layers to create two masked averages, z i and z r i , representing the document without rationales and the document containing only the rationales, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_40",
            "start": 398,
            "end": 646,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_40@3",
            "content": "There are several strategies for aggregating many sentence vectors into a single document vector; we use the average pooling strategy for the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_40",
            "start": 648,
            "end": 801,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_41@0",
            "content": "The LwR-RC approach can be used to train any model that uses cross-entropy loss functions, including logistic regression and deep neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_41",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_41@1",
            "content": "It can also work with several representations, including one-hot encoding of the words, word2vec (Mikolov et al., 2013), and doc2vec (Le and Mikolov, 2014), as well as more recent language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_41",
            "start": 147,
            "end": 436,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_41@2",
            "content": "For example, if we remove the embedding layer and the hidden layers, and represent the sentences using one-hot encoding of the words, we would get a simple logistic regression classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_41",
            "start": 438,
            "end": 624,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_41@3",
            "content": "If we use BERT for encoding the sentences in the embedding layer, then we can either use BERT embeddings directly or fine-tune the BERT model on downstream classification tasks by optimizing the ranking-constrained loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_41",
            "start": 626,
            "end": 854,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_42@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_42",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_43@0",
            "content": "In this section, we describe the three datasets, several baselines, and the experimental settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_43",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_44@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_44",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_45@0",
            "content": "We used two publicly available datasets: a sentiment classification dataset and an aviation safety dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_45",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_45@1",
            "content": "Both datasets were annotated with labels and rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_45",
            "start": 108,
            "end": 163,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_45@2",
            "content": "Additionally, we introduce a new scientific article classification dataset that we annotated with labels and rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_45",
            "start": 165,
            "end": 284,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_46@0",
            "content": "IMDb is a movie review dataset annotated by Zaidan et al. (2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_46",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_46@1",
            "content": "It consists of 1,800 documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_46",
            "start": 66,
            "end": 96,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_46@2",
            "content": "We used 600 reviews as the training set, 600 reviews as the validation set, and 600 reviews as the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_46",
            "start": 98,
            "end": 205,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_47@0",
            "content": "ASRS is an Aviation Safety Reporting System dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_47",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_47@1",
            "content": "We used the same balanced binary classification dataset created by Melamud et al. (2019), consisting of reports labeled with either 'Proficiency' or 'Physical Environment.' The original split had 386 documents for training and 392 documents for testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_47",
            "start": 53,
            "end": 305,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_47@2",
            "content": "We split the test set into two and use 196 documents for validation set and 196 documents for test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_47",
            "start": 307,
            "end": 409,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@0",
            "content": "AIvsCR contains scientific articles that we collected from arXiv and annotated with rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@1",
            "content": "This dataset contains 2,394 documents from Artificial Intelligence (cs.AI) and Cryptography and Security (cs.CR) categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 96,
            "end": 219,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@2",
            "content": "Two annotators independently annotated 394 documents with rationales for the ground truth label, and we computed the inter-annotator agreement for the rationales in the same manner as Zaidan et al. (2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 221,
            "end": 425,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@3",
            "content": "We used 394 human-annotated documents as the training set, 1,000 documents as the validation set, and 1,000 documents as the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 427,
            "end": 560,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@4",
            "content": "Note that the validation and test sets do not need rationales; they only need the documents and their labels for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 562,
            "end": 685,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_48@5",
            "content": "We make this dataset publicly available, and provide a complete description of this dataset in the appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_48",
            "start": 687,
            "end": 794,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_49@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_49",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@0",
            "content": "For training LwR-RC, we fine-tuned a pre-trained 'bert-base-uncased' version of the BERT (Devlin et al., 2018) model on downstream classification task using our ranking-constrained loss function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@1",
            "content": "We input each sentence within a document to BERT and used the '[CLS]' logits from the last hidden layer as the sentence embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 196,
            "end": 326,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@2",
            "content": "To fit the model into GPU (NVIDIA Quadro RTX 5000) memory, we truncated each input sentence to at most 48 tokens (including two special tokens '[CLS]' and '[SEP]'), and each document to at most 64 sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 328,
            "end": 534,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@3",
            "content": "We used only one hidden layer with 100 nodes in the hidden layers section of Figure 1, and used tanh as the activation function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 536,
            "end": 663,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@4",
            "content": "The total number of model parameters for LwR-RC is 109,559,241.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 665,
            "end": 727,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@5",
            "content": "The running time of training LwR-RC is similar to training a fine-tuned BERT model without using rationales; LwR-RC needs to make two more forward passes to compute \u00b5 i and \u00b5 r i for x i and x r i , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 729,
            "end": 940,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@6",
            "content": "We present average learning curves over 5 different runs to assess how the models would perform under varying labeling regiments, and plot error bars showing the standard error.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 942,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@7",
            "content": "Each learning curve starts with a bootstrap of 5 randomly selected documents from each label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1120,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@8",
            "content": "Each step of the learning curve corresponds to labeling 20 additional documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1214,
            "end": 1293,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@9",
            "content": "For a fair comparison between various learning strategies, all learning strategies (our approach and the baselines) are fed the same sequence of documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1295,
            "end": 1449,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@10",
            "content": "After the bootstrap phase, we run 10 more steps, and hence the budget of learning curves runs up to 10 + 20 \u00d7 10 = 210 documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1451,
            "end": 1579,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@11",
            "content": "Tuning Hyper-parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1581,
            "end": 1604,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@12",
            "content": "For a fair comparison between our method and the baselines, at each iteration of learning, we performed grid search to optimize the tunable hyper-parameters of each method using the held-out validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1606,
            "end": 1811,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@13",
            "content": "For LwR-RC, we experimented with different pairs of hyperparameters, \u03bb 1 and \u03bb 2 , whose values were selected from the set {0, 0.125, 0.25, 0.5}. We fine-tuned BERT model for LwR-RC for 10 epochs, and selected the best model across different epochs using the held-out validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 1813,
            "end": 2095,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_50@14",
            "content": "We next discuss the details of the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_50",
            "start": 2097,
            "end": 2141,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_51@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_51",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@0",
            "content": "We compare our approach with one Learning without Rationales (Lw/oR) baseline and four Learning with Rationales (LwR) baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@1",
            "content": "Learning without Rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 129,
            "end": 156,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@2",
            "content": "The Lw/oR-BERT baseline fine-tunes the BERT model for downstream classification tasks, and optimizes the model by only minimizing the classification loss function, L clf , according to Equation (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 158,
            "end": 355,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@3",
            "content": "It is worth noting that traditional Lw/oR approaches that fine-tune BERT model on classification tasks have shown impressive performances, and therefore, Lw/oR-BERT is a strong baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 357,
            "end": 542,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@4",
            "content": "For example, Sun et al. (2019) achieved the state-of-the-art performances on eight text classification tasks by fine-tuning the BERT model, outperforming both CNN and LSTM based models as well as using just pre-trained BERT embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 544,
            "end": 778,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_52@5",
            "content": "We observed similar trends in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_52",
            "start": 780,
            "end": 825,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@0",
            "content": "Learning with Rationales Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@1",
            "content": "We conducted experiments using four learning-withrationales baselines from the literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 36,
            "end": 125,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@2",
            "content": "1) Rationale-Augmented SVM (RA-SVM): This approach is Zaidan et al. (2007)'s model that translates the importance of rationales into additional constraints for training support vector machines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 127,
            "end": 319,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@3",
            "content": "This method requires three hyper-parameters: regularization C for the original samples, regularization C contrast for the contrast samples, and margin \u00b5 between the original and contrast samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 321,
            "end": 515,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@4",
            "content": "We optimized these hyper-parameters using grid search, and selected the values of both C and C contrast from the set {0.01, 0.1, 1, 10, 100} and the value of \u00b5 from the set {0.01, 0.1, 1, 10}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 517,
            "end": 708,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@5",
            "content": "2) Rationale-Augmented LR (RA-LR): This approach is Sharma and Bilgic (2018)'s approach that emphasizes the rationales and de-emphasizes non-rationales in the vectorized feature matrix representation of the documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 710,
            "end": 926,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_53@6",
            "content": "It has three hyper-parameters, weight r for the rationale terms, weight o for the non-rationale terms, and regularization C. We selected the value of r from the set {1, 10, 100}, the value of o from the set {0.01, 0.1, 1}, and the value of C from the set {0.01, 0.1, 1, 10, 100} to optimize the hyper-parameters using grid search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_53",
            "start": 928,
            "end": 1257,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@0",
            "content": "3) RB-BOW-PROTO and 4) RB-WAVG-BERT: These are two models proposed by Melamud et al. (2019) that achieved the state-of-the-art performance in their experiments compared to Rationale-Augmented CNN (Zhang et al., 2016), Rationale-Augmented SVM (Sharma and Bilgic, 2018), and ULMFiT (Howard and Ruder, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@1",
            "content": "RB-BOW-PROTO uses a pre-trained word2vec embedding to construct rationale-biased text vectors for each class as prototypes, and then uses nearest-neighbor classification, instead of training a model to finetune the embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 306,
            "end": 531,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@2",
            "content": "This method has one hyperparameter, \u03b1, that controls the impact of rationale biases on the rationale-bias function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 533,
            "end": 647,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@3",
            "content": "We selected the value of \u03b1 from the set {1, 3, 6, 12} to optimize it using grid search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 649,
            "end": 735,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@4",
            "content": "The second approach, RB-WAVG-BERT, which is more closely related to our work, fine-tunes BERT model to jointly learn the labels on documents and the labels on tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 737,
            "end": 902,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@5",
            "content": "We fine-tuned this model for 10 epochs and selected the best model across different epochs, using the learning rate of 5e-6, as suggested by the paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 904,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_54@6",
            "content": "Melamud et al. (2019) found that RB-BOW-PROTO performed better under extremely-limited labeling settings, and that RB-WAVG-BERT performed better when the training size was larger; hence, we included both approaches as baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_54",
            "start": 1056,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_55@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_55",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@0",
            "content": "We first present results comparing LwR-RC with the baselines, and then discuss the effects of the two ranking-constrained losses on the performance of LwR-RC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@1",
            "content": "embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 159,
            "end": 169,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@2",
            "content": "Zaidan et al. (2007) and Sharma and Bilgic (2018) showed that RA-SVM and RA-LR outperformed several Lw/oR approaches, and hence these two are strong LwR baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 171,
            "end": 333,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@3",
            "content": "Still, a finetuned BERT model that does not use rationales is able to outperform two strong baselines that used rationales but did not utilize the BERT embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 335,
            "end": 497,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@4",
            "content": "This result highlights the added benefit of the \"existing knowledge\" that pretrained embeddings provide.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 499,
            "end": 602,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@5",
            "content": "BERT Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 604,
            "end": 618,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@6",
            "content": "RB-WAVG-BERT, the baseline that fine-tuned BERT model and utilized rationales, outperforms Lw/oR-BERT, the baseline that did not use rationales, showing the benefits of utilizing rationales with recent deep learning models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 620,
            "end": 842,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@7",
            "content": "However, the improvements provided by RB-WAVG-BERT become noticeable only after the model has seen enough data (e.g., more than 50 documents), which was also noted by Melamud et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 844,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@8",
            "content": "LwR-RC vs. the Best Baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1034,
            "end": 1062,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@9",
            "content": "We next turn our attention to a fairer comparison: LwR-RC versus RB-WAVG-BERT; both used and fine-tuned BERT embeddings and both utilized rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1064,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@10",
            "content": "LwR-RC provides statistically significant improvements 2 over RB-WAVG-BERT, with a p-value of less than 0.05, especially when the annotation budget is small, and it performs comparably at larger budgets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1214,
            "end": 1416,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@11",
            "content": "For IMDb, LwR-RC provides up to 22.3% improvements in accuracy over RB-WAVG-BERT; for ASRS, LwR-RC provides up to 21.7% improvements in accuracy over RB-WAVG-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1418,
            "end": 1580,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@12",
            "content": "For AIvsCR dataset, Lw/oR-BERT can quickly reach 90% accuracy even without utilizing rationales, and thus the improvements provided by LwR-RC on this dataset for most training budgets are not as large as the improvements on the other two datasets; however, LwR-RC can still provide up to 8.67% improvements in accuracy over RB-WAVG-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1582,
            "end": 1918,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_56@13",
            "content": "Regarding RB-BOW-PROTO, as Melamud et al. (2019) also observed, it performs well only under extremely-limited budget settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_56",
            "start": 1920,
            "end": 2045,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_57@0",
            "content": "Comparison with the Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_57",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_58@0",
            "content": "Corresponding to the learning curves presented in Figure 2, Table 1 shows the number of annotated documents needed for training LwR-RC as well as the two fine-tuned BERT baselines, Lw/oR-BERT and RB-WAVG-BERT, to achieve a target accuracy (ranging from 65% to 90%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_58",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_58@1",
            "content": "As Table 1 shows, LwR-RC usually needs 2 and sometimes 3 times fewer number of annotated documents compared to Lw/oR-BERT and RB-WAVG-BERT to achieve the same level of accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_58",
            "start": 266,
            "end": 442,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_59@0",
            "content": "The Effects of the Loss Functions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_59",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_60@0",
            "content": "We further investigate the effects of the two ranking-constrained losses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_60",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_60@1",
            "content": "Specifically, we want to understand how LwR-RC behaves with the two ranking-constrained losses: LwR-RC DvM that uses only L DvM (setting \u03bb 1 to 0.25 and \u03bb 2 to 0 in Equation ( 7)), and LwR-RC RvM that uses only L RvM (setting \u03bb 1 to 0 and \u03bb 2 to 0.25 in Equation ( 7)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_60",
            "start": 74,
            "end": 342,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_60@2",
            "content": "Figure 3 presents the learning curves for these settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_60",
            "start": 344,
            "end": 400,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_60@3",
            "content": "For the IMDb dataset, LwR-RC RvM achieves a slightly higher accuracy than LwR-RC DvM after 100 training documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_60",
            "start": 402,
            "end": 515,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_60@4",
            "content": "For ASRS dataset, LwR-RC DvM performs the best, and for AIvsCR dataset, LwR-RC RvM performs the best.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_60",
            "start": 517,
            "end": 617,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_61@0",
            "content": "To investigate it further, we provide average statistics for the number of sentences, the number of rationale sentences, and the percentage of rationale sentences within the documents for each dataset in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_61",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_61@1",
            "content": "We observe that LwR-RC RvM performs better when the percentage of rationale sentences in documents is high, e.g., IMDb and AIvsCR datasets, and LwR-RC DvM performs better when the percentage of rationale sentences is low in the documents, e.g., ASRS dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_61",
            "start": 213,
            "end": 470,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_62@0",
            "content": "We hypothesize that different ranking constraints may be affected differently by a number of factors, including the budget for training documents, the diversity of rationales, the number of rationales provided for each document, how thorough the annotator was in providing rationales, and the domain, to name a few.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_62",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_62@1",
            "content": "Table 2 provides only a glimpse of such a study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_62",
            "start": 316,
            "end": 363,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_62@2",
            "content": "An exhaustive study is needed for making a definitive conclusion about how various document and rationale statistics affect different ranking-constrained losses, which is beyond the scope of this study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_62",
            "start": 365,
            "end": 566,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_62@3",
            "content": "However, the tuning strategy that picks the best \u03bb parameters for LwR-RC at each iteration of learning using a validation set, and hence chooses the appropriate balance between the two loss functions, works well in practice, as was shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_62",
            "start": 568,
            "end": 817,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_63@0",
            "content": "Limitations and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_63",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_64@0",
            "content": "We presented experimental results for binary classification tasks in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_64",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_64@1",
            "content": "To the best of our knowledge, prior learning-with-rationales frameworks also focused on binary classification tasks in their experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_64",
            "start": 81,
            "end": 217,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_64@2",
            "content": "Extending the framework to multi-class settings is a promising future direction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_64",
            "start": 219,
            "end": 298,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_64@3",
            "content": "Such an extension would require adapting the loss functions to multi-class settings and creating multi-class classification datasets with rationales.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_64",
            "start": 300,
            "end": 448,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_64@4",
            "content": "Extending the framework to multi-label settings where a document can be assigned more than one label, however, is more challenging, both for formulating the problem as well as annotating the datasets with rationales, because rationales need to be assigned to their respective labels, which might be more than one in a single document.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_64",
            "start": 450,
            "end": 783,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_65@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_65",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_66@0",
            "content": "We presented a novel approach to incorporate rationales as ranking-constraints into the training of classification models with cross-entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_66",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_66@1",
            "content": "The proposed approach is general enough that it can be used for simple models, such as logistic regression with one-hot encoding of documents, as well as deep learning models combined with text embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_66",
            "start": 147,
            "end": 351,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_66@2",
            "content": "We conducted empirical evaluations comparing the proposed approach to several baselines and observed that the proposed approach outperformed the baselines in most settings, and was comparable to them at the remaining settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_66",
            "start": 353,
            "end": 578,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_67@0",
            "content": "In this section, we supplement the results presented in the paper with the following: \u2022 In the paper, we focused on experimental results with a budget of up to 210 training documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_67",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_68@0",
            "content": "Here, we supplement the main results in the paper with a larger budget of up to 310 documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_68",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_68@1",
            "content": "\u2022 We present the improvements in accuracy provided by LwR-RC over the two fine-tuned BERT baselines, Lw/oR-BERT and RB-WAVG-BERT, for all three datasets at varying budgets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_68",
            "start": 95,
            "end": 266,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_69@0",
            "content": "LwR-RC to Lw/oR-BERT and RB-WAVG-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_69",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_69@1",
            "content": "\u2022 In the paper, we provided the formulation of LwR-RC for binary classification for the ease of exposition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_69",
            "start": 39,
            "end": 145,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_69@2",
            "content": "Here, we extend the formulation of LwR-RC to multi-class classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_69",
            "start": 147,
            "end": 218,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_69@3",
            "content": "\u2022 We provide a complete description of the AIvsCR dataset that we collected and annotated with rationales for the ground truth labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_69",
            "start": 220,
            "end": 353,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_69@4",
            "content": "\u2022 Additionally, we provide the AIvsCR dataset and the other two datasets (IMDb and ASRS), as well as the source code for all the experiments in our paper with this submission as separate .zip files.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_69",
            "start": 355,
            "end": 552,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_70@0",
            "content": "In the paper, we focused on experimental results with a budget of up to 210 training documents (Figure 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_70",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_70@1",
            "content": "We supplement the results in Figure 2 with a larger budget of up to 310 training documents in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_70",
            "start": 107,
            "end": 209,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_70@2",
            "content": "As can be seen in Figure 4, the trends of all the results in the paper remain the same even with larger budgets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_70",
            "start": 211,
            "end": 322,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_70@3",
            "content": "For IMDb and AIvsCR datasets, LwR-RC still performs better or comparably to the most competitive baseline, RB-WAVG-BERT; for ASRS dataset, LwR-RC still outperforms all the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_70",
            "start": 324,
            "end": 505,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_70@4",
            "content": "However, as the number of labeled documents grows, we expect our models and the baselines to converge to a similar accuracy, as the models no longer need the human-provided rationales and can learn statistically \"what is important\" from a large collection of documents that are simply annotated with labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_70",
            "start": 507,
            "end": 813,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_71@0",
            "content": "We present the improvements in accuracy provided by LwR-RC compared to the baselines for the three datasets across different training budgets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_71",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_71@1",
            "content": "Specifically, we compare LwR-RC with the two fine-tuned BERT based approaches, Lw/oR-BERT and RB-WAVG-BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_71",
            "start": 143,
            "end": 249,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_71@2",
            "content": "As shown in Table 3, LwR-RC provides significant improvements in accuracy over the two baselines across most training budgets: for IMDb, the improvements are up to 23.68%; for ASRS, the improvements are up to 28.31%; for AIvsCR, the improvements are up to 8.67%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_71",
            "start": 251,
            "end": 512,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_72@0",
            "content": "In this section, we provide a summary of pairwise one-tailed t-tests comparing LwR-RC with the two most competitive baselines, Lw/oR-BERT and RB-WAVG-BERT, for all three datasets at varying budget regiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_72",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_72@1",
            "content": "Table 4 shows the p-values of onetailed paired t-tests with the alternative hypothesis \"the performance of LwR-RC is better than the baseline approach\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_72",
            "start": 208,
            "end": 359,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_72@2",
            "content": "As this result shows, LwR-RC statistically significantly outperforms both Lw/oR-BERT and RB-WAVG-BERT at most budget regiments with a p-value of less than 0.05.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_72",
            "start": 361,
            "end": 520,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_73@0",
            "content": "In our paper, we focused on binary classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_73",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_73@1",
            "content": "LwR-RC, can be extended to multi-class classification with a few modifications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_73",
            "start": 51,
            "end": 129,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_73@2",
            "content": "For multi-class classification, let",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_73",
            "start": 131,
            "end": 165,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_74@0",
            "content": "y i \u2208 {c 1 , c 2 , \u2022 \u2022 \u2022 , c k }: f (x i ) = p(y i = c | x i ) = sof tmax(W z z i )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_74",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_75@0",
            "content": "for some parameter vector/matrix W z , where c is the correct label for instance x i and z i is the vector representation of x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_75",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_75@1",
            "content": "Assuming that y i is encoded as onehot representation, the classification loss function, L clf , will then change from binary cross-entropy to categorical cross-entropy:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_75",
            "start": 131,
            "end": 299,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_76@0",
            "content": "L clf = \u2212 1 |L| i (y i \u2022 log(p(y i |x i )))(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_76",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_77@0",
            "content": "For modeling the DvM objective of LwR-RC, let \u00b5 i = W z z i and \u00b5 i = W z z i , where z i is the vector representation of x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_77",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_77@1",
            "content": "Then, for the correct label c, we would like \u00b5 c i > 0 and \u00b5 c i > \u00b5 c i , which results in the following objective function:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_77",
            "start": 128,
            "end": 252,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_78@0",
            "content": "L DvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x i , x i )) (9) where, p(y i |x i , x i ) = sof tmax(\u2212(\u00b5 i \u2212 \u00b5 i ))(10",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_78",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_79@0",
            "content": ") We define the ranking loss similarly for the RvM component, this time using the R and M documents and their respective scores \u00b5 r i = W z z r i and \u00b5 i = W z z i , where z r i is the vector representation of x r i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_79",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_79@1",
            "content": "The ranking loss L RvM is then defined as: Figure 4: Comparison between our approach, LwR-RC, and the five baselines using the best hyper-parameter setting for each method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_79",
            "start": 218,
            "end": 389,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_79@2",
            "content": "where, p(y i |x r i , x i ) = sof tmax(\u2212(\u00b5 r i \u2212 \u00b5 i )) (12) We combine the classification loss L clf with the ranking losses, L DvM and L RvM , resulting in the main objective function for our approach for multi-class classification: RC to the two fine-tuned BERT baselines for all three datasets at varying budget regiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_79",
            "start": 391,
            "end": 716,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_79@3",
            "content": "We report the pvalues for one-tailed paired t-tests with the alternative hypothesis \"the performance of our approach is better than the baseline approach\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_79",
            "start": 718,
            "end": 872,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_79@4",
            "content": "The results where LwR-RC performs statistically significantly better than the baselines (p-value of less than 0.05) are boldfaced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_79",
            "start": 874,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_80@0",
            "content": "L RvM = \u2212 1 |L| i (y i \u2022 log(p(y i |x r i , x i ))(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_80",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_81@0",
            "content": "L = (1 \u2212 \u03bb 1 \u2212 \u03bb 2 )L clf + \u03bb 1 L DvM + \u03bb 2 L RvM (13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_81",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_82@0",
            "content": "In our study, we experimented with three humanannotated datasets, IMDb, ASRS, and AIvsCR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_82",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_82@1",
            "content": "We collected and annotated the AIvsCR dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_82",
            "start": 90,
            "end": 135,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_82@2",
            "content": "To construct this dataset, we first collected 6,000 articles equally from two categories, cs.AI and cs.CR, from arXiv.org using a custom search query in the arXiv API.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_82",
            "start": 137,
            "end": 303,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_82@3",
            "content": "We provide the code, including the custom search queries, that we used to collect the data from arXiv.org with the supplementary material.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_82",
            "start": 305,
            "end": 442,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_83@0",
            "content": "For annotating the AIvsCR dataset, two annotators, A1 and A2, were provided with the same instructions as Zaidan et al. (2007) described in their paper: highlight the rationales at your best but do not mark everything.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_83",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_84@0",
            "content": "A1 A2 # rationales per document 3.8 8.4 # rationale words per document 17.4 31.3 % rationales overlapping with A1 100 30.5 % rationales overlapping with A2 64.0 100 Table 5: Average statistics for AIvsCR dataset and the two annotators, A1 and A2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_84",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_84@1",
            "content": "The table presents the number of rationales and the number of rationale words per document provided by the two annotators, as well as the inter-annotator agreement for their rationale annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_84",
            "start": 247,
            "end": 441,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_85@0",
            "content": "We calculated the inter-annotator agreement for the rationales, where the rationales provided by the two annotators for the same document are considered as overlapping if they have at least one word in common, following the same manner of Zaidan et al. (2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_85",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_85@1",
            "content": "The relevant statistics are shown in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_85",
            "start": 261,
            "end": 305,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_85@2",
            "content": "To make the best use of each annotator's effort, for every document, we kept the overlapping words, phrases, and sentences between the two annotators' highlighted rationales as the final rationales, as illustrated in the following example: \u2022 A1: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_85",
            "start": 307,
            "end": 664,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_85@3",
            "content": "\u2022 A2: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_85",
            "start": 666,
            "end": 783,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_85@4",
            "content": "\u2022 Final: rectified linear units are among the most widely used activation function in a broad variety of tasks in vision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_85",
            "start": 785,
            "end": 905,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_86@0",
            "content": "C Charu, Chengxiang Aggarwal,  Zhai, A survey of text classification algorithms, 2012, Mining text data, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_86",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2014, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_87",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2018, Deriving machine attention from human rationales, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_88",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_89",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_90@0",
            "content": "Jeff Donahue, Kristen Grauman, Annotator rationales for visual recognition, 2011, 2011 International Conference on Computer Vision, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_90",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2020, Concept matching for low-resource classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_91",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_92@0",
            "content": "Fatemeh Hemmatian, Mohammad Sohrabi, A survey on classification techniques for opinion mining and sentiment analysis, 2019, Artificial Intelligence Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_92",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2018, Universal language model fine-tuning for text classification, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_93",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_94@0",
            "content": "Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, Krister Lind\u00e9n, Automatic language identification in texts: A survey, 2019, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_94",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_95@0",
            "content": "Quoc Le, Tomas Mikolov, Distributed representations of sentences and documents, 2014, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_95",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_96@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_96",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_97@0",
            "content": "Oren Melamud, Mihaela Bornea, Ken Barker, Combining unsupervised pre-training and annotator rationales to improve low-shot text classification, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_97",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2013, Efficient estimation of word representations in vector space, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_98",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_99@0",
            "content": "Shama Chandramouli, Evangelos Sastry,  Milios, Active neural learners for text with dual supervision, 2020, Neural Computing and Applications, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_99",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2018, Learning with rationales for document classification. Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_100",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_101@0",
            "content": "Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang, How to fine-tune bert for text classification?, 2019, Chinese Computational Linguistics, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_101",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_102@0",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_102",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_103@0",
            "content": "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy, Hierarchical attention networks for document classification, 2016, Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_103",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_104@0",
            "content": "Omar Zaidan, Jason Eisner, Modeling annotators: A generative approach to learning from annotator rationales, 2008, Proceedings of the 2008 conference on Empirical methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_104",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_105@0",
            "content": "Omar Zaidan, Jason Eisner, Christine Piatko, Using \"annotator rationales\" to improve machine learning for text categorization, 2007, Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_105",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_106@0",
            "content": "Jason Omar F Zaidan, Christine Eisner,  Piatko, Machine learning with annotator rationales to reduce annotation cost, 2008, Proceedings of the NIPS* 2008 workshop on cost sensitive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_106",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_107@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Lecun, Character-level convolutional networks for text classification, 2015, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_107",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "448-ARR_v1_108@0",
            "content": "Ye Zhang, Iain Marshall, Byron C Wallace, Rationale-augmented convolutional neural networks for text classification, 2016, Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, NIH Public Access.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "448-ARR_v1_108",
            "start": 0,
            "end": 287,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_1",
            "tgt_ix": "448-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_1",
            "tgt_ix": "448-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_2",
            "tgt_ix": "448-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_4",
            "tgt_ix": "448-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_5",
            "tgt_ix": "448-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_7",
            "tgt_ix": "448-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_13",
            "tgt_ix": "448-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_14",
            "tgt_ix": "448-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_15",
            "tgt_ix": "448-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_19",
            "tgt_ix": "448-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_20",
            "tgt_ix": "448-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_22",
            "tgt_ix": "448-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_24",
            "tgt_ix": "448-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_25",
            "tgt_ix": "448-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_27",
            "tgt_ix": "448-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_28",
            "tgt_ix": "448-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_29",
            "tgt_ix": "448-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_30",
            "tgt_ix": "448-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_31",
            "tgt_ix": "448-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_32",
            "tgt_ix": "448-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_33",
            "tgt_ix": "448-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_34",
            "tgt_ix": "448-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_35",
            "tgt_ix": "448-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_36",
            "tgt_ix": "448-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_37",
            "tgt_ix": "448-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_38",
            "tgt_ix": "448-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_40",
            "tgt_ix": "448-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_39",
            "tgt_ix": "448-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_39",
            "tgt_ix": "448-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_39",
            "tgt_ix": "448-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_41",
            "tgt_ix": "448-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_43",
            "tgt_ix": "448-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_45",
            "tgt_ix": "448-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_46",
            "tgt_ix": "448-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_47",
            "tgt_ix": "448-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_49",
            "tgt_ix": "448-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_49",
            "tgt_ix": "448-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_51",
            "tgt_ix": "448-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_51",
            "tgt_ix": "448-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_51",
            "tgt_ix": "448-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_51",
            "tgt_ix": "448-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_55",
            "tgt_ix": "448-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_55",
            "tgt_ix": "448-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_55",
            "tgt_ix": "448-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_57",
            "tgt_ix": "448-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_57",
            "tgt_ix": "448-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_55",
            "tgt_ix": "448-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_58",
            "tgt_ix": "448-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_61",
            "tgt_ix": "448-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_59",
            "tgt_ix": "448-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_59",
            "tgt_ix": "448-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_59",
            "tgt_ix": "448-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_59",
            "tgt_ix": "448-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_62",
            "tgt_ix": "448-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_63",
            "tgt_ix": "448-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_63",
            "tgt_ix": "448-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_67",
            "tgt_ix": "448-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_66",
            "tgt_ix": "448-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_68",
            "tgt_ix": "448-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_71",
            "tgt_ix": "448-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_73",
            "tgt_ix": "448-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_74",
            "tgt_ix": "448-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_75",
            "tgt_ix": "448-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_76",
            "tgt_ix": "448-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_77",
            "tgt_ix": "448-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_78",
            "tgt_ix": "448-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_80",
            "tgt_ix": "448-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_72",
            "tgt_ix": "448-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_82",
            "tgt_ix": "448-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_81",
            "tgt_ix": "448-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_84",
            "tgt_ix": "448-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_83",
            "tgt_ix": "448-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "448-ARR_v1_0",
            "tgt_ix": "448-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_1",
            "tgt_ix": "448-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_2",
            "tgt_ix": "448-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_2",
            "tgt_ix": "448-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_2",
            "tgt_ix": "448-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_3",
            "tgt_ix": "448-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_4",
            "tgt_ix": "448-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_4",
            "tgt_ix": "448-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_4",
            "tgt_ix": "448-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_4",
            "tgt_ix": "448-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_5",
            "tgt_ix": "448-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_5",
            "tgt_ix": "448-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_5",
            "tgt_ix": "448-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_5",
            "tgt_ix": "448-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_6",
            "tgt_ix": "448-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_7",
            "tgt_ix": "448-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_7",
            "tgt_ix": "448-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_8",
            "tgt_ix": "448-ARR_v1_8@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_9",
            "tgt_ix": "448-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_10",
            "tgt_ix": "448-ARR_v1_10@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_11",
            "tgt_ix": "448-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_12",
            "tgt_ix": "448-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_13",
            "tgt_ix": "448-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_14",
            "tgt_ix": "448-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_14",
            "tgt_ix": "448-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_14",
            "tgt_ix": "448-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_15",
            "tgt_ix": "448-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_16",
            "tgt_ix": "448-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_17",
            "tgt_ix": "448-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_18",
            "tgt_ix": "448-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_19",
            "tgt_ix": "448-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_19",
            "tgt_ix": "448-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_19",
            "tgt_ix": "448-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_19",
            "tgt_ix": "448-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_20",
            "tgt_ix": "448-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_21",
            "tgt_ix": "448-ARR_v1_21@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_22",
            "tgt_ix": "448-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_23",
            "tgt_ix": "448-ARR_v1_23@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_24",
            "tgt_ix": "448-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_25",
            "tgt_ix": "448-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_26",
            "tgt_ix": "448-ARR_v1_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_27",
            "tgt_ix": "448-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_28",
            "tgt_ix": "448-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_29",
            "tgt_ix": "448-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_30",
            "tgt_ix": "448-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_31",
            "tgt_ix": "448-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_32",
            "tgt_ix": "448-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_32",
            "tgt_ix": "448-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_33",
            "tgt_ix": "448-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_34",
            "tgt_ix": "448-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_35",
            "tgt_ix": "448-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_36",
            "tgt_ix": "448-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_37",
            "tgt_ix": "448-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_38",
            "tgt_ix": "448-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_38",
            "tgt_ix": "448-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_38",
            "tgt_ix": "448-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_38",
            "tgt_ix": "448-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_39",
            "tgt_ix": "448-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_40",
            "tgt_ix": "448-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_40",
            "tgt_ix": "448-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_40",
            "tgt_ix": "448-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_40",
            "tgt_ix": "448-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_41",
            "tgt_ix": "448-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_41",
            "tgt_ix": "448-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_41",
            "tgt_ix": "448-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_41",
            "tgt_ix": "448-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_42",
            "tgt_ix": "448-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_43",
            "tgt_ix": "448-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_44",
            "tgt_ix": "448-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_45",
            "tgt_ix": "448-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_45",
            "tgt_ix": "448-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_45",
            "tgt_ix": "448-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_46",
            "tgt_ix": "448-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_46",
            "tgt_ix": "448-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_46",
            "tgt_ix": "448-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_47",
            "tgt_ix": "448-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_47",
            "tgt_ix": "448-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_47",
            "tgt_ix": "448-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_48",
            "tgt_ix": "448-ARR_v1_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_49",
            "tgt_ix": "448-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_50",
            "tgt_ix": "448-ARR_v1_50@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_51",
            "tgt_ix": "448-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_52",
            "tgt_ix": "448-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_53",
            "tgt_ix": "448-ARR_v1_53@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_54",
            "tgt_ix": "448-ARR_v1_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_55",
            "tgt_ix": "448-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_56",
            "tgt_ix": "448-ARR_v1_56@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_57",
            "tgt_ix": "448-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_58",
            "tgt_ix": "448-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_58",
            "tgt_ix": "448-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_59",
            "tgt_ix": "448-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_60",
            "tgt_ix": "448-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_61",
            "tgt_ix": "448-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_61",
            "tgt_ix": "448-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_62",
            "tgt_ix": "448-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_62",
            "tgt_ix": "448-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_62",
            "tgt_ix": "448-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_62",
            "tgt_ix": "448-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_63",
            "tgt_ix": "448-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_64",
            "tgt_ix": "448-ARR_v1_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_65",
            "tgt_ix": "448-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_66",
            "tgt_ix": "448-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_66",
            "tgt_ix": "448-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_66",
            "tgt_ix": "448-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_67",
            "tgt_ix": "448-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_68",
            "tgt_ix": "448-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_68",
            "tgt_ix": "448-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_69",
            "tgt_ix": "448-ARR_v1_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_70",
            "tgt_ix": "448-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_71",
            "tgt_ix": "448-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_71",
            "tgt_ix": "448-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_71",
            "tgt_ix": "448-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_72",
            "tgt_ix": "448-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_72",
            "tgt_ix": "448-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_72",
            "tgt_ix": "448-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_73",
            "tgt_ix": "448-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_73",
            "tgt_ix": "448-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_73",
            "tgt_ix": "448-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_74",
            "tgt_ix": "448-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_75",
            "tgt_ix": "448-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_75",
            "tgt_ix": "448-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_76",
            "tgt_ix": "448-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_77",
            "tgt_ix": "448-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_77",
            "tgt_ix": "448-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_78",
            "tgt_ix": "448-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_79",
            "tgt_ix": "448-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_80",
            "tgt_ix": "448-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_81",
            "tgt_ix": "448-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_82",
            "tgt_ix": "448-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_82",
            "tgt_ix": "448-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_82",
            "tgt_ix": "448-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_82",
            "tgt_ix": "448-ARR_v1_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_83",
            "tgt_ix": "448-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_84",
            "tgt_ix": "448-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_84",
            "tgt_ix": "448-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_85",
            "tgt_ix": "448-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_85",
            "tgt_ix": "448-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_85",
            "tgt_ix": "448-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_85",
            "tgt_ix": "448-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_85",
            "tgt_ix": "448-ARR_v1_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_86",
            "tgt_ix": "448-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_87",
            "tgt_ix": "448-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_88",
            "tgt_ix": "448-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_89",
            "tgt_ix": "448-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_90",
            "tgt_ix": "448-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_91",
            "tgt_ix": "448-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_92",
            "tgt_ix": "448-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_93",
            "tgt_ix": "448-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_94",
            "tgt_ix": "448-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_95",
            "tgt_ix": "448-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_96",
            "tgt_ix": "448-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_97",
            "tgt_ix": "448-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_98",
            "tgt_ix": "448-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_99",
            "tgt_ix": "448-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_100",
            "tgt_ix": "448-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_101",
            "tgt_ix": "448-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_102",
            "tgt_ix": "448-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_103",
            "tgt_ix": "448-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_104",
            "tgt_ix": "448-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_105",
            "tgt_ix": "448-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_106",
            "tgt_ix": "448-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_107",
            "tgt_ix": "448-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "448-ARR_v1_108",
            "tgt_ix": "448-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1332,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "448-ARR",
        "version": 1
    }
}