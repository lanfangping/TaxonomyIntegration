{
    "nodes": [
        {
            "ix": "66-ARR_v1_0",
            "content": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_2",
            "content": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives. On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives. Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "66-ARR_v1_4",
            "content": "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2018), RoBERTa , and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_5",
            "content": "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020;Petroni et al., 2021;Schick and Sch\u00fctze, 2020). Specifically, one type of knowledge that models often struggle with is the visual knowledge of common objects such as attributes (e.g. appearance, measurable quantity) and affordances. This is because this kind of knowledge is rarely explicitly described in the training text due to reporting bias. For example, as shown in Figure 1, people tend to report what interests them rather than general facts such as a shape or color of oranges they already know.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_6",
            "content": "Towards better knowledge-enhanced PTLMs, recent works incorporate external knowledge bases (e.g., knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019;Peters et al., 2019;Wang et al., 2021;Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019;Wang et al., 2020). However, these approaches still suffer from a lack of visual knowledge that is important to understand the real world.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_7",
            "content": "In this paper, we conduct systematic experiments to understand whether such visual knowledge can be transferred into LMs, and if so, how to perform effective knowledge transfer. Specifically, we look into a series of analysis question as follows: (1) Can intermediate pre-training (Pruksachatkun et al., 2020) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_8",
            "content": "For the text knowledge transfer, we utilize text corpus from visual domain, e.g., image captions. We leverage two training objectives for the language model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_9",
            "content": "(2) text contrastive learning augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_10",
            "content": "For the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2019); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_11",
            "content": "We perform comprehensive comparisons on five downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2018), OBQA (Mihaylov et al., 2018), and Rid-dleSense (Lin et al., 2021). Results suggest that:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_12",
            "content": "(1) Simple intermediate pre-training on captions can help improving performance on commonsense reasoning that needs physical or visual knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_13",
            "content": "(2) Cross-modal knowledge transfer approaches consistently improve the performance in a large margin when only few train examples are available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_14",
            "content": "(3) Cross-modal contrastive learning shows that it is best for packaging visual knowledge into LMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_15",
            "content": "Analysis Setup",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "66-ARR_v1_16",
            "content": "In this work, we study how to transfer the visual knowledge into language models. For this study, we introduce our analysis setup: problem formulation, analysis questions, and knowledge corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_17",
            "content": "Problem Formulation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "66-ARR_v1_18",
            "content": "We focus on a pre-trained text encoder f L and an image encoder f V if images are available. f L and f V are initialized with pre-trained model and we continue to pre-train the models on different sources and tasks, which we call intermediate pretraining. After the intermediate pre-training, we fine-tune f L on downstream NLU tasks. Existing NLU benchmarks have been trained against standard supervised learning paradigms that typically require a large number of question answering examples which need a large annotation efforts. However, in scenarios where the number of labeled examples is small, the model tends to overfit the training examples and shows poor generalization performance on test set. Here, we evaluate the intermediate pre-training objective's generalization ability on test set in both fully supervised and lowresource settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_19",
            "content": "Analysis Questions",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "66-ARR_v1_20",
            "content": "In this paper, we provide a comprehensive study for transferring the visual knowledge into LMs. Visual knowledge transfer can be done in two approaches, depending on the source to be trained:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_21",
            "content": "(1) Text knowledge transfer using the text corpus in the visual domain, e.g., image captions and (2) cross-modal knowledge transfer which passes visual knowledge about common objects to LMs by training over paired image and captions. By evaluating the model on 5 downstream datasets that require physical and visual commonsense knowledge, we explore following three research questions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_22",
            "content": "Q1: Can intermediate pre-training on external knowledge sources help transfer visual knowledge to augment text encoders? We investigate diverse intermediate pre-training methods with external knowledge sources including caption data to inject visual information from images and captions into LMs. We first analyze the performance of text and cross-modal knowledge transfer methods with a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a cross-modal contrastive learning with adversarial negative samplings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_23",
            "content": "Pre-training Data",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "66-ARR_v1_24",
            "content": "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014;Chen et al., 2015). MS COCO is a large scale dataset that contains images reflecting the composition of actual everyday scenes and corresponding captions which describe contextual reasoning between objects in the scene. We only use captions for text knowledge transfer while we use both images and captions for cross-modal knowledge transfer. As an ablation study, we explore other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_25",
            "content": "Downstream Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "66-ARR_v1_26",
            "content": "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2018) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and Riddle-Sense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding. We use in-house test sets made from training sets for PIQA and CSQA since test set is not provided to public. We list the data statics in Table 1. Moreover, We additionally test on GLUE (Wang et al., 2018) to evaluate the general text understanding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_27",
            "content": "Evaluation Protocol",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "66-ARR_v1_28",
            "content": "We evaluate the models in both fully supervised and low-resource settings. For both settings, we consider accuracy for 5 different classification tasks and get average performance over tasks to check the final performance. In fully supervised setting, we evaluate models with 3 different random seeds and report the average accuracy. In a low-resource setting, we consider the size of train data to 64 or 128. For each experiment, we run over 5 different sub-samples and show the average accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_29",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "66-ARR_v1_30",
            "content": "In this section, we introduce the following two approaches to integrate visual knowledge into LMs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_31",
            "content": "(1) text knowledge transfer; and (2) cross-modal knowledge transfer. Throughout this section, we assume the data is a collection of image-caption pairs",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_32",
            "content": "(x v i , x l i ) m i=1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_33",
            "content": "and image encoder f V and text encoder f L are given.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_34",
            "content": "Text Knowledge Transfer",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "66-ARR_v1_35",
            "content": "For text knowledge transfer, we investigate following pre-training objectives: (1) masked language modeling; and (2) text contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_36",
            "content": "Masked Language Modeling (MLM) Following BERT (Devlin et al., 2018), we select 15% of input tokens and replace them with [MASK]. Of the selected tokens, 80% are replaced, 10% are not changed and 10% are replaced by random vocabulary token. Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples . MLM objective is the cross-entropy loss for masked token predictions :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_37",
            "content": "\u2113 MLM (x l i ) = \u2212 log p(x l i |x masked ),(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_38",
            "content": "where x i is the i-th token and x masked is a mask.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_39",
            "content": "Text Contrastive Learning (TCL) Contrastive learning aims to learn representations by pulling positive pairs closer and pushing negative pairs apart. Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a;Gao et al., 2021). Given a text encoder f L , and a caption x l i , we first get text representations using the encoders h l i = f L (x l i ). Following Gao et al. (2021), we augment identical positive sample h l + i by different dropout representations. The contrastive loss is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_40",
            "content": "\u2113 l i = \u2212 log e sim(h l i ,h l + i )/\u03c4 N j=1 e sim(h l i ,h l + i )/\u03c4 ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_41",
            "content": "A girl puts an apple in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_42",
            "content": "A girl puts an [MASK] in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_43",
            "content": "A girl puts an envelope in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_44",
            "content": "Figure 3: LM perturbation. We create adversarial negatives using language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_45",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity, i.e., sim(u, v) = u \u2022 v/\u2225u\u2225\u2225v\u2225. \u03c4 represents a temperature parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_46",
            "content": "Cross-modal Knowledge Transfer",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "66-ARR_v1_47",
            "content": "Language models might learn additional information from visual sources such as images and captions. So we include a variety of vision-based approaches and investigate the approaches whether they can benefit from visual sources. We introduce vision-based approaches as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_48",
            "content": "Voken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image retrieval to transfer visual knowledge. It aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs, and call \"voken classification\". Given text x and a voken v i for the i-th token, the loss is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_49",
            "content": "\u2113 voken i = \u2212 log(p(v i |x)).(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_50",
            "content": "Similar to masked language modeling, it classifies each token to a corresponding voken. Vokenization trains language models with the voken classification task and MLM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_51",
            "content": "Masked Language Modeling with Visual Clues VL-BERT (Su et al., 2019) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues. VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora. It shows its effectiveness in many vision-language tasks. We investigate whether this model also succeed in NLP tasks and compare it with others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_52",
            "content": "To harness the visual knowledge from imagecaption datasets, we adopt contrastive loss on image and text vectors. Given an image encoder f V , a text encoder f L , and an image-caption pair (x v i , x l i ), we first get image and text representations using the encoders",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_53",
            "content": "h v i = f V (x v i ), h l i = f L (x l i ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_54",
            "content": "Then the contrastive learning objective contains two loss functions: an image-to-text contrastive loss \u2113 (v,l) and and a text-to-image contrastive loss \u2113 (l,v) . The image-to-text contrastive loss is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_55",
            "content": "\u2113 (v,l) i = \u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 , (4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_56",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_57",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity. This loss encourages a closer distance between representations of aligned imagecaption pairs than unaligned pairs given an image and multiple captions. Similarly, the text-to-image contrastive loss \u2113 (l,v) is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_58",
            "content": "\u2113 (l,v) i = \u2212 log e sim(h l i ,h v i )/\u03c4 N j=1 e sim(h l i ,h v j )/\u03c4 . (5",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_59",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_60",
            "content": "The final loss is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_61",
            "content": "L = 1 N N i=1 (\u2113 (v,l) i + \u2113 (l,v) i ).(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_62",
            "content": "CLIP (Radford et al., 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_63",
            "content": "CMCL with Adversarial Negative Samples (ANS) As in-batch negatives in CMCL are not challenging enough for models to distinguish, we present adversarial negative sampling strategy to improve CMCL. Given an image-caption pair (x v i , x l i ), we define a LM-perturbed sentence x l \u2212 i , which is a hard negative where n is replaced with a different word n \u2032 from a probability distribution of PTLMs. We expect the l \u2212 is syntactically correct and plausible sentence even the word n is replaced to n \u2032 , while it does not semantically match to the corresponding image x v i . With such hard negative, we try to make more challenging task so that models can effectively learn from the task. For example, we choose a word 'girl' in the sentence 'A girl puts an apple in her bag.' in Figure 3. Then we mask the word with [MASK] token to do masked token predictions by PTLMs. Then we get topk predictions from language models and replace the masked tokens with one of the predicted ones. To avoid false negative sentences which may have the same semantics as the original sentence, we introduce an additional filtering step: if the masked predictions are synonyms or hypernyms of the original tokens, we discard the predictions. We use WordNet (Miller, 1995) to find synonyms and hypernyms. The contrastive loss with hard negative is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_64",
            "content": "\u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 + M k=1 e sim(h v i ,h l \u2212 j )/\u03c4 , (7",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_65",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_66",
            "content": "where M is the number of hard negative samples per positive pair. This formula is only for image-totext contrastive loss \u2113 (v,l) and final loss is defined to same as equation ( 6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_67",
            "content": "In ANS, we filter perturbed sentences where the masked predictions are synonyms or hypernyms of the original tokens. Instead of excluding these perturbed sentences, another option is to include them as additional positive samples l + to the paired images. We name this as positive sample augmentation (PSA). It also adopts LM-perturbed negative samples as in ANS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_68",
            "content": "Cross-model knowledge distillation is to transfer knowledge between different modalities, e.g., image modality and text modality. In this category, CMKD is to transfer knowledge from a teacher model which is knowledgeable about visual information. VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding. A teacher model is first trained using contrastive learning on a video-text dataset, and then it transfers its knowledge to a student language model using KD on a text corpus. Their contrastive learning loss (hinge loss) is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_69",
            "content": "L = N i [max(0, \u03b1\u2212sim(h v i , h l i )+sim(h v \u2032 i , h l i )) + max(0, \u03b1 \u2212 sim(h v i , h l i ) + sim(h v i , h l \u2032 i ))],(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_70",
            "content": "where v \u2032 and l \u2032 are a random image and caption text, respectively. \u03b1 is the margin between the similarities of a positive pair and a negative pair. Instead of video datasets, we use a MS COCO dataset to train a teacher model and use two versions of contrastive learning, equations ( 6) and ( 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_71",
            "content": "As another version of CMKD, we consider distilling visual knowledge from a pre-trained vision- language model, VL-BERT, which is knowledgeable about grounded language. We adopt masked language modeling on Wikitext103 (Merity et al., 2016), a subset of English Wikipedia, in the knowledge distillation step. For knowledge distillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_72",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "66-ARR_v1_73",
            "content": "For all the approaches, we use bert-base-uncased (Devlin et al., 2018) as text encoder f L and ResNeXt101 (Xie et al., 2017) as an image encoder f V . For text knowledge transfer, (1) MLM follows the exact setting of codebase in huggingface 2 which uses dynamic masking strategy to conduct language modling task. (2) TCL conducts contrastive learning with f L . We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017). For cross-modal knowledge transfer, (1) CMKD explores VL-BERT, Vokenization, and VidLanKD approaches. Here, we use VL-BERT-large model to do CMKD. Vokenization uses a checkpoint from their official codebase 3 and VidLanKD trains a teacher model by two versions of contrastive learning (equations ( 6) and ( 8)) on MS COCO dataset. We set \u03b1 = 1 in VidLanKD (equation ( 8)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_74",
            "content": "(2) CMCL conducts contrastive learning with f L and f V . Here, we set \u03c4 = 0.05 (equations ( 4) and ( 5)). ( 3 use top-5 predictions from f L as replacement. We filter out synonyms and hypernyms of original words using WordNet (Miller, 1995). ( 4) CMCL with PSA includes the perturbed sentences with synonyms and hypernyms as additional positive samples. In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder f V and BERT as a text encoder f L . TCL and CMCL train with batch size 64, maximum sequence length 20, learning rate 1e-4 for 3 epochs. For fine-tuning on downstream tasks, we do grid search on learning rates {5e-5, 1e-4, 3e-4, 4e-4, 5e-4, 6e-4} and choose the best learning rate. We set maximum epochs to 30 in low-resource and 15 in fully supervised settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_75",
            "content": "Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "66-ARR_v1_76",
            "content": "We analyze the main results of intermediate pretraining. Tables 2 and 3 show the main results of low-resource learning and fully supervised learning with the MS COCO captioning dataset, respectively. We train the models with a few training examples, 2 and 3). Specifically, CMKD with VidLanKD variant outperforms the baseline by 1.6% point on the PIQA dataset in fully supervised setting. CMCL also shows its effectiveness. However, we could find that it becomes more powerful when equipped with PSA and ANS. It suggests that data augmentation for positive and negative sampling is an important factor for CMCL. In low-resource setting, we find that cross-modal knowledge transfer helps better initialization and let models learn new tasks faster.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_77",
            "content": "What intermediate pre-training objectives are effective for cross-modal knowledge transfer? Among various cross-modal knowledge transfer methods, we study which method is the most effective for cross-modal knowledge transfer. Overall, CMCL with PSA and ANS shows the best performance among all cross-modal methods. Interestingly, VL-BERT also shows better performance than BERT-base on all datasets in the low-resource setting. This suggesting that exploiting images in masked language modeling task help transfer the knowledge to language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_78",
            "content": "What types of knowledge sources are most helpful? Here, we investigate whether using an image source in addition to a text source can further improve the model. To answer this question, we analyze methods from different types of sources: text-only and text-image pair sources. We focus on the methods that use the contrastive learning objective: TCL and CMCL. Note that these two methods share the same objective but CMCL trains on cross modalities which are images and captions while TCL only trains on captions Overall, TCL performs slightly better than CMCL in low-resource and fully supervised settings. Interestingly, additional negative samples (ANS) and positive samples in TCL decreases the performance while they help CMCL to improve the performance. We conjecture that perturbed sentences in ANS might not be semantically negative to the original sentence so models learn from wrong labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_79",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "66-ARR_v1_80",
            "content": "How do models perform on general NLU tasks? (Lin et al., 2014;Chen et al., 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al., 2016) gests including perturbed sentences as positive and negative samples are useful to cross-modal knowledge transfer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_81",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "66-ARR_v1_82",
            "content": "Text Knowledge enhanced methods. Recently, huge efforts on integrating knowledge into PTLMs have been made. One typical form of knowledge is a knowledge graph. There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019;Peters et al., 2019;He et al., 2020). Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019;Wang et al., 2020). Another existing form of knowledge is extra large-scale corpus. Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al., 2021). They are mostly focused on injecting world knowledge presented in text, rather than physical and visual commonsense knowledge that can be found in images.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_83",
            "content": "There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2019), VisualBert (Li et al., 2019), and Uniter (Chen et al., 2020b). These models aim to improve vision-language tasks, e.g., VQA (Goyal et al., 2017), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020). Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021). Vokenization employs token-level text-toimage retrieval to transfer visual knowledge to language models. For this, Vokenization introduces 30k vokens and matches each token into the limited voken space; it may have approximation errors. VidLanKD adopts contrastive learning to train a teacher model on video datasets and uses distillation approaches to distill visual knowledge from the teacher to a student model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_84",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "66-ARR_v1_85",
            "content": "We study whether intermediate pre-training on visual knowledge can help transfer visual knowledge into LMs. We investigate text knowledge transfer and cross-modal knowledge transfer using images and captions. In our empirical analysis, we observe that intermediate pre-training on captions can help improving performance and cross-modal knowledge transfer approaches consistently improve performance. When the transfer methods are equipped with additional positive and negative samples, they show better performance. Future works include improving both commonsense reasoning and general language understanding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_86",
            "content": "PIQA is a multiple-choice question answering task, which chooses the most appropriate solution for physical commonsense questions, which may need illustration or description of physical interaction in the real world. VP is to tell if two descriptions are describing the same scene or two different scenes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_87",
            "content": "While they seem like purely textual tasks, they require visual common sense to answer. CSQA is a multiple-choice question answering task that requires commonsense reasoning to answer. It is built from ConceptNet (Speer et al., 2017). OBQA is a multiple-choice question answering task, which is modeled after open book exams on elementarylevel core science questions. The task generally requires open book fact but also additional commonsense which can be learnt from scientific illustration. RiddleSense is a multiple-choice riddlestyle question answering which requires complex commonsense reasoning ability and understanding of figurative language which may benefit from visual knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v1_88",
            "content": "Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark, Genericskb: A knowledge base of generic statements, 2020, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Sumithra Bhakthavatsalam",
                    "Chloe Anastasiades",
                    "Peter Clark"
                ],
                "title": "Genericskb: A knowledge base of generic statements",
                "pub_date": "2020",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_89",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Piqa: Reasoning about physical commonsense in natural language, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Yonatan Bisk",
                    "Rowan Zellers",
                    "Jianfeng Gao",
                    "Yejin Choi"
                ],
                "title": "Piqa: Reasoning about physical commonsense in natural language",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_90",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Daniel Cer",
                    "Mona Diab",
                    "Eneko Agirre",
                    "I\u00f1igo Lopez-Gazpio",
                    "Lucia Specia"
                ],
                "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v1_91",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "66-ARR_v1_92",
            "content": "UNKNOWN, None, 2015, Microsoft coco captions: Data collection and evaluation server, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Microsoft coco captions: Data collection and evaluation server",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_93",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "Uniter: Universal image-text representation learning",
                "pub_date": "2020",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "66-ARR_v1_94",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_95",
            "content": "UNKNOWN, None, 2021, Simcse: Simple contrastive learning of sentence embeddings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Simcse: Simple contrastive learning of sentence embeddings",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_96",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_97",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, 2020. Don't stop pretraining: Adapt language models to domains and tasks, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ana Suchin Gururangan",
                    "Swabha Marasovi\u0107",
                    "Kyle Swayamdipta",
                    "Iz Lo",
                    "Doug Beltagy",
                    "Noah Downey",
                    " Smith"
                ],
                "title": "2020. Don't stop pretraining: Adapt language models to domains and tasks",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v1_98",
            "content": "Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Yuan, Tong Xu, BERT-MK: Integrating graph contextualized knowledge into pretrained language models, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Bin He",
                    "Di Zhou",
                    "Jinghui Xiao",
                    "Xin Jiang",
                    "Qun Liu",
                    "Nicholas Yuan",
                    "Tong Xu"
                ],
                "title": "BERT-MK: Integrating graph contextualized knowledge into pretrained language models",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_99",
            "content": "UNKNOWN, None, 2017, Like what you like: Knowledge distill via neuron selectivity transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Like what you like: Knowledge distill via neuron selectivity transfer",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_100",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visualbert: A simple and performant baseline for vision and language",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_101",
            "content": "Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen,  Ren, KagNet: Knowledge-aware graph networks for commonsense reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Xinyue Bill Yuchen Lin",
                    "Jamin Chen",
                    "Xiang Chen",
                    " Ren"
                ],
                "title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v1_102",
            "content": "Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee,  Ren, Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Ziyi Bill Yuchen Lin",
                    "Yichi Wu",
                    "Dong-Ho Yang",
                    "Xiang Lee",
                    " Ren"
                ],
                "title": "Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_103",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Tsung-Yi Lin",
                    "Michael Maire",
                    "Serge Belongie",
                    "James Hays",
                    "Pietro Perona",
                    "Deva Ramanan",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco: Common objects in context",
                "pub_date": "2014",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "66-ARR_v1_104",
            "content": "Xiao Lin, Devi Parikh, Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Xiao Lin",
                    "Devi Parikh"
                ],
                "title": "Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_105",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_106",
            "content": "UNKNOWN, None, 2016, Pointer sentinel mixture models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Pointer sentinel mixture models",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_107",
            "content": "UNKNOWN, None, 2018, Can a suit of armor conduct electricity? a new dataset for open book question answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_108",
            "content": "A George,  Miller, Wordnet: a lexical database for english, 1995, Communications of the ACM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "A George",
                    " Miller"
                ],
                "title": "Wordnet: a lexical database for english",
                "pub_date": "1995",
                "pub_title": "Communications of the ACM",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_109",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Matthew Peters",
                    "Mark Neumann",
                    "Robert Logan",
                    "Roy Schwartz",
                    "Vidur Joshi",
                    "Sameer Singh",
                    "Noah Smith"
                ],
                "title": "Knowledge enhanced contextual word representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v1_110",
            "content": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, KILT: a benchmark for knowledge intensive language tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Fabio Petroni",
                    "Aleksandra Piktus",
                    "Angela Fan",
                    "Patrick Lewis",
                    "Majid Yazdani",
                    "Nicola Cao",
                    "James Thorne",
                    "Yacine Jernite",
                    "Vladimir Karpukhin",
                    "Jean Maillard",
                    "Vassilis Plachouras",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel"
                ],
                "title": "KILT: a benchmark for knowledge intensive language tasks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_111",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Yada Pruksachatkun",
                    "Jason Phang",
                    "Haokun Liu",
                    "Xiaoyi Phu Mon Htut",
                    "Richard Zhang",
                    "Clara Pang",
                    "Katharina Vania",
                    "Samuel Kann"
                ],
                "title": "Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_112",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, , Learning transferable visual models from natural language supervision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal"
                ],
                "title": "Girish Sastry",
                "pub_date": null,
                "pub_title": "Learning transferable visual models from natural language supervision",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_113",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_114",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "66-ARR_v1_115",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Piyush Sharma",
                    "Nan Ding",
                    "Sebastian Goodman",
                    "Radu Soricut"
                ],
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "66-ARR_v1_116",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "66-ARR_v1_117",
            "content": "UNKNOWN, None, 2019, Vl-bert: Pre-training of generic visual-linguistic representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Vl-bert: Pre-training of generic visual-linguistic representations",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_118",
            "content": "UNKNOWN, None, 2018, Commonsenseqa: A question answering challenge targeting commonsense knowledge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_119",
            "content": "UNKNOWN, None, 2020, Vokenization: improving language understanding with contextualized, visual-grounded supervision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Vokenization: improving language understanding with contextualized, visual-grounded supervision",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_120",
            "content": "UNKNOWN, None, 2021, Vidlankd: Improving language understanding via video-distilled knowledge transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Vidlankd: Improving language understanding via video-distilled knowledge transfer",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_121",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_122",
            "content": "Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, Connecting the dots: A knowledgeable path generator for commonsense question answering, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Peifeng Wang",
                    "Nanyun Peng",
                    "Filip Ilievski",
                    "Pedro Szekely",
                    "Xiang Ren"
                ],
                "title": "Connecting the dots: A knowledgeable path generator for commonsense question answering",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v1_123",
            "content": "Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Ruize Wang",
                    "Duyu Tang",
                    "Nan Duan",
                    "Zhongyu Wei",
                    "Xuanjing Huang",
                    "Jianshu Ji",
                    "Guihong Cao",
                    "Daxin Jiang",
                    "Ming Zhou"
                ],
                "title": "2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_124",
            "content": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Aggregated residual transformations for deep neural networks, 2017, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Saining Xie",
                    "Ross Girshick",
                    "Piotr Doll\u00e1r",
                    "Zhuowen Tu",
                    "Kaiming He"
                ],
                "title": "Aggregated residual transformations for deep neural networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_125",
            "content": "UNKNOWN, None, 2020, Jaket: Joint pre-training of knowledge graph and language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Jaket: Joint pre-training of knowledge graph and language understanding",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_126",
            "content": "UNKNOWN, None, 2021, Dict-bert: Enhancing language model pre-training with dictionary, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Dict-bert: Enhancing language model pre-training with dictionary",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_127",
            "content": "UNKNOWN, None, 2020, Contrastive learning of medical visual representations from paired images and text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Contrastive learning of medical visual representations from paired images and text",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_128",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Zhengyan Zhang",
                    "Xu Han",
                    "Zhiyuan Liu",
                    "Xin Jiang",
                    "Maosong Sun",
                    "Qun Liu"
                ],
                "title": "ERNIE: Enhanced language representation with informative entities",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_129",
            "content": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Selvam, Seyeon Lee, Xiang Ren, Pre-training text-to-text transformers for concept-centric common sense, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Wangchunshu Zhou",
                    "Dong-Ho Lee",
                    "Ravi Selvam",
                    "Seyeon Lee",
                    "Xiang Ren"
                ],
                "title": "Pre-training text-to-text transformers for concept-centric common sense",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_130",
            "content": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, The IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Yukun Zhu",
                    "Ryan Kiros",
                    "Rich Zemel",
                    "Ruslan Salakhutdinov",
                    "Raquel Urtasun",
                    "Antonio Torralba",
                    "Sanja Fidler"
                ],
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "pub_date": "2015",
                "pub_title": "The IEEE International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v1_131",
            "content": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Yukun Zhu",
                    "Ryan Kiros",
                    "Rich Zemel",
                    "Ruslan Salakhutdinov",
                    "Raquel Urtasun",
                    "Antonio Torralba",
                    "Sanja Fidler"
                ],
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE international conference on computer vision",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "66-ARR_v1_0@0",
            "content": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_0",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@0",
            "content": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@1",
            "content": "In this work, we study whether integrating visual knowledge into a language model can fill the gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 266,
            "end": 364,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@2",
            "content": "We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 366,
            "end": 616,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@3",
            "content": "On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 618,
            "end": 765,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@4",
            "content": "Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 767,
            "end": 893,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_2@5",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_2",
            "start": 895,
            "end": 895,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_4@0",
            "content": "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2018), RoBERTa , and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_4",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_5@0",
            "content": "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020;Petroni et al., 2021;Schick and Sch\u00fctze, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_5",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_5@1",
            "content": "Specifically, one type of knowledge that models often struggle with is the visual knowledge of common objects such as attributes (e.g. appearance, measurable quantity) and affordances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_5",
            "start": 273,
            "end": 456,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_5@2",
            "content": "This is because this kind of knowledge is rarely explicitly described in the training text due to reporting bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_5",
            "start": 458,
            "end": 570,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_5@3",
            "content": "For example, as shown in Figure 1, people tend to report what interests them rather than general facts such as a shape or color of oranges they already know.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_5",
            "start": 572,
            "end": 728,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_6@0",
            "content": "Towards better knowledge-enhanced PTLMs, recent works incorporate external knowledge bases (e.g., knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019;Peters et al., 2019;Wang et al., 2021;Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019;Wang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_6",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_6@1",
            "content": "However, these approaches still suffer from a lack of visual knowledge that is important to understand the real world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_6",
            "start": 351,
            "end": 468,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_7@0",
            "content": "In this paper, we conduct systematic experiments to understand whether such visual knowledge can be transferred into LMs, and if so, how to perform effective knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_7",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_7@1",
            "content": "Specifically, we look into a series of analysis question as follows: (1) Can intermediate pre-training (Pruksachatkun et al., 2020) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_7",
            "start": 178,
            "end": 654,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_8@0",
            "content": "For the text knowledge transfer, we utilize text corpus from visual domain, e.g., image captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_8",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_8@1",
            "content": "We leverage two training objectives for the language model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_8",
            "start": 98,
            "end": 350,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_9@0",
            "content": "(2) text contrastive learning augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_9",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_10@0",
            "content": "For the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2019); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_10",
            "start": 0,
            "end": 914,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_11@0",
            "content": "We perform comprehensive comparisons on five downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2018), OBQA (Mihaylov et al., 2018), and Rid-dleSense (Lin et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_11",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_11@1",
            "content": "Results suggest that:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_11",
            "start": 301,
            "end": 321,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_12@0",
            "content": "(1) Simple intermediate pre-training on captions can help improving performance on commonsense reasoning that needs physical or visual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_12",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_13@0",
            "content": "(2) Cross-modal knowledge transfer approaches consistently improve the performance in a large margin when only few train examples are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_13",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_14@0",
            "content": "(3) Cross-modal contrastive learning shows that it is best for packaging visual knowledge into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_14",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_15@0",
            "content": "Analysis Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_15",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_16@0",
            "content": "In this work, we study how to transfer the visual knowledge into language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_16",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_16@1",
            "content": "For this study, we introduce our analysis setup: problem formulation, analysis questions, and knowledge corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_16",
            "start": 82,
            "end": 193,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_17@0",
            "content": "Problem Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_17",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@0",
            "content": "We focus on a pre-trained text encoder f L and an image encoder f V if images are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@1",
            "content": "f L and f V are initialized with pre-trained model and we continue to pre-train the models on different sources and tasks, which we call intermediate pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 93,
            "end": 254,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@2",
            "content": "After the intermediate pre-training, we fine-tune f L on downstream NLU tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 256,
            "end": 333,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@3",
            "content": "Existing NLU benchmarks have been trained against standard supervised learning paradigms that typically require a large number of question answering examples which need a large annotation efforts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 335,
            "end": 530,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@4",
            "content": "However, in scenarios where the number of labeled examples is small, the model tends to overfit the training examples and shows poor generalization performance on test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 532,
            "end": 703,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_18@5",
            "content": "Here, we evaluate the intermediate pre-training objective's generalization ability on test set in both fully supervised and lowresource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_18",
            "start": 705,
            "end": 849,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_19@0",
            "content": "Analysis Questions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_19",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_20@0",
            "content": "In this paper, we provide a comprehensive study for transferring the visual knowledge into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_20",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_20@1",
            "content": "Visual knowledge transfer can be done in two approaches, depending on the source to be trained:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_20",
            "start": 96,
            "end": 190,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_21@0",
            "content": "(1) Text knowledge transfer using the text corpus in the visual domain, e.g., image captions and (2) cross-modal knowledge transfer which passes visual knowledge about common objects to LMs by training over paired image and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_21",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_21@1",
            "content": "By evaluating the model on 5 downstream datasets that require physical and visual commonsense knowledge, we explore following three research questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_21",
            "start": 234,
            "end": 384,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_22@0",
            "content": "Q1: Can intermediate pre-training on external knowledge sources help transfer visual knowledge to augment text encoders?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_22",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_22@1",
            "content": "We investigate diverse intermediate pre-training methods with external knowledge sources including caption data to inject visual information from images and captions into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_22",
            "start": 121,
            "end": 295,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_22@2",
            "content": "We first analyze the performance of text and cross-modal knowledge transfer methods with a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a cross-modal contrastive learning with adversarial negative samplings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_22",
            "start": 297,
            "end": 676,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_23@0",
            "content": "Pre-training Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_23",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_24@0",
            "content": "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014;Chen et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_24",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_24@1",
            "content": "MS COCO is a large scale dataset that contains images reflecting the composition of actual everyday scenes and corresponding captions which describe contextual reasoning between objects in the scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_24",
            "start": 121,
            "end": 319,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_24@2",
            "content": "We only use captions for text knowledge transfer while we use both images and captions for cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_24",
            "start": 321,
            "end": 442,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_24@3",
            "content": "As an ablation study, we explore other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2016) and BookCorpus (Zhu et al., 2015a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_24",
            "start": 444,
            "end": 611,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_25@0",
            "content": "Downstream Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_26@0",
            "content": "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2018) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and Riddle-Sense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_26",
            "start": 0,
            "end": 569,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_26@1",
            "content": "We use in-house test sets made from training sets for PIQA and CSQA since test set is not provided to public.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_26",
            "start": 571,
            "end": 679,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_26@2",
            "content": "We list the data statics in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_26",
            "start": 681,
            "end": 716,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_26@3",
            "content": "Moreover, We additionally test on GLUE (Wang et al., 2018) to evaluate the general text understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_26",
            "start": 718,
            "end": 819,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_27@0",
            "content": "Evaluation Protocol",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_27",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_28@0",
            "content": "We evaluate the models in both fully supervised and low-resource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_28",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_28@1",
            "content": "For both settings, we consider accuracy for 5 different classification tasks and get average performance over tasks to check the final performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_28",
            "start": 75,
            "end": 221,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_28@2",
            "content": "In fully supervised setting, we evaluate models with 3 different random seeds and report the average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_28",
            "start": 223,
            "end": 332,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_28@3",
            "content": "In a low-resource setting, we consider the size of train data to 64 or 128.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_28",
            "start": 334,
            "end": 408,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_28@4",
            "content": "For each experiment, we run over 5 different sub-samples and show the average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_28",
            "start": 410,
            "end": 496,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_29@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_29",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_30@0",
            "content": "In this section, we introduce the following two approaches to integrate visual knowledge into LMs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_30",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_31@0",
            "content": "(1) text knowledge transfer; and (2) cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_31",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_31@1",
            "content": "Throughout this section, we assume the data is a collection of image-caption pairs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_31",
            "start": 69,
            "end": 150,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_32@0",
            "content": "(x v i , x l i ) m i=1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_32",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_33@0",
            "content": "and image encoder f V and text encoder f L are given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_33",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_34@0",
            "content": "Text Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_34",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_35@0",
            "content": "For text knowledge transfer, we investigate following pre-training objectives: (1) masked language modeling; and (2) text contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_35",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_36@0",
            "content": "Masked Language Modeling (MLM) Following BERT (Devlin et al., 2018), we select 15% of input tokens and replace them with [MASK].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_36",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_36@1",
            "content": "Of the selected tokens, 80% are replaced, 10% are not changed and 10% are replaced by random vocabulary token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_36",
            "start": 129,
            "end": 238,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_36@2",
            "content": "Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_36",
            "start": 240,
            "end": 385,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_36@3",
            "content": "MLM objective is the cross-entropy loss for masked token predictions :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_36",
            "start": 387,
            "end": 456,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_37@0",
            "content": "\u2113 MLM (x l i ) = \u2212 log p(x l i |x masked ),(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_37",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_38@0",
            "content": "where x i is the i-th token and x masked is a mask.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_38",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_39@0",
            "content": "Text Contrastive Learning (TCL) Contrastive learning aims to learn representations by pulling positive pairs closer and pushing negative pairs apart.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_39",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_39@1",
            "content": "Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_39",
            "start": 150,
            "end": 281,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_39@2",
            "content": "Given a text encoder f L , and a caption x l i , we first get text representations using the encoders h l i = f L (x l i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_39",
            "start": 283,
            "end": 405,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_39@3",
            "content": "Following Gao et al. (2021), we augment identical positive sample h l + i by different dropout representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_39",
            "start": 407,
            "end": 517,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_39@4",
            "content": "The contrastive loss is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_39",
            "start": 519,
            "end": 561,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_40@0",
            "content": "\u2113 l i = \u2212 log e sim(h l i ,h l + i )/\u03c4 N j=1 e sim(h l i ,h l + i )/\u03c4 ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_40",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_41@0",
            "content": "A girl puts an apple in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_41",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_42@0",
            "content": "A girl puts an [MASK] in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_42",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_43@0",
            "content": "A girl puts an envelope in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_43",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_44@0",
            "content": "Figure 3: LM perturbation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_44",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_44@1",
            "content": "We create adversarial negatives using language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_44",
            "start": 27,
            "end": 80,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_45@0",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity, i.e., sim(u, v) = u \u2022 v/\u2225u\u2225\u2225v\u2225. \u03c4 represents a temperature parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_45",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_46@0",
            "content": "Cross-modal Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_46",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_47@0",
            "content": "Language models might learn additional information from visual sources such as images and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_47",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_47@1",
            "content": "So we include a variety of vision-based approaches and investigate the approaches whether they can benefit from visual sources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_47",
            "start": 100,
            "end": 226,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_47@2",
            "content": "We introduce vision-based approaches as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_47",
            "start": 228,
            "end": 275,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_48@0",
            "content": "Voken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image retrieval to transfer visual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_48",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_48@1",
            "content": "It aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs, and call \"voken classification\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_48",
            "start": 131,
            "end": 269,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_48@2",
            "content": "Given text x and a voken v i for the i-th token, the loss is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_48",
            "start": 271,
            "end": 341,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_49@0",
            "content": "\u2113 voken i = \u2212 log(p(v i |x)).(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_49",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_50@0",
            "content": "Similar to masked language modeling, it classifies each token to a corresponding voken.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_50",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_50@1",
            "content": "Vokenization trains language models with the voken classification task and MLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_50",
            "start": 88,
            "end": 166,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_51@0",
            "content": "Masked Language Modeling with Visual Clues VL-BERT (Su et al., 2019) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_51",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_51@1",
            "content": "VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_51",
            "start": 237,
            "end": 411,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_51@2",
            "content": "It shows its effectiveness in many vision-language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_51",
            "start": 413,
            "end": 469,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_51@3",
            "content": "We investigate whether this model also succeed in NLP tasks and compare it with others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_51",
            "start": 471,
            "end": 557,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_52@0",
            "content": "To harness the visual knowledge from imagecaption datasets, we adopt contrastive loss on image and text vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_52",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_52@1",
            "content": "Given an image encoder f V , a text encoder f L , and an image-caption pair (x v i , x l i ), we first get image and text representations using the encoders",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_52",
            "start": 113,
            "end": 268,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_53@0",
            "content": "h v i = f V (x v i ), h l i = f L (x l i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_53",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_54@0",
            "content": "Then the contrastive learning objective contains two loss functions: an image-to-text contrastive loss \u2113 (v,l) and and a text-to-image contrastive loss \u2113 (l,v) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_54",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_54@1",
            "content": "The image-to-text contrastive loss is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_54",
            "start": 162,
            "end": 218,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_55@0",
            "content": "\u2113 (v,l) i = \u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 , (4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_55",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_56@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_56",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_57@0",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_57",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_57@1",
            "content": "This loss encourages a closer distance between representations of aligned imagecaption pairs than unaligned pairs given an image and multiple captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_57",
            "start": 65,
            "end": 215,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_57@2",
            "content": "Similarly, the text-to-image contrastive loss \u2113 (l,v) is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_57",
            "start": 217,
            "end": 292,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_58@0",
            "content": "\u2113 (l,v) i = \u2212 log e sim(h l i ,h v i )/\u03c4 N j=1 e sim(h l i ,h v j )/\u03c4 . (5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_58",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_59@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_59",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_60@0",
            "content": "The final loss is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_60",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_61@0",
            "content": "L = 1 N N i=1 (\u2113 (v,l) i + \u2113 (l,v) i ).(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_61",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_62@0",
            "content": "CLIP (Radford et al., 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_62",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@0",
            "content": "CMCL with Adversarial Negative Samples (ANS) As in-batch negatives in CMCL are not challenging enough for models to distinguish, we present adversarial negative sampling strategy to improve CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@1",
            "content": "Given an image-caption pair (x v i , x l i ), we define a LM-perturbed sentence x l \u2212 i , which is a hard negative where n is replaced with a different word n \u2032 from a probability distribution of PTLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 196,
            "end": 397,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@2",
            "content": "We expect the l \u2212 is syntactically correct and plausible sentence even the word n is replaced to n \u2032 , while it does not semantically match to the corresponding image x v i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 399,
            "end": 572,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@3",
            "content": "With such hard negative, we try to make more challenging task so that models can effectively learn from the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 574,
            "end": 686,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@4",
            "content": "For example, we choose a word 'girl' in the sentence 'A girl puts an apple in her bag.' in Figure 3. Then we mask the word with [MASK] token to do masked token predictions by PTLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 688,
            "end": 868,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@5",
            "content": "Then we get topk predictions from language models and replace the masked tokens with one of the predicted ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 870,
            "end": 980,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@6",
            "content": "To avoid false negative sentences which may have the same semantics as the original sentence, we introduce an additional filtering step: if the masked predictions are synonyms or hypernyms of the original tokens, we discard the predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 982,
            "end": 1221,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@7",
            "content": "We use WordNet (Miller, 1995) to find synonyms and hypernyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 1223,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_63@8",
            "content": "The contrastive loss with hard negative is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_63",
            "start": 1285,
            "end": 1346,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_64@0",
            "content": "\u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 + M k=1 e sim(h v i ,h l \u2212 j )/\u03c4 , (7",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_64",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_65@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_65",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_66@0",
            "content": "where M is the number of hard negative samples per positive pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_66",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_66@1",
            "content": "This formula is only for image-totext contrastive loss \u2113 (v,l) and final loss is defined to same as equation ( 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_66",
            "start": 66,
            "end": 179,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_67@0",
            "content": "In ANS, we filter perturbed sentences where the masked predictions are synonyms or hypernyms of the original tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_67",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_67@1",
            "content": "Instead of excluding these perturbed sentences, another option is to include them as additional positive samples l + to the paired images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_67",
            "start": 117,
            "end": 254,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_67@2",
            "content": "We name this as positive sample augmentation (PSA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_67",
            "start": 256,
            "end": 306,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_67@3",
            "content": "It also adopts LM-perturbed negative samples as in ANS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_67",
            "start": 308,
            "end": 362,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_68@0",
            "content": "Cross-model knowledge distillation is to transfer knowledge between different modalities, e.g., image modality and text modality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_68",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_68@1",
            "content": "In this category, CMKD is to transfer knowledge from a teacher model which is knowledgeable about visual information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_68",
            "start": 130,
            "end": 246,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_68@2",
            "content": "VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_68",
            "start": 248,
            "end": 378,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_68@3",
            "content": "A teacher model is first trained using contrastive learning on a video-text dataset, and then it transfers its knowledge to a student language model using KD on a text corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_68",
            "start": 380,
            "end": 554,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_68@4",
            "content": "Their contrastive learning loss (hinge loss) is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_68",
            "start": 556,
            "end": 613,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_69@0",
            "content": "L = N i [max(0, \u03b1\u2212sim(h v i , h l i )+sim(h v \u2032 i , h l i )) + max(0, \u03b1 \u2212 sim(h v i , h l i ) + sim(h v i , h l \u2032 i ))],(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_69",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_70@0",
            "content": "where v \u2032 and l \u2032 are a random image and caption text, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_70",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_70@1",
            "content": "\u03b1 is the margin between the similarities of a positive pair and a negative pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_70",
            "start": 69,
            "end": 148,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_70@2",
            "content": "Instead of video datasets, we use a MS COCO dataset to train a teacher model and use two versions of contrastive learning, equations ( 6) and ( 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_70",
            "start": 150,
            "end": 296,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_71@0",
            "content": "As another version of CMKD, we consider distilling visual knowledge from a pre-trained vision- language model, VL-BERT, which is knowledgeable about grounded language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_71",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_71@1",
            "content": "We adopt masked language modeling on Wikitext103 (Merity et al., 2016), a subset of English Wikipedia, in the knowledge distillation step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_71",
            "start": 168,
            "end": 305,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_71@2",
            "content": "For knowledge distillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_71",
            "start": 307,
            "end": 464,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_72@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_72",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@0",
            "content": "For all the approaches, we use bert-base-uncased (Devlin et al., 2018) as text encoder f L and ResNeXt101 (Xie et al., 2017) as an image encoder f V .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@1",
            "content": "For text knowledge transfer, (1) MLM follows the exact setting of codebase in huggingface 2 which uses dynamic masking strategy to conduct language modling task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 151,
            "end": 311,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@2",
            "content": "(2) TCL conducts contrastive learning with f L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 313,
            "end": 360,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@3",
            "content": "We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 362,
            "end": 451,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@4",
            "content": "For cross-modal knowledge transfer, (1) CMKD explores VL-BERT, Vokenization, and VidLanKD approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 453,
            "end": 553,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@5",
            "content": "Here, we use VL-BERT-large model to do CMKD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 555,
            "end": 598,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@6",
            "content": "Vokenization uses a checkpoint from their official codebase 3 and VidLanKD trains a teacher model by two versions of contrastive learning (equations ( 6) and ( 8)) on MS COCO dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 600,
            "end": 782,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_73@7",
            "content": "We set \u03b1 = 1 in VidLanKD (equation ( 8)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_73",
            "start": 784,
            "end": 824,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@0",
            "content": "(2) CMCL conducts contrastive learning with f L and f V .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@1",
            "content": "Here, we set \u03c4 = 0.05 (equations ( 4) and ( 5)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 58,
            "end": 105,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@2",
            "content": "( 3 use top-5 predictions from f L as replacement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 107,
            "end": 156,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@3",
            "content": "We filter out synonyms and hypernyms of original words using WordNet (Miller, 1995).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 158,
            "end": 241,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@4",
            "content": "( 4) CMCL with PSA includes the perturbed sentences with synonyms and hypernyms as additional positive samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 243,
            "end": 353,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@5",
            "content": "In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder f V and BERT as a text encoder f L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 355,
            "end": 458,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@6",
            "content": "TCL and CMCL train with batch size 64, maximum sequence length 20, learning rate 1e-4 for 3 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 460,
            "end": 558,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@7",
            "content": "For fine-tuning on downstream tasks, we do grid search on learning rates {5e-5, 1e-4, 3e-4, 4e-4, 5e-4, 6e-4} and choose the best learning rate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 560,
            "end": 703,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_74@8",
            "content": "We set maximum epochs to 30 in low-resource and 15 in fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_74",
            "start": 705,
            "end": 784,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_75@0",
            "content": "Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_75",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@0",
            "content": "We analyze the main results of intermediate pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@1",
            "content": "Tables 2 and 3 show the main results of low-resource learning and fully supervised learning with the MS COCO captioning dataset, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 57,
            "end": 198,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@2",
            "content": "We train the models with a few training examples, 2 and 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 200,
            "end": 258,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@3",
            "content": "Specifically, CMKD with VidLanKD variant outperforms the baseline by 1.6% point on the PIQA dataset in fully supervised setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 260,
            "end": 387,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@4",
            "content": "CMCL also shows its effectiveness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 389,
            "end": 422,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@5",
            "content": "However, we could find that it becomes more powerful when equipped with PSA and ANS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 424,
            "end": 507,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@6",
            "content": "It suggests that data augmentation for positive and negative sampling is an important factor for CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 509,
            "end": 610,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_76@7",
            "content": "In low-resource setting, we find that cross-modal knowledge transfer helps better initialization and let models learn new tasks faster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_76",
            "start": 612,
            "end": 746,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_77@0",
            "content": "What intermediate pre-training objectives are effective for cross-modal knowledge transfer? Among various cross-modal knowledge transfer methods, we study which method is the most effective for cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_77",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_77@1",
            "content": "Overall, CMCL with PSA and ANS shows the best performance among all cross-modal methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_77",
            "start": 226,
            "end": 313,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_77@2",
            "content": "Interestingly, VL-BERT also shows better performance than BERT-base on all datasets in the low-resource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_77",
            "start": 315,
            "end": 426,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_77@3",
            "content": "This suggesting that exploiting images in masked language modeling task help transfer the knowledge to language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_77",
            "start": 428,
            "end": 546,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@0",
            "content": "What types of knowledge sources are most helpful?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@1",
            "content": "Here, we investigate whether using an image source in addition to a text source can further improve the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 50,
            "end": 159,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@2",
            "content": "To answer this question, we analyze methods from different types of sources: text-only and text-image pair sources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 161,
            "end": 275,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@3",
            "content": "We focus on the methods that use the contrastive learning objective: TCL and CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 277,
            "end": 358,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@4",
            "content": "Note that these two methods share the same objective but CMCL trains on cross modalities which are images and captions while TCL only trains on captions Overall, TCL performs slightly better than CMCL in low-resource and fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 360,
            "end": 606,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@5",
            "content": "Interestingly, additional negative samples (ANS) and positive samples in TCL decreases the performance while they help CMCL to improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 608,
            "end": 758,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_78@6",
            "content": "We conjecture that perturbed sentences in ANS might not be semantically negative to the original sentence so models learn from wrong labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_78",
            "start": 760,
            "end": 899,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_79@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_79",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_80@0",
            "content": "How do models perform on general NLU tasks? (Lin et al., 2014;Chen et al., 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al., 2016) gests including perturbed sentences as positive and negative samples are useful to cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_80",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_81@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_81",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@0",
            "content": "Text Knowledge enhanced methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@1",
            "content": "Recently, huge efforts on integrating knowledge into PTLMs have been made.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 33,
            "end": 106,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@2",
            "content": "One typical form of knowledge is a knowledge graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 108,
            "end": 158,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@3",
            "content": "There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019;Peters et al., 2019;He et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 160,
            "end": 368,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@4",
            "content": "Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019;Wang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 370,
            "end": 501,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@5",
            "content": "Another existing form of knowledge is extra large-scale corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 503,
            "end": 565,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@6",
            "content": "Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 567,
            "end": 797,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_82@7",
            "content": "They are mostly focused on injecting world knowledge presented in text, rather than physical and visual commonsense knowledge that can be found in images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_82",
            "start": 799,
            "end": 952,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@0",
            "content": "There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2019), VisualBert (Li et al., 2019), and Uniter (Chen et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@1",
            "content": "These models aim to improve vision-language tasks, e.g., VQA (Goyal et al., 2017), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 173,
            "end": 345,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@2",
            "content": "Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 347,
            "end": 489,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@3",
            "content": "Vokenization employs token-level text-toimage retrieval to transfer visual knowledge to language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 491,
            "end": 594,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@4",
            "content": "For this, Vokenization introduces 30k vokens and matches each token into the limited voken space; it may have approximation errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 596,
            "end": 726,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_83@5",
            "content": "VidLanKD adopts contrastive learning to train a teacher model on video datasets and uses distillation approaches to distill visual knowledge from the teacher to a student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_83",
            "start": 728,
            "end": 904,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_84@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_84",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_85@0",
            "content": "We study whether intermediate pre-training on visual knowledge can help transfer visual knowledge into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_85",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_85@1",
            "content": "We investigate text knowledge transfer and cross-modal knowledge transfer using images and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_85",
            "start": 108,
            "end": 207,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_85@2",
            "content": "In our empirical analysis, we observe that intermediate pre-training on captions can help improving performance and cross-modal knowledge transfer approaches consistently improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_85",
            "start": 209,
            "end": 399,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_85@3",
            "content": "When the transfer methods are equipped with additional positive and negative samples, they show better performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_85",
            "start": 401,
            "end": 515,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_85@4",
            "content": "Future works include improving both commonsense reasoning and general language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_85",
            "start": 517,
            "end": 609,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_86@0",
            "content": "PIQA is a multiple-choice question answering task, which chooses the most appropriate solution for physical commonsense questions, which may need illustration or description of physical interaction in the real world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_86",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_86@1",
            "content": "VP is to tell if two descriptions are describing the same scene or two different scenes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_86",
            "start": 217,
            "end": 304,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@0",
            "content": "While they seem like purely textual tasks, they require visual common sense to answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@1",
            "content": "CSQA is a multiple-choice question answering task that requires commonsense reasoning to answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 87,
            "end": 182,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@2",
            "content": "It is built from ConceptNet (Speer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 184,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@3",
            "content": "OBQA is a multiple-choice question answering task, which is modeled after open book exams on elementarylevel core science questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 234,
            "end": 365,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@4",
            "content": "The task generally requires open book fact but also additional commonsense which can be learnt from scientific illustration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 367,
            "end": 490,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_87@5",
            "content": "RiddleSense is a multiple-choice riddlestyle question answering which requires complex commonsense reasoning ability and understanding of figurative language which may benefit from visual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_87",
            "start": 492,
            "end": 689,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_88@0",
            "content": "Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark, Genericskb: A knowledge base of generic statements, 2020, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_88",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_89@0",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Piqa: Reasoning about physical commonsense in natural language, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_89",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_90@0",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_90",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_91@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_91",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2015, Microsoft coco captions: Data collection and evaluation server, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_92",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_93@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_93",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_94",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2021, Simcse: Simple contrastive learning of sentence embeddings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_95",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_96@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_96",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_97@0",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, 2020. Don't stop pretraining: Adapt language models to domains and tasks, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_97",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_98@0",
            "content": "Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Yuan, Tong Xu, BERT-MK: Integrating graph contextualized knowledge into pretrained language models, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_98",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_99@0",
            "content": "UNKNOWN, None, 2017, Like what you like: Knowledge distill via neuron selectivity transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_99",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_100",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_101@0",
            "content": "Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen,  Ren, KagNet: Knowledge-aware graph networks for commonsense reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_101",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_102@0",
            "content": "Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee,  Ren, Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_102",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_103@0",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_103",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_104@0",
            "content": "Xiao Lin, Devi Parikh, Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks, 2015, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_104",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_105",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_106@0",
            "content": "UNKNOWN, None, 2016, Pointer sentinel mixture models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_106",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2018, Can a suit of armor conduct electricity? a new dataset for open book question answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_107",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_108@0",
            "content": "A George,  Miller, Wordnet: a lexical database for english, 1995, Communications of the ACM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_108",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_109@0",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_109",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_110@0",
            "content": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, KILT: a benchmark for knowledge intensive language tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_110",
            "start": 0,
            "end": 417,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_111@0",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_111",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_112@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, , Learning transferable visual models from natural language supervision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_112",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_113@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_113",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_114@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_114",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_115@0",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_115",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_116@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_116",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_117@0",
            "content": "UNKNOWN, None, 2019, Vl-bert: Pre-training of generic visual-linguistic representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_117",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_118@0",
            "content": "UNKNOWN, None, 2018, Commonsenseqa: A question answering challenge targeting commonsense knowledge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_118",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_119@0",
            "content": "UNKNOWN, None, 2020, Vokenization: improving language understanding with contextualized, visual-grounded supervision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_119",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_120@0",
            "content": "UNKNOWN, None, 2021, Vidlankd: Improving language understanding via video-distilled knowledge transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_120",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_121@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_121",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_122@0",
            "content": "Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, Connecting the dots: A knowledgeable path generator for commonsense question answering, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_122",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_123@0",
            "content": "Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_123",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_124@0",
            "content": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Aggregated residual transformations for deep neural networks, 2017, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_124",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_125@0",
            "content": "UNKNOWN, None, 2020, Jaket: Joint pre-training of knowledge graph and language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_125",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_126@0",
            "content": "UNKNOWN, None, 2021, Dict-bert: Enhancing language model pre-training with dictionary, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_126",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_127@0",
            "content": "UNKNOWN, None, 2020, Contrastive learning of medical visual representations from paired images and text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_127",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_128@0",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_128",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_129@0",
            "content": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Selvam, Seyeon Lee, Xiang Ren, Pre-training text-to-text transformers for concept-centric common sense, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_129",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_130@0",
            "content": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, The IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_130",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "66-ARR_v1_131@0",
            "content": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v1_131",
            "start": 0,
            "end": 284,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_1",
            "tgt_ix": "66-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_1",
            "tgt_ix": "66-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_4",
            "tgt_ix": "66-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_5",
            "tgt_ix": "66-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_6",
            "tgt_ix": "66-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_7",
            "tgt_ix": "66-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_8",
            "tgt_ix": "66-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_9",
            "tgt_ix": "66-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_10",
            "tgt_ix": "66-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_11",
            "tgt_ix": "66-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_12",
            "tgt_ix": "66-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_13",
            "tgt_ix": "66-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_14",
            "tgt_ix": "66-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_16",
            "tgt_ix": "66-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_17",
            "tgt_ix": "66-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_17",
            "tgt_ix": "66-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_20",
            "tgt_ix": "66-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_21",
            "tgt_ix": "66-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_19",
            "tgt_ix": "66-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_19",
            "tgt_ix": "66-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_19",
            "tgt_ix": "66-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_19",
            "tgt_ix": "66-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_22",
            "tgt_ix": "66-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_23",
            "tgt_ix": "66-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_23",
            "tgt_ix": "66-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_24",
            "tgt_ix": "66-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_25",
            "tgt_ix": "66-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_25",
            "tgt_ix": "66-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_26",
            "tgt_ix": "66-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_27",
            "tgt_ix": "66-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_27",
            "tgt_ix": "66-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_30",
            "tgt_ix": "66-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_31",
            "tgt_ix": "66-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_32",
            "tgt_ix": "66-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_33",
            "tgt_ix": "66-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_35",
            "tgt_ix": "66-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_36",
            "tgt_ix": "66-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_37",
            "tgt_ix": "66-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_38",
            "tgt_ix": "66-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_40",
            "tgt_ix": "66-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_41",
            "tgt_ix": "66-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_42",
            "tgt_ix": "66-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_44",
            "tgt_ix": "66-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_43",
            "tgt_ix": "66-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_45",
            "tgt_ix": "66-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_47",
            "tgt_ix": "66-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_48",
            "tgt_ix": "66-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_49",
            "tgt_ix": "66-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_50",
            "tgt_ix": "66-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_52",
            "tgt_ix": "66-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_53",
            "tgt_ix": "66-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_54",
            "tgt_ix": "66-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_55",
            "tgt_ix": "66-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_56",
            "tgt_ix": "66-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_57",
            "tgt_ix": "66-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_58",
            "tgt_ix": "66-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_59",
            "tgt_ix": "66-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_60",
            "tgt_ix": "66-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_61",
            "tgt_ix": "66-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_62",
            "tgt_ix": "66-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_64",
            "tgt_ix": "66-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_65",
            "tgt_ix": "66-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_51",
            "tgt_ix": "66-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_66",
            "tgt_ix": "66-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_69",
            "tgt_ix": "66-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_70",
            "tgt_ix": "66-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_67",
            "tgt_ix": "66-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_71",
            "tgt_ix": "66-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_72",
            "tgt_ix": "66-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_72",
            "tgt_ix": "66-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_72",
            "tgt_ix": "66-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_77",
            "tgt_ix": "66-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_79",
            "tgt_ix": "66-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_79",
            "tgt_ix": "66-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_80",
            "tgt_ix": "66-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_81",
            "tgt_ix": "66-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_81",
            "tgt_ix": "66-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_81",
            "tgt_ix": "66-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_84",
            "tgt_ix": "66-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_84",
            "tgt_ix": "66-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_86",
            "tgt_ix": "66-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_84",
            "tgt_ix": "66-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_84",
            "tgt_ix": "66-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v1_0",
            "tgt_ix": "66-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_1",
            "tgt_ix": "66-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_2",
            "tgt_ix": "66-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_3",
            "tgt_ix": "66-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_4",
            "tgt_ix": "66-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_5",
            "tgt_ix": "66-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_5",
            "tgt_ix": "66-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_5",
            "tgt_ix": "66-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_5",
            "tgt_ix": "66-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_6",
            "tgt_ix": "66-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_6",
            "tgt_ix": "66-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_7",
            "tgt_ix": "66-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_7",
            "tgt_ix": "66-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_8",
            "tgt_ix": "66-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_8",
            "tgt_ix": "66-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_9",
            "tgt_ix": "66-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_10",
            "tgt_ix": "66-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_11",
            "tgt_ix": "66-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_11",
            "tgt_ix": "66-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_12",
            "tgt_ix": "66-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_13",
            "tgt_ix": "66-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_14",
            "tgt_ix": "66-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_15",
            "tgt_ix": "66-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_16",
            "tgt_ix": "66-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_16",
            "tgt_ix": "66-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_17",
            "tgt_ix": "66-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_18",
            "tgt_ix": "66-ARR_v1_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_19",
            "tgt_ix": "66-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_20",
            "tgt_ix": "66-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_20",
            "tgt_ix": "66-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_21",
            "tgt_ix": "66-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_21",
            "tgt_ix": "66-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_22",
            "tgt_ix": "66-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_22",
            "tgt_ix": "66-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_22",
            "tgt_ix": "66-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_23",
            "tgt_ix": "66-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_24",
            "tgt_ix": "66-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_24",
            "tgt_ix": "66-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_24",
            "tgt_ix": "66-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_24",
            "tgt_ix": "66-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_25",
            "tgt_ix": "66-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_26",
            "tgt_ix": "66-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_26",
            "tgt_ix": "66-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_26",
            "tgt_ix": "66-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_26",
            "tgt_ix": "66-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_27",
            "tgt_ix": "66-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_28",
            "tgt_ix": "66-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_29",
            "tgt_ix": "66-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_30",
            "tgt_ix": "66-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_31",
            "tgt_ix": "66-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_31",
            "tgt_ix": "66-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_32",
            "tgt_ix": "66-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_33",
            "tgt_ix": "66-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_34",
            "tgt_ix": "66-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_35",
            "tgt_ix": "66-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_36",
            "tgt_ix": "66-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_36",
            "tgt_ix": "66-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_36",
            "tgt_ix": "66-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_36",
            "tgt_ix": "66-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_37",
            "tgt_ix": "66-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_38",
            "tgt_ix": "66-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_39",
            "tgt_ix": "66-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_40",
            "tgt_ix": "66-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_41",
            "tgt_ix": "66-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_42",
            "tgt_ix": "66-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_43",
            "tgt_ix": "66-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_44",
            "tgt_ix": "66-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_44",
            "tgt_ix": "66-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_45",
            "tgt_ix": "66-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_46",
            "tgt_ix": "66-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_47",
            "tgt_ix": "66-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_47",
            "tgt_ix": "66-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_47",
            "tgt_ix": "66-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_48",
            "tgt_ix": "66-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_48",
            "tgt_ix": "66-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_48",
            "tgt_ix": "66-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_49",
            "tgt_ix": "66-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_50",
            "tgt_ix": "66-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_50",
            "tgt_ix": "66-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_51",
            "tgt_ix": "66-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_51",
            "tgt_ix": "66-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_51",
            "tgt_ix": "66-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_51",
            "tgt_ix": "66-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_52",
            "tgt_ix": "66-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_52",
            "tgt_ix": "66-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_53",
            "tgt_ix": "66-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_54",
            "tgt_ix": "66-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_54",
            "tgt_ix": "66-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_55",
            "tgt_ix": "66-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_56",
            "tgt_ix": "66-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_57",
            "tgt_ix": "66-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_57",
            "tgt_ix": "66-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_57",
            "tgt_ix": "66-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_58",
            "tgt_ix": "66-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_59",
            "tgt_ix": "66-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_60",
            "tgt_ix": "66-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_61",
            "tgt_ix": "66-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_62",
            "tgt_ix": "66-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_63",
            "tgt_ix": "66-ARR_v1_63@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_64",
            "tgt_ix": "66-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_65",
            "tgt_ix": "66-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_66",
            "tgt_ix": "66-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_66",
            "tgt_ix": "66-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_67",
            "tgt_ix": "66-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_67",
            "tgt_ix": "66-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_67",
            "tgt_ix": "66-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_67",
            "tgt_ix": "66-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_68",
            "tgt_ix": "66-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_69",
            "tgt_ix": "66-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_70",
            "tgt_ix": "66-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_70",
            "tgt_ix": "66-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_70",
            "tgt_ix": "66-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_71",
            "tgt_ix": "66-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_71",
            "tgt_ix": "66-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_71",
            "tgt_ix": "66-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_72",
            "tgt_ix": "66-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_73",
            "tgt_ix": "66-ARR_v1_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_74",
            "tgt_ix": "66-ARR_v1_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_75",
            "tgt_ix": "66-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_76",
            "tgt_ix": "66-ARR_v1_76@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_77",
            "tgt_ix": "66-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_77",
            "tgt_ix": "66-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_77",
            "tgt_ix": "66-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_77",
            "tgt_ix": "66-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_78",
            "tgt_ix": "66-ARR_v1_78@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_79",
            "tgt_ix": "66-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_80",
            "tgt_ix": "66-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_81",
            "tgt_ix": "66-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_82",
            "tgt_ix": "66-ARR_v1_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_83",
            "tgt_ix": "66-ARR_v1_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_84",
            "tgt_ix": "66-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_85",
            "tgt_ix": "66-ARR_v1_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_86",
            "tgt_ix": "66-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_86",
            "tgt_ix": "66-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_87",
            "tgt_ix": "66-ARR_v1_87@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_88",
            "tgt_ix": "66-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_89",
            "tgt_ix": "66-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_90",
            "tgt_ix": "66-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_91",
            "tgt_ix": "66-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_92",
            "tgt_ix": "66-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_93",
            "tgt_ix": "66-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_94",
            "tgt_ix": "66-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_95",
            "tgt_ix": "66-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_96",
            "tgt_ix": "66-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_97",
            "tgt_ix": "66-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_98",
            "tgt_ix": "66-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_99",
            "tgt_ix": "66-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_100",
            "tgt_ix": "66-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_101",
            "tgt_ix": "66-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_102",
            "tgt_ix": "66-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_103",
            "tgt_ix": "66-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_104",
            "tgt_ix": "66-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_105",
            "tgt_ix": "66-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_106",
            "tgt_ix": "66-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_107",
            "tgt_ix": "66-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_108",
            "tgt_ix": "66-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_109",
            "tgt_ix": "66-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_110",
            "tgt_ix": "66-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_111",
            "tgt_ix": "66-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_112",
            "tgt_ix": "66-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_113",
            "tgt_ix": "66-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_114",
            "tgt_ix": "66-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_115",
            "tgt_ix": "66-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_116",
            "tgt_ix": "66-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_117",
            "tgt_ix": "66-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_118",
            "tgt_ix": "66-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_119",
            "tgt_ix": "66-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_120",
            "tgt_ix": "66-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_121",
            "tgt_ix": "66-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_122",
            "tgt_ix": "66-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_123",
            "tgt_ix": "66-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_124",
            "tgt_ix": "66-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_125",
            "tgt_ix": "66-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_126",
            "tgt_ix": "66-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_127",
            "tgt_ix": "66-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_128",
            "tgt_ix": "66-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_129",
            "tgt_ix": "66-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_130",
            "tgt_ix": "66-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v1_131",
            "tgt_ix": "66-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1050,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "66-ARR",
        "version": 1
    }
}