{
    "nodes": [
        {
            "ix": "66-ARR_v2_0",
            "content": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_2",
            "content": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives. On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives. Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings. 1 * Authors contributed equally.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "66-ARR_v2_4",
            "content": "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2019), RoBERTa , and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_5",
            "content": "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020a;Petroni et al., 2021;Schick and Sch\u00fctze, 2020). Specifically, one type of knowledge that models often struggle with is the visual knowledge of common objects such as attributes (e.g. appearance, measurable quantity) and affordances. This is because this kind of knowledge is rarely explicitly described in the training text due to reporting bias. For example, as shown in Figure 1, people tend to report what interests them rather than general facts such as a shape or color of oranges they already know.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_6",
            "content": "Towards better knowledge-enhanced PTLMs, recent works incorporate external knowledge bases (e.g., knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019;Peters et al., 2019;Wang et al., 2021;Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019;Wang et al., 2020). However, these approaches still suffer from a lack of visual knowledge that is important to understand the real world.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_7",
            "content": "In this paper, we conduct systematic experiments to understand whether such visual knowledge can be transferred into LMs, and if so, how to perform effective knowledge transfer. Specifically, we look into a series of analysis question as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_8",
            "content": "(1) Can intermediate pre-training (Pruksachatkun et al., 2020a) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020a) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_9",
            "content": "For the text knowledge transfer, we utilize text corpus from visual domain, e.g., image captions. We leverage two training objectives for the language model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020a), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_10",
            "content": "(2) text contrastive learning augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_11",
            "content": "For the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2020); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_12",
            "content": "We perform comprehensive comparisons on five downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), and Rid-dleSense (Lin et al., 2021). Results suggest that:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_13",
            "content": "(1) Simple intermediate pre-training on captions can help improving performance on commonsense reasoning that needs physical or visual knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_14",
            "content": "(2) Cross-modal knowledge transfer approaches consistently improve the performance in a large margin when only few train examples are available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_15",
            "content": "(3) Cross-modal contrastive learning shows that it is best for packaging visual knowledge into LMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_16",
            "content": "we fine-tune f L on downstream NLU tasks. Existing NLU benchmarks have been trained against standard supervised learning paradigms that typically require a large number of question answering examples which need a large annotation efforts. However, in scenarios where the number of labeled examples is small, the model tends to overfit the training examples and shows poor generalization performance on test set. Here, we evaluate the intermediate pre-training objective's generalization ability on test set in both fully supervised and lowresource settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_17",
            "content": "Analysis Questions",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "66-ARR_v2_18",
            "content": "In this paper, we provide a comprehensive study for transferring the visual knowledge into LMs. Visual knowledge transfer can be done in two approaches, depending on the source to be trained:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_19",
            "content": "(1) Text knowledge transfer using the text corpus in the visual domain, e.g., image captions and (2) cross-modal knowledge transfer which passes visual knowledge about common objects to LMs by training over paired image and captions. By evaluating the model on 5 downstream datasets that require physical and visual commonsense knowledge, we explore following three research questions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_20",
            "content": "Q1: Can intermediate pre-training on external knowledge sources help transfer visual knowledge to augment text encoders? We investigate diverse intermediate pre-training methods with external knowledge sources including caption data to inject visual information from images and captions into LMs. We first analyze the performance of text and cross-modal knowledge transfer methods with a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2017) and BookCorpus (Zhu et al., 2015a edge distillation. Here, we want to present which strategy is best suited for cross-modal knowledge transfer. Furthermore, we study how to enhance cross-modal contrastive learning with adversarial negative samplings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_21",
            "content": "Pre-training Data",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "66-ARR_v2_22",
            "content": "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014;Chen et al., 2015). MS COCO contains images reflecting the composition of actual everyday scenes and corresponding captions which describe contextual reasoning between objects in the scene. We only use captions for text knowledge transfer while we use both images and captions for crossmodal knowledge transfer. As an ablation study, we explore other text corpora such as Generic-sKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2017 and BookCorpus (Zhu et al., 2015a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_23",
            "content": "Downstream Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "66-ARR_v2_24",
            "content": "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2019) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and Riddle-Sense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding. We use in-house test sets made from training sets for PIQA and CSQA since test set is not provided to public. We list the data statics in Table 1. Moreover, We additionally test on GLUE (Wang et al., 2019) to evaluate the general text understanding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_25",
            "content": "Evaluation Protocol",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "66-ARR_v2_26",
            "content": "We evaluate the models in both fully supervised and low-resource settings. For both settings, we consider accuracy for 5 different classification tasks and get average performance over tasks to check the final performance. In the fully supervised setting, we evaluate models with 3 different random seeds and report the average accuracy. In the lowresource setting, we set the size of the train data to 64 or 128. For each experiment, we run over 5 different sub-samples and show the average accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_27",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "66-ARR_v2_28",
            "content": "In this section, we introduce the following two approaches to integrate visual knowledge into LMs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_29",
            "content": "(1) text knowledge transfer; and (2) cross-modal knowledge transfer. Throughout this section, we assume the data is a collection of image x v and caption",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_30",
            "content": "x l pairs (x v i , x l i ) m i=1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_31",
            "content": "(m is the size of the pairs) and image encoder f V and text encoder f L are given. Note that we use the same text encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_32",
            "content": "Text Knowledge Transfer",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "66-ARR_v2_33",
            "content": "For text knowledge transfer, we investigate following pre-training objectives: (1) masked language modeling; and (2) text contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_34",
            "content": "Masked Language Modeling (MLM) Following BERT (Devlin et al., 2019), we select 15% of input tokens and replace them with [MASK]. Of the selected tokens, 80% are replaced, 10% are not changed and 10% are replaced by random vocabulary token. Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples . MLM objective is the cross-entropy loss for masked token predictions :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_35",
            "content": "\u2113 MLM (x l i ) = \u2212 log p(x l i |x masked ),(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_36",
            "content": "where x i is the i-th token and x masked is a mask.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_37",
            "content": "Text Contrastive Learning (TCL) Contrastive learning aims to learn representations by pulling positive pairs closer and pushing negative pairs apart. Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a;Gao et al., 2021). Given a text encoder f L , and a caption x l i , we first get text representations using the encoders h l i = f L (x l i ). Following Gao et al. (2021), we create identical A girl puts an apple in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_38",
            "content": "A girl puts an [MASK] in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_39",
            "content": "A girl puts an envelope in her bag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_40",
            "content": "Figure 3: LM perturbation. We create adversarial negatives using language models. positive sample h l + i by different dropout representations. The contrastive loss is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_41",
            "content": "\u2113 l i = \u2212 log e sim(h l i ,h l + i )/\u03c4 N j=1 e sim(h l i ,h l j )/\u03c4 , (2",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_42",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_43",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity, i.e., sim(u, v) = u \u2022 v/\u2225u\u2225\u2225v\u2225. \u03c4 represents a temperature parameter.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_44",
            "content": "Cross-modal Knowledge Transfer",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "66-ARR_v2_45",
            "content": "Language models might learn additional information from visual sources such as images and captions. So we include a variety of vision-based approaches and investigate the approaches whether they can benefit from visual sources. We introduce vision-based approaches as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_46",
            "content": "Voken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image retrieval to transfer visual knowledge. It aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs, and call it \"voken classification\". Given text x and a voken v i for the i-th token, the loss is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_47",
            "content": "\u2113 voken i = \u2212 log(p(v i |x)).(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_48",
            "content": "Similar to masked language modeling, it classifies each token to a corresponding voken. Vokenization trains language models with the voken classification task and MLM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_49",
            "content": "Masked Language Modeling with Visual Clues VL-BERT (Su et al., 2020) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues. VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora. It shows its effectiveness in many vision-language tasks. We investigate whether this model also succeed in NLP tasks and compare it with others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_50",
            "content": "To harness the visual knowledge from imagecaption datasets, we adopt contrastive loss on image and text vectors. Given an image encoder f V , a text encoder f L , and an image-caption pair (x v i , x l i ), we first get image and text representations using the encoders",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_51",
            "content": "h v i = f V (x v i ), h l i = f L (x l i ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_52",
            "content": "Then the contrastive learning objective contains two loss functions: an image-to-text contrastive loss \u2113 (v,l) and a text-to-image contrastive loss \u2113 (l,v) . The image-to-text contrastive loss is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_53",
            "content": "\u2113 (v,l) i = \u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 , (4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_54",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_55",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity. This loss encourages a closer distance between representations of aligned imagecaption pairs than unaligned pairs given an image and multiple captions. Similarly, the text-to-image contrastive loss \u2113 (l,v) is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_56",
            "content": "\u2113 (l,v) i = \u2212 log e sim(h l i ,h v i )/\u03c4 N j=1 e sim(h l i ,h v j )/\u03c4 .(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_57",
            "content": "The final loss is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_58",
            "content": "L = 1 N N i=1 (\u2113 (v,l) i + \u2113 (l,v) i ).(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_59",
            "content": "CLIP (Radford et al., 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_60",
            "content": "CMCL with Adversarial Negative Samples (ANS) As in-batch negatives in CMCL are not challenging enough for models to distinguish, we present adversarial negative sampling strategy to improve CMCL. Given an image-caption pair (x v i , x l i ), we define a LM-perturbed sentence x l \u2212 i , which is a hard negative where n is replaced with a different word n \u2032 from a probability distribution of PTLMs. We expect the l \u2212 is syntactically correct and plausible sentence even the word n is replaced to n \u2032 , while it does not semantically match to the corresponding image x v i . With such hard negative, we try to make more challenging task so that models can effectively learn from the task. For example, we choose a word 'girl' in the sentence 'A girl puts an apple in her bag.' in Figure 3. Then we mask the word with [MASK] token to do masked token predictions by PTLMs. Then we get topk predictions from language models and replace the masked tokens with one of the predicted ones. To avoid false negative sentences which may have the same semantics as the original sentence, we introduce an additional filtering step: if the masked predictions are synonyms or hypernyms of the original tokens, we discard the predictions. We use WordNet (Miller, 1992) to find synonyms and hypernyms. The contrastive loss with hard negative is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_61",
            "content": "\u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 + M k=1 e sim(h v i ,h l \u2212 j )/\u03c4 ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_62",
            "content": "where M is the number of hard negative samples per positive pair. This formula is only for image-totext contrastive loss \u2113 (v,l) and final loss is defined to same as equation ( 6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_63",
            "content": "In ANS, we filter perturbed sentences where the masked predictions are synonyms or hypernyms of the original tokens. Instead of excluding these perturbed sentences, another option is to include them as additional positive samples l + to the paired images. We name this as positive sample augmentation (PSA). It also adopts LM-perturbed negative samples as in ANS.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_64",
            "content": "Cross-modal knowledge distillation is to transfer knowledge between different modalities, e.g., image modality and text modality. In this category, CMKD is to transfer knowledge from a teacher model which is knowledgeable about visual information. VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding. A teacher model is first trained using contrastive learning on a video-text dataset, and then it transfers its knowledge to a student language model using KD on a text corpus. Their contrastive learning loss (hinge loss) is defined as ties of a positive pair and a negative pair. Instead of video datasets, we use a MS COCO dataset to train a teacher model and use two versions of contrastive learning, equations ( 6) and ( 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_65",
            "content": "L = N i [max(0, \u03b1\u2212sim(h v i , h l i )+sim(h v \u2032 i , h l i )) + max(0, \u03b1 \u2212 sim(h v i , h l i ) + sim(h v i , h l \u2032 i ))],(",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_66",
            "content": "As another version of CMKD, we consider distilling visual knowledge from a pre-trained visionlanguage model, VL-BERT, which is knowledgeable about grounded language. We adopt masked language modeling on Wikitext103 (Merity et al., 2017), a subset of English Wikipedia, in the knowledge distillation step. For knowledge distillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_67",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "66-ARR_v2_68",
            "content": "For all the approaches, we use bert-base-uncased (Devlin et al., 2019) as text encoder f L and ResNeXt101 (Xie et al., 2017) as an image encoder f V . We continue to pre-train the encoders in our experiments. For text knowledge transfer, (1) MLM follows the exact setting of codebase in huggingface 2 which uses dynamic masking strategy to conduct language modeling task. (2) TCL conducts contrastive learning with f L . We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017). For cross-modal knowledge transfer, (1) CMKD explores VL-BERT, Vokenization, and VidLanKD approaches. Here, we use VL-BERTlarge model to do CMKD. We use the VL-BERT and Vokenization checkpoints from their official codebases 3 . VidLanKD trains a teacher model by two versions of contrastive learning (equations ( 6) and ( 8)) on MS COCO dataset. We set \u03b1 = 1 in VidLanKD (equation ( 8)). ( 2) CMCL conducts contrastive learning with f L and f V . Here, we set \u03c4 = 0.05 (equations ( 2) and ( 4)). ( 3) CMCL with ANS chooses three noun words or verb words to do masked prediction and use top-5 predictions from f L as replacement. We filter out synonyms and hypernyms of original words using WordNet (Miller, 1992). ( 4) CMCL with PSA includes the perturbed sentences with synonyms and hypernyms as additional positive samples. In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder f V and BERT as a text encoder f L . TCL and CMCL train with batch size 64, maximum sequence length 20, learning rate 1e-4 for 3 epochs. For fine-tuning on downstream tasks, we do grid search on learning rates {5e-5, 1e-4, 3e-4, 4e-4, 5e-4, 6e-4} and choose the best learning rate. We set maximum epochs to 30 in low-resource and 15 in fully supervised settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_69",
            "content": "Results and Analysis",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "66-ARR_v2_70",
            "content": "We analyze the main results of intermediate pretraining. Tables 2 and 3 show the main results of low-resource learning and fully supervised learning with the MS COCO captioning dataset, respectively. We train the models with a few training examples, 64 and 128, to understand the better initialization. We argue that if a model obtains better performance in the low-resource setup, then it is a faster learner and has better generalization on downstream tasks. 2 and 3). Specifically, CMKD with VidLanKD variant outperforms the baseline by 1.6% point on the PIQA dataset in fully supervised setting. CMCL also shows its effectiveness. However, we could find that it becomes more powerful when equipped with PSA and ANS. It suggests that data augmentation for positive and negative sampling is an important factor for CMCL. In low-resource setting, we find that cross-modal knowledge transfer helps better initialization and lets models learn new tasks faster.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_71",
            "content": "What intermediate pre-training objectives are effective for cross-modal knowledge transfer? Among various cross-modal knowledge transfer methods, we study which method is the most effective for cross-modal knowledge transfer. Overall, CMCL with PSA and ANS shows the best performance among all cross-modal methods. Interestingly, VL-BERT also shows better performance than BERT-base on all datasets in the low-resource setting. This suggests that exploiting images in masked language modeling task help transfer the knowledge to language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_72",
            "content": "What types of knowledge sources are most helpful? Here, we investigate whether using an image source in addition to a text source can further improve the model. To answer this question, we analyze methods from different types of sources: text-only and text-image pair sources. We focus on the methods that use the contrastive learning objective: TCL and CMCL. Note that these two methods share the same objective but CMCL trains on cross modalities which are images and captions while TCL only trains on captions. Overall, TCL performs slightly better than CMCL in low-resource and fully supervised settings. Interestingly, additional negative samples (ANS) and positive samples in TCL decreases the performance while they help CMCL to improve the performance. We conjecture that perturbed sentences in ANS might not be semantically negative to the original sentence so models learn from wrong labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_73",
            "content": "Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "66-ARR_v2_74",
            "content": "How do models perform on general NLU tasks? (Lin et al., 2014;Chen et al., 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al., 2017). We sample 250k sentences from each corpus for a fair comparison. We notice that caption datasets are useful on OBQA and RiddleSense datasets while GenericsKB are the most helpful on PIQA datasets. Results are expected since GenericsKB contains a lot of everyday",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_75",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "66-ARR_v2_76",
            "content": "Text Knowledge enhanced methods. Recently, huge efforts on integrating knowledge into PTLMs have been made. One typical form of knowledge is a knowledge graph. There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019;Xu et al., 2021a;Peters et al., 2019;He et al., 2020;Xu et al., 2021b). Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019;Wang et al., 2020). Another existing form of knowledge is extra largescale corpus. Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al., 2021;Calixto et al., 2021). They are mostly focused on injecting world knowledge presented in text, rather than physical and visual commonsense knowledge that can be found in images.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_77",
            "content": "There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2020), VisualBert (Li et al., 2019), and Uniter (Chen et al., 2020b). These models aim to improve vision-language tasks, e.g., VQA (Goyal et al., 2017) and event understanding (Li et al., 2022), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_78",
            "content": "Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_79",
            "content": "Vokenization employs token-level text-to-image retrieval to transfer visual knowledge to language models. For this, Vokenization introduces 30k vokens and matches each token into the limited voken space. VidLanKD adopts contrastive learning to train a teacher model on video datasets and uses distillation approaches to distill visual knowledge from the teacher to a student model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_80",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "66-ARR_v2_81",
            "content": "We",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "66-ARR_v2_82",
            "content": "UNKNOWN, None, 2020, Genericskb: A knowledge base of generic statements, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Genericskb: A knowledge base of generic statements",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_83",
            "content": "Yonatan Bisk, Rowan Zellers, Ronan Lebras, Jianfeng Gao, Yejin Choi, PIQA: reasoning about physical commonsense in natural language, 2020-02-07, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Yonatan Bisk",
                    "Rowan Zellers",
                    "Ronan Lebras",
                    "Jianfeng Gao",
                    "Yejin Choi"
                ],
                "title": "PIQA: reasoning about physical commonsense in natural language",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "66-ARR_v2_84",
            "content": "Iacer Calixto, Alessandro Raganato, Tommaso Pasini, Wikipedia entities as rendezvous across languages: Grounding multilingual language models by predicting Wikipedia hyperlinks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Iacer Calixto",
                    "Alessandro Raganato",
                    "Tommaso Pasini"
                ],
                "title": "Wikipedia entities as rendezvous across languages: Grounding multilingual language models by predicting Wikipedia hyperlinks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_85",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020-07, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020-07",
                "pub_title": "Proceedings of the 37th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "66-ARR_v2_86",
            "content": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco captions: Data collection and evaluation server, 2015, ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Xinlei Chen",
                    "Hao Fang",
                    "Tsung-Yi Lin",
                    "Ramakrishna Vedantam",
                    "Saurabh Gupta",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco captions: Data collection and evaluation server",
                "pub_date": "2015",
                "pub_title": "ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_87",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "Uniter: Universal image-text representation learning",
                "pub_date": "2020",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "66-ARR_v2_88",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_89",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tianyu Gao",
                    "Xingcheng Yao",
                    "Danqi Chen"
                ],
                "title": "SimCSE: Simple contrastive learning of sentence embeddings",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_90",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017-07-21",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "66-ARR_v2_91",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ana Suchin Gururangan",
                    "Swabha Marasovi\u0107",
                    "Kyle Swayamdipta",
                    "Iz Lo",
                    "Doug Beltagy",
                    "Noah Downey",
                    " Smith"
                ],
                "title": "Don't stop pretraining: Adapt language models to domains and tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_92",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Ana Suchin Gururangan",
                    "Swabha Marasovi\u0107",
                    "Kyle Swayamdipta",
                    "Iz Lo",
                    "Doug Beltagy",
                    "Noah Downey",
                    " Smith"
                ],
                "title": "Don't stop pretraining: Adapt language models to domains and tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_93",
            "content": "Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Yuan, Tong Xu, BERT-MK: Integrating graph contextualized knowledge into pretrained language models, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Bin He",
                    "Di Zhou",
                    "Jinghui Xiao",
                    "Xin Jiang",
                    "Qun Liu",
                    "Nicholas Yuan",
                    "Tong Xu"
                ],
                "title": "BERT-MK: Integrating graph contextualized knowledge into pretrained language models",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_94",
            "content": "UNKNOWN, None, 2017, Like what you like: Knowledge distill via neuron selectivity transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Like what you like: Knowledge distill via neuron selectivity transfer",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_95",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visualbert: A simple and performant baseline for vision and language",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_96",
            "content": "UNKNOWN, None, 2022, Clip-event: Connecting text and images with event structures, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2022",
                "pub_title": "Clip-event: Connecting text and images with event structures",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_97",
            "content": "Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen,  Ren, KagNet: Knowledge-aware graph networks for commonsense reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xinyue Bill Yuchen Lin",
                    "Jamin Chen",
                    "Xiang Chen",
                    " Ren"
                ],
                "title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_98",
            "content": "Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee,  Ren, RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Ziyi Bill Yuchen Lin",
                    "Yichi Wu",
                    "Dong-Ho Yang",
                    "Xiang Lee",
                    " Ren"
                ],
                "title": "RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_99",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Tsung-Yi Lin",
                    "Michael Maire",
                    "Serge Belongie",
                    "James Hays",
                    "Pietro Perona",
                    "Deva Ramanan",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco: Common objects in context",
                "pub_date": "2014",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "66-ARR_v2_100",
            "content": "Xiao Lin, Devi Parikh, Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Xiao Lin",
                    "Devi Parikh"
                ],
                "title": "Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks",
                "pub_date": "2015-06-07",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "66-ARR_v2_101",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_102",
            "content": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, Pointer sentinel mixture models, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Stephen Merity",
                    "Caiming Xiong",
                    "James Bradbury",
                    "Richard Socher"
                ],
                "title": "Pointer sentinel mixture models",
                "pub_date": "2017-04-24",
                "pub_title": "5th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_103",
            "content": "Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Todor Mihaylov",
                    "Peter Clark",
                    "Tushar Khot",
                    "Ashish Sabharwal"
                ],
                "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_104",
            "content": "A George,  Miller, WordNet: A lexical database for English, 1992-02-23, Speech and Natural Language: Proceedings of a Workshop Held at, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "A George",
                    " Miller"
                ],
                "title": "WordNet: A lexical database for English",
                "pub_date": "1992-02-23",
                "pub_title": "Speech and Natural Language: Proceedings of a Workshop Held at",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_105",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Matthew Peters",
                    "Mark Neumann",
                    "Robert Logan",
                    "Roy Schwartz",
                    "Vidur Joshi",
                    "Sameer Singh",
                    "Noah Smith"
                ],
                "title": "Knowledge enhanced contextual word representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_106",
            "content": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, KILT: a benchmark for knowledge intensive language tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Fabio Petroni",
                    "Aleksandra Piktus",
                    "Angela Fan",
                    "Patrick Lewis",
                    "Majid Yazdani",
                    "Nicola Cao",
                    "James Thorne",
                    "Yacine Jernite",
                    "Vladimir Karpukhin",
                    "Jean Maillard",
                    "Vassilis Plachouras",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel"
                ],
                "title": "KILT: a benchmark for knowledge intensive language tasks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_107",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020a. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yada Pruksachatkun",
                    "Jason Phang",
                    "Haokun Liu",
                    "Xiaoyi Phu Mon Htut",
                    "Richard Zhang",
                    "Clara Pang",
                    "Katharina Vania",
                    "Samuel Kann"
                ],
                "title": "Bowman. 2020a. Intermediate-task transfer learning with pretrained language models: When and why does it work?",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_108",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020b. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Yada Pruksachatkun",
                    "Jason Phang",
                    "Haokun Liu",
                    "Xiaoyi Phu Mon Htut",
                    "Richard Zhang",
                    "Clara Pang",
                    "Katharina Vania",
                    "Samuel Kann"
                ],
                "title": "Bowman. 2020b. Intermediate-task transfer learning with pretrained language models: When and why does it work?",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_109",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark",
                    "Gretchen Krueger",
                    "Ilya Sutskever"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021-07-24",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "66-ARR_v2_110",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_111",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "66-ARR_v2_112",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Piyush Sharma",
                    "Nan Ding",
                    "Sebastian Goodman",
                    "Radu Soricut"
                ],
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "66-ARR_v2_113",
            "content": "UNKNOWN, None, , , Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_114",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017-02-04, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017-02-04",
                "pub_title": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "66-ARR_v2_115",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, VL-BERT: pretraining of generic visual-linguistic representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Weijie Su",
                    "Xizhou Zhu",
                    "Yue Cao",
                    "Bin Li",
                    "Lewei Lu",
                    "Furu Wei",
                    "Jifeng Dai"
                ],
                "title": "VL-BERT: pretraining of generic visual-linguistic representations",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_116",
            "content": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, CommonsenseQA: A question answering challenge targeting commonsense knowledge, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Alon Talmor",
                    "Jonathan Herzig",
                    "Nicholas Lourie",
                    "Jonathan Berant"
                ],
                "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_117",
            "content": "Hao Tan, Mohit Bansal, Vokenization: Improving language understanding with contextualized, visual-grounded supervision, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "Vokenization: Improving language understanding with contextualized, visual-grounded supervision",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_118",
            "content": "Zineng Tang, Jaemin Cho, Hao Tan, Mohit Bansal, Vidlankd: Improving language understanding via video-distilled knowledge transfer, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Zineng Tang",
                    "Jaemin Cho",
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "Vidlankd: Improving language understanding via video-distilled knowledge transfer",
                "pub_date": "2021",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_119",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_120",
            "content": "Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, Connecting the dots: A knowledgeable path generator for commonsense question answering, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Peifeng Wang",
                    "Nanyun Peng",
                    "Filip Ilievski",
                    "Pedro Szekely",
                    "Xiang Ren"
                ],
                "title": "Connecting the dots: A knowledgeable path generator for commonsense question answering",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "66-ARR_v2_121",
            "content": "Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Ruize Wang",
                    "Duyu Tang",
                    "Nan Duan",
                    "Zhongyu Wei",
                    "Xuanjing Huang",
                    "Jianshu Ji",
                    "Guihong Cao",
                    "Daxin Jiang",
                    "Ming Zhou"
                ],
                "title": "2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_122",
            "content": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Aggregated residual transformations for deep neural networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Saining Xie",
                    "Ross Girshick",
                    "Piotr Doll\u00e1r",
                    "Zhuowen Tu",
                    "Kaiming He"
                ],
                "title": "Aggregated residual transformations for deep neural networks",
                "pub_date": "2017-07-21",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_123",
            "content": "UNKNOWN, None, 2021, Does knowledge help general nlu? an empirical study, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Does knowledge help general nlu? an empirical study",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_124",
            "content": "Yichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu, Michael Zeng, Xuedong Huang, Fusing context into knowledge graph for commonsense question answering, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Yichong Xu",
                    "Chenguang Zhu",
                    "Ruochen Xu",
                    "Yang Liu",
                    "Michael Zeng",
                    "Xuedong Huang"
                ],
                "title": "Fusing context into knowledge graph for commonsense question answering",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_125",
            "content": "Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, Jaket: Joint pre-training of knowledge graph and language understanding, 2020, ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Donghan Yu",
                    "Chenguang Zhu",
                    "Yiming Yang",
                    "Michael Zeng"
                ],
                "title": "Jaket: Joint pre-training of knowledge graph and language understanding",
                "pub_date": "2020",
                "pub_title": "ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_126",
            "content": "Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, Meng Jiang, Dict-bert: Enhancing language model pre-training with dictionary, 2021, ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Wenhao Yu",
                    "Chenguang Zhu",
                    "Yuwei Fang",
                    "Donghan Yu",
                    "Shuohang Wang",
                    "Yichong Xu",
                    "Michael Zeng",
                    "Meng Jiang"
                ],
                "title": "Dict-bert: Enhancing language model pre-training with dictionary",
                "pub_date": "2021",
                "pub_title": "ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_127",
            "content": "UNKNOWN, None, 2020, Contrastive learning of medical visual representations from paired images and text. ArXiv preprint, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Contrastive learning of medical visual representations from paired images and text. ArXiv preprint",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_128",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Zhengyan Zhang",
                    "Xu Han",
                    "Zhiyuan Liu",
                    "Xin Jiang",
                    "Maosong Sun",
                    "Qun Liu"
                ],
                "title": "ERNIE: Enhanced language representation with informative entities",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_129",
            "content": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Selvam, Seyeon Lee, Xiang Ren, Pre-training text-to-text transformers for concept-centric common sense, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Wangchunshu Zhou",
                    "Dong-Ho Lee",
                    "Ravi Selvam",
                    "Seyeon Lee",
                    "Xiang Ren"
                ],
                "title": "Pre-training text-to-text transformers for concept-centric common sense",
                "pub_date": "2021-05-03",
                "pub_title": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
                "pub": null
            }
        },
        {
            "ix": "66-ARR_v2_130",
            "content": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015-12-07, 2015 IEEE International Conference on Computer Vision, ICCV 2015, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Yukun Zhu",
                    "Ryan Kiros",
                    "Richard Zemel",
                    "Ruslan Salakhutdinov",
                    "Raquel Urtasun",
                    "Antonio Torralba",
                    "Sanja Fidler"
                ],
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "pub_date": "2015-12-07",
                "pub_title": "2015 IEEE International Conference on Computer Vision, ICCV 2015",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "66-ARR_v2_131",
            "content": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, 2015 IEEE International Conference on Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Yukun Zhu",
                    "Ryan Kiros",
                    "Richard Zemel",
                    "Ruslan Salakhutdinov",
                    "Raquel Urtasun",
                    "Antonio Torralba",
                    "Sanja Fidler"
                ],
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "pub_date": "2015",
                "pub_title": "2015 IEEE International Conference on Computer Vision",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "66-ARR_v2_0@0",
            "content": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_0",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@0",
            "content": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@1",
            "content": "In this work, we study whether integrating visual knowledge into a language model can fill the gap.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 266,
            "end": 364,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@2",
            "content": "We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 366,
            "end": 616,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@3",
            "content": "On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 618,
            "end": 765,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@4",
            "content": "Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 767,
            "end": 893,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_2@5",
            "content": "1 * Authors contributed equally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_2",
            "start": 895,
            "end": 926,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_4@0",
            "content": "Pre-trained language models (PTLMs) such as BERT (Devlin et al., 2019), RoBERTa , and T5 (Raffel et al., 2020) have shown impressive results in various conventional natural language understanding (NLU) tasks by capturing syntactic and semantic knowledge from the pretraining tasks of masked language modeling and masked span infilling tasks on massive text corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_4",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_5@0",
            "content": "Though yielding good performance on various NLU downstream tasks, these pre-training objectives suffer from a lack of out-of-domain knowledge that is not explicitly present in the pre-training corpus (Gururangan et al., 2020a;Petroni et al., 2021;Schick and Sch\u00fctze, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_5",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_5@1",
            "content": "Specifically, one type of knowledge that models often struggle with is the visual knowledge of common objects such as attributes (e.g. appearance, measurable quantity) and affordances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_5",
            "start": 274,
            "end": 457,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_5@2",
            "content": "This is because this kind of knowledge is rarely explicitly described in the training text due to reporting bias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_5",
            "start": 459,
            "end": 571,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_5@3",
            "content": "For example, as shown in Figure 1, people tend to report what interests them rather than general facts such as a shape or color of oranges they already know.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_5",
            "start": 573,
            "end": 729,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_6@0",
            "content": "Towards better knowledge-enhanced PTLMs, recent works incorporate external knowledge bases (e.g., knowledge graph, dictionary) to inject entity knowledge into PTLMs (Zhang et al., 2019;Peters et al., 2019;Wang et al., 2021;Yu et al., 2021) or retrieve knowledge from external knowledge bases to solve the problem (Lin et al., 2019;Wang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_6",
            "start": 0,
            "end": 349,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_6@1",
            "content": "However, these approaches still suffer from a lack of visual knowledge that is important to understand the real world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_6",
            "start": 351,
            "end": 468,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_7@0",
            "content": "In this paper, we conduct systematic experiments to understand whether such visual knowledge can be transferred into LMs, and if so, how to perform effective knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_7",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_7@1",
            "content": "Specifically, we look into a series of analysis question as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_7",
            "start": 178,
            "end": 245,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_8@0",
            "content": "(1) Can intermediate pre-training (Pruksachatkun et al., 2020a) on image-caption pairs help transfer the knowledge? (2) What types of knowledge sources are more helpful? To answer questions, we explore various intermediate pre-training tasks (Pruksachatkun et al., 2020a) on two different sources: text-only (text knowledge transfer from visual domains) and image-caption pairs (crossmodal knowledge transfer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_8",
            "start": 0,
            "end": 409,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_9@0",
            "content": "For the text knowledge transfer, we utilize text corpus from visual domain, e.g., image captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_9",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_9@1",
            "content": "We leverage two training objectives for the language model: (1) masked language modeling follows the domain adaptive pre-training scheme (Gururangan et al., 2020a), assuming the corpus contains enriched visual knowledge or physical commonsense knowledge;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_9",
            "start": 98,
            "end": 351,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_10@0",
            "content": "(2) text contrastive learning augments the sentence representation with dropout to create positive samples while considering all others in the batch as negative samples for the contrastive learning (Gao et al., 2021), assuming training better sentence representations leads to better understanding of the corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_10",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_11@0",
            "content": "For the cross-modal knowledge transfer, we explore multiple methods to transfer visual-related knowledge to LMs: (1) masked language modeling with visual clues incorporates visual clues to capture dependencies between visual and linguistic contents (Su et al., 2020); (2) voken classification contextually aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs (Tan and Bansal, 2020); (3) cross-modal contrastive learning aims to improve text representations by maximizing the agreement between correct image-text pairs versus random (inbatch) and adversarial negative pairs by contrastive learning between image and text modalities; and (4) cross-modal knowledge distillation transfers the knowledge from the teacher model, which is trained by cross-modal contrastive learning on image and text modalities, to the student language model using knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_11",
            "start": 0,
            "end": 914,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_12@0",
            "content": "We perform comprehensive comparisons on five downstream tasks that may require visual or physical commonsense knowledge, including PIQA (Bisk et al., 2020), Visual Paraphrasing (VP) (Lin and Parikh, 2015), CSQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), and Rid-dleSense (Lin et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_12",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_12@1",
            "content": "Results suggest that:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_12",
            "start": 301,
            "end": 321,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_13@0",
            "content": "(1) Simple intermediate pre-training on captions can help improving performance on commonsense reasoning that needs physical or visual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_13",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_14@0",
            "content": "(2) Cross-modal knowledge transfer approaches consistently improve the performance in a large margin when only few train examples are available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_14",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_15@0",
            "content": "(3) Cross-modal contrastive learning shows that it is best for packaging visual knowledge into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_15",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_16@0",
            "content": "we fine-tune f L on downstream NLU tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_16",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_16@1",
            "content": "Existing NLU benchmarks have been trained against standard supervised learning paradigms that typically require a large number of question answering examples which need a large annotation efforts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_16",
            "start": 42,
            "end": 237,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_16@2",
            "content": "However, in scenarios where the number of labeled examples is small, the model tends to overfit the training examples and shows poor generalization performance on test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_16",
            "start": 239,
            "end": 410,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_16@3",
            "content": "Here, we evaluate the intermediate pre-training objective's generalization ability on test set in both fully supervised and lowresource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_16",
            "start": 412,
            "end": 556,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_17@0",
            "content": "Analysis Questions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_17",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_18@0",
            "content": "In this paper, we provide a comprehensive study for transferring the visual knowledge into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_18",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_18@1",
            "content": "Visual knowledge transfer can be done in two approaches, depending on the source to be trained:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_18",
            "start": 96,
            "end": 190,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_19@0",
            "content": "(1) Text knowledge transfer using the text corpus in the visual domain, e.g., image captions and (2) cross-modal knowledge transfer which passes visual knowledge about common objects to LMs by training over paired image and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_19",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_19@1",
            "content": "By evaluating the model on 5 downstream datasets that require physical and visual commonsense knowledge, we explore following three research questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_19",
            "start": 234,
            "end": 384,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_20@0",
            "content": "Q1: Can intermediate pre-training on external knowledge sources help transfer visual knowledge to augment text encoders?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_20",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_20@1",
            "content": "We investigate diverse intermediate pre-training methods with external knowledge sources including caption data to inject visual information from images and captions into LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_20",
            "start": 121,
            "end": 295,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_20@2",
            "content": "We first analyze the performance of text and cross-modal knowledge transfer methods with a image-caption dataset, and we additionally study text knowledge transfer methods with other text corpora such as GenericsKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2017) and BookCorpus (Zhu et al., 2015a edge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_20",
            "start": 297,
            "end": 625,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_20@3",
            "content": "Here, we want to present which strategy is best suited for cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_20",
            "start": 627,
            "end": 716,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_20@4",
            "content": "Furthermore, we study how to enhance cross-modal contrastive learning with adversarial negative samplings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_20",
            "start": 718,
            "end": 823,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_21@0",
            "content": "Pre-training Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_21",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_22@0",
            "content": "To transfer the visual knowledge, we collect 250K image-caption pairs from MS COCO (Lin et al., 2014;Chen et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_22",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_22@1",
            "content": "MS COCO contains images reflecting the composition of actual everyday scenes and corresponding captions which describe contextual reasoning between objects in the scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_22",
            "start": 121,
            "end": 289,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_22@2",
            "content": "We only use captions for text knowledge transfer while we use both images and captions for crossmodal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_22",
            "start": 291,
            "end": 411,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_22@3",
            "content": "As an ablation study, we explore other text corpora such as Generic-sKB (Bhakthavatsalam et al., 2020), Wiki103 (Merity et al., 2017 and BookCorpus (Zhu et al., 2015a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_22",
            "start": 413,
            "end": 580,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_23@0",
            "content": "Downstream Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_23",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_24@0",
            "content": "For downstream benchmarks, we find tasks that can benefit from visual knowledge: multiple choice question answering tasks including PIQA (Bisk et al., 2020) which requires physical commonsense reasoning, CSQA (Talmor et al., 2019) for general understanding of commonsense reasoning, OBQA (Mihaylov et al., 2018) that needs elemenatry-level science knowledge, and Riddle-Sense (RS) (Lin et al., 2021) for complex understanding of figurative language, and binary classification task including Visual Paraphrasing (VP) (Lin and Parikh, 2015) that needs scene understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_24",
            "start": 0,
            "end": 569,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_24@1",
            "content": "We use in-house test sets made from training sets for PIQA and CSQA since test set is not provided to public.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_24",
            "start": 571,
            "end": 679,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_24@2",
            "content": "We list the data statics in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_24",
            "start": 681,
            "end": 716,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_24@3",
            "content": "Moreover, We additionally test on GLUE (Wang et al., 2019) to evaluate the general text understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_24",
            "start": 718,
            "end": 819,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_25@0",
            "content": "Evaluation Protocol",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_25",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_26@0",
            "content": "We evaluate the models in both fully supervised and low-resource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_26",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_26@1",
            "content": "For both settings, we consider accuracy for 5 different classification tasks and get average performance over tasks to check the final performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_26",
            "start": 75,
            "end": 221,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_26@2",
            "content": "In the fully supervised setting, we evaluate models with 3 different random seeds and report the average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_26",
            "start": 223,
            "end": 336,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_26@3",
            "content": "In the lowresource setting, we set the size of the train data to 64 or 128.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_26",
            "start": 338,
            "end": 412,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_26@4",
            "content": "For each experiment, we run over 5 different sub-samples and show the average accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_26",
            "start": 414,
            "end": 500,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_27@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_27",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_28@0",
            "content": "In this section, we introduce the following two approaches to integrate visual knowledge into LMs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_28",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_29@0",
            "content": "(1) text knowledge transfer; and (2) cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_29",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_29@1",
            "content": "Throughout this section, we assume the data is a collection of image x v and caption",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_29",
            "start": 69,
            "end": 152,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_30@0",
            "content": "x l pairs (x v i , x l i ) m i=1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_30",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_31@0",
            "content": "(m is the size of the pairs) and image encoder f V and text encoder f L are given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_31",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_31@1",
            "content": "Note that we use the same text encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_31",
            "start": 83,
            "end": 121,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_32@0",
            "content": "Text Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_32",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_33@0",
            "content": "For text knowledge transfer, we investigate following pre-training objectives: (1) masked language modeling; and (2) text contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_33",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_34@0",
            "content": "Masked Language Modeling (MLM) Following BERT (Devlin et al., 2019), we select 15% of input tokens and replace them with [MASK].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_34",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_34@1",
            "content": "Of the selected tokens, 80% are replaced, 10% are not changed and 10% are replaced by random vocabulary token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_34",
            "start": 129,
            "end": 238,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_34@2",
            "content": "Here, we employ dynamic masking, which performs random masking and replacement during training to prevent the same masking for the same examples .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_34",
            "start": 240,
            "end": 385,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_34@3",
            "content": "MLM objective is the cross-entropy loss for masked token predictions :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_34",
            "start": 387,
            "end": 456,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_35@0",
            "content": "\u2113 MLM (x l i ) = \u2212 log p(x l i |x masked ),(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_35",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_36@0",
            "content": "where x i is the i-th token and x masked is a mask.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_36",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_37@0",
            "content": "Text Contrastive Learning (TCL) Contrastive learning aims to learn representations by pulling positive pairs closer and pushing negative pairs apart.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_37",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_37@1",
            "content": "Here, we employ the contrastive framework with cross-entropy objective and in-batch negatives (Chen et al., 2020a;Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_37",
            "start": 150,
            "end": 281,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_37@2",
            "content": "Given a text encoder f L , and a caption x l i , we first get text representations using the encoders h l i = f L (x l i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_37",
            "start": 283,
            "end": 405,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_37@3",
            "content": "Following Gao et al. (2021), we create identical A girl puts an apple in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_37",
            "start": 407,
            "end": 487,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_38@0",
            "content": "A girl puts an [MASK] in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_38",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_39@0",
            "content": "A girl puts an envelope in her bag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_39",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_40@0",
            "content": "Figure 3: LM perturbation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_40",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_40@1",
            "content": "We create adversarial negatives using language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_40",
            "start": 27,
            "end": 80,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_40@2",
            "content": "positive sample h l + i by different dropout representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_40",
            "start": 82,
            "end": 142,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_40@3",
            "content": "The contrastive loss is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_40",
            "start": 144,
            "end": 186,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_41@0",
            "content": "\u2113 l i = \u2212 log e sim(h l i ,h l + i )/\u03c4 N j=1 e sim(h l i ,h l j )/\u03c4 , (2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_41",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_42@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_42",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_43@0",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity, i.e., sim(u, v) = u \u2022 v/\u2225u\u2225\u2225v\u2225. \u03c4 represents a temperature parameter.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_43",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_44@0",
            "content": "Cross-modal Knowledge Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_44",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_45@0",
            "content": "Language models might learn additional information from visual sources such as images and captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_45",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_45@1",
            "content": "So we include a variety of vision-based approaches and investigate the approaches whether they can benefit from visual sources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_45",
            "start": 100,
            "end": 226,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_45@2",
            "content": "We introduce vision-based approaches as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_45",
            "start": 228,
            "end": 275,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_46@0",
            "content": "Voken Classification Vokenization (Tan and Bansal, 2020) employs token-level text-to-image retrieval to transfer visual knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_46",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_46@1",
            "content": "It aligns language tokens to their related images (called \"vokens\") to transfer visual knowledge into LMs, and call it \"voken classification\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_46",
            "start": 131,
            "end": 272,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_46@2",
            "content": "Given text x and a voken v i for the i-th token, the loss is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_46",
            "start": 274,
            "end": 344,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_47@0",
            "content": "\u2113 voken i = \u2212 log(p(v i |x)).(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_47",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_48@0",
            "content": "Similar to masked language modeling, it classifies each token to a corresponding voken.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_48",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_48@1",
            "content": "Vokenization trains language models with the voken classification task and MLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_48",
            "start": 88,
            "end": 166,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_49@0",
            "content": "Masked Language Modeling with Visual Clues VL-BERT (Su et al., 2020) adopts masked language modeling with visual clues in which models are given a caption with masked tokens and an image and predict the masked tokens using visual clues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_49",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_49@1",
            "content": "VL-BERT is pre-trained on Conceptual Captions (Sharma et al., 2018) as an image-caption corpus, and BooksCorpus (Zhu et al., 2015b) and English Wikipedia as text-only corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_49",
            "start": 237,
            "end": 411,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_49@2",
            "content": "It shows its effectiveness in many vision-language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_49",
            "start": 413,
            "end": 469,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_49@3",
            "content": "We investigate whether this model also succeed in NLP tasks and compare it with others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_49",
            "start": 471,
            "end": 557,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_50@0",
            "content": "To harness the visual knowledge from imagecaption datasets, we adopt contrastive loss on image and text vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_50",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_50@1",
            "content": "Given an image encoder f V , a text encoder f L , and an image-caption pair (x v i , x l i ), we first get image and text representations using the encoders",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_50",
            "start": 113,
            "end": 268,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_51@0",
            "content": "h v i = f V (x v i ), h l i = f L (x l i ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_51",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_52@0",
            "content": "Then the contrastive learning objective contains two loss functions: an image-to-text contrastive loss \u2113 (v,l) and a text-to-image contrastive loss \u2113 (l,v) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_52",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_52@1",
            "content": "The image-to-text contrastive loss is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_52",
            "start": 158,
            "end": 214,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_53@0",
            "content": "\u2113 (v,l) i = \u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 , (4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_53",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_54@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_54",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_55@0",
            "content": "where N is a batch size and sim(\u2022) represents cosine similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_55",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_55@1",
            "content": "This loss encourages a closer distance between representations of aligned imagecaption pairs than unaligned pairs given an image and multiple captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_55",
            "start": 65,
            "end": 215,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_55@2",
            "content": "Similarly, the text-to-image contrastive loss \u2113 (l,v) is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_55",
            "start": 217,
            "end": 292,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_56@0",
            "content": "\u2113 (l,v) i = \u2212 log e sim(h l i ,h v i )/\u03c4 N j=1 e sim(h l i ,h v j )/\u03c4 .(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_56",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_57@0",
            "content": "The final loss is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_57",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_58@0",
            "content": "L = 1 N N i=1 (\u2113 (v,l) i + \u2113 (l,v) i ).(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_58",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_59@0",
            "content": "CLIP (Radford et al., 2021) and ConVIRT (Zhang et al., 2020) also adopt contrastive learning, but we freeze the image encoder in training and use the trained text encoder for downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_59",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@0",
            "content": "CMCL with Adversarial Negative Samples (ANS) As in-batch negatives in CMCL are not challenging enough for models to distinguish, we present adversarial negative sampling strategy to improve CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@1",
            "content": "Given an image-caption pair (x v i , x l i ), we define a LM-perturbed sentence x l \u2212 i , which is a hard negative where n is replaced with a different word n \u2032 from a probability distribution of PTLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 196,
            "end": 397,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@2",
            "content": "We expect the l \u2212 is syntactically correct and plausible sentence even the word n is replaced to n \u2032 , while it does not semantically match to the corresponding image x v i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 399,
            "end": 572,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@3",
            "content": "With such hard negative, we try to make more challenging task so that models can effectively learn from the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 574,
            "end": 686,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@4",
            "content": "For example, we choose a word 'girl' in the sentence 'A girl puts an apple in her bag.' in Figure 3. Then we mask the word with [MASK] token to do masked token predictions by PTLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 688,
            "end": 868,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@5",
            "content": "Then we get topk predictions from language models and replace the masked tokens with one of the predicted ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 870,
            "end": 980,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@6",
            "content": "To avoid false negative sentences which may have the same semantics as the original sentence, we introduce an additional filtering step: if the masked predictions are synonyms or hypernyms of the original tokens, we discard the predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 982,
            "end": 1221,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@7",
            "content": "We use WordNet (Miller, 1992) to find synonyms and hypernyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 1223,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_60@8",
            "content": "The contrastive loss with hard negative is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_60",
            "start": 1285,
            "end": 1346,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_61@0",
            "content": "\u2212 log e sim(h v i ,h l i )/\u03c4 N j=1 e sim(h v i ,h l j )/\u03c4 + M k=1 e sim(h v i ,h l \u2212 j )/\u03c4 ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_61",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_62@0",
            "content": "where M is the number of hard negative samples per positive pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_62",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_62@1",
            "content": "This formula is only for image-totext contrastive loss \u2113 (v,l) and final loss is defined to same as equation ( 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_62",
            "start": 66,
            "end": 179,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_63@0",
            "content": "In ANS, we filter perturbed sentences where the masked predictions are synonyms or hypernyms of the original tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_63",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_63@1",
            "content": "Instead of excluding these perturbed sentences, another option is to include them as additional positive samples l + to the paired images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_63",
            "start": 117,
            "end": 254,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_63@2",
            "content": "We name this as positive sample augmentation (PSA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_63",
            "start": 256,
            "end": 306,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_63@3",
            "content": "It also adopts LM-perturbed negative samples as in ANS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_63",
            "start": 308,
            "end": 362,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@0",
            "content": "Cross-modal knowledge distillation is to transfer knowledge between different modalities, e.g., image modality and text modality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@1",
            "content": "In this category, CMKD is to transfer knowledge from a teacher model which is knowledgeable about visual information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 130,
            "end": 246,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@2",
            "content": "VidLanKD (Tang et al., 2021) also utilizes a cross-modal knowledge distillation method to help with general language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 248,
            "end": 378,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@3",
            "content": "A teacher model is first trained using contrastive learning on a video-text dataset, and then it transfers its knowledge to a student language model using KD on a text corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 380,
            "end": 554,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@4",
            "content": "Their contrastive learning loss (hinge loss) is defined as ties of a positive pair and a negative pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 556,
            "end": 658,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_64@5",
            "content": "Instead of video datasets, we use a MS COCO dataset to train a teacher model and use two versions of contrastive learning, equations ( 6) and ( 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_64",
            "start": 660,
            "end": 806,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_65@0",
            "content": "L = N i [max(0, \u03b1\u2212sim(h v i , h l i )+sim(h v \u2032 i , h l i )) + max(0, \u03b1 \u2212 sim(h v i , h l i ) + sim(h v i , h l \u2032 i ))],(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_65",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_66@0",
            "content": "As another version of CMKD, we consider distilling visual knowledge from a pre-trained visionlanguage model, VL-BERT, which is knowledgeable about grounded language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_66",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_66@1",
            "content": "We adopt masked language modeling on Wikitext103 (Merity et al., 2017), a subset of English Wikipedia, in the knowledge distillation step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_66",
            "start": 166,
            "end": 303,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_66@2",
            "content": "For knowledge distillation, we adopt Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), which proves the effectiveness in VidLanKD (Tang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_66",
            "start": 305,
            "end": 462,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_67@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_67",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@0",
            "content": "For all the approaches, we use bert-base-uncased (Devlin et al., 2019) as text encoder f L and ResNeXt101 (Xie et al., 2017) as an image encoder f V .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@1",
            "content": "We continue to pre-train the encoders in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 151,
            "end": 207,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@2",
            "content": "For text knowledge transfer, (1) MLM follows the exact setting of codebase in huggingface 2 which uses dynamic masking strategy to conduct language modeling task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 209,
            "end": 370,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@3",
            "content": "(2) TCL conducts contrastive learning with f L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 372,
            "end": 419,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@4",
            "content": "We choose the best checkpoint by the best spearman correlation on STSb (Cer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 421,
            "end": 510,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@5",
            "content": "For cross-modal knowledge transfer, (1) CMKD explores VL-BERT, Vokenization, and VidLanKD approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 512,
            "end": 612,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@6",
            "content": "Here, we use VL-BERTlarge model to do CMKD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 614,
            "end": 656,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@7",
            "content": "We use the VL-BERT and Vokenization checkpoints from their official codebases 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 658,
            "end": 738,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@8",
            "content": "VidLanKD trains a teacher model by two versions of contrastive learning (equations ( 6) and ( 8)) on MS COCO dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 740,
            "end": 856,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@9",
            "content": "We set \u03b1 = 1 in VidLanKD (equation ( 8)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 858,
            "end": 898,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@10",
            "content": "( 2) CMCL conducts contrastive learning with f L and f V .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 900,
            "end": 957,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@11",
            "content": "Here, we set \u03c4 = 0.05 (equations ( 2) and ( 4)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 959,
            "end": 1006,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@12",
            "content": "( 3) CMCL with ANS chooses three noun words or verb words to do masked prediction and use top-5 predictions from f L as replacement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1008,
            "end": 1139,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@13",
            "content": "We filter out synonyms and hypernyms of original words using WordNet (Miller, 1992).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1141,
            "end": 1224,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@14",
            "content": "( 4) CMCL with PSA includes the perturbed sentences with synonyms and hypernyms as additional positive samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1226,
            "end": 1336,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@15",
            "content": "In CMCL, we adopt ResNeXt101 (Xie et al., 2017) as an image encoder f V and BERT as a text encoder f L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1338,
            "end": 1441,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@16",
            "content": "TCL and CMCL train with batch size 64, maximum sequence length 20, learning rate 1e-4 for 3 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1443,
            "end": 1541,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@17",
            "content": "For fine-tuning on downstream tasks, we do grid search on learning rates {5e-5, 1e-4, 3e-4, 4e-4, 5e-4, 6e-4} and choose the best learning rate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1543,
            "end": 1686,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_68@18",
            "content": "We set maximum epochs to 30 in low-resource and 15 in fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_68",
            "start": 1688,
            "end": 1767,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_69@0",
            "content": "Results and Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_69",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@0",
            "content": "We analyze the main results of intermediate pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@1",
            "content": "Tables 2 and 3 show the main results of low-resource learning and fully supervised learning with the MS COCO captioning dataset, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 57,
            "end": 198,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@2",
            "content": "We train the models with a few training examples, 64 and 128, to understand the better initialization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 200,
            "end": 301,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@3",
            "content": "We argue that if a model obtains better performance in the low-resource setup, then it is a faster learner and has better generalization on downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 303,
            "end": 459,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@4",
            "content": "2 and 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 461,
            "end": 469,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@5",
            "content": "Specifically, CMKD with VidLanKD variant outperforms the baseline by 1.6% point on the PIQA dataset in fully supervised setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 471,
            "end": 598,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@6",
            "content": "CMCL also shows its effectiveness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 600,
            "end": 633,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@7",
            "content": "However, we could find that it becomes more powerful when equipped with PSA and ANS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 635,
            "end": 718,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@8",
            "content": "It suggests that data augmentation for positive and negative sampling is an important factor for CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 720,
            "end": 821,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_70@9",
            "content": "In low-resource setting, we find that cross-modal knowledge transfer helps better initialization and lets models learn new tasks faster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_70",
            "start": 823,
            "end": 958,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_71@0",
            "content": "What intermediate pre-training objectives are effective for cross-modal knowledge transfer? Among various cross-modal knowledge transfer methods, we study which method is the most effective for cross-modal knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_71",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_71@1",
            "content": "Overall, CMCL with PSA and ANS shows the best performance among all cross-modal methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_71",
            "start": 226,
            "end": 313,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_71@2",
            "content": "Interestingly, VL-BERT also shows better performance than BERT-base on all datasets in the low-resource setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_71",
            "start": 315,
            "end": 426,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_71@3",
            "content": "This suggests that exploiting images in masked language modeling task help transfer the knowledge to language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_71",
            "start": 428,
            "end": 544,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@0",
            "content": "What types of knowledge sources are most helpful?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@1",
            "content": "Here, we investigate whether using an image source in addition to a text source can further improve the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 50,
            "end": 159,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@2",
            "content": "To answer this question, we analyze methods from different types of sources: text-only and text-image pair sources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 161,
            "end": 275,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@3",
            "content": "We focus on the methods that use the contrastive learning objective: TCL and CMCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 277,
            "end": 358,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@4",
            "content": "Note that these two methods share the same objective but CMCL trains on cross modalities which are images and captions while TCL only trains on captions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 360,
            "end": 512,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@5",
            "content": "Overall, TCL performs slightly better than CMCL in low-resource and fully supervised settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 514,
            "end": 607,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@6",
            "content": "Interestingly, additional negative samples (ANS) and positive samples in TCL decreases the performance while they help CMCL to improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 609,
            "end": 759,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_72@7",
            "content": "We conjecture that perturbed sentences in ANS might not be semantically negative to the original sentence so models learn from wrong labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_72",
            "start": 761,
            "end": 900,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_73@0",
            "content": "Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_73",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_74@0",
            "content": "How do models perform on general NLU tasks? (Lin et al., 2014;Chen et al., 2015), GenericsKB (Bhakthavatsalam et al., 2020), BooksCorpus (Zhu et al., 2015a), and WikiText103 (Merity et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_74",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_74@1",
            "content": "We sample 250k sentences from each corpus for a fair comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_74",
            "start": 197,
            "end": 260,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_74@2",
            "content": "We notice that caption datasets are useful on OBQA and RiddleSense datasets while GenericsKB are the most helpful on PIQA datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_74",
            "start": 262,
            "end": 392,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_74@3",
            "content": "Results are expected since GenericsKB contains a lot of everyday",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_74",
            "start": 394,
            "end": 457,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_75@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_75",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@0",
            "content": "Text Knowledge enhanced methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@1",
            "content": "Recently, huge efforts on integrating knowledge into PTLMs have been made.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 33,
            "end": 106,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@2",
            "content": "One typical form of knowledge is a knowledge graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 108,
            "end": 158,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@3",
            "content": "There have been efforts of using knowledge graph to inject entity and relation representations, which are pre-computed from external source, into PTLMs (Zhang et al., 2019;Xu et al., 2021a;Peters et al., 2019;He et al., 2020;Xu et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 160,
            "end": 402,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@4",
            "content": "Some other works try to retrieve or generate the sub-graph from the graph to solve the problem (Lin et al., 2019;Wang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 404,
            "end": 535,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@5",
            "content": "Another existing form of knowledge is extra largescale corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 537,
            "end": 598,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@6",
            "content": "Works that use such corpus present knowledge-related pre-training objectives such as concept order recovering (Zhou et al., 2021), entity category prediction (Yu et al., 2020) and source of knowledge prediction (Wang et al., 2021;Calixto et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 600,
            "end": 851,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_76@7",
            "content": "They are mostly focused on injecting world knowledge presented in text, rather than physical and visual commonsense knowledge that can be found in images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_76",
            "start": 853,
            "end": 1006,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_77@0",
            "content": "There is a extensive line of works for a variety of vision-language tasks, such as VL-BERT (Su et al., 2020), VisualBert (Li et al., 2019), and Uniter (Chen et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_77",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_77@1",
            "content": "These models aim to improve vision-language tasks, e.g., VQA (Goyal et al., 2017) and event understanding (Li et al., 2022), and they are found to be not effective in improving language tasks (Tan and Bansal, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_77",
            "start": 173,
            "end": 387,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_78@0",
            "content": "Another line of works is to transfer visual knowledge to language models: Vokenization (Tan and Bansal, 2020) and VidLanKD (Tang et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_78",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_79@0",
            "content": "Vokenization employs token-level text-to-image retrieval to transfer visual knowledge to language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_79",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_79@1",
            "content": "For this, Vokenization introduces 30k vokens and matches each token into the limited voken space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_79",
            "start": 106,
            "end": 202,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_79@2",
            "content": "VidLanKD adopts contrastive learning to train a teacher model on video datasets and uses distillation approaches to distill visual knowledge from the teacher to a student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_79",
            "start": 204,
            "end": 380,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_80@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_80",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_81@0",
            "content": "We",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_81",
            "start": 0,
            "end": 1,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_82@0",
            "content": "UNKNOWN, None, 2020, Genericskb: A knowledge base of generic statements, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_82",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_83@0",
            "content": "Yonatan Bisk, Rowan Zellers, Ronan Lebras, Jianfeng Gao, Yejin Choi, PIQA: reasoning about physical commonsense in natural language, 2020-02-07, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_83",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_84@0",
            "content": "Iacer Calixto, Alessandro Raganato, Tommaso Pasini, Wikipedia entities as rendezvous across languages: Grounding multilingual language models by predicting Wikipedia hyperlinks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_84",
            "start": 0,
            "end": 377,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_85@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020-07, Proceedings of the 37th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_85",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_86@0",
            "content": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco captions: Data collection and evaluation server, 2015, ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_86",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_87@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_87",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_88@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_88",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_89@0",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, SimCSE: Simple contrastive learning of sentence embeddings, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_89",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_90@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the V in VQA matter: Elevating the role of image understanding in visual question answering, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_90",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_91@0",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_91",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_92@0",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_92",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_93@0",
            "content": "Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Yuan, Tong Xu, BERT-MK: Integrating graph contextualized knowledge into pretrained language models, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_93",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2017, Like what you like: Knowledge distill via neuron selectivity transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_94",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_95@0",
            "content": "UNKNOWN, None, 2019, Visualbert: A simple and performant baseline for vision and language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_95",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_96@0",
            "content": "UNKNOWN, None, 2022, Clip-event: Connecting text and images with event structures, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_96",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_97@0",
            "content": "Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen,  Ren, KagNet: Knowledge-aware graph networks for commonsense reasoning, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_97",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_98@0",
            "content": "Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee,  Ren, RiddleSense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_98",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_99@0",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_99",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_100@0",
            "content": "Xiao Lin, Devi Parikh, Don't just listen, use your imagination: Leveraging visual common sense for non-visual tasks, 2015-06-07, IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_100",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_101@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_101",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_102@0",
            "content": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, Pointer sentinel mixture models, 2017-04-24, 5th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_102",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_103@0",
            "content": "Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_103",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_104@0",
            "content": "A George,  Miller, WordNet: A lexical database for English, 1992-02-23, Speech and Natural Language: Proceedings of a Workshop Held at, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_104",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_105@0",
            "content": "Matthew Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah Smith, Knowledge enhanced contextual word representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_105",
            "start": 0,
            "end": 373,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_106@0",
            "content": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, KILT: a benchmark for knowledge intensive language tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_106",
            "start": 0,
            "end": 417,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_107@0",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020a. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_107",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_108@0",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020b. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_108",
            "start": 0,
            "end": 326,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_109@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07-24, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_109",
            "start": 0,
            "end": 345,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_110@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_110",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_111@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_111",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_112@0",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_112",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_113@0",
            "content": "UNKNOWN, None, , , Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_113",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_114@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017-02-04, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_114",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_115@0",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, VL-BERT: pretraining of generic visual-linguistic representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_115",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_116@0",
            "content": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, CommonsenseQA: A question answering challenge targeting commonsense knowledge, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_116",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_117@0",
            "content": "Hao Tan, Mohit Bansal, Vokenization: Improving language understanding with contextualized, visual-grounded supervision, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_117",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_118@0",
            "content": "Zineng Tang, Jaemin Cho, Hao Tan, Mohit Bansal, Vidlankd: Improving language understanding via video-distilled knowledge transfer, 2021, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_118",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_119@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_119",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_120@0",
            "content": "Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, Connecting the dots: A knowledgeable path generator for commonsense question answering, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_120",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_121@0",
            "content": "Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_121",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_122@0",
            "content": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Aggregated residual transformations for deep neural networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_122",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_123@0",
            "content": "UNKNOWN, None, 2021, Does knowledge help general nlu? an empirical study, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_123",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_124@0",
            "content": "Yichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu, Michael Zeng, Xuedong Huang, Fusing context into knowledge graph for commonsense question answering, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_124",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_125@0",
            "content": "Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, Jaket: Joint pre-training of knowledge graph and language understanding, 2020, ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_125",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_126@0",
            "content": "Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, Meng Jiang, Dict-bert: Enhancing language model pre-training with dictionary, 2021, ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_126",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2020, Contrastive learning of medical visual representations from paired images and text. ArXiv preprint, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_127",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_128@0",
            "content": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ERNIE: Enhanced language representation with informative entities, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_128",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_129@0",
            "content": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Selvam, Seyeon Lee, Xiang Ren, Pre-training text-to-text transformers for concept-centric common sense, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_129",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_130@0",
            "content": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015-12-07, 2015 IEEE International Conference on Computer Vision, ICCV 2015, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_130",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "66-ARR_v2_131@0",
            "content": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015, 2015 IEEE International Conference on Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "66-ARR_v2_131",
            "start": 0,
            "end": 273,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_1",
            "tgt_ix": "66-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_1",
            "tgt_ix": "66-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_4",
            "tgt_ix": "66-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_5",
            "tgt_ix": "66-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_6",
            "tgt_ix": "66-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_7",
            "tgt_ix": "66-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_8",
            "tgt_ix": "66-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_9",
            "tgt_ix": "66-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_10",
            "tgt_ix": "66-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_11",
            "tgt_ix": "66-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_12",
            "tgt_ix": "66-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_13",
            "tgt_ix": "66-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_14",
            "tgt_ix": "66-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_15",
            "tgt_ix": "66-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_16",
            "tgt_ix": "66-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_18",
            "tgt_ix": "66-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_19",
            "tgt_ix": "66-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_17",
            "tgt_ix": "66-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_17",
            "tgt_ix": "66-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_17",
            "tgt_ix": "66-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_17",
            "tgt_ix": "66-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_21",
            "tgt_ix": "66-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_21",
            "tgt_ix": "66-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_22",
            "tgt_ix": "66-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_23",
            "tgt_ix": "66-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_23",
            "tgt_ix": "66-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_24",
            "tgt_ix": "66-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_25",
            "tgt_ix": "66-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_25",
            "tgt_ix": "66-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_28",
            "tgt_ix": "66-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_29",
            "tgt_ix": "66-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_30",
            "tgt_ix": "66-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_31",
            "tgt_ix": "66-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_33",
            "tgt_ix": "66-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_34",
            "tgt_ix": "66-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_35",
            "tgt_ix": "66-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_36",
            "tgt_ix": "66-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_37",
            "tgt_ix": "66-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_38",
            "tgt_ix": "66-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_40",
            "tgt_ix": "66-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_41",
            "tgt_ix": "66-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_42",
            "tgt_ix": "66-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_39",
            "tgt_ix": "66-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_43",
            "tgt_ix": "66-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_45",
            "tgt_ix": "66-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_46",
            "tgt_ix": "66-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_47",
            "tgt_ix": "66-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_48",
            "tgt_ix": "66-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_50",
            "tgt_ix": "66-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_51",
            "tgt_ix": "66-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_52",
            "tgt_ix": "66-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_53",
            "tgt_ix": "66-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_54",
            "tgt_ix": "66-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_55",
            "tgt_ix": "66-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_56",
            "tgt_ix": "66-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_57",
            "tgt_ix": "66-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_58",
            "tgt_ix": "66-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_59",
            "tgt_ix": "66-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_61",
            "tgt_ix": "66-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_49",
            "tgt_ix": "66-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_62",
            "tgt_ix": "66-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_65",
            "tgt_ix": "66-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_63",
            "tgt_ix": "66-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_66",
            "tgt_ix": "66-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_67",
            "tgt_ix": "66-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_67",
            "tgt_ix": "66-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_71",
            "tgt_ix": "66-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_73",
            "tgt_ix": "66-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_73",
            "tgt_ix": "66-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_74",
            "tgt_ix": "66-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_77",
            "tgt_ix": "66-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_78",
            "tgt_ix": "66-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_79",
            "tgt_ix": "66-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_80",
            "tgt_ix": "66-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_80",
            "tgt_ix": "66-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "66-ARR_v2_0",
            "tgt_ix": "66-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_1",
            "tgt_ix": "66-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_2",
            "tgt_ix": "66-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_3",
            "tgt_ix": "66-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_4",
            "tgt_ix": "66-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_5",
            "tgt_ix": "66-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_5",
            "tgt_ix": "66-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_5",
            "tgt_ix": "66-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_5",
            "tgt_ix": "66-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_6",
            "tgt_ix": "66-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_6",
            "tgt_ix": "66-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_7",
            "tgt_ix": "66-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_7",
            "tgt_ix": "66-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_8",
            "tgt_ix": "66-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_9",
            "tgt_ix": "66-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_9",
            "tgt_ix": "66-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_10",
            "tgt_ix": "66-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_11",
            "tgt_ix": "66-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_12",
            "tgt_ix": "66-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_12",
            "tgt_ix": "66-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_13",
            "tgt_ix": "66-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_14",
            "tgt_ix": "66-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_15",
            "tgt_ix": "66-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_16",
            "tgt_ix": "66-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_16",
            "tgt_ix": "66-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_16",
            "tgt_ix": "66-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_16",
            "tgt_ix": "66-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_17",
            "tgt_ix": "66-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_18",
            "tgt_ix": "66-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_18",
            "tgt_ix": "66-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_19",
            "tgt_ix": "66-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_19",
            "tgt_ix": "66-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_20",
            "tgt_ix": "66-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_21",
            "tgt_ix": "66-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_22",
            "tgt_ix": "66-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_22",
            "tgt_ix": "66-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_22",
            "tgt_ix": "66-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_22",
            "tgt_ix": "66-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_23",
            "tgt_ix": "66-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_24",
            "tgt_ix": "66-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_24",
            "tgt_ix": "66-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_24",
            "tgt_ix": "66-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_24",
            "tgt_ix": "66-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_25",
            "tgt_ix": "66-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_26",
            "tgt_ix": "66-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_27",
            "tgt_ix": "66-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_28",
            "tgt_ix": "66-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_29",
            "tgt_ix": "66-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_29",
            "tgt_ix": "66-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_30",
            "tgt_ix": "66-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_31",
            "tgt_ix": "66-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_31",
            "tgt_ix": "66-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_32",
            "tgt_ix": "66-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_33",
            "tgt_ix": "66-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_34",
            "tgt_ix": "66-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_34",
            "tgt_ix": "66-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_34",
            "tgt_ix": "66-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_34",
            "tgt_ix": "66-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_35",
            "tgt_ix": "66-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_36",
            "tgt_ix": "66-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_37",
            "tgt_ix": "66-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_37",
            "tgt_ix": "66-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_37",
            "tgt_ix": "66-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_37",
            "tgt_ix": "66-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_38",
            "tgt_ix": "66-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_39",
            "tgt_ix": "66-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_40",
            "tgt_ix": "66-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_40",
            "tgt_ix": "66-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_40",
            "tgt_ix": "66-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_40",
            "tgt_ix": "66-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_41",
            "tgt_ix": "66-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_42",
            "tgt_ix": "66-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_43",
            "tgt_ix": "66-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_44",
            "tgt_ix": "66-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_45",
            "tgt_ix": "66-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_45",
            "tgt_ix": "66-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_45",
            "tgt_ix": "66-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_46",
            "tgt_ix": "66-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_46",
            "tgt_ix": "66-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_46",
            "tgt_ix": "66-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_47",
            "tgt_ix": "66-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_48",
            "tgt_ix": "66-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_48",
            "tgt_ix": "66-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_49",
            "tgt_ix": "66-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_49",
            "tgt_ix": "66-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_49",
            "tgt_ix": "66-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_49",
            "tgt_ix": "66-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_50",
            "tgt_ix": "66-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_50",
            "tgt_ix": "66-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_51",
            "tgt_ix": "66-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_52",
            "tgt_ix": "66-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_52",
            "tgt_ix": "66-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_53",
            "tgt_ix": "66-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_54",
            "tgt_ix": "66-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_55",
            "tgt_ix": "66-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_55",
            "tgt_ix": "66-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_55",
            "tgt_ix": "66-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_56",
            "tgt_ix": "66-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_57",
            "tgt_ix": "66-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_58",
            "tgt_ix": "66-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_59",
            "tgt_ix": "66-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_60",
            "tgt_ix": "66-ARR_v2_60@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_61",
            "tgt_ix": "66-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_62",
            "tgt_ix": "66-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_62",
            "tgt_ix": "66-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_63",
            "tgt_ix": "66-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_63",
            "tgt_ix": "66-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_63",
            "tgt_ix": "66-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_63",
            "tgt_ix": "66-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_64",
            "tgt_ix": "66-ARR_v2_64@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_65",
            "tgt_ix": "66-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_66",
            "tgt_ix": "66-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_66",
            "tgt_ix": "66-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_66",
            "tgt_ix": "66-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_67",
            "tgt_ix": "66-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_68",
            "tgt_ix": "66-ARR_v2_68@18",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_69",
            "tgt_ix": "66-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_70",
            "tgt_ix": "66-ARR_v2_70@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_71",
            "tgt_ix": "66-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_71",
            "tgt_ix": "66-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_71",
            "tgt_ix": "66-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_71",
            "tgt_ix": "66-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_72",
            "tgt_ix": "66-ARR_v2_72@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_73",
            "tgt_ix": "66-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_74",
            "tgt_ix": "66-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_74",
            "tgt_ix": "66-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_74",
            "tgt_ix": "66-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_74",
            "tgt_ix": "66-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_75",
            "tgt_ix": "66-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_76",
            "tgt_ix": "66-ARR_v2_76@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_77",
            "tgt_ix": "66-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_77",
            "tgt_ix": "66-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_78",
            "tgt_ix": "66-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_79",
            "tgt_ix": "66-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_79",
            "tgt_ix": "66-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_79",
            "tgt_ix": "66-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_80",
            "tgt_ix": "66-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_81",
            "tgt_ix": "66-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_82",
            "tgt_ix": "66-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_83",
            "tgt_ix": "66-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_84",
            "tgt_ix": "66-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_85",
            "tgt_ix": "66-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_86",
            "tgt_ix": "66-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_87",
            "tgt_ix": "66-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_88",
            "tgt_ix": "66-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_89",
            "tgt_ix": "66-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_90",
            "tgt_ix": "66-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_91",
            "tgt_ix": "66-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_92",
            "tgt_ix": "66-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_93",
            "tgt_ix": "66-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_94",
            "tgt_ix": "66-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_95",
            "tgt_ix": "66-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_96",
            "tgt_ix": "66-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_97",
            "tgt_ix": "66-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_98",
            "tgt_ix": "66-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_99",
            "tgt_ix": "66-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_100",
            "tgt_ix": "66-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_101",
            "tgt_ix": "66-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_102",
            "tgt_ix": "66-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_103",
            "tgt_ix": "66-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_104",
            "tgt_ix": "66-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_105",
            "tgt_ix": "66-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_106",
            "tgt_ix": "66-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_107",
            "tgt_ix": "66-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_108",
            "tgt_ix": "66-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_109",
            "tgt_ix": "66-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_110",
            "tgt_ix": "66-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_111",
            "tgt_ix": "66-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_112",
            "tgt_ix": "66-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_113",
            "tgt_ix": "66-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_114",
            "tgt_ix": "66-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_115",
            "tgt_ix": "66-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_116",
            "tgt_ix": "66-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_117",
            "tgt_ix": "66-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_118",
            "tgt_ix": "66-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_119",
            "tgt_ix": "66-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_120",
            "tgt_ix": "66-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_121",
            "tgt_ix": "66-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_122",
            "tgt_ix": "66-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_123",
            "tgt_ix": "66-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_124",
            "tgt_ix": "66-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_125",
            "tgt_ix": "66-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_126",
            "tgt_ix": "66-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_127",
            "tgt_ix": "66-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_128",
            "tgt_ix": "66-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_129",
            "tgt_ix": "66-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_130",
            "tgt_ix": "66-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "66-ARR_v2_131",
            "tgt_ix": "66-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 839,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "66-ARR",
        "version": 2
    }
}