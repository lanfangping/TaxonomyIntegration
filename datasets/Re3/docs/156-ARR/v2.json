{
    "nodes": [
        {
            "ix": "156-ARR_v2_0",
            "content": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_2",
            "content": "Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "156-ARR_v2_4",
            "content": "Pre-trained language models (PLMs) have been widely explored both in natural language understanding (NLU) and generation (NLG) in recent years, this pre-training and fine-tuning paradigm sheds light on various downstream tasks in natural language processing (NLP). Compared with general pre-trained models, task-oriented pre-trained models (such as Summarization, Dialog and etc.), which is designed in line with task characteristics, may achieve better performance and be more robust. In this paper, we proposes a novel pre-trained dialog response generation model based on previous research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_5",
            "content": "Dialogue Response Generation (DSG) in open domain is a challenging task with a wide range of application scenarios. Recent advances in DSG utilize pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) in two major categories. The first one focuses on how to fine-tune PLMs in downstream tasks and address the various application-specific needs and challenges (Lin et al., 2020). The second one augments dialog specific tasks into the PLM training Bao et al., 2020) and then fine-tunes the new pre-trained model in downstream tasks. We study the latter in this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_6",
            "content": "There is a proverbial one-to-many problem in DSG, i.e., a single dialog context could be followed by multiple reasonable responses. Existing works introduce latent variables to model this problem. For example, VHRED (Serban et al., 2017) incorporates latent continuous variable into the sequenceto-sequence (Seq2Seq) RNN model to improve the diversity of generated responses. VAE-Seq2Seq (Bahuleyan et al., 2017) proposes variational attention to replace the vanilla encoder-decoder attention (Luong et al., 2015), to avoid attention to bypass the latent space and invalidate the latent variable. For controllability and interpretability, some discrete VAEs have also been proposed, such as (Oord et al., 2017;Vahdat et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_7",
            "content": "Recently, PLATO (Bao et al., 2020) firstly introduces latent variables into their pre-training dialog model, where the authors introduce a K-way (K = 20) categorical latent variable, and the pretrained model shows significant gains in multiple downstream response generation tasks. Continuous latent variables besides discrete latent variables is popularly used for modeling one-to-many mapping in dialog system, but the potential of incorporating continuous latent variables with large-scale language pretraining is less explored.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_8",
            "content": "In this paper, we propose a pre-trained latent Variable Encoder-Decoder model for Dialog generation, which is called DialogVED. In this model, we introduce a continuous latent variable into the enhanced encoder-decoder pre-training framework and we adopt the optimization techniques based on the VAEs literature to learn the model with continuous latent variables. More specifically, we conduct the pre-training by optimizing the following 4 pre-training objectives simultaneously: 1) masked language spans loss to enhance the encoder's understanding of context, 2) response generation with n-gram loss to improve the decoder's planning ability, 3) Kullback-Leibler divergence loss to minimize the difference between the posterior and prior distribution of the latent variables, and 4) bag-ofwords loss to reduce posterior distribution collapse. In addition, we also explore the effect of absolute and relative position embeddings specific for conversational data on the model performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_9",
            "content": "We conduct experiments on three different kinds of conversation tasks: chit-chat, knowledge grounded conversation, and conversational question answering. Experimental results verify the effectiveness and superiority of our model compared with the previous state-of-the-art method. We further carry out ablation study to better understand the impact of different components in the DialogVED on model performance including latent space sizes, different decoding strategies, and position embeddings for turns and roles.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_10",
            "content": "The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; 2) We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; 3) Extensive experiments show that the proposed model achieves the new state-of-theart (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_11",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "156-ARR_v2_12",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "156-ARR_v2_13",
            "content": "In response generation, there are three elements: dialogue context c, response r and latent variable z. The dialogue context c may consist of several history utterances (i.e., multi turns) and the response r is one piece of appropriate reply towards the given context. Additionally, the latent variable z in the latent space represents many unobserved factors associating the context and the response.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_14",
            "content": "We assume the latent variable z is continuous, which is different from PLATO (Bao et al., 2020), and portrays a certain conditional probability distribution related to the response given context. We then define the conditional distribution p(r, z|c) = p(r|c, z)p(z|c) and our goal is to use encoder-decoder models (parameterized by \u03b8) to approximate p(r|c, z) and a multi-layer perceptron (parametrized by \u03d5) to estimate p(z|c), which is called the prior network in VAE literature. We call the final pre-trained model DialogVED, which is a transformer-based encoder-decoder model with an extra prior network for modeling the latent space. Figure 1 gives a overview of our model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_15",
            "content": "Encoder",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "156-ARR_v2_16",
            "content": "We use multi-layer Transformer-based (Vaswani et al., 2017) encoder to encode the dialogue context. First, an input sequence of tokens is mapped to a sequence of embeddings, which are then passed into the encoder. The encoder consists of a stack of \"blocks\", each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Compared to the vanilla transformer encoder, our encoder has slight differences in position embeddings and self-attention layer in fine-tuning phase, which contains richer location information and will be introduced in \u00a7 2.7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_17",
            "content": "Decoder",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "156-ARR_v2_18",
            "content": "Future predicting strategy has been concerned in recent research (Qi et al., 2020;Xiao et al., 2020), instead of predicting only the next token at each time step, the decoder using future predicting predicts n future tokens simultaneously.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_19",
            "content": "Specifically, the original Seq2Seq model aims to optimize the conditional likelihood P (r t |r <t , c), while future predicting strategy changes the optimization of predicting next single token to P (r t:t+n\u22121 |r <t , c) at each time step t, where r t:t+n\u22121 denotes the next continuous n future tokens. The future n-gram prediction loss can explicitly encourage the model to plan for future token prediction and prevent over-fitting on strong local correlations (Qi et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_20",
            "content": "We adopt the n-stream self-attention proposed in ProphetNet (Qi et al., 2020) in our decoder. The n-stream self-attention mechanism incorporates n extra self-attention predicting streams besides main stream to predict next n continuous future tokens respectively at each time step. Memory Scheme To incorporate the latent variable into decoder, we adopt a memory scheme similar to OPTIMUS , where latent variable z \u2208 R P is mapped to a additional memory vector, denoted as h M em , which is an additional key-value pair for decoder to attend. We have memory vector",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_21",
            "content": "h M em = z key z value = W M z (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_22",
            "content": "where W M \u2208 R H\u00d7P is the weight matrix, and the memory vector is shared and propagated across all layers in decoder as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_23",
            "content": "H (k+1) = MultiHead(H (k) , h (k) M em \u2295 H (k) , h (k) M em \u2295 H (k) )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_24",
            "content": "where H (k) refers to the hidden state of the k-th layer of decoder. The memory vector is equivalent to adding a virtual token during decoding to participate in the calculation of self-attention main stream, and the predicting streams are implicitly affected by h M em through interaction with the main stream. The latent variable guides the generation of each step of the decoder through the memory vector.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_25",
            "content": "Latent Variable",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "156-ARR_v2_26",
            "content": "Intuitively, introducing latent variables provides a hierarchical generation procedure: 1) sample a latent variable z from the prior network p(z|c); 2) generate r through the decoder network p(r|c, z). From previous research (Zhao et al., 2017a), z \u223c p(z|c) may determine the high-level semantics, and the auto-regressive decoding is followed to produce the output sentences with low-level syntactic and lexical details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_27",
            "content": "Similar to the Variational Autoencoders (VAEs), we learn the parameters \u03b8 by maximizing the marginal log likelihood:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_28",
            "content": "log p \u03b8 (r|c) = log p \u03d5 (z|c)p \u03b8 (r|c, z)dz,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_29",
            "content": "where p \u03d5 involves an intractable marginalization over the latent variable z. (Kingma et al., 2016;, We will optimize its lower bound, which is equivalent to minimize the two terms below: reconstruction loss (or negative loglikelihood)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_30",
            "content": "L rc = \u2212E q(z) [log p \u03b8 (r|c, z)] = \u2212E q(z) [log t p \u03b8 (r t:t+n\u22121 |r <t , c)] (2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_31",
            "content": "and K-L regularization term",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_32",
            "content": "L kl = KL(q(z)||p \u03d5 (z|c)).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_33",
            "content": "(3)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_34",
            "content": "Here q(z) is a multivariable normal distribution with mean \u00b5 \u2208 R P and diagonal variance matrix with diagonal taiking values \u03c3 2 \u2208 R P , denoted as diag(\u03c3 2 ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_35",
            "content": "To connect to the hidden space, we add a special classification token ([CLS]) to the beginning of the context, and the first hidden state denoted as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_36",
            "content": "h [CLS] \u2208 R H in last-layer is used to represent the global dialog context. We assume \u00b5 log(\u03c3 2 ) = MLP h h [CLS] (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_37",
            "content": "where MLP h is a multilayer perceptron and this multilayer perceptron is called the prior network in VAEs literature. We can then sample P random variables with each variable is from standard normal distribution and via transformation, we obtain samples of z \u2208 R P from N (\u00b5, diag(\u03c3 2 )), and feed them to the decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_38",
            "content": "Mask Language Spans",
            "ntype": "title",
            "meta": {
                "section": "2.5"
            }
        },
        {
            "ix": "156-ARR_v2_39",
            "content": "To improve the understanding ability of the encoder and the robustness to noise, we randomly mask part of the context before encoding. Recent research (Joshi et al., 2020;Lewis et al., 2020) on masked language models show the advantages of masking spans over masking individual words or subword units.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_40",
            "content": "We adopt a simple method to mask spans: 1) randomly select n tokens in context, denote as S; 2) for each token t \u2208 S, extend it to a text span with a fixed length of m; 3) mask all selected tokens after sorting, deduplication and boundary checking.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_41",
            "content": "Following BERT (Devlin et al., 2019), the total number of masked tokens in the context accounts for approximately 15%, and we replace the masked token with: 1) the [MASK] token 80% of the time; 2) a random token 10% of the time; 3) the unchanged masked token 10% of the time. Then, the last-layer hidden states h x \u2208 R H of each masked token x will be used to predict the original token and the encoder is trained to optimize the cross entropy loss:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_42",
            "content": "L M = \u2212 x LSM(W 2 tanh(W 1 h x +b 1 ))(x) (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_43",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_44",
            "content": "W 1 \u2208 R H\u00d7H , b 1 \u2208 R H and W 2 \u2208 R H\u00d7|V |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_45",
            "content": "denote the weight matrices of one fully-connected layer, |V | is the vocabulary size, LSM is log softmax function and LSM(. . . )(x) means to take the log probability value corresponding to token x. In this paper, we share the parameters of W 2 with parameters of embedding layers in the encoder and decoder. Note that we only mask the context only the pre-training stage.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_46",
            "content": "Reduce KL-vanishing",
            "ntype": "title",
            "meta": {
                "section": "2.6"
            }
        },
        {
            "ix": "156-ARR_v2_47",
            "content": "DialogVED allows the decoder to attend the hidden states of context (i.e., the output of the encoder), and thus direct training will cause the decoder to ignore the latent variable z, and the KL loss will rapidly decrease to 0 and the latent space loses its expressive power, which is called posterior collapse or KL-vanishing (Bowman et al., 2016). This paper adopts two methods developed in VAEs literature to reduce posterior collapse:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_48",
            "content": "Free Bits (Kingma et al., 2016), which replaces the K-L regularization term in (3) with a hinge loss term that maximize each component of the original K-L term with a constant \u03bb:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_49",
            "content": "L \u2032 kl = \u2212 i max(\u03bb, KL(q(z i )||p \u03d5 (z i |c))) (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_50",
            "content": "Bag-of-words Loss (Zhao et al., 2017b), which is used to encourage the latent variable to predict the words in response r in a non-autoregressive way:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_51",
            "content": "L BOW = \u2212 T t=1 log f rt (7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_52",
            "content": "where T is the number of tokens in response r, and f rt denotes the estimated probability of word r t . More specifically, f is the function outputting the probability of words within the target response:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_53",
            "content": "f = softmax(MLP z [z \u2295 h [CLS] ]) \u2208 R |V | (8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_54",
            "content": "where MLP z is a multilayer perceptron and V refers to the whole vocabulary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_55",
            "content": "Position Embeddings",
            "ntype": "title",
            "meta": {
                "section": "2.7"
            }
        },
        {
            "ix": "156-ARR_v2_56",
            "content": "Besides tokenlevel learned position embeddings used in original Transformer, we also consider turn level and speaker-level position embeddings like PLATO (Bao et al., 2020). To better model the meaning of a turn in a dialog, We introduce embedding for turn position and role position in one conversation, the final input embedding of each token is the sum of corresponding turn, role and token embeddings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_57",
            "content": "Relative Position Embeddings It has recently become more common to use relative position embeddings, which produce a different learned embedding according to the offset between the \"key\" and \"query\" being compared in the self-attention mechanism (Shaw et al., 2018;Raffel et al., 2019). We extend the element of the original relative distance matrix in T5 (Raffel et al., 2019) to two-tuple.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_58",
            "content": "e ij = x i W Q (x j W K + a K ij ) T \u221a d z , a K ij = f (d token , d turn , x i , x j )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_59",
            "content": "In the mapping function f , we consider both token relative distance d token and turn relative distance d turn , where these tuples are mapped through a bucket function, and then a K ij is queried in predefined embedding layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_60",
            "content": "Pre-training Objectives",
            "ntype": "title",
            "meta": {
                "section": "2.8"
            }
        },
        {
            "ix": "156-ARR_v2_61",
            "content": "Combining the losses detailed in the Equations (2) (5) ( 6) and ( 7), we have pre-training objective, which we use to pre-train the DialogVED on the large-scale conversation corpus:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_62",
            "content": "loss = L M + L rc + L \u2032 kl + L BOW(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_63",
            "content": "To sum up, we mask text spans in the context c, sample a latent variable z from prior network, and then let the encoder and decoder predict the masked spans and response r respectively with the guidance of the latent variable z.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_64",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "156-ARR_v2_65",
            "content": "In this section, we firstly introduce the pre-training datasets and fine-tuning benchmarks in \u00a7 3.1, and implement details in \u00a7 3.2. Then we present the main results in \u00a7 3.3. Lastly, we analyze the influence of parameters and position embeddings in \u00a7 3.4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_66",
            "content": "DataSets and Baselines",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "156-ARR_v2_67",
            "content": "Pre-training Corpus",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "156-ARR_v2_68",
            "content": "Large-scale Reddit comments dataset (Zhou et al., 2018;Galley et al., 2019) is employed for pretraining our dialog language model. This dataset has been proved to be helpful in various conversation downstream tasks (Bao et al., 2020;. We use the script provided by Di-aloGPT to obtain the latest Reddit comment data. We obtain 215 million 1 training samples (42GB in total) for pre-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_69",
            "content": "To accelerate the training process and accommodate GPU memory limitations, we adopt two methods. First, we sort the samples according to the length of the context. Samples with similar length (i.e. number of tokens in context) are assembled into a batch to minimize the amount of padding. Secondly, due to the uneven distribution of sample lengths, we divide the Reddit corpus into two sub-datasets: Reddit-Short and Reddit-Long according to the length of context and response. with some statistics in Table 1, and optimize the batch size for each sub-dataset to avoid reserving a large amount of memory for a few long response samples during the training process. Within an epoch, we first pre-train on Reddit-Short with a larger batch size, and then pre-train Reddit-Long with a smaller batch size. We split the reddit comment dataset here mainly for efficiency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_70",
            "content": "Fine-tuning Benchmarks",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "156-ARR_v2_71",
            "content": "Following PLATO (Bao et al., 2020), we select three datasets as our benchmarks: DailyDialog (Li et al., 2017), a chit-chat dataset, which contains high-quality human conversations about daily life.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_72",
            "content": "Persona-Chat (Zhang et al., 2018), a knowledge grounded conversation dataset. It provides both manually annotated conversations and corresponding persona profiles (background knowledge), where two participants chat naturally and try to get to know each other.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_73",
            "content": "DSTC7-AVSD (Alamri et al., 2019a), a conversational question answering dataset, shorts for Audio Visual Scene-aware Dialog of the DSTC7 challenge. The system needs to generate an answer given dialogue context and background knowledge. There are multiple reference responses for each context in DSTC7-AVSD test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_74",
            "content": "For evaluation, we use the same metrics as used in PLATO, except for knowledge-related metrics, since this paper does not focus on utilizing knowledge. So we will focus the following metrics:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_75",
            "content": "BLEU-1/2 (Papineni et al., 2002), which measures the relevance of generated text to the reference text by calculating the 1/2-gram overlapping between them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_76",
            "content": "Distinct-1/2 (Li et al., 2016a), which measures the diversity of a generated sentence by focusing on the number of distinct 1/2-gram of a sentence and thus penalizing sentences with lots of repeated words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_77",
            "content": "Other word-overlap-based metrics, METEOR, ROUGE-L, and CIDEr, which are also reported for the DSTC7-AVSD dataset, same as DSTC7 reviews (Alamri et al., 2019b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_78",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "3.1.3"
            }
        },
        {
            "ix": "156-ARR_v2_79",
            "content": "Vanilla sequence to sequence (Seq2Seq) models, dialog pre-training models, and general natural language pre-training models are used as our baselines: Seq2Seq (Vinyals and Le, 2015) is a sequenceto-sequence model with attention. iVAE MI (Fang et al., 2019) is an implicit deep latent variable model based on Variational Autoencoder for better latent representations and diverse responses. LIC (Golovanov et al., 2019) obtains the best performance during the contest, and is one transformer based generation method. PLATO (Bao et al., 2020) utilizes a discrete latent variable for dialog generation pre-training to address the one-to-many problem. ProphetNet (Qi et al., 2020) is a pretrained LM model with predicting more than one future tokens as the pre-training objective. We finetune ProphetNet-Large model released in (Qi et al., 2020) with downstream training data directly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_80",
            "content": "For benchmark DSTC7-AVSD, we include AVSD Baseline (Alamri et al., 2019a) system provided by the the challenge organizer, as well as the best performing model developed by the team of CMU Sinbad's (Sanabria et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_81",
            "content": "Model Configuration",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "156-ARR_v2_82",
            "content": "DialogVED is composed of a 12-layer encoder and a 12-layer decoder, with 1024 embedding/hidden size and 4096 feed-forward filter size. The dimension P of hidden states z is set to 64 and we will analyze the effect of P in \u00a7 3.4.1. We use Adam optimizer (Kingma and Ba, 2014) with a learning rate of 3 \u00d7 10 \u22124 for pre-training. We set ngram as 2 following ProphetNet (Qi et al., 2020). The pre-training of dialogue generation is carried out on 32 Nvidia Telsa V100 32G GPU (4 nodes) for 6 epochs, taking about 5 days to reach convergence. Mixed precision training is also adopted for efficiently training and inference, and we use the Fairseq (Ott et al., 2019) framework to conduct all experiments. We use the BERT-uncased dictionary, and replace some unused tokens to custom special symbols (such as [SOT], denoting the beginning of the conversation, which is suitable for conversation datasets containing knowledge, like PersonaChat and DSTC7-AVSD). We used package WordPiece (Devlin et al., 2019) for tokenization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_83",
            "content": "For fine-tuning, we use exactly the same hyperparameter settings in all three datasets, and they are slightly different from the hyperparameter in pre-training. The learning rate is set to 1 \u00d7 10 \u22124 and the batch size is fixed to 512. We also adopt an additional warmup strategy where we linearly increase the learning rate from initial learning rate (1 \u00d7 10 \u22127 ), the number of warmup updates is set to 2000. For each dataset, we train 10 epochs, and select the checkpoint with the lowest validation loss for inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_84",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "156-ARR_v2_85",
            "content": "In Table 2, we compare several DialogVED variants with baseline models. DialogVED represents inferencing DialogVED with beam search. Compared with DialogVED, DialogVED w/o latent is not equipped with latent variable, thus the loss function does not include bag-of-words loss and K-L loss. DialogVED Greedy means DialogVED inference with greedy search. For DialogVED Sampling, we sample from the top K tokens with the highest output probability at each decoding step. For the latent space, we always sample each latent variable from the prior distribution standard normal distribution. Here, beam size is set to 5 and K is set to 100.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_86",
            "content": "As shown in Table 2 and Table 3, our model Di-alogVED is very competitive compared to PLATO and other models. In particular, decoding using Top-K (K = 100) sampling with DialogVED beats the PLATO in BLEU-1/2 and Distinct-1/2 on Dai-lyDialog and PersonaChat (see in Table 2). In fact, as K increases, the overlap of n-grams decreases and the diversity increases. Based on our observations, K taking 100 is a good balance, Table 4 shows more detailed results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_87",
            "content": "On the DSTC7-AVSD, the diversity of the responses is not as important as the accuracy. From Table 3, We observe that DialogVED w/o latent variable perform the best in overall metrics. However, DialogVED equipped with beam search or greedy search, can still easily beat PLATO even though it has a post-generation ranking component.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_88",
            "content": "There are 2 essential components that contribute greatly the success of our model: Firstly, We adopt a newly developed pretrained LM as the initializer and further continue its pretraining pipeline on our dialog dataset (Reddit) and thus we have a really powerful encoder-decoder. This is demonstrated in the fact that our model (DialogVED w/o latent variable) beat PLATO (w/o latent variable) in all metrics on all the three datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_89",
            "content": "Secondly, the special structure of our model combines the benefits of both seq2seq models and VAE models. Compared to general VAEs, DialogVED allows encoder-decoder interaction in the decoding, which avoids insufficient representation of lowdimensional latent variable. At the same time, compared with seq2seq model, predicting the bag of words pushes the latent variable to give extra guidance to decoder. This is demonstrated by the fact that when compared with DialogVED w/o latent variable, we observe the additional gains in terms of both accuracy and diversity (see Table 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_90",
            "content": "Overall, our DialogVED achieves new state-ofthe-art results in all three downstream tasks of dialogue response generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_91",
            "content": "Parameters and Position Analysis",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "156-ARR_v2_92",
            "content": "Balancing Accuracy and Diversity with Sampling",
            "ntype": "title",
            "meta": {
                "section": "3.4.1"
            }
        },
        {
            "ix": "156-ARR_v2_93",
            "content": "We investigate the effect of latent space sizes, P , defined as the dimension of the latent variable z and the different K in sampling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_94",
            "content": "The results in Table 4 show that smaller latent size (P = 32) is more dominant in n-gram based metrics (BLEU-1/2), while larger latent size generates more diverse texts. From the results of top-K sampling, we see that the two metric (BLEU-1/2 and Distinct-1/2) have a negative correlation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_95",
            "content": "We can flexibly choose the decoding strategy depends on specific scene.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_96",
            "content": "Position Embeddings",
            "ntype": "title",
            "meta": {
                "section": "3.4.2"
            }
        },
        {
            "ix": "156-ARR_v2_97",
            "content": "We study the impact of position embeddings as described in section 2.7, we define two types of position embeddings: absolute position embeddings (APE) and relative position embeddings (RPE).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_98",
            "content": "We report the metrics of their different combinations, these independent components are TurnAPE (turn absolute embedding), RoleAPE (role absolute embedding), TokenRPE (token relative embedding) and TurnRPE(turn relative embedding) respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_99",
            "content": "As the results shown in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_100",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "156-ARR_v2_101",
            "content": "Automated metrics (BLEU 1/2, Distinct-1/2, etc.) have limitations for evaluating open-domain dialog tasks. To make it more convincing, we conduct a human evaluation. Specifically, we randomly select 100 dialogue contexts and generate responses with the following methods: PLATO, DialogVED and DialogVED-Sampling. Following PLATO, annotators are asked to compare the response (win, tie or lose) quality from four aspects: fluency, coherence, informativeness and overall. The results of human comparison are shown in Table 6, where the average Cohen's kappa (Kraemer, 2014) of group 1 and 2 is 0.729 and 0.743 respectively, indicating annotators have reached moderate agreement. It can be seen that most of the time they are tied, and the three models sometimes generate exactly the same response. For Di-alogVED, it beats Plato more in coherence but with close informativeness; while DialogVED-sampling beats Plato significantly in informativeness but with a slightly weaker coherence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_102",
            "content": "In general, DialogVED can generate both relevant and diverse response, we show some case study to help illustrate the effectiveness of our model in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_103",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "156-ARR_v2_104",
            "content": "Encoder-Decoder dialog models Unlike retrieval based dialogue systems (Boussaha et al., 2019;Chen et al., 2021), encoder-decoder models are widely used in dialog response generation, but it tends to generate generic responses and dull responses (e.g., I don't know). To enhance encoderdecode models and generate diverse responses, researchers have tried different approaches: using diversity promotion objectives (Li et al., 2016a), using different decoding algorithms (Li et al., 2016b), adding additional contents (Xu et al., 2019), or introducing large-scale knowledge graphs into dialog generation (Liu et al., 2018;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_105",
            "content": "Another class of methods is using the latent variable to address the one-to-many problem in response generation. These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017a. In this paper, we also adopt this approach and further we incorporate the latent variables both in the pre-training and fine-tuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_106",
            "content": "Pre-trained language models have been successfully used in NLG and NLU tasks (Devlin et al., 2019;Radford et al., 2019). Recently, various new pre-trained language models have been pre-trained including BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), T5 (Raffel et al., 2020). In these papers, they demonstrate that better performance can be obtained with fine-tuning PLMs than training from scratch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_107",
            "content": "Due to the fact that there are many important applications in the dialog domain and the dialog corpus has different linguistic features from general documents, pre-trained dialog models with open domain dialog data such as Reddit is very important. DialoGPT continues to pre-train GPT-2 model directly on Reddit comments data, and the new pre-trained model achieves better performance on downstream tasks including several dialog response generation benchmarks. PLATO (Bao et al., 2020) proposes a new model specifically for dialog generation, which introduces a discrete variable for one-to-many relationship modeling. The pre-trained model helps to achieve state-of-the-art results on several response generation tasks. This is the closest work in literature to ours. However, in our paper, we introduce continuous latent variables during pre-training on dialog corpus instead of a discrete latent variable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_108",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "156-ARR_v2_109",
            "content": "This paper proposes a new pre-training framework for dialogue response generation called Di-alogVED. The latent variable is incorporated into the sequence-to-sequence framework based on Transformer, and obtains a robust and diverse response generation model through 4 training targets. our pre-trained model has achieved new state-ofthe-art in multiple downstream tasks of dialogue response generation. Extensive experiments prove the effectiveness of our model. Additional human evaluation demonstrates the advantages of our proposed model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "156-ARR_v2_110",
            "content": "Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim Marks, Chiori Hori, Peter Anderson, Audio visual scene-aware dialog, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Huda Alamri",
                    "Vincent Cartillier",
                    "Abhishek Das",
                    "Jue Wang",
                    "Anoop Cherian",
                    "Irfan Essa",
                    "Dhruv Batra",
                    "Tim Marks",
                    "Chiori Hori",
                    "Peter Anderson"
                ],
                "title": "Audio visual scene-aware dialog",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_111",
            "content": "Huda Alamri, Chiori Hori, Tim Marks, Dhruv Batra, Devi Parikh, Audio visual scene-aware dialog (AVSD) track for natural language generation in DSTC7, 2019, AAAI workshop on the 7th edition of Dialog System Technology Challenge (DSTC7), .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Huda Alamri",
                    "Chiori Hori",
                    "Tim Marks",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Audio visual scene-aware dialog (AVSD) track for natural language generation in DSTC7",
                "pub_date": "2019",
                "pub_title": "AAAI workshop on the 7th edition of Dialog System Technology Challenge (DSTC7)",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_112",
            "content": "UNKNOWN, None, 2017, Variational attention for sequence-to-sequence models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Variational attention for sequence-to-sequence models",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_113",
            "content": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, PLATO: Pre-trained dialogue generation model with discrete latent variable, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Siqi Bao",
                    "Huang He",
                    "Fan Wang",
                    "Hua Wu",
                    "Haifeng Wang"
                ],
                "title": "PLATO: Pre-trained dialogue generation model with discrete latent variable",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "156-ARR_v2_114",
            "content": "UNKNOWN, None, 2019, Deep retrieval-based dialogue systems: A short review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Deep retrieval-based dialogue systems: A short review",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_115",
            "content": "Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, Samy Bengio, Generating sentences from a continuous space, 2016, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Samuel Bowman",
                    "Luke Vilnis",
                    "Oriol Vinyals",
                    "Andrew Dai",
                    "Rafal Jozefowicz",
                    "Samy Bengio"
                ],
                "title": "Generating sentences from a continuous space",
                "pub_date": "2016",
                "pub_title": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_116",
            "content": "UNKNOWN, None, 2021, Contextual fine-tocoarse distillation for coarse-grained response selection in open-domain conversations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Contextual fine-tocoarse distillation for coarse-grained response selection in open-domain conversations",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_117",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_118",
            "content": "Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, Changyou Chen, Implicit deep latent variable models for text generation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Le Fang",
                    "Chunyuan Li",
                    "Jianfeng Gao",
                    "Wen Dong",
                    "Changyou Chen"
                ],
                "title": "Implicit deep latent variable models for text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_119",
            "content": "Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, Bill Dolan, Grounded response generation task at dstc7, 2019, AAAI Dialog System Technology Challenges Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Michel Galley",
                    "Chris Brockett",
                    "Xiang Gao",
                    "Jianfeng Gao",
                    "Bill Dolan"
                ],
                "title": "Grounded response generation task at dstc7",
                "pub_date": "2019",
                "pub_title": "AAAI Dialog System Technology Challenges Workshop",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_120",
            "content": "Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Shuming Shi, Generating multiple diverse responses for short-text conversation, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jun Gao",
                    "Wei Bi",
                    "Xiaojiang Liu",
                    "Junhui Li",
                    "Shuming Shi"
                ],
                "title": "Generating multiple diverse responses for short-text conversation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_121",
            "content": "Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov, Thomas Wolf, Large-scale transfer learning for natural language generation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Sergey Golovanov",
                    "Rauf Kurbanov",
                    "Sergey Nikolenko",
                    "Kyryl Truskovskyi",
                    "Alexander Tselousov",
                    "Thomas Wolf"
                ],
                "title": "Large-scale transfer learning for natural language generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_122",
            "content": "Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer,  Levy, Spanbert: Improving pre-training by representing and predicting spans, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Mandar Joshi",
                    "Danqi Chen",
                    "Yinhan Liu",
                    "S Daniel",
                    "Luke Weld",
                    "Omer Zettlemoyer",
                    " Levy"
                ],
                "title": "Spanbert: Improving pre-training by representing and predicting spans",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_123",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_124",
            "content": "UNKNOWN, None, 2016, Improving variational inference with inverse autoregressive flow, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Improving variational inference with inverse autoregressive flow",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_125",
            "content": "UNKNOWN, None, 2014, Kappa coefficient. Wiley StatsRef: Statistics Reference Online, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Kappa coefficient. Wiley StatsRef: Statistics Reference Online",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_126",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_127",
            "content": "UNKNOWN, None, 2020, Optimus: Organizing sentences via pre-trained modeling of a latent space, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Optimus: Organizing sentences via pre-trained modeling of a latent space",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_128",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Jiwei Li",
                    "Michel Galley",
                    "Chris Brockett",
                    "Jianfeng Gao",
                    "William B Dolan"
                ],
                "title": "A diversity-promoting objective function for neural conversation models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_129",
            "content": "UNKNOWN, None, 2016, A simple, fast diverse decoding algorithm for neural generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "A simple, fast diverse decoding algorithm for neural generation",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_130",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, Dailydialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Yanran Li",
                    "Hui Su",
                    "Xiaoyu Shen",
                    "Wenjie Li",
                    "Ziqiang Cao",
                    "Shuzi Niu"
                ],
                "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Eighth International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "156-ARR_v2_131",
            "content": "Zhaojiang Lin, Andrea Madotto, Pascale Fung, Exploring versatile generative language model via parameter-efficient transfer learning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Zhaojiang Lin",
                    "Andrea Madotto",
                    "Pascale Fung"
                ],
                "title": "Exploring versatile generative language model via parameter-efficient transfer learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_132",
            "content": "Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng, Qun Liu, Dawei Yin, Knowledge diffusion for neural dialogue generation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Shuman Liu",
                    "Hongshen Chen",
                    "Zhaochun Ren",
                    "Yang Feng",
                    "Qun Liu",
                    "Dawei Yin"
                ],
                "title": "Knowledge diffusion for neural dialogue generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "156-ARR_v2_133",
            "content": "UNKNOWN, None, 2015, Effective approaches to attentionbased neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Effective approaches to attentionbased neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_134",
            "content": "UNKNOWN, None, 2017, Neural discrete representation learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Neural discrete representation learning",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_135",
            "content": "UNKNOWN, None, 2019, fairseq: A fast, extensible toolkit for sequence modeling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_136",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_137",
            "content": "Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Weizhen Qi",
                    "Yu Yan",
                    "Yeyun Gong",
                    "Dayiheng Liu",
                    "Nan Duan",
                    "Jiusheng Chen",
                    "Ruofei Zhang",
                    "Ming Zhou"
                ],
                "title": "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_138",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Language models are unsupervised multitask learners",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_139",
            "content": "UNKNOWN, None, 2019, Exploring the limits of transfer learning with a unified text-to-text transformer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_140",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter J Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_141",
            "content": "UNKNOWN, None, 2019, Cmu sinbad's submission for the dstc7 avsd challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Cmu sinbad's submission for the dstc7 avsd challenge",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_142",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoder-decoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Iulian Serban",
                    "Alessandro Sordoni",
                    "Ryan Lowe",
                    "Laurent Charlin",
                    "Joelle Pineau",
                    "Aaron Courville",
                    "Yoshua Bengio"
                ],
                "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
                "pub_date": "2017",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_143",
            "content": "UNKNOWN, None, 2018, Self-attention with relative position representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Self-attention with relative position representations",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_144",
            "content": "Arash Vahdat, William Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash, Dvae++: Discrete variational autoencoders with overlapping transformations, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Arash Vahdat",
                    "William Macready",
                    "Zhengbing Bian",
                    "Amir Khoshaman",
                    "Evgeny Andriyash"
                ],
                "title": "Dvae++: Discrete variational autoencoders with overlapping transformations",
                "pub_date": "2018",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "156-ARR_v2_145",
            "content": "UNKNOWN, None, 2017, Attention is all you need, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Attention is all you need",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_146",
            "content": "UNKNOWN, None, 2015, A neural conversational model, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "A neural conversational model",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_147",
            "content": "Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, Zhonghai Wu, Diverse and informative dialogue generation with context-specific commonsense knowledge awareness, 2020, Proceedings of the 58th annual meeting of the association for computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Sixing Wu",
                    "Ying Li",
                    "Dawei Zhang",
                    "Yang Zhou",
                    "Zhonghai Wu"
                ],
                "title": "Diverse and informative dialogue generation with context-specific commonsense knowledge awareness",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th annual meeting of the association for computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_148",
            "content": "UNKNOWN, None, 2020, Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_149",
            "content": "Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt Schuerman, Ying Wang, Neural response generation with meta-words, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Can Xu",
                    "Wei Wu",
                    "Chongyang Tao",
                    "Huang Hu",
                    "Matt Schuerman",
                    "Ying Wang"
                ],
                "title": "Neural response generation with meta-words",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "156-ARR_v2_150",
            "content": "UNKNOWN, None, 2018, Personalizing dialogue agents: I have a dog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Personalizing dialogue agents: I have a dog",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_151",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, 2020, ACL, system demonstration, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Yizhe Zhang",
                    "Siqi Sun",
                    "Michel Galley",
                    "Yen-Chun Chen",
                    "Chris Brockett",
                    "Xiang Gao",
                    "Jianfeng Gao",
                    "Jingjing Liu",
                    "Bill Dolan"
                ],
                "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
                "pub_date": "2020",
                "pub_title": "ACL, system demonstration",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_152",
            "content": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi, Unsupervised discrete sentence representation learning for interpretable neural dialog generation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Tiancheng Zhao",
                    "Kyusong Lee",
                    "Maxine Eskenazi"
                ],
                "title": "Unsupervised discrete sentence representation learning for interpretable neural dialog generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "156-ARR_v2_153",
            "content": "Tiancheng Zhao, Ran Zhao, Maxine Eskenazi, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Tiancheng Zhao",
                    "Ran Zhao",
                    "Maxine Eskenazi"
                ],
                "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "156-ARR_v2_154",
            "content": "Tiancheng Zhao, Ran Zhao, Maxine Eskenazi, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Tiancheng Zhao",
                    "Ran Zhao",
                    "Maxine Eskenazi"
                ],
                "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "156-ARR_v2_155",
            "content": "Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu, Commonsense knowledge aware conversation generation with graph attention, 2018, IJCAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Hao Zhou",
                    "Tom Young",
                    "Minlie Huang",
                    "Haizhou Zhao",
                    "Jingfang Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Commonsense knowledge aware conversation generation with graph attention",
                "pub_date": "2018",
                "pub_title": "IJCAI",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "156-ARR_v2_0@0",
            "content": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_0",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@0",
            "content": "Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@1",
            "content": "In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 145,
            "end": 379,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@2",
            "content": "With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 381,
            "end": 677,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@3",
            "content": "We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 679,
            "end": 803,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@4",
            "content": "We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 805,
            "end": 906,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_2@5",
            "content": "Experimental results show that our model achieves the new state-of-the-art results on all these datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_2",
            "start": 908,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_4@0",
            "content": "Pre-trained language models (PLMs) have been widely explored both in natural language understanding (NLU) and generation (NLG) in recent years, this pre-training and fine-tuning paradigm sheds light on various downstream tasks in natural language processing (NLP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_4",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_4@1",
            "content": "Compared with general pre-trained models, task-oriented pre-trained models (such as Summarization, Dialog and etc.), which is designed in line with task characteristics, may achieve better performance and be more robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_4",
            "start": 265,
            "end": 484,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_4@2",
            "content": "In this paper, we proposes a novel pre-trained dialog response generation model based on previous research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_4",
            "start": 486,
            "end": 592,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_5@0",
            "content": "Dialogue Response Generation (DSG) in open domain is a challenging task with a wide range of application scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_5",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_5@1",
            "content": "Recent advances in DSG utilize pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) in two major categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_5",
            "start": 116,
            "end": 272,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_5@2",
            "content": "The first one focuses on how to fine-tune PLMs in downstream tasks and address the various application-specific needs and challenges (Lin et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_5",
            "start": 274,
            "end": 425,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_5@3",
            "content": "The second one augments dialog specific tasks into the PLM training Bao et al., 2020) and then fine-tunes the new pre-trained model in downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_5",
            "start": 427,
            "end": 578,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_5@4",
            "content": "We study the latter in this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_5",
            "start": 580,
            "end": 613,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_6@0",
            "content": "There is a proverbial one-to-many problem in DSG, i.e., a single dialog context could be followed by multiple reasonable responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_6",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_6@1",
            "content": "Existing works introduce latent variables to model this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_6",
            "start": 132,
            "end": 195,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_6@2",
            "content": "For example, VHRED (Serban et al., 2017) incorporates latent continuous variable into the sequenceto-sequence (Seq2Seq) RNN model to improve the diversity of generated responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_6",
            "start": 197,
            "end": 374,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_6@3",
            "content": "VAE-Seq2Seq (Bahuleyan et al., 2017) proposes variational attention to replace the vanilla encoder-decoder attention (Luong et al., 2015), to avoid attention to bypass the latent space and invalidate the latent variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_6",
            "start": 376,
            "end": 595,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_6@4",
            "content": "For controllability and interpretability, some discrete VAEs have also been proposed, such as (Oord et al., 2017;Vahdat et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_6",
            "start": 597,
            "end": 730,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_7@0",
            "content": "Recently, PLATO (Bao et al., 2020) firstly introduces latent variables into their pre-training dialog model, where the authors introduce a K-way (K = 20) categorical latent variable, and the pretrained model shows significant gains in multiple downstream response generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_7",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_7@1",
            "content": "Continuous latent variables besides discrete latent variables is popularly used for modeling one-to-many mapping in dialog system, but the potential of incorporating continuous latent variables with large-scale language pretraining is less explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_7",
            "start": 282,
            "end": 530,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_8@0",
            "content": "In this paper, we propose a pre-trained latent Variable Encoder-Decoder model for Dialog generation, which is called DialogVED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_8",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_8@1",
            "content": "In this model, we introduce a continuous latent variable into the enhanced encoder-decoder pre-training framework and we adopt the optimization techniques based on the VAEs literature to learn the model with continuous latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_8",
            "start": 128,
            "end": 363,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_8@2",
            "content": "More specifically, we conduct the pre-training by optimizing the following 4 pre-training objectives simultaneously: 1) masked language spans loss to enhance the encoder's understanding of context, 2) response generation with n-gram loss to improve the decoder's planning ability, 3) Kullback-Leibler divergence loss to minimize the difference between the posterior and prior distribution of the latent variables, and 4) bag-ofwords loss to reduce posterior distribution collapse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_8",
            "start": 365,
            "end": 844,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_8@3",
            "content": "In addition, we also explore the effect of absolute and relative position embeddings specific for conversational data on the model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_8",
            "start": 846,
            "end": 988,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_9@0",
            "content": "We conduct experiments on three different kinds of conversation tasks: chit-chat, knowledge grounded conversation, and conversational question answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_9",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_9@1",
            "content": "Experimental results verify the effectiveness and superiority of our model compared with the previous state-of-the-art method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_9",
            "start": 154,
            "end": 279,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_9@2",
            "content": "We further carry out ablation study to better understand the impact of different components in the DialogVED on model performance including latent space sizes, different decoding strategies, and position embeddings for turns and roles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_9",
            "start": 281,
            "end": 515,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_10@0",
            "content": "The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; 2) We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; 3) Extensive experiments show that the proposed model achieves the new state-of-theart (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_10",
            "start": 0,
            "end": 587,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_11@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_11",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_12@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_12",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_13@0",
            "content": "In response generation, there are three elements: dialogue context c, response r and latent variable z. The dialogue context c may consist of several history utterances (i.e., multi turns) and the response r is one piece of appropriate reply towards the given context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_13",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_13@1",
            "content": "Additionally, the latent variable z in the latent space represents many unobserved factors associating the context and the response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_13",
            "start": 269,
            "end": 400,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_14@0",
            "content": "We assume the latent variable z is continuous, which is different from PLATO (Bao et al., 2020), and portrays a certain conditional probability distribution related to the response given context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_14",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_14@1",
            "content": "We then define the conditional distribution p(r, z|c) = p(r|c, z)p(z|c) and our goal is to use encoder-decoder models (parameterized by \u03b8) to approximate p(r|c, z) and a multi-layer perceptron (parametrized by \u03d5) to estimate p(z|c), which is called the prior network in VAE literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_14",
            "start": 196,
            "end": 480,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_14@2",
            "content": "We call the final pre-trained model DialogVED, which is a transformer-based encoder-decoder model with an extra prior network for modeling the latent space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_14",
            "start": 482,
            "end": 637,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_14@3",
            "content": "Figure 1 gives a overview of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_14",
            "start": 639,
            "end": 677,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_15@0",
            "content": "Encoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_15",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_16@0",
            "content": "We use multi-layer Transformer-based (Vaswani et al., 2017) encoder to encode the dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_16",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_16@1",
            "content": "First, an input sequence of tokens is mapped to a sequence of embeddings, which are then passed into the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_16",
            "start": 100,
            "end": 212,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_16@2",
            "content": "The encoder consists of a stack of \"blocks\", each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_16",
            "start": 214,
            "end": 365,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_16@3",
            "content": "Compared to the vanilla transformer encoder, our encoder has slight differences in position embeddings and self-attention layer in fine-tuning phase, which contains richer location information and will be introduced in \u00a7 2.7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_16",
            "start": 367,
            "end": 591,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_17@0",
            "content": "Decoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_17",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_18@0",
            "content": "Future predicting strategy has been concerned in recent research (Qi et al., 2020;Xiao et al., 2020), instead of predicting only the next token at each time step, the decoder using future predicting predicts n future tokens simultaneously.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_18",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_19@0",
            "content": "Specifically, the original Seq2Seq model aims to optimize the conditional likelihood P (r t |r <t , c), while future predicting strategy changes the optimization of predicting next single token to P (r t:t+n\u22121 |r <t , c) at each time step t, where r t:t+n\u22121 denotes the next continuous n future tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_19",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_19@1",
            "content": "The future n-gram prediction loss can explicitly encourage the model to plan for future token prediction and prevent over-fitting on strong local correlations (Qi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_19",
            "start": 303,
            "end": 479,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_20@0",
            "content": "We adopt the n-stream self-attention proposed in ProphetNet (Qi et al., 2020) in our decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_20",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_20@1",
            "content": "The n-stream self-attention mechanism incorporates n extra self-attention predicting streams besides main stream to predict next n continuous future tokens respectively at each time step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_20",
            "start": 94,
            "end": 280,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_20@2",
            "content": "Memory Scheme To incorporate the latent variable into decoder, we adopt a memory scheme similar to OPTIMUS , where latent variable z \u2208 R P is mapped to a additional memory vector, denoted as h M em , which is an additional key-value pair for decoder to attend.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_20",
            "start": 282,
            "end": 541,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_20@3",
            "content": "We have memory vector",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_20",
            "start": 543,
            "end": 563,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_21@0",
            "content": "h M em = z key z value = W M z (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_21",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_22@0",
            "content": "where W M \u2208 R H\u00d7P is the weight matrix, and the memory vector is shared and propagated across all layers in decoder as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_22",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_23@0",
            "content": "H (k+1) = MultiHead(H (k) , h (k) M em \u2295 H (k) , h (k) M em \u2295 H (k) )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_23",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_24@0",
            "content": "where H (k) refers to the hidden state of the k-th layer of decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_24",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_24@1",
            "content": "The memory vector is equivalent to adding a virtual token during decoding to participate in the calculation of self-attention main stream, and the predicting streams are implicitly affected by h M em through interaction with the main stream.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_24",
            "start": 69,
            "end": 309,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_24@2",
            "content": "The latent variable guides the generation of each step of the decoder through the memory vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_24",
            "start": 311,
            "end": 406,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_25@0",
            "content": "Latent Variable",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_25",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_26@0",
            "content": "Intuitively, introducing latent variables provides a hierarchical generation procedure: 1) sample a latent variable z from the prior network p(z|c); 2) generate r through the decoder network p(r|c, z).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_26",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_26@1",
            "content": "From previous research (Zhao et al., 2017a), z \u223c p(z|c) may determine the high-level semantics, and the auto-regressive decoding is followed to produce the output sentences with low-level syntactic and lexical details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_26",
            "start": 202,
            "end": 419,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_27@0",
            "content": "Similar to the Variational Autoencoders (VAEs), we learn the parameters \u03b8 by maximizing the marginal log likelihood:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_27",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_28@0",
            "content": "log p \u03b8 (r|c) = log p \u03d5 (z|c)p \u03b8 (r|c, z)dz,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_28",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_29@0",
            "content": "where p \u03d5 involves an intractable marginalization over the latent variable z. (Kingma et al., 2016;, We will optimize its lower bound, which is equivalent to minimize the two terms below: reconstruction loss (or negative loglikelihood)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_29",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_30@0",
            "content": "L rc = \u2212E q(z) [log p \u03b8 (r|c, z)] = \u2212E q(z) [log t p \u03b8 (r t:t+n\u22121 |r <t , c)] (2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_30",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_31@0",
            "content": "and K-L regularization term",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_31",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_32@0",
            "content": "L kl = KL(q(z)||p \u03d5 (z|c)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_32",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_33@0",
            "content": "(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_33",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_34@0",
            "content": "Here q(z) is a multivariable normal distribution with mean \u00b5 \u2208 R P and diagonal variance matrix with diagonal taiking values \u03c3 2 \u2208 R P , denoted as diag(\u03c3 2 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_34",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_35@0",
            "content": "To connect to the hidden space, we add a special classification token ([CLS]) to the beginning of the context, and the first hidden state denoted as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_35",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_36@0",
            "content": "h [CLS] \u2208 R H in last-layer is used to represent the global dialog context. We assume \u00b5 log(\u03c3 2 ) = MLP h h [CLS] (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_36",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_37@0",
            "content": "where MLP h is a multilayer perceptron and this multilayer perceptron is called the prior network in VAEs literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_37",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_37@1",
            "content": "We can then sample P random variables with each variable is from standard normal distribution and via transformation, we obtain samples of z \u2208 R P from N (\u00b5, diag(\u03c3 2 )), and feed them to the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_37",
            "start": 118,
            "end": 317,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_38@0",
            "content": "Mask Language Spans",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_38",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_39@0",
            "content": "To improve the understanding ability of the encoder and the robustness to noise, we randomly mask part of the context before encoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_39",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_39@1",
            "content": "Recent research (Joshi et al., 2020;Lewis et al., 2020) on masked language models show the advantages of masking spans over masking individual words or subword units.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_39",
            "start": 135,
            "end": 300,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_40@0",
            "content": "We adopt a simple method to mask spans: 1) randomly select n tokens in context, denote as S; 2) for each token t \u2208 S, extend it to a text span with a fixed length of m; 3) mask all selected tokens after sorting, deduplication and boundary checking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_40",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_41@0",
            "content": "Following BERT (Devlin et al., 2019), the total number of masked tokens in the context accounts for approximately 15%, and we replace the masked token with: 1) the [MASK] token 80% of the time; 2) a random token 10% of the time; 3) the unchanged masked token 10% of the time. Then, the last-layer hidden states h x \u2208 R H of each masked token x will be used to predict the original token and the encoder is trained to optimize the cross entropy loss:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_41",
            "start": 0,
            "end": 448,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_42@0",
            "content": "L M = \u2212 x LSM(W 2 tanh(W 1 h x +b 1 ))(x) (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_42",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_43@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_43",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_44@0",
            "content": "W 1 \u2208 R H\u00d7H , b 1 \u2208 R H and W 2 \u2208 R H\u00d7|V |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_44",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_45@0",
            "content": "denote the weight matrices of one fully-connected layer, |V | is the vocabulary size, LSM is log softmax function and LSM(. . . )(x) means to take the log probability value corresponding to token x. In this paper, we share the parameters of W 2 with parameters of embedding layers in the encoder and decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_45",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_45@1",
            "content": "Note that we only mask the context only the pre-training stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_45",
            "start": 309,
            "end": 371,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_46@0",
            "content": "Reduce KL-vanishing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_46",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_47@0",
            "content": "DialogVED allows the decoder to attend the hidden states of context (i.e., the output of the encoder), and thus direct training will cause the decoder to ignore the latent variable z, and the KL loss will rapidly decrease to 0 and the latent space loses its expressive power, which is called posterior collapse or KL-vanishing (Bowman et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_47",
            "start": 0,
            "end": 348,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_47@1",
            "content": "This paper adopts two methods developed in VAEs literature to reduce posterior collapse:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_47",
            "start": 350,
            "end": 437,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_48@0",
            "content": "Free Bits (Kingma et al., 2016), which replaces the K-L regularization term in (3) with a hinge loss term that maximize each component of the original K-L term with a constant \u03bb:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_48",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_49@0",
            "content": "L \u2032 kl = \u2212 i max(\u03bb, KL(q(z i )||p \u03d5 (z i |c))) (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_49",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_50@0",
            "content": "Bag-of-words Loss (Zhao et al., 2017b), which is used to encourage the latent variable to predict the words in response r in a non-autoregressive way:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_50",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_51@0",
            "content": "L BOW = \u2212 T t=1 log f rt (7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_51",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_52@0",
            "content": "where T is the number of tokens in response r, and f rt denotes the estimated probability of word r t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_52",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_52@1",
            "content": "More specifically, f is the function outputting the probability of words within the target response:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_52",
            "start": 104,
            "end": 203,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_53@0",
            "content": "f = softmax(MLP z [z \u2295 h [CLS] ]) \u2208 R |V | (8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_53",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_54@0",
            "content": "where MLP z is a multilayer perceptron and V refers to the whole vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_54",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_55@0",
            "content": "Position Embeddings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_55",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_56@0",
            "content": "Besides tokenlevel learned position embeddings used in original Transformer, we also consider turn level and speaker-level position embeddings like PLATO (Bao et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_56",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_56@1",
            "content": "To better model the meaning of a turn in a dialog, We introduce embedding for turn position and role position in one conversation, the final input embedding of each token is the sum of corresponding turn, role and token embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_56",
            "start": 174,
            "end": 404,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_57@0",
            "content": "Relative Position Embeddings It has recently become more common to use relative position embeddings, which produce a different learned embedding according to the offset between the \"key\" and \"query\" being compared in the self-attention mechanism (Shaw et al., 2018;Raffel et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_57",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_57@1",
            "content": "We extend the element of the original relative distance matrix in T5 (Raffel et al., 2019) to two-tuple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_57",
            "start": 287,
            "end": 390,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_58@0",
            "content": "e ij = x i W Q (x j W K + a K ij ) T \u221a d z , a K ij = f (d token , d turn , x i , x j )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_58",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_59@0",
            "content": "In the mapping function f , we consider both token relative distance d token and turn relative distance d turn , where these tuples are mapped through a bucket function, and then a K ij is queried in predefined embedding layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_59",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_60@0",
            "content": "Pre-training Objectives",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_60",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_61@0",
            "content": "Combining the losses detailed in the Equations (2) (5) ( 6) and ( 7), we have pre-training objective, which we use to pre-train the DialogVED on the large-scale conversation corpus:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_61",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_62@0",
            "content": "loss = L M + L rc + L \u2032 kl + L BOW(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_62",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_63@0",
            "content": "To sum up, we mask text spans in the context c, sample a latent variable z from prior network, and then let the encoder and decoder predict the masked spans and response r respectively with the guidance of the latent variable z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_63",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_64@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_64",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_65@0",
            "content": "In this section, we firstly introduce the pre-training datasets and fine-tuning benchmarks in \u00a7 3.1, and implement details in \u00a7 3.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_65",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_65@1",
            "content": "Then we present the main results in \u00a7 3.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_65",
            "start": 133,
            "end": 174,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_65@2",
            "content": "Lastly, we analyze the influence of parameters and position embeddings in \u00a7 3.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_65",
            "start": 176,
            "end": 255,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_66@0",
            "content": "DataSets and Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_66",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_67@0",
            "content": "Pre-training Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_67",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_68@0",
            "content": "Large-scale Reddit comments dataset (Zhou et al., 2018;Galley et al., 2019) is employed for pretraining our dialog language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_68",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_68@1",
            "content": "This dataset has been proved to be helpful in various conversation downstream tasks (Bao et al., 2020;. We use the script provided by Di-aloGPT to obtain the latest Reddit comment data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_68",
            "start": 131,
            "end": 315,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_68@2",
            "content": "We obtain 215 million 1 training samples (42GB in total) for pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_68",
            "start": 317,
            "end": 390,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@0",
            "content": "To accelerate the training process and accommodate GPU memory limitations, we adopt two methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@1",
            "content": "First, we sort the samples according to the length of the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 97,
            "end": 162,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@2",
            "content": "Samples with similar length (i.e. number of tokens in context) are assembled into a batch to minimize the amount of padding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 164,
            "end": 287,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@3",
            "content": "Secondly, due to the uneven distribution of sample lengths, we divide the Reddit corpus into two sub-datasets: Reddit-Short and Reddit-Long according to the length of context and response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 289,
            "end": 476,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@4",
            "content": "with some statistics in Table 1, and optimize the batch size for each sub-dataset to avoid reserving a large amount of memory for a few long response samples during the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 478,
            "end": 663,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@5",
            "content": "Within an epoch, we first pre-train on Reddit-Short with a larger batch size, and then pre-train Reddit-Long with a smaller batch size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 665,
            "end": 799,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_69@6",
            "content": "We split the reddit comment dataset here mainly for efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_69",
            "start": 801,
            "end": 863,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_70@0",
            "content": "Fine-tuning Benchmarks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_70",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_71@0",
            "content": "Following PLATO (Bao et al., 2020), we select three datasets as our benchmarks: DailyDialog (Li et al., 2017), a chit-chat dataset, which contains high-quality human conversations about daily life.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_71",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_72@0",
            "content": "Persona-Chat (Zhang et al., 2018), a knowledge grounded conversation dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_72",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_72@1",
            "content": "It provides both manually annotated conversations and corresponding persona profiles (background knowledge), where two participants chat naturally and try to get to know each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_72",
            "start": 78,
            "end": 258,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_73@0",
            "content": "DSTC7-AVSD (Alamri et al., 2019a), a conversational question answering dataset, shorts for Audio Visual Scene-aware Dialog of the DSTC7 challenge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_73",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_73@1",
            "content": "The system needs to generate an answer given dialogue context and background knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_73",
            "start": 147,
            "end": 233,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_73@2",
            "content": "There are multiple reference responses for each context in DSTC7-AVSD test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_73",
            "start": 235,
            "end": 313,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_74@0",
            "content": "For evaluation, we use the same metrics as used in PLATO, except for knowledge-related metrics, since this paper does not focus on utilizing knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_74",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_74@1",
            "content": "So we will focus the following metrics:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_74",
            "start": 152,
            "end": 190,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_75@0",
            "content": "BLEU-1/2 (Papineni et al., 2002), which measures the relevance of generated text to the reference text by calculating the 1/2-gram overlapping between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_75",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_76@0",
            "content": "Distinct-1/2 (Li et al., 2016a), which measures the diversity of a generated sentence by focusing on the number of distinct 1/2-gram of a sentence and thus penalizing sentences with lots of repeated words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_76",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_77@0",
            "content": "Other word-overlap-based metrics, METEOR, ROUGE-L, and CIDEr, which are also reported for the DSTC7-AVSD dataset, same as DSTC7 reviews (Alamri et al., 2019b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_77",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_78@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_78",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@0",
            "content": "Vanilla sequence to sequence (Seq2Seq) models, dialog pre-training models, and general natural language pre-training models are used as our baselines: Seq2Seq (Vinyals and Le, 2015) is a sequenceto-sequence model with attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@1",
            "content": "iVAE MI (Fang et al., 2019) is an implicit deep latent variable model based on Variational Autoencoder for better latent representations and diverse responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 229,
            "end": 387,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@2",
            "content": "LIC (Golovanov et al., 2019) obtains the best performance during the contest, and is one transformer based generation method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 389,
            "end": 513,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@3",
            "content": "PLATO (Bao et al., 2020) utilizes a discrete latent variable for dialog generation pre-training to address the one-to-many problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 515,
            "end": 645,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@4",
            "content": "ProphetNet (Qi et al., 2020) is a pretrained LM model with predicting more than one future tokens as the pre-training objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 647,
            "end": 774,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_79@5",
            "content": "We finetune ProphetNet-Large model released in (Qi et al., 2020) with downstream training data directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_79",
            "start": 776,
            "end": 879,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_80@0",
            "content": "For benchmark DSTC7-AVSD, we include AVSD Baseline (Alamri et al., 2019a) system provided by the the challenge organizer, as well as the best performing model developed by the team of CMU Sinbad's (Sanabria et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_80",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_81@0",
            "content": "Model Configuration",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_81",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@0",
            "content": "DialogVED is composed of a 12-layer encoder and a 12-layer decoder, with 1024 embedding/hidden size and 4096 feed-forward filter size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@1",
            "content": "The dimension P of hidden states z is set to 64 and we will analyze the effect of P in \u00a7 3.4.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 135,
            "end": 229,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@2",
            "content": "We use Adam optimizer (Kingma and Ba, 2014) with a learning rate of 3 \u00d7 10 \u22124 for pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 231,
            "end": 325,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@3",
            "content": "We set ngram as 2 following ProphetNet (Qi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 327,
            "end": 383,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@4",
            "content": "The pre-training of dialogue generation is carried out on 32 Nvidia Telsa V100 32G GPU (4 nodes) for 6 epochs, taking about 5 days to reach convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 385,
            "end": 536,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@5",
            "content": "Mixed precision training is also adopted for efficiently training and inference, and we use the Fairseq (Ott et al., 2019) framework to conduct all experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 538,
            "end": 697,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@6",
            "content": "We use the BERT-uncased dictionary, and replace some unused tokens to custom special symbols (such as [SOT], denoting the beginning of the conversation, which is suitable for conversation datasets containing knowledge, like PersonaChat and DSTC7-AVSD).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 699,
            "end": 950,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_82@7",
            "content": "We used package WordPiece (Devlin et al., 2019) for tokenization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_82",
            "start": 952,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_83@0",
            "content": "For fine-tuning, we use exactly the same hyperparameter settings in all three datasets, and they are slightly different from the hyperparameter in pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_83",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_83@1",
            "content": "The learning rate is set to 1 \u00d7 10 \u22124 and the batch size is fixed to 512.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_83",
            "start": 161,
            "end": 233,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_83@2",
            "content": "We also adopt an additional warmup strategy where we linearly increase the learning rate from initial learning rate (1 \u00d7 10 \u22127 ), the number of warmup updates is set to 2000.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_83",
            "start": 235,
            "end": 408,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_83@3",
            "content": "For each dataset, we train 10 epochs, and select the checkpoint with the lowest validation loss for inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_83",
            "start": 410,
            "end": 519,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_84@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_84",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@0",
            "content": "In Table 2, we compare several DialogVED variants with baseline models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@1",
            "content": "DialogVED represents inferencing DialogVED with beam search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 72,
            "end": 131,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@2",
            "content": "Compared with DialogVED, DialogVED w/o latent is not equipped with latent variable, thus the loss function does not include bag-of-words loss and K-L loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 133,
            "end": 287,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@3",
            "content": "DialogVED Greedy means DialogVED inference with greedy search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 289,
            "end": 350,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@4",
            "content": "For DialogVED Sampling, we sample from the top K tokens with the highest output probability at each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 352,
            "end": 465,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@5",
            "content": "For the latent space, we always sample each latent variable from the prior distribution standard normal distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 467,
            "end": 583,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_85@6",
            "content": "Here, beam size is set to 5 and K is set to 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_85",
            "start": 585,
            "end": 632,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_86@0",
            "content": "As shown in Table 2 and Table 3, our model Di-alogVED is very competitive compared to PLATO and other models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_86",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_86@1",
            "content": "In particular, decoding using Top-K (K = 100) sampling with DialogVED beats the PLATO in BLEU-1/2 and Distinct-1/2 on Dai-lyDialog and PersonaChat (see in Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_86",
            "start": 110,
            "end": 273,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_86@2",
            "content": "In fact, as K increases, the overlap of n-grams decreases and the diversity increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_86",
            "start": 275,
            "end": 360,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_86@3",
            "content": "Based on our observations, K taking 100 is a good balance, Table 4 shows more detailed results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_86",
            "start": 362,
            "end": 456,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_87@0",
            "content": "On the DSTC7-AVSD, the diversity of the responses is not as important as the accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_87",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_87@1",
            "content": "From Table 3, We observe that DialogVED w/o latent variable perform the best in overall metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_87",
            "start": 87,
            "end": 182,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_87@2",
            "content": "However, DialogVED equipped with beam search or greedy search, can still easily beat PLATO even though it has a post-generation ranking component.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_87",
            "start": 184,
            "end": 329,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_88@0",
            "content": "There are 2 essential components that contribute greatly the success of our model: Firstly, We adopt a newly developed pretrained LM as the initializer and further continue its pretraining pipeline on our dialog dataset (Reddit) and thus we have a really powerful encoder-decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_88",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_88@1",
            "content": "This is demonstrated in the fact that our model (DialogVED w/o latent variable) beat PLATO (w/o latent variable) in all metrics on all the three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_88",
            "start": 281,
            "end": 434,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_89@0",
            "content": "Secondly, the special structure of our model combines the benefits of both seq2seq models and VAE models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_89",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_89@1",
            "content": "Compared to general VAEs, DialogVED allows encoder-decoder interaction in the decoding, which avoids insufficient representation of lowdimensional latent variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_89",
            "start": 106,
            "end": 268,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_89@2",
            "content": "At the same time, compared with seq2seq model, predicting the bag of words pushes the latent variable to give extra guidance to decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_89",
            "start": 270,
            "end": 405,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_89@3",
            "content": "This is demonstrated by the fact that when compared with DialogVED w/o latent variable, we observe the additional gains in terms of both accuracy and diversity (see Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_89",
            "start": 407,
            "end": 580,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_90@0",
            "content": "Overall, our DialogVED achieves new state-ofthe-art results in all three downstream tasks of dialogue response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_90",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_91@0",
            "content": "Parameters and Position Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_91",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_92@0",
            "content": "Balancing Accuracy and Diversity with Sampling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_92",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_93@0",
            "content": "We investigate the effect of latent space sizes, P , defined as the dimension of the latent variable z and the different K in sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_93",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_94@0",
            "content": "The results in Table 4 show that smaller latent size (P = 32) is more dominant in n-gram based metrics (BLEU-1/2), while larger latent size generates more diverse texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_94",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_94@1",
            "content": "From the results of top-K sampling, we see that the two metric (BLEU-1/2 and Distinct-1/2) have a negative correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_94",
            "start": 170,
            "end": 288,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_95@0",
            "content": "We can flexibly choose the decoding strategy depends on specific scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_95",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_96@0",
            "content": "Position Embeddings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_96",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_97@0",
            "content": "We study the impact of position embeddings as described in section 2.7, we define two types of position embeddings: absolute position embeddings (APE) and relative position embeddings (RPE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_97",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_98@0",
            "content": "We report the metrics of their different combinations, these independent components are TurnAPE (turn absolute embedding), RoleAPE (role absolute embedding), TokenRPE (token relative embedding) and TurnRPE(turn relative embedding) respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_98",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_99@0",
            "content": "As the results shown in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_99",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_100@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_100",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@0",
            "content": "Automated metrics (BLEU 1/2, Distinct-1/2, etc.) have limitations for evaluating open-domain dialog tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@1",
            "content": "To make it more convincing, we conduct a human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 107,
            "end": 164,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@2",
            "content": "Specifically, we randomly select 100 dialogue contexts and generate responses with the following methods: PLATO, DialogVED and DialogVED-Sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 166,
            "end": 311,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@3",
            "content": "Following PLATO, annotators are asked to compare the response (win, tie or lose) quality from four aspects: fluency, coherence, informativeness and overall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 313,
            "end": 468,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@4",
            "content": "The results of human comparison are shown in Table 6, where the average Cohen's kappa (Kraemer, 2014) of group 1 and 2 is 0.729 and 0.743 respectively, indicating annotators have reached moderate agreement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 470,
            "end": 675,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@5",
            "content": "It can be seen that most of the time they are tied, and the three models sometimes generate exactly the same response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 677,
            "end": 794,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_101@6",
            "content": "For Di-alogVED, it beats Plato more in coherence but with close informativeness; while DialogVED-sampling beats Plato significantly in informativeness but with a slightly weaker coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_101",
            "start": 796,
            "end": 983,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_102@0",
            "content": "In general, DialogVED can generate both relevant and diverse response, we show some case study to help illustrate the effectiveness of our model in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_102",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_103@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_103",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_104@0",
            "content": "Encoder-Decoder dialog models Unlike retrieval based dialogue systems (Boussaha et al., 2019;Chen et al., 2021), encoder-decoder models are widely used in dialog response generation, but it tends to generate generic responses and dull responses (e.g., I don't know).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_104",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_104@1",
            "content": "To enhance encoderdecode models and generate diverse responses, researchers have tried different approaches: using diversity promotion objectives (Li et al., 2016a), using different decoding algorithms (Li et al., 2016b), adding additional contents (Xu et al., 2019), or introducing large-scale knowledge graphs into dialog generation (Liu et al., 2018;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_104",
            "start": 267,
            "end": 620,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_105@0",
            "content": "Another class of methods is using the latent variable to address the one-to-many problem in response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_105",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_105@1",
            "content": "These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017a.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_105",
            "start": 113,
            "end": 251,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_105@2",
            "content": "In this paper, we also adopt this approach and further we incorporate the latent variables both in the pre-training and fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_105",
            "start": 253,
            "end": 384,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_106@0",
            "content": "Pre-trained language models have been successfully used in NLG and NLU tasks (Devlin et al., 2019;Radford et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_106",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_106@1",
            "content": "Recently, various new pre-trained language models have been pre-trained including BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), T5 (Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_106",
            "start": 121,
            "end": 284,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_106@2",
            "content": "In these papers, they demonstrate that better performance can be obtained with fine-tuning PLMs than training from scratch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_106",
            "start": 286,
            "end": 408,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@0",
            "content": "Due to the fact that there are many important applications in the dialog domain and the dialog corpus has different linguistic features from general documents, pre-trained dialog models with open domain dialog data such as Reddit is very important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@1",
            "content": "DialoGPT continues to pre-train GPT-2 model directly on Reddit comments data, and the new pre-trained model achieves better performance on downstream tasks including several dialog response generation benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 249,
            "end": 460,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@2",
            "content": "PLATO (Bao et al., 2020) proposes a new model specifically for dialog generation, which introduces a discrete variable for one-to-many relationship modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 462,
            "end": 618,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@3",
            "content": "The pre-trained model helps to achieve state-of-the-art results on several response generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 620,
            "end": 720,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@4",
            "content": "This is the closest work in literature to ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 722,
            "end": 768,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_107@5",
            "content": "However, in our paper, we introduce continuous latent variables during pre-training on dialog corpus instead of a discrete latent variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_107",
            "start": 770,
            "end": 908,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_108@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_108",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_109@0",
            "content": "This paper proposes a new pre-training framework for dialogue response generation called Di-alogVED.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_109",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_109@1",
            "content": "The latent variable is incorporated into the sequence-to-sequence framework based on Transformer, and obtains a robust and diverse response generation model through 4 training targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_109",
            "start": 101,
            "end": 284,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_109@2",
            "content": "our pre-trained model has achieved new state-ofthe-art in multiple downstream tasks of dialogue response generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_109",
            "start": 286,
            "end": 401,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_109@3",
            "content": "Extensive experiments prove the effectiveness of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_109",
            "start": 403,
            "end": 461,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_109@4",
            "content": "Additional human evaluation demonstrates the advantages of our proposed model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_109",
            "start": 463,
            "end": 540,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_110@0",
            "content": "Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim Marks, Chiori Hori, Peter Anderson, Audio visual scene-aware dialog, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_110",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_111@0",
            "content": "Huda Alamri, Chiori Hori, Tim Marks, Dhruv Batra, Devi Parikh, Audio visual scene-aware dialog (AVSD) track for natural language generation in DSTC7, 2019, AAAI workshop on the 7th edition of Dialog System Technology Challenge (DSTC7), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_111",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2017, Variational attention for sequence-to-sequence models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_112",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_113@0",
            "content": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, PLATO: Pre-trained dialogue generation model with discrete latent variable, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_113",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2019, Deep retrieval-based dialogue systems: A short review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_114",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_115@0",
            "content": "Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, Samy Bengio, Generating sentences from a continuous space, 2016, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_115",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_116@0",
            "content": "UNKNOWN, None, 2021, Contextual fine-tocoarse distillation for coarse-grained response selection in open-domain conversations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_116",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_117@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_117",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_118@0",
            "content": "Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, Changyou Chen, Implicit deep latent variable models for text generation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_118",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_119@0",
            "content": "Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, Bill Dolan, Grounded response generation task at dstc7, 2019, AAAI Dialog System Technology Challenges Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_119",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_120@0",
            "content": "Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Shuming Shi, Generating multiple diverse responses for short-text conversation, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_120",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_121@0",
            "content": "Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov, Thomas Wolf, Large-scale transfer learning for natural language generation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_121",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_122@0",
            "content": "Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer,  Levy, Spanbert: Improving pre-training by representing and predicting spans, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_122",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_123@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_123",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2016, Improving variational inference with inverse autoregressive flow, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_124",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_125@0",
            "content": "UNKNOWN, None, 2014, Kappa coefficient. Wiley StatsRef: Statistics Reference Online, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_125",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_126@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_126",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2020, Optimus: Organizing sentences via pre-trained modeling of a latent space, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_127",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_128@0",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_128",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_129@0",
            "content": "UNKNOWN, None, 2016, A simple, fast diverse decoding algorithm for neural generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_129",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_130@0",
            "content": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, Dailydialog: A manually labelled multi-turn dialogue dataset, 2017, Proceedings of the Eighth International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_130",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_131@0",
            "content": "Zhaojiang Lin, Andrea Madotto, Pascale Fung, Exploring versatile generative language model via parameter-efficient transfer learning, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_131",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_132@0",
            "content": "Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng, Qun Liu, Dawei Yin, Knowledge diffusion for neural dialogue generation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_132",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_133@0",
            "content": "UNKNOWN, None, 2015, Effective approaches to attentionbased neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_133",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_134@0",
            "content": "UNKNOWN, None, 2017, Neural discrete representation learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_134",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_135@0",
            "content": "UNKNOWN, None, 2019, fairseq: A fast, extensible toolkit for sequence modeling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_135",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_136@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_136",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_137@0",
            "content": "Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_137",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_138@0",
            "content": "UNKNOWN, None, 2019, Language models are unsupervised multitask learners, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_138",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_139@0",
            "content": "UNKNOWN, None, 2019, Exploring the limits of transfer learning with a unified text-to-text transformer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_139",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_140@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_140",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_141@0",
            "content": "UNKNOWN, None, 2019, Cmu sinbad's submission for the dstc7 avsd challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_141",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_142@0",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoder-decoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_142",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_143@0",
            "content": "UNKNOWN, None, 2018, Self-attention with relative position representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_143",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_144@0",
            "content": "Arash Vahdat, William Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash, Dvae++: Discrete variational autoencoders with overlapping transformations, 2018, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_144",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_145@0",
            "content": "UNKNOWN, None, 2017, Attention is all you need, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_145",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_146@0",
            "content": "UNKNOWN, None, 2015, A neural conversational model, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_146",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_147@0",
            "content": "Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, Zhonghai Wu, Diverse and informative dialogue generation with context-specific commonsense knowledge awareness, 2020, Proceedings of the 58th annual meeting of the association for computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_147",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_148@0",
            "content": "UNKNOWN, None, 2020, Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_148",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_149@0",
            "content": "Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt Schuerman, Ying Wang, Neural response generation with meta-words, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_149",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_150@0",
            "content": "UNKNOWN, None, 2018, Personalizing dialogue agents: I have a dog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_150",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_151@0",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, 2020, ACL, system demonstration, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_151",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_152@0",
            "content": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi, Unsupervised discrete sentence representation learning for interpretable neural dialog generation, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_152",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_153@0",
            "content": "Tiancheng Zhao, Ran Zhao, Maxine Eskenazi, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_153",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_154@0",
            "content": "Tiancheng Zhao, Ran Zhao, Maxine Eskenazi, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_154",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "156-ARR_v2_155@0",
            "content": "Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu, Commonsense knowledge aware conversation generation with graph attention, 2018, IJCAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "156-ARR_v2_155",
            "start": 0,
            "end": 162,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_1",
            "tgt_ix": "156-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_1",
            "tgt_ix": "156-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_4",
            "tgt_ix": "156-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_7",
            "tgt_ix": "156-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_8",
            "tgt_ix": "156-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_9",
            "tgt_ix": "156-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_10",
            "tgt_ix": "156-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_13",
            "tgt_ix": "156-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_12",
            "tgt_ix": "156-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_12",
            "tgt_ix": "156-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_12",
            "tgt_ix": "156-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_14",
            "tgt_ix": "156-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_15",
            "tgt_ix": "156-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_15",
            "tgt_ix": "156-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_16",
            "tgt_ix": "156-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_18",
            "tgt_ix": "156-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_19",
            "tgt_ix": "156-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_20",
            "tgt_ix": "156-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_21",
            "tgt_ix": "156-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_22",
            "tgt_ix": "156-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_23",
            "tgt_ix": "156-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_24",
            "tgt_ix": "156-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_26",
            "tgt_ix": "156-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_27",
            "tgt_ix": "156-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_28",
            "tgt_ix": "156-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_29",
            "tgt_ix": "156-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_30",
            "tgt_ix": "156-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_31",
            "tgt_ix": "156-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_32",
            "tgt_ix": "156-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_33",
            "tgt_ix": "156-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_34",
            "tgt_ix": "156-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_35",
            "tgt_ix": "156-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_36",
            "tgt_ix": "156-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_37",
            "tgt_ix": "156-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_39",
            "tgt_ix": "156-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_40",
            "tgt_ix": "156-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_41",
            "tgt_ix": "156-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_42",
            "tgt_ix": "156-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_43",
            "tgt_ix": "156-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_44",
            "tgt_ix": "156-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_45",
            "tgt_ix": "156-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_47",
            "tgt_ix": "156-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_48",
            "tgt_ix": "156-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_49",
            "tgt_ix": "156-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_50",
            "tgt_ix": "156-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_51",
            "tgt_ix": "156-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_52",
            "tgt_ix": "156-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_53",
            "tgt_ix": "156-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_54",
            "tgt_ix": "156-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_56",
            "tgt_ix": "156-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_57",
            "tgt_ix": "156-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_58",
            "tgt_ix": "156-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_59",
            "tgt_ix": "156-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_61",
            "tgt_ix": "156-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_62",
            "tgt_ix": "156-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_60",
            "tgt_ix": "156-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_60",
            "tgt_ix": "156-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_60",
            "tgt_ix": "156-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_60",
            "tgt_ix": "156-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_63",
            "tgt_ix": "156-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_65",
            "tgt_ix": "156-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_66",
            "tgt_ix": "156-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_68",
            "tgt_ix": "156-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_67",
            "tgt_ix": "156-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_67",
            "tgt_ix": "156-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_67",
            "tgt_ix": "156-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_71",
            "tgt_ix": "156-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_72",
            "tgt_ix": "156-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_73",
            "tgt_ix": "156-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_74",
            "tgt_ix": "156-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_75",
            "tgt_ix": "156-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_76",
            "tgt_ix": "156-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_77",
            "tgt_ix": "156-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_78",
            "tgt_ix": "156-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_78",
            "tgt_ix": "156-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_78",
            "tgt_ix": "156-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_80",
            "tgt_ix": "156-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_81",
            "tgt_ix": "156-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_81",
            "tgt_ix": "156-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_81",
            "tgt_ix": "156-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_83",
            "tgt_ix": "156-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_86",
            "tgt_ix": "156-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_87",
            "tgt_ix": "156-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_88",
            "tgt_ix": "156-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_89",
            "tgt_ix": "156-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_90",
            "tgt_ix": "156-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_91",
            "tgt_ix": "156-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_93",
            "tgt_ix": "156-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_94",
            "tgt_ix": "156-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_92",
            "tgt_ix": "156-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_92",
            "tgt_ix": "156-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_92",
            "tgt_ix": "156-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_92",
            "tgt_ix": "156-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_95",
            "tgt_ix": "156-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_97",
            "tgt_ix": "156-ARR_v2_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_98",
            "tgt_ix": "156-ARR_v2_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_96",
            "tgt_ix": "156-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_96",
            "tgt_ix": "156-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_96",
            "tgt_ix": "156-ARR_v2_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_96",
            "tgt_ix": "156-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_99",
            "tgt_ix": "156-ARR_v2_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_102",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_100",
            "tgt_ix": "156-ARR_v2_101",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_100",
            "tgt_ix": "156-ARR_v2_102",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_100",
            "tgt_ix": "156-ARR_v2_101",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_103",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_102",
            "tgt_ix": "156-ARR_v2_103",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_104",
            "tgt_ix": "156-ARR_v2_105",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_104",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_105",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_104",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_106",
            "tgt_ix": "156-ARR_v2_107",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_106",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_107",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_105",
            "tgt_ix": "156-ARR_v2_106",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_108",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_108",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_108",
            "tgt_ix": "156-ARR_v2_109",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_108",
            "tgt_ix": "156-ARR_v2_109",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "156-ARR_v2_0",
            "tgt_ix": "156-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_1",
            "tgt_ix": "156-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_2",
            "tgt_ix": "156-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_3",
            "tgt_ix": "156-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_4",
            "tgt_ix": "156-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_4",
            "tgt_ix": "156-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_4",
            "tgt_ix": "156-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_5",
            "tgt_ix": "156-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_6",
            "tgt_ix": "156-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_7",
            "tgt_ix": "156-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_7",
            "tgt_ix": "156-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_8",
            "tgt_ix": "156-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_8",
            "tgt_ix": "156-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_8",
            "tgt_ix": "156-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_8",
            "tgt_ix": "156-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_9",
            "tgt_ix": "156-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_9",
            "tgt_ix": "156-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_9",
            "tgt_ix": "156-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_10",
            "tgt_ix": "156-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_11",
            "tgt_ix": "156-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_12",
            "tgt_ix": "156-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_13",
            "tgt_ix": "156-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_13",
            "tgt_ix": "156-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_14",
            "tgt_ix": "156-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_14",
            "tgt_ix": "156-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_14",
            "tgt_ix": "156-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_14",
            "tgt_ix": "156-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_15",
            "tgt_ix": "156-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_16",
            "tgt_ix": "156-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_16",
            "tgt_ix": "156-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_16",
            "tgt_ix": "156-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_16",
            "tgt_ix": "156-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_17",
            "tgt_ix": "156-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_18",
            "tgt_ix": "156-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_19",
            "tgt_ix": "156-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_19",
            "tgt_ix": "156-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_20",
            "tgt_ix": "156-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_20",
            "tgt_ix": "156-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_20",
            "tgt_ix": "156-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_20",
            "tgt_ix": "156-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_21",
            "tgt_ix": "156-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_22",
            "tgt_ix": "156-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_23",
            "tgt_ix": "156-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_24",
            "tgt_ix": "156-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_24",
            "tgt_ix": "156-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_24",
            "tgt_ix": "156-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_25",
            "tgt_ix": "156-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_26",
            "tgt_ix": "156-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_26",
            "tgt_ix": "156-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_27",
            "tgt_ix": "156-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_28",
            "tgt_ix": "156-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_29",
            "tgt_ix": "156-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_30",
            "tgt_ix": "156-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_31",
            "tgt_ix": "156-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_32",
            "tgt_ix": "156-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_33",
            "tgt_ix": "156-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_34",
            "tgt_ix": "156-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_35",
            "tgt_ix": "156-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_36",
            "tgt_ix": "156-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_37",
            "tgt_ix": "156-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_37",
            "tgt_ix": "156-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_38",
            "tgt_ix": "156-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_39",
            "tgt_ix": "156-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_39",
            "tgt_ix": "156-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_40",
            "tgt_ix": "156-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_41",
            "tgt_ix": "156-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_42",
            "tgt_ix": "156-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_43",
            "tgt_ix": "156-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_44",
            "tgt_ix": "156-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_45",
            "tgt_ix": "156-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_45",
            "tgt_ix": "156-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_46",
            "tgt_ix": "156-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_47",
            "tgt_ix": "156-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_47",
            "tgt_ix": "156-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_48",
            "tgt_ix": "156-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_49",
            "tgt_ix": "156-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_50",
            "tgt_ix": "156-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_51",
            "tgt_ix": "156-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_52",
            "tgt_ix": "156-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_52",
            "tgt_ix": "156-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_53",
            "tgt_ix": "156-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_54",
            "tgt_ix": "156-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_55",
            "tgt_ix": "156-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_56",
            "tgt_ix": "156-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_56",
            "tgt_ix": "156-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_57",
            "tgt_ix": "156-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_57",
            "tgt_ix": "156-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_58",
            "tgt_ix": "156-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_59",
            "tgt_ix": "156-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_60",
            "tgt_ix": "156-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_61",
            "tgt_ix": "156-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_62",
            "tgt_ix": "156-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_63",
            "tgt_ix": "156-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_64",
            "tgt_ix": "156-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_65",
            "tgt_ix": "156-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_65",
            "tgt_ix": "156-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_65",
            "tgt_ix": "156-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_66",
            "tgt_ix": "156-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_67",
            "tgt_ix": "156-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_68",
            "tgt_ix": "156-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_68",
            "tgt_ix": "156-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_68",
            "tgt_ix": "156-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_69",
            "tgt_ix": "156-ARR_v2_69@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_70",
            "tgt_ix": "156-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_71",
            "tgt_ix": "156-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_72",
            "tgt_ix": "156-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_72",
            "tgt_ix": "156-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_73",
            "tgt_ix": "156-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_73",
            "tgt_ix": "156-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_73",
            "tgt_ix": "156-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_74",
            "tgt_ix": "156-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_74",
            "tgt_ix": "156-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_75",
            "tgt_ix": "156-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_76",
            "tgt_ix": "156-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_77",
            "tgt_ix": "156-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_78",
            "tgt_ix": "156-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_79",
            "tgt_ix": "156-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_80",
            "tgt_ix": "156-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_81",
            "tgt_ix": "156-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_82",
            "tgt_ix": "156-ARR_v2_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_83",
            "tgt_ix": "156-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_83",
            "tgt_ix": "156-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_83",
            "tgt_ix": "156-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_83",
            "tgt_ix": "156-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_84",
            "tgt_ix": "156-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_85",
            "tgt_ix": "156-ARR_v2_85@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_86",
            "tgt_ix": "156-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_86",
            "tgt_ix": "156-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_86",
            "tgt_ix": "156-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_86",
            "tgt_ix": "156-ARR_v2_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_87",
            "tgt_ix": "156-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_87",
            "tgt_ix": "156-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_87",
            "tgt_ix": "156-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_88",
            "tgt_ix": "156-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_88",
            "tgt_ix": "156-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_89",
            "tgt_ix": "156-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_89",
            "tgt_ix": "156-ARR_v2_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_89",
            "tgt_ix": "156-ARR_v2_89@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_89",
            "tgt_ix": "156-ARR_v2_89@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_90",
            "tgt_ix": "156-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_91",
            "tgt_ix": "156-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_92",
            "tgt_ix": "156-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_93",
            "tgt_ix": "156-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_94",
            "tgt_ix": "156-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_94",
            "tgt_ix": "156-ARR_v2_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_95",
            "tgt_ix": "156-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_96",
            "tgt_ix": "156-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_97",
            "tgt_ix": "156-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_98",
            "tgt_ix": "156-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_99",
            "tgt_ix": "156-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_100",
            "tgt_ix": "156-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_101",
            "tgt_ix": "156-ARR_v2_101@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_102",
            "tgt_ix": "156-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_103",
            "tgt_ix": "156-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_104",
            "tgt_ix": "156-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_104",
            "tgt_ix": "156-ARR_v2_104@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_105",
            "tgt_ix": "156-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_105",
            "tgt_ix": "156-ARR_v2_105@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_105",
            "tgt_ix": "156-ARR_v2_105@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_106",
            "tgt_ix": "156-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_106",
            "tgt_ix": "156-ARR_v2_106@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_106",
            "tgt_ix": "156-ARR_v2_106@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_107",
            "tgt_ix": "156-ARR_v2_107@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_108",
            "tgt_ix": "156-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_109",
            "tgt_ix": "156-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_109",
            "tgt_ix": "156-ARR_v2_109@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_109",
            "tgt_ix": "156-ARR_v2_109@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_109",
            "tgt_ix": "156-ARR_v2_109@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_109",
            "tgt_ix": "156-ARR_v2_109@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_110",
            "tgt_ix": "156-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_111",
            "tgt_ix": "156-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_112",
            "tgt_ix": "156-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_113",
            "tgt_ix": "156-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_114",
            "tgt_ix": "156-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_115",
            "tgt_ix": "156-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_116",
            "tgt_ix": "156-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_117",
            "tgt_ix": "156-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_118",
            "tgt_ix": "156-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_119",
            "tgt_ix": "156-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_120",
            "tgt_ix": "156-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_121",
            "tgt_ix": "156-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_122",
            "tgt_ix": "156-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_123",
            "tgt_ix": "156-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_124",
            "tgt_ix": "156-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_125",
            "tgt_ix": "156-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_126",
            "tgt_ix": "156-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_127",
            "tgt_ix": "156-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_128",
            "tgt_ix": "156-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_129",
            "tgt_ix": "156-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_130",
            "tgt_ix": "156-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_131",
            "tgt_ix": "156-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_132",
            "tgt_ix": "156-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_133",
            "tgt_ix": "156-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_134",
            "tgt_ix": "156-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_135",
            "tgt_ix": "156-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_136",
            "tgt_ix": "156-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_137",
            "tgt_ix": "156-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_138",
            "tgt_ix": "156-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_139",
            "tgt_ix": "156-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_140",
            "tgt_ix": "156-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_141",
            "tgt_ix": "156-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_142",
            "tgt_ix": "156-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_143",
            "tgt_ix": "156-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_144",
            "tgt_ix": "156-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_145",
            "tgt_ix": "156-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_146",
            "tgt_ix": "156-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_147",
            "tgt_ix": "156-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_148",
            "tgt_ix": "156-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_149",
            "tgt_ix": "156-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_150",
            "tgt_ix": "156-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_151",
            "tgt_ix": "156-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_152",
            "tgt_ix": "156-ARR_v2_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_153",
            "tgt_ix": "156-ARR_v2_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_154",
            "tgt_ix": "156-ARR_v2_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "156-ARR_v2_155",
            "tgt_ix": "156-ARR_v2_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 886,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "156-ARR",
        "version": 2
    }
}