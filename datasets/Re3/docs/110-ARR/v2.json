{
    "nodes": [
        {
            "ix": "110-ARR_v2_0",
            "content": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_2",
            "content": "The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff \u00acp is false), is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs' LNP understanding. Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics. Through experiments, we observe that PLMs violate the LNP frequently. To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis. Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks. Finally, we observe that our proposed approach 1 outperforms our previous counterparts despite its time and resource efficiency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "110-ARR_v2_4",
            "content": "Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019b) benchmark datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_5",
            "content": "However, their reliability is recently being challenged. Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2021;Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021). These issues raise concerns about PLMs' stability and reliability, precluding them from applications in practice, especially in risk-sensitive areas.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_6",
            "content": "Another critical problem of PLMs is their inaccurate behaviour on negation, which is a principal property in many language understanding tasks. For tasks where the LNP holds (p is true iff \u00acp is false; see Aina et al. 2018), PLMs should make different answers for the original and negated inputs. However, several studies observed that PLMs violate this property. In masked knowledge retrieval tasks, PLMs frequently generate incorrect answers for negated input queries (Ettinger, 2020;Kassner and Sch\u00fctze, 2020). In other studies, PLMs show a poor generalisation ability on negated natural language inference (NLI) datasets (Naik et al., 2018;Hossain et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_7",
            "content": "Although the aforementioned studies produced promising analysis results, they limited the scope of the LNP only to adding negation expressions (e.g., \"no\" and \"not\"). However, other perturbations that generate the opposite meaning also can be applied to the property. Therefore, a consideration of such perturbation methods is necessary to fully assess whether PLMs satisfy the LNP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_8",
            "content": "Also, remedies to alleviate the problem have not been studied much yet. Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2020) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM). However, this approach has several downsides. First, like previous works, Hosseini et al. (2021) only considered negation expressions. Second, the data augmentation method is contingent on many additional linguistic compo-nents, which causes the dependency of a model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable. Finally, the model should be pre-trained from scratch with the unlikelihood objective, which consumes considerable time and resources.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_9",
            "content": "In this paper, we expand the boundary of the LNP to lexical semantics, i.e., synonyms and antonyms, and ascertain that PLMs are prone to violate the LNP. Next, we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components. We hypothesise that a leading cause lies in the MLM training objective, which assumes the distributional hypothesis for learning the meaning of the text (Sinha et al., 2021a). Instead, we design a model that directly learns the correspondence between words and their semantic contents. Through experiments, we verify that our approach improves the model's comprehension of the LNP, while showing a stable performance on multiple downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_10",
            "content": "Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy, named IM 2 , which is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_11",
            "content": "Probing Tasks for Investigating the Logical Negation Property",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "110-ARR_v2_12",
            "content": "We design three probing tasks to evaluate whether PLMs satisfy the LNP: masked knowledge retrieval on negated queries (MKR-NQ), masked word retrieval (MWR), and synonym/antonym recognition (SAR). Brief illustrations of each task are in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_13",
            "content": "Masked Knowledge Retrieval on Negated Queries",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "110-ARR_v2_14",
            "content": "The MKR-NQ task examines whether PLMs generate incorrect answers for negated queries. Following the work of Kassner and Sch\u00fctze (2020), we constructed the evaluation dataset by negating the LAMA dataset (Petroni et al., 2019), which contains masked free-text forms of ConceptNet (Speer et al., 2017) triplets and their corresponding answers (e.g., (bird, CapableOf, fly) \u2192 (\"A bird can [MASK]\", fly)). The task aims to generate a correct word through MLM.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_15",
            "content": "According to the LNP, a model must not generate the original answer if the query is negated. To measure how likely PLMs generate wrong predictions for negated queries, we collected pairs of (negated_query, wrong_predictions). We selected several relations in the LAMA dataset that ensure mutual exclusiveness between the original and negated queries. 2 For negating sentences, we selected LAMA data points that contain a single verb using the Spacy parts of speech (POS) tagger (Honnibal and Johnson, 2015). Next, we added negation expressions, such as \"not\" and \"don't\", or removed such expressions if they existed. Finally, we collected the wrong predictions from Concept-Net by using the head entity and relation. As a result, we collected 3,360 data points for this task. The list of the relations that we used and examples of the data are in Table 10 in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_16",
            "content": "Masked Word Retrieval",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "110-ARR_v2_17",
            "content": "To expand the boundary of the LNP to lexical semantics, we design the MWR task, which generates an answer of a masked query, asking for the synonym/antonym of a target word through MLM (e.g., \"happy is the synonym of [MASK]\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_18",
            "content": "Let s w and a w denote masked queries that ask the synonym and antonym of the word w, respectively. Also, let A s and A a refer to the list of correct answers for s w and a w , respectively. Intuitively, A a becomes the wrong predictions of s w , because s w and a w have the opposite meaning. Therefore, we can evaluate the violation of the LNP by investigating whether a PLM generates wrong predictions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_19",
            "content": "To extract commonly-used words for our experiment, we first extracted nouns, adjectives, and adverbs that appear more than five times in the SNLI dataset (Bowman et al., 2015). Among the extracted candidates, we filtered words that have synonyms or antonyms in ConceptNet. Finally, we generated masked queries by employing templates used by Camburu et al. (2020). As a result, we collected about 27K data points for MWR. The templates and examples of the data are in Table 11 in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_20",
            "content": "Synonym/Antonym Recognition",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "110-ARR_v2_21",
            "content": "SAR is a classification that distinguishes whether two given words are synonyms or antonyms. It aims to evaluate whether the contextualised representations of PLMs reflect the lexical meaning of words. Therefore, we use a parametric probing model (Adi et al., 2017;Liu et al., 2019a;Belinkov and Glass, 2019;Sinha et al., 2021a) for the experiment. Specifically, the experiment is performed on the final layer of each PLMs, i.e., we only train the classifier while keeping the encoder frozen. We use ConceptNet to build the dataset. ConceptNet has much more synonym triplets compared to antonyms. As a result, we randomly sample the synonym triplets to maintain a balance. To that end, we collect 33K, 1K, and 2K data points for the train, dev, and test datasets, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_22",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "2.4"
            }
        },
        {
            "ix": "110-ARR_v2_23",
            "content": "We use the top-k hit rate (HR@k) to evaluate the performance on the MKR-NQ and MWR tasks. Assume that P = {(p 1 , c 1 ), (p 2 , c 2 ), . . . , (p n , c n )} denotes the set of predictions for a data point x, where p t and c t refer to the predicted word and confidence score of the t-th prediction, respectively. Then, the top-k hit rate for a data point x is defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_24",
            "content": "HR@k(x) = k i=1 1(p i \u2208 W x ) k ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_25",
            "content": "where W x is the wrong prediction set of x. Intuitively, the metric measures the ratio of top-k predicted words that belong to the wrong prediction set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_26",
            "content": "To reflect the prediction confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights. It is worth to mention that lower metrics mean a better model performance in both cases as the metrics assess how likely the models make inaccurate answers that they must avoid. The weighted metric can be defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_27",
            "content": "WHR@k(x) = k i=1 c i \u00d7 1(p i \u2208 W x ) k i=1 c i .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_28",
            "content": "For the SAR task, we employ accuracy as an evaluation metric, because each data point has its own label, and the label distribution is not skewed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_29",
            "content": "PLMs Lack Information of Negation and Lexical Semantics",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "110-ARR_v2_30",
            "content": "We select the following PLMs for the experiments: bidirectional encoder representations from transformers (BERT)-base/large (Devlin et al., 2019), RoBERTa-base/large (Liu et al., 2019b), and ALBERT-base/large (Lan et al., 2019). These PLMs are pre-trained with the MLM training objective. We added the ELECTRA-small/base/large models (Clark et al., 2020) for the SAR task, but it is not used for the MKR-NQ and MWR experiments, as the discriminator of the ELECTRA models are trained with the replaced token prediction (RTP) training objective and have no MLM classifier. No additional training is required for the MKR-NQ and MWR tasks. For the SAR task, we fine-tune each PLM for 10 epochs and apply the early stopping technique. We use the AdamW optimiser (Loshchilov and Hutter, 2019) for training with a learning rate of 5e \u22126 and a batch size of 32.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_31",
            "content": "Results for MKR-NQ",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "110-ARR_v2_32",
            "content": "The results for the MKR-NQ task are summarised in Table 1. In general, the results are consistent with previous works (Ettinger, 2020;Kassner and Sch\u00fctze, 2020). We observe three important characteristics from the experimental results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_33",
            "content": "Model MKR-NQ MWR HR@1 HR@3 WHR@3 HR@5 WHR@5 HR@1 HR@3 WHR@3 HR@5 WHR@5 BERT-base 9.57 First, large models produce a higher hit rate than their corresponding base-size models in all three PLMs, recording an average of about 1.5 times higher values. This implies that large-size models are more likely to generate wrong predictions for negated queries, even though they perform better than small-size models in many benchmark tests. The results suggest that evaluating a model's performance solely based on the accuracy metric is unwise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_34",
            "content": "Second, the hit rate decreases as k increases, which implies that the majority of PLMs' top predictions (e.g., k=1 or k=2) are incorrect. Finally, the weighted hit rate is much higher than the vanilla hit rate, suggesting that PLMs generate wrong predictions with high confidence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_35",
            "content": "Results for MWR",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "110-ARR_v2_36",
            "content": "The results of the MWR task are summarised in Table 1. The three characteristics found in the MKR-NQ task are also observed in the MWR task. Also, we found the following additional patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_37",
            "content": "In general, the hit rates are extremely high compared to the MKR-NQ task in all the PLMs. Analysing their predictions, we find that PLMs generate incorrect predictions primarily in antonym-asking queries. Specifically, the average HR@1 of the antonymasking queries is 41.9%, while that of the synonymasking queries is only 1.4%. A leading cause is that PLMs simply replicate the word presented in the input query. Table 2 shows the ratio of instances where each PLM reproduces the same word in a question. While the values are quite high for both synonym-asking and antonym-asking queries, the problem is more severe in the latter case, because the generated predictions are definitely incorrect. Based on our results, we conclude that PLMs' contextualised representations lack lexical semantic information. Our conclusion is in line with the findings of Liu et al. (2019a) showing that encoderfixed PLMs are not suitable to deal with tasks that require fine-grained linguistic knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_38",
            "content": "Issues are more severe with nouns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_39",
            "content": "We observe that the hit rates are higher when a word in a question is a noun. Specifically, the average HR@1 values of nouns, adjectives, and adverbs are 35.1%, 27.4%, and 11.8%, respectively. Interestingly, PLMs have a high error rate when dealing with nouns even though they are trained with a large written English corpus, where nouns form the greatest portion (at least 37%) of all POS tags (Hudson, 1994;Liang and Liu, 2013).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_40",
            "content": "Results for SAR",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "110-ARR_v2_41",
            "content": "As part of the comparison, we fine-tune each PLM on the SAR task, i.e., train the entire set of parameters. The results are summarised in Table 3. We observe a huge gap between the performance of fine-tuned models and that of encoder-fixed models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_42",
            "content": "In contrast to the fine-tuned models that produce a high accuracy, encoder-fixed models fall short of expectations, even recording almost a random guess performance in BERT models. Also, just as a common belief, large models' performance is greatly improved when fine-tuned. However, the difference between the large and small encoderfixed models is insignificant, except for the ELEC-TRA models that exhibit only a marginal improvement. The two phenomenons suggest that PLMs' outstanding performance is predicated on updating many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_43",
            "content": "Intermediate Training on Meaning",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "110-ARR_v2_44",
            "content": "Matching Task: IM 2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_45",
            "content": "Issue of PLMs",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "110-ARR_v2_46",
            "content": "Through the previous experiments, we observe that PLMs contain little information about negation and especially lexical semantics. We hypothesise a leading cause lies in the training objective of PLMs: the language modelling (LM) objective, which is a backbone pre-training task of almost all PLMs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_47",
            "content": "In the LM objective, words are generated based on given contexts. The distributional hypothesis (Harris, 1954), which assumes that semantically related or similar words will appear in similar contexts (Mrk\u0161i\u0107 et al., 2016), is the underpinning assumption of the LM objective (Sinha et al., 2021a). Under this assumption, a model learns the meaning of texts based on their correlation to others. This is a great benefit, because a model can learn the meaning of texts using only the text form, allowing unsupervised training. Based on this advantage, many unsupervised representations, such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), and current PLMs, have been developed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_48",
            "content": "However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts. For instance, consider the two words \"boy\" and \"girl\". We can readily imagine sentences in which the two words appear in the same context, e.g., \"the little boy/girl cuddled the teddy bear closely\". As a result, a model can learn their common functional meanings, i.e., young human beings, and the vector representations would be very similar if they were trained based on the distributional hypothesis. However, the representation hardly captures their semantic antonomy, e.g., gender. Similarly, negated sentences have almost identical contexts to their original forms. As a result, models cannot effectively learn the semantic meaning of words and negation expressions, provided they leverage only the text forms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_49",
            "content": "Meaning-Matching Task",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "110-ARR_v2_50",
            "content": "In the light of meaning-text theory, there is a correspondence between linguistic expressions (text) and semantic contents (meaning) (Mel'\u010duk and \u017dolkovskij, 1970;Mili\u0107evi\u0107, 2006). Instead of solely relying on the distributional hypothesis, we propose the new meaning-matching task, which can directly learn the correspondence. Specifically, meaning-matching is a classification that takes a word and a sentence as input and determines whether the sentence defines the word correctly. Through this task, a model can learn both meaning-text correspondences and correlations between a word and other words in a definition, which is rarely found in general corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_51",
            "content": "For training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018;Wang et al., 2019a;Liu et al., 2019a;Pruksachatkun et al., 2020;Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks. It has been shown that training on intermediate tasks that require high-level linguistic knowledge and inference ability could improve performance (Liu et al., 2019a;Pruksachatkun et al., 2020). Furthermore, it is more efficient in time and resources than pretraining models on large corpora (e.g., BERTNOT model (Hosseini et al., 2021)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_52",
            "content": "Dataset. We collect about 150K free-text definitions that depict the meaning of English words from WordNet (Miller, 1995) and the English Word, Meaning, and Usage Examples dataset. 3 In cases when a word appears in both datasets, we concatenate the word's definitions. Several examples of our data are presented in Table 12 in Appendix A. We use publicly available English datasets for convenience, but our approach is easily adaptable to other languages, since most of them have their own dictionaries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_53",
            "content": "Model MKR-NQ MWR HR@1 HR@3 WHR@3 HR@5 WHR@5 HR@1 HR@3 WHR@3 HR@5 WHR@5 BERT-large 13.33 7.70 11.17 Table 4: Results of BERT-large and RoBERTa-large after applying the IM 2 approach. We multiply 100 to each value for a better readability. Note that the lower the values the better. Table 5: PLMs' accuracy change in the SAR task when we apply IM 2 . We record the average across 5 runs. Our models show a statistically significant difference with p-value < 0.05 (*) compared to the baseline results in Table 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_54",
            "content": "It is necessary to generate false word-definition pairs to train PLMs on the meaning-matching task. To achieve this, we use a negative sampling technique. We investigate the proper k in the range of 3, 5, 10, and 20. For a hyperparameter search, the performance of the RoBERTa-base model on the SAR task is used as a criterion. Figure 2 illustrates the SAR performance of the RoBERTa-base model with different k values. Intuitively, a large k value will lead the model to a better performance by investigating more wordmeaning combinations. However, we observe that the model performs the best when k is 10, and the performance decreases if k is too large. We conjecture that a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem. As a result, large k values can increase the possibility of recognising the meaning of such similar words as different.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_55",
            "content": "To avoid the class-imbalance issue in a batch, we duplicate the correct word-definition pairs k times when we construct the training data. For training, the AdamW optimiser is used with a learning rate of 5e \u22126 . We use 5% of data points for validation and train the models for 15 epochs with a batch size of 32. The early stopping technique is used to prevent overfitting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_56",
            "content": "Experiments and Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "110-ARR_v2_57",
            "content": "We conduct the same probing tasks after the intermediate training on the meaning-matching task. 4",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_58",
            "content": "SAR Results",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "110-ARR_v2_59",
            "content": "We first focus on the SAR task. After the intermediate training, all models are fine-tuned on the SAR task with the same hyperparameters described in Section 3. The results are summarised in Table 5. Improved lexical semantic information. We generally observe marginal or no significant improvements when fine-tuning the whole parameters, especially for large-size PLMs. However, with fixed encoder, the performance is significantly improved for PLMs with more than 100M parameters, and the improvements are more significant for large PLMs. Our results show that the proposed approach assists PLMs to learn enhanced representations with more abundant lexical semantic information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_60",
            "content": "We find that small PLMs, such as ELECTRA-small and ALBERT models, show no significant increase in performance or are negatively impacted. Because all PLMs achieve a comparable performance on the meaning-matching task, we hypothesise that a leading cause is catastrophic forgetting (Pruksachatkun et al., 2020;Wallat et al., 2020), where the model forgets previous knowledge learned through pretraining to accept new information from the intermediate task. To verify this, we measure the change of parameter values after IM 2 . Concretely, let M i and M mm i denote the parameter of i-th layer before and after IM 2 . We calculate the average Frobenius norm for each layer:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_61",
            "content": "F i = 1 |M i | |M i \u2212 M mm i | F .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_62",
            "content": "Figure 3 shows the boxplots of F i for each PLMs. We observe that the parameters of the ELECTRA-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M. The results suggest that the size of PLMs is an important property to prevent the catastrophic forgetting issue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_63",
            "content": "MKR-NQ and MWR Results",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "110-ARR_v2_64",
            "content": "Next, we perform the MKR-NQ and MWR tasks after applying the IM 2 method. Since our models are not trained with the MLM objective, we replace the encoder of original PLMs with that of the models after fine-tuning on the meaning-matching task and reuse the MLM classifier. For the experiments, we use BERT-large and RoBERTa-large, because they are pre-trained based on the MLM objective, and parameters are hardly changed after applying the IM 2 method. The results are summarised in Table 4.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_65",
            "content": "We observe substantial decreases in the hit rates of incorrect predictions in both PLMs. For the MWR task, we find that the issue of regenerating a word in a given query is greatly relieved after applying the IM 2 method. Specifically, the percentage of such instances drops from 40.3% to 19.6% and from 33.8% to 25.2% for BERT-large and RoBERTa-large, respectively. Several examples of the predicted results are presented in Table 6. The results lend support to our claim that the IM 2 approach is of benefit to learning lexical semantic information and the meaning of negated expressions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_66",
            "content": "Fine-Tuning on the GLUE Benchmark",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "110-ARR_v2_67",
            "content": "A critical drawback of intermediate training is that the target task performance could be negatively impacted if the intermediate task is not related to the target task (Liu et al., 2019a;Pruksachatkun et al., 2020). To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and ELECTRA-large on 7 GLUE benchmark datasets (Wang et al., 2018) with their IM 2 counterparts. We train the models for 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3. It is observed that the training is generally finished within 8 epochs for all the models. The batch size per GPU and learning rates used for each dataset are described in Table 8. Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_68",
            "content": "The results are presented in Table 7. We find no significant difference in performance for tasks with large datasets, such as MNLI, QNLI, QQP, and SST2. On the contrary, tasks with small datasets, like MRPC and RTE, are slightly improved. The result is consistent with Pruksachatkun et al. (2020) and Vu et al. (2020), which showed that smaller tasks benefit much more from the intermediate training. Furthermore, unlike the previous studies that observed a negative transfer with the COLA dataset (Phang et al., 2018;Pruksachatkun et al., 2020), the performance is improved in our approach. The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_69",
            "content": "Experiments on the NegNLI Dataset",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "110-ARR_v2_70",
            "content": "Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020), where negation plays an important role for NLI tasks. As a baseline, we compare the reported performance of BERTNOT (Hosseini et al., 2021), which is a recently proposed remedy to improve PLMs' ability to understand negation. Since Hosseini et al. (2021) used BERT-base as a backbone model, we also apply the IM 2 method to BERT-base. The results are summarised in Table 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_71",
            "content": "For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in the NegNLI datasets, while yielding a comparable performance in the original development datasets. It is interesting that our approach improves the understanding of negation in both MKR-NQ and NegNLI tasks. We conjecture that a leading cause is that the definitions of the meaning-matching dataset contain many negation expressions, which enables a model to learn their proposed meaning (see Table 12). The results suggest that our proposed approach is more efficient than BERTNOT, because the IM 2 method leverages less time and resources for training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_72",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "110-ARR_v2_73",
            "content": "PLMs are at the core of many success stories in natural language processing (NLP). However, it remains unclear to what extent PLMs understand the syntactic and semantic properties of the human language. A series of probing tasks have been conducted on PLMs and have found them lacking or falling short on some language properties. Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2021;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_74",
            "content": "In addition to the above faulty behaviours, Ettinger (2020) and Kassner and Sch\u00fctze (2020) show that PLMs fail to comprehend negation, which is an important property of language in many natural language understanding (NLU) tasks. Ettinger (2020) check the ability of PLMs to understand the meaning of negation in given contexts. In their work, they check whether models are sensitive in their completions of sentences that either include negation or not. Under normal circumstances, the completions are expected to vary in truth depending on the presence or absence of negation in given sentences. Their results show that PLMs are insensitive to the impacts of negations when completing sentences. Kassner and Sch\u00fctze (2020) construct the negated LAMA dataset by inserting negation elements (e.g., \"not\") in the LAMA cloze questions (Petroni et al., 2019). They use negated and original question pairs to query PLMs and establish that models are equally prone to make the same predictions for both the original and negated questions. In a well-informed setting, it is expected that PLMs should make different predictions for the original and negated questions. This shows that PLMs struggle to comprehend negation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_75",
            "content": "In light of the highlighted faulty behaviours of PLMs, especially their struggle to comprehend negation, Hosseini et al. (2021) propose a remedy to alleviate the problem. In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2020) based on negated sentences from the training corpus. They use a syntactic augmentation method to generate negated sentences. In this method, the dependency parse of the sentences, POS tags, and morphological information of each word are taken as input, and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007). During training, they replace objects in negated sentences with [MASK] tokens and use unlikelihood training to make the masked-out tokens unlikely under the PLM distribution. To ensure that negated sentences are factually false, they use the corresponding positive sentences as context for the unlikelihood prediction task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_76",
            "content": "Previous studies (e.g., Kassner and Sch\u00fctze (2020)) have mostly limited the scope of the logical negation property only to the negation expressions (e.g., \"no\" and \"not\"). However, the core spirit of this property is the opposite meaning, which is not only limited to the negation. Welleck et al. (2020) consider negating sentences using dependency tree regular expression patterns. This widens the scope of negation, as it is not only limited to the negation expressions \"no\" and \"not\". However, their approach relies on other components, such as Semgrex, and dependency and POS parsers, which could impact the quality of the data, hence impact the models' performance. In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_77",
            "content": "Summary and Outlook",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "110-ARR_v2_78",
            "content": "In this work, we investigated PLMs' LNP. Compared to previous works that only examine negation expressions, we expanded the boundary of LNP to lexical semantics. We confirmed that PLMs are likely to violate LNP through extensive experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_79",
            "content": "We hypothesise that the distributional hypothesis is an insufficient basis for understanding the semantic meaning of texts. To alleviate the issue, we proposed a novel intermediate task: meaning-matching. Via experiments, we verified that meaningmatching is a stable intermediate task that substantially improves PLMs' understanding of negation and lexical semantic information while guaranteeing a positive transfer with multiple downstream tasks. Also, our approach produces a better performance on the negated NLI datasets compared to the unlikelihood training-based method, which leverages much more time and resources. Our work suggests that it is time to move beyond the distributional hypothesis to develop logically consistent and stable language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "110-ARR_v2_80",
            "content": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg, Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 2017, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Yossi Adi",
                    "Einat Kermany",
                    "Yonatan Belinkov",
                    "Ofer Lavi",
                    "Yoav Goldberg"
                ],
                "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
                "pub_date": "2017",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_81",
            "content": "Laura Aina, Raffaella Bernardi, Raquel Fern\u00e1ndez, A distributional study of negated adjectives and antonyms, 2018, Proceedings of the 5th Italian Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Laura Aina",
                    "Raffaella Bernardi",
                    "Raquel Fern\u00e1ndez"
                ],
                "title": "A distributional study of negated adjectives and antonyms",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 5th Italian Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_82",
            "content": "Yonatan Belinkov, James Glass, Analysis methods in neural language processing: A survey, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Yonatan Belinkov",
                    "James Glass"
                ],
                "title": "Analysis methods in neural language processing: A survey",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_83",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "R Samuel",
                    "Gabor Bowman",
                    "Christopher Angeli",
                    "Christopher Potts",
                    " Manning"
                ],
                "title": "A large annotated corpus for learning natural language inference",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_84",
            "content": "UNKNOWN, None, , Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot In Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot In Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "110-ARR_v2_85",
            "content": "Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom, Make up your mind! Adversarial generation of inconsistent natural language explanations, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Oana-Maria Camburu",
                    "Brendan Shillingford",
                    "Pasquale Minervini",
                    "Thomas Lukasiewicz",
                    "Phil Blunsom"
                ],
                "title": "Make up your mind! Adversarial generation of inconsistent natural language explanations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_86",
            "content": "Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill Maccartney, Marie-Catherine De Marneffe, Daniel Ramage, Eric Yeh, Christopher Manning, Learning alignments and leveraging natural logic, 2007, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Nathanael Chambers",
                    "Daniel Cer",
                    "Trond Grenager",
                    "David Hall",
                    "Chloe Kiddon",
                    "Bill Maccartney",
                    "Marie-Catherine De Marneffe",
                    "Daniel Ramage",
                    "Eric Yeh",
                    "Christopher Manning"
                ],
                "title": "Learning alignments and leveraging natural logic",
                "pub_date": "2007",
                "pub_title": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_87",
            "content": "UNKNOWN, None, 2020, ELECTRA: Pretraining text encoders as discriminators rather than generators. International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "ELECTRA: Pretraining text encoders as discriminators rather than generators. International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_88",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_89",
            "content": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, Yoav Goldberg, Measuring and improving consistency in pretrained language models, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Yanai Elazar",
                    "Nora Kassner",
                    "Shauli Ravfogel",
                    "Abhilasha Ravichander",
                    "Eduard Hovy",
                    "Hinrich Sch\u00fctze",
                    "Yoav Goldberg"
                ],
                "title": "Measuring and improving consistency in pretrained language models",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_90",
            "content": "Allyson Ettinger, What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Allyson Ettinger"
                ],
                "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_91",
            "content": "Ashim Gupta, Giorgi Kvernadze, Vivek Srikumar, Bert & family eat word salad: Experiments with text understanding, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ashim Gupta",
                    "Giorgi Kvernadze",
                    "Vivek Srikumar"
                ],
                "title": "Bert & family eat word salad: Experiments with text understanding",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_92",
            "content": "UNKNOWN, None, 1954, Distributional structure. Word, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "1954",
                "pub_title": "Distributional structure. Word",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_93",
            "content": "Matthew Honnibal, Mark Johnson, An improved non-monotonic transition system for dependency parsing, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Matthew Honnibal",
                    "Mark Johnson"
                ],
                "title": "An improved non-monotonic transition system for dependency parsing",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_94",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Venelin Md Mosharaf Hossain",
                    "Pranoy Kovatchev",
                    "Tiffany Dutta",
                    "Elizabeth Kao",
                    "Eduardo Wei",
                    " Blanco"
                ],
                "title": "An analysis of natural language inference benchmarks through the lens of negation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_95",
            "content": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Hjelm, Alessandro Sordoni, Aaron Courville, Understanding by understanding not: Modeling negation in language models, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Arian Hosseini",
                    "Siva Reddy",
                    "Dzmitry Bahdanau",
                    "R Hjelm",
                    "Alessandro Sordoni",
                    "Aaron Courville"
                ],
                "title": "Understanding by understanding not: Modeling negation in language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_96",
            "content": "Richard Hudson, About 37% of word-tokens are nouns, 1994, Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Richard Hudson"
                ],
                "title": "About 37% of word-tokens are nouns",
                "pub_date": "1994",
                "pub_title": "Language",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_97",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Nora Kassner",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_98",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Zhenzhong Lan",
                    "Mingda Chen",
                    "Sebastian Goodman",
                    "Kevin Gimpel",
                    "Piyush Sharma",
                    "Radu Soricut"
                ],
                "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_99",
            "content": "Junying Liang, Haitao Liu, Noun distribution in natural languages, 2013, Pozna\u0144 Studies in Contemporary Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Junying Liang",
                    "Haitao Liu"
                ],
                "title": "Noun distribution in natural languages",
                "pub_date": "2013",
                "pub_title": "Pozna\u0144 Studies in Contemporary Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_100",
            "content": "Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna,  Ren, Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pretrained language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Seyeon Bill Yuchen Lin",
                    "Rahul Lee",
                    "Xiang Khanna",
                    " Ren"
                ],
                "title": "Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pretrained language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_101",
            "content": "Nelson Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, Noah Smith, Linguistic knowledge and transferability of contextual 2039 representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Nelson Liu",
                    "Matt Gardner",
                    "Yonatan Belinkov",
                    "Matthew Peters",
                    "Noah Smith"
                ],
                "title": "Linguistic knowledge and transferability of contextual 2039 representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_102",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "RoBERTa: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_103",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ilya Loshchilov",
                    "Frank Hutter"
                ],
                "title": "Decoupled weight decay regularization",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_104",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Tom Mccoy",
                    "Ellie Pavlick",
                    "Tal Linzen"
                ],
                "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_105",
            "content": "I , A \u017dolkovskij, Towards a functioning 'meaning-text' model of language, 1970, Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "I ",
                    "A \u017dolkovskij"
                ],
                "title": "Towards a functioning 'meaning-text' model of language",
                "pub_date": "1970",
                "pub_title": "Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_106",
            "content": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeff Dean, Distributed representations of words and phrases and their compositionality, 2013, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Tomas Mikolov",
                    "Ilya Sutskever",
                    "Kai Chen",
                    "Greg Corrado",
                    "Jeff Dean"
                ],
                "title": "Distributed representations of words and phrases and their compositionality",
                "pub_date": "2013",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "110-ARR_v2_107",
            "content": "Jasmina Mili\u0107evi\u0107, A short guide to the meaningtext linguistic theory, 2006, Journal of Koralex, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Jasmina Mili\u0107evi\u0107"
                ],
                "title": "A short guide to the meaningtext linguistic theory",
                "pub_date": "2006",
                "pub_title": "Journal of Koralex",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_108",
            "content": "George Miller, WordNet: A lexical database for English, 1995, Communications of the ACM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "George Miller"
                ],
                "title": "WordNet: A lexical database for English",
                "pub_date": "1995",
                "pub_title": "Communications of the ACM",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_109",
            "content": "Nikola Mrk\u0161i\u0107, \u00d3 Diarmuid, Blaise S\u00e9aghdha, Milica Thomson, Lina Ga\u0161i\u0107, Pei-Hao Rojas-Barahona, David Su, Tsung-Hsien Vandyke, Steve Wen,  Young, Counter-fitting word vectors to linguistic constraints, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Nikola Mrk\u0161i\u0107",
                    "\u00d3 Diarmuid",
                    "Blaise S\u00e9aghdha",
                    "Milica Thomson",
                    "Lina Ga\u0161i\u0107",
                    "Pei-Hao Rojas-Barahona",
                    "David Su",
                    "Tsung-Hsien Vandyke",
                    "Steve Wen",
                    " Young"
                ],
                "title": "Counter-fitting word vectors to linguistic constraints",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_110",
            "content": "Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, Stress test evaluation for natural language inference, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Aakanksha Naik",
                    "Abhilasha Ravichander",
                    "Norman Sadeh",
                    "Carolyn Rose",
                    "Graham Neubig"
                ],
                "title": "Stress test evaluation for natural language inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_111",
            "content": "Timothy Niven, Hung-Yu Kao, Probing neural network comprehension of natural language arguments, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Timothy Niven",
                    "Hung-Yu Kao"
                ],
                "title": "Probing neural network comprehension of natural language arguments",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_112",
            "content": "UNKNOWN, None, 2021, Investigating the limitations of transformers with simple arithmetic tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Investigating the limitations of transformers with simple arithmetic tasks",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_113",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "GloVe: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_114",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Language models as knowledge bases?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_115",
            "content": "Thang Pham, Trung Bui, Long Mai, Anh Nguyen, Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Thang Pham",
                    "Trung Bui",
                    "Long Mai",
                    "Anh Nguyen"
                ],
                "title": "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_116",
            "content": "UNKNOWN, None, 2018, Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_117",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Yada Pruksachatkun",
                    "Jason Phang",
                    "Haokun Liu",
                    "Xiaoyi Phu Mon Htut",
                    "Richard Zhang",
                    "Clara Pang",
                    "Katharina Vania",
                    "Samuel Kann"
                ],
                "title": "Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_118",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI blog",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_119",
            "content": "Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung, On the systematicity of probing contextualized word representations: The case of hypernymy in BERT, 2020, Proceedings of the 9th Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Abhilasha Ravichander",
                    "Eduard Hovy",
                    "Kaheer Suleman",
                    "Adam Trischler",
                    "Jackie Chi Kit Cheung"
                ],
                "title": "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 9th Joint Conference on Lexical and Computational Semantics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_120",
            "content": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Koustuv Sinha",
                    "Robin Jia",
                    "Dieuwke Hupkes",
                    "Joelle Pineau",
                    "Adina Williams",
                    "Douwe Kiela"
                ],
                "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_121",
            "content": "Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams, UnNatural Language Inference, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Koustuv Sinha",
                    "Prasanna Parthasarathi",
                    "Joelle Pineau",
                    "Adina Williams"
                ],
                "title": "UnNatural Language Inference",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "110-ARR_v2_122",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, ConceptNet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the 31st AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "ConceptNet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 31st AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_123",
            "content": "Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer, Exploring and predicting transferability across NLP tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Tu Vu",
                    "Tong Wang",
                    "Tsendsuren Munkhdalai",
                    "Alessandro Sordoni",
                    "Adam Trischler",
                    "Andrew Mattarella-Micke",
                    "Subhransu Maji",
                    "Mohit Iyyer"
                ],
                "title": "Exploring and predicting transferability across NLP tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_124",
            "content": "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, Do NLP models know numbers? probing numeracy in embeddings, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019), .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Eric Wallace",
                    "Yizhong Wang",
                    "Sujian Li",
                    "Sameer Singh",
                    "Matt Gardner"
                ],
                "title": "Do NLP models know numbers? probing numeracy in embeddings",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019)",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_125",
            "content": "Jonas Wallat, Jaspreet Singh, Avishek Anand, BERTnesia: Investigating the capture and forgetting of knowledge in BERT, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Jonas Wallat",
                    "Jaspreet Singh",
                    "Avishek Anand"
                ],
                "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_126",
            "content": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R Mccoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, Samuel Bowman, Can you tell me how to get past Sesame Street? Sentence-level pretraining beyond language modeling, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Alex Wang",
                    "Jan Hula",
                    "Patrick Xia",
                    "Raghavendra Pappagari",
                    "R Mccoy",
                    "Roma Patel",
                    "Najoung Kim",
                    "Ian Tenney",
                    "Yinghui Huang",
                    "Katherin Yu",
                    "Shuning Jin",
                    "Berlin Chen",
                    "Benjamin Van Durme",
                    "Edouard Grave",
                    "Ellie Pavlick",
                    "Samuel Bowman"
                ],
                "title": "Can you tell me how to get past Sesame Street? Sentence-level pretraining beyond language modeling",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_127",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Alex Wang",
                    "Yada Pruksachatkun",
                    "Nikita Nangia",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "110-ARR_v2_128",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "110-ARR_v2_129",
            "content": "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, Neural text generation with unlikelihood training, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Sean Welleck",
                    "Ilia Kulikov",
                    "Stephen Roller",
                    "Emily Dinan",
                    "Kyunghyun Cho",
                    "Jason Weston"
                ],
                "title": "Neural text generation with unlikelihood training",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "110-ARR_v2_0@0",
            "content": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_0",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@0",
            "content": "The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff \u00acp is false), is an important property that a trustworthy language model must satisfy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@1",
            "content": "However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 219,
            "end": 334,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@2",
            "content": "In this paper, we perform experiments using probing tasks to assess PLMs' LNP understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 336,
            "end": 427,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@3",
            "content": "Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 429,
            "end": 558,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@4",
            "content": "Through experiments, we observe that PLMs violate the LNP frequently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 560,
            "end": 628,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@5",
            "content": "To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 630,
            "end": 834,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@6",
            "content": "Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 836,
            "end": 938,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@7",
            "content": "Also, through fine-tuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 940,
            "end": 1109,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_2@8",
            "content": "Finally, we observe that our proposed approach 1 outperforms our previous counterparts despite its time and resource efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_2",
            "start": 1111,
            "end": 1238,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_4@0",
            "content": "Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019b) benchmark datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_4",
            "start": 0,
            "end": 332,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_5@0",
            "content": "However, their reliability is recently being challenged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_5",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_5@1",
            "content": "Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2021;Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_5",
            "start": 57,
            "end": 465,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_5@2",
            "content": "These issues raise concerns about PLMs' stability and reliability, precluding them from applications in practice, especially in risk-sensitive areas.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_5",
            "start": 467,
            "end": 615,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_6@0",
            "content": "Another critical problem of PLMs is their inaccurate behaviour on negation, which is a principal property in many language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_6",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_6@1",
            "content": "For tasks where the LNP holds (p is true iff \u00acp is false; see Aina et al. 2018), PLMs should make different answers for the original and negated inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_6",
            "start": 144,
            "end": 295,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_6@2",
            "content": "However, several studies observed that PLMs violate this property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_6",
            "start": 297,
            "end": 362,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_6@3",
            "content": "In masked knowledge retrieval tasks, PLMs frequently generate incorrect answers for negated input queries (Ettinger, 2020;Kassner and Sch\u00fctze, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_6",
            "start": 364,
            "end": 512,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_6@4",
            "content": "In other studies, PLMs show a poor generalisation ability on negated natural language inference (NLI) datasets (Naik et al., 2018;Hossain et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_6",
            "start": 514,
            "end": 665,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_7@0",
            "content": "Although the aforementioned studies produced promising analysis results, they limited the scope of the LNP only to adding negation expressions (e.g., \"no\" and \"not\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_7",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_7@1",
            "content": "However, other perturbations that generate the opposite meaning also can be applied to the property.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_7",
            "start": 167,
            "end": 266,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_7@2",
            "content": "Therefore, a consideration of such perturbation methods is necessary to fully assess whether PLMs satisfy the LNP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_7",
            "start": 268,
            "end": 381,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@0",
            "content": "Also, remedies to alleviate the problem have not been studied much yet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@1",
            "content": "Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2020) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 72,
            "end": 301,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@2",
            "content": "However, this approach has several downsides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 303,
            "end": 347,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@3",
            "content": "First, like previous works, Hosseini et al. (2021) only considered negation expressions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 349,
            "end": 436,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@4",
            "content": "Second, the data augmentation method is contingent on many additional linguistic compo-nents, which causes the dependency of a model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 438,
            "end": 693,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_8@5",
            "content": "Finally, the model should be pre-trained from scratch with the unlikelihood objective, which consumes considerable time and resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_8",
            "start": 695,
            "end": 828,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_9@0",
            "content": "In this paper, we expand the boundary of the LNP to lexical semantics, i.e., synonyms and antonyms, and ascertain that PLMs are prone to violate the LNP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_9",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_9@1",
            "content": "Next, we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_9",
            "start": 154,
            "end": 292,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_9@2",
            "content": "We hypothesise that a leading cause lies in the MLM training objective, which assumes the distributional hypothesis for learning the meaning of the text (Sinha et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_9",
            "start": 294,
            "end": 468,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_9@3",
            "content": "Instead, we design a model that directly learns the correspondence between words and their semantic contents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_9",
            "start": 470,
            "end": 578,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_9@4",
            "content": "Through experiments, we verify that our approach improves the model's comprehension of the LNP, while showing a stable performance on multiple downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_9",
            "start": 580,
            "end": 739,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_10@0",
            "content": "Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy, named IM 2 , which is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_10",
            "start": 0,
            "end": 694,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_11@0",
            "content": "Probing Tasks for Investigating the Logical Negation Property",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_11",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_12@0",
            "content": "We design three probing tasks to evaluate whether PLMs satisfy the LNP: masked knowledge retrieval on negated queries (MKR-NQ), masked word retrieval (MWR), and synonym/antonym recognition (SAR).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_12",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_12@1",
            "content": "Brief illustrations of each task are in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_12",
            "start": 196,
            "end": 244,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_13@0",
            "content": "Masked Knowledge Retrieval on Negated Queries",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_13",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_14@0",
            "content": "The MKR-NQ task examines whether PLMs generate incorrect answers for negated queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_14",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_14@1",
            "content": "Following the work of Kassner and Sch\u00fctze (2020), we constructed the evaluation dataset by negating the LAMA dataset (Petroni et al., 2019), which contains masked free-text forms of ConceptNet (Speer et al., 2017) triplets and their corresponding answers (e.g., (bird, CapableOf, fly) \u2192 (\"A bird can [MASK]\", fly)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_14",
            "start": 86,
            "end": 400,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_14@2",
            "content": "The task aims to generate a correct word through MLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_14",
            "start": 402,
            "end": 454,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@0",
            "content": "According to the LNP, a model must not generate the original answer if the query is negated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@1",
            "content": "To measure how likely PLMs generate wrong predictions for negated queries, we collected pairs of (negated_query, wrong_predictions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 93,
            "end": 224,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@2",
            "content": "We selected several relations in the LAMA dataset that ensure mutual exclusiveness between the original and negated queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 226,
            "end": 349,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@3",
            "content": "2 For negating sentences, we selected LAMA data points that contain a single verb using the Spacy parts of speech (POS) tagger (Honnibal and Johnson, 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 351,
            "end": 506,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@4",
            "content": "Next, we added negation expressions, such as \"not\" and \"don't\", or removed such expressions if they existed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 508,
            "end": 615,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@5",
            "content": "Finally, we collected the wrong predictions from Concept-Net by using the head entity and relation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 617,
            "end": 715,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@6",
            "content": "As a result, we collected 3,360 data points for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 717,
            "end": 774,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_15@7",
            "content": "The list of the relations that we used and examples of the data are in Table 10 in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_15",
            "start": 776,
            "end": 869,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_16@0",
            "content": "Masked Word Retrieval",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_16",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_17@0",
            "content": "To expand the boundary of the LNP to lexical semantics, we design the MWR task, which generates an answer of a masked query, asking for the synonym/antonym of a target word through MLM (e.g., \"happy is the synonym of [MASK]\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_17",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_18@0",
            "content": "Let s w and a w denote masked queries that ask the synonym and antonym of the word w, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_18",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_18@1",
            "content": "Also, let A s and A a refer to the list of correct answers for s w and a w , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_18",
            "start": 100,
            "end": 189,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_18@2",
            "content": "Intuitively, A a becomes the wrong predictions of s w , because s w and a w have the opposite meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_18",
            "start": 191,
            "end": 292,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_18@3",
            "content": "Therefore, we can evaluate the violation of the LNP by investigating whether a PLM generates wrong predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_18",
            "start": 294,
            "end": 404,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_19@0",
            "content": "To extract commonly-used words for our experiment, we first extracted nouns, adjectives, and adverbs that appear more than five times in the SNLI dataset (Bowman et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_19",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_19@1",
            "content": "Among the extracted candidates, we filtered words that have synonyms or antonyms in ConceptNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_19",
            "start": 177,
            "end": 271,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_19@2",
            "content": "Finally, we generated masked queries by employing templates used by Camburu et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_19",
            "start": 273,
            "end": 362,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_19@3",
            "content": "As a result, we collected about 27K data points for MWR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_19",
            "start": 364,
            "end": 419,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_19@4",
            "content": "The templates and examples of the data are in Table 11 in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_19",
            "start": 421,
            "end": 489,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_20@0",
            "content": "Synonym/Antonym Recognition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_20",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@0",
            "content": "SAR is a classification that distinguishes whether two given words are synonyms or antonyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@1",
            "content": "It aims to evaluate whether the contextualised representations of PLMs reflect the lexical meaning of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 93,
            "end": 200,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@2",
            "content": "Therefore, we use a parametric probing model (Adi et al., 2017;Liu et al., 2019a;Belinkov and Glass, 2019;Sinha et al., 2021a) for the experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 202,
            "end": 347,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@3",
            "content": "Specifically, the experiment is performed on the final layer of each PLMs, i.e., we only train the classifier while keeping the encoder frozen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 349,
            "end": 491,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@4",
            "content": "We use ConceptNet to build the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 493,
            "end": 531,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@5",
            "content": "ConceptNet has much more synonym triplets compared to antonyms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 533,
            "end": 595,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@6",
            "content": "As a result, we randomly sample the synonym triplets to maintain a balance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 597,
            "end": 671,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_21@7",
            "content": "To that end, we collect 33K, 1K, and 2K data points for the train, dev, and test datasets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_21",
            "start": 673,
            "end": 776,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_22@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_22",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_23@0",
            "content": "We use the top-k hit rate (HR@k) to evaluate the performance on the MKR-NQ and MWR tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_23",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_23@1",
            "content": "Assume that P = {(p 1 , c 1 ), (p 2 , c 2 ), . . . , (p n , c n )} denotes the set of predictions for a data point x, where p t and c t refer to the predicted word and confidence score of the t-th prediction, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_23",
            "start": 90,
            "end": 311,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_23@2",
            "content": "Then, the top-k hit rate for a data point x is defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_23",
            "start": 313,
            "end": 378,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_24@0",
            "content": "HR@k(x) = k i=1 1(p i \u2208 W x ) k ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_24",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_25@0",
            "content": "where W x is the wrong prediction set of x. Intuitively, the metric measures the ratio of top-k predicted words that belong to the wrong prediction set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_25",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_26@0",
            "content": "To reflect the prediction confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_26",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_26@1",
            "content": "It is worth to mention that lower metrics mean a better model performance in both cases as the metrics assess how likely the models make inaccurate answers that they must avoid.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_26",
            "start": 171,
            "end": 347,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_26@2",
            "content": "The weighted metric can be defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_26",
            "start": 349,
            "end": 394,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_27@0",
            "content": "WHR@k(x) = k i=1 c i \u00d7 1(p i \u2208 W x ) k i=1 c i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_27",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_28@0",
            "content": "For the SAR task, we employ accuracy as an evaluation metric, because each data point has its own label, and the label distribution is not skewed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_28",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_29@0",
            "content": "PLMs Lack Information of Negation and Lexical Semantics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_29",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@0",
            "content": "We select the following PLMs for the experiments: bidirectional encoder representations from transformers (BERT)-base/large (Devlin et al., 2019), RoBERTa-base/large (Liu et al., 2019b), and ALBERT-base/large (Lan et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@1",
            "content": "These PLMs are pre-trained with the MLM training objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 229,
            "end": 287,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@2",
            "content": "We added the ELECTRA-small/base/large models (Clark et al., 2020) for the SAR task, but it is not used for the MKR-NQ and MWR experiments, as the discriminator of the ELECTRA models are trained with the replaced token prediction (RTP) training objective and have no MLM classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 289,
            "end": 569,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@3",
            "content": "No additional training is required for the MKR-NQ and MWR tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 571,
            "end": 634,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@4",
            "content": "For the SAR task, we fine-tune each PLM for 10 epochs and apply the early stopping technique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 636,
            "end": 728,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_30@5",
            "content": "We use the AdamW optimiser (Loshchilov and Hutter, 2019) for training with a learning rate of 5e \u22126 and a batch size of 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_30",
            "start": 730,
            "end": 852,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_31@0",
            "content": "Results for MKR-NQ",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_31",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_32@0",
            "content": "The results for the MKR-NQ task are summarised in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_32",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_32@1",
            "content": "In general, the results are consistent with previous works (Ettinger, 2020;Kassner and Sch\u00fctze, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_32",
            "start": 59,
            "end": 160,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_32@2",
            "content": "We observe three important characteristics from the experimental results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_32",
            "start": 162,
            "end": 234,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_33@0",
            "content": "Model MKR-NQ MWR HR@1 HR@3 WHR@3 HR@5 WHR@5 HR@1 HR@3 WHR@3 HR@5 WHR@5 BERT-base 9.57 First, large models produce a higher hit rate than their corresponding base-size models in all three PLMs, recording an average of about 1.5 times higher values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_33",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_33@1",
            "content": "This implies that large-size models are more likely to generate wrong predictions for negated queries, even though they perform better than small-size models in many benchmark tests.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_33",
            "start": 248,
            "end": 429,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_33@2",
            "content": "The results suggest that evaluating a model's performance solely based on the accuracy metric is unwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_33",
            "start": 431,
            "end": 534,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_34@0",
            "content": "Second, the hit rate decreases as k increases, which implies that the majority of PLMs' top predictions (e.g., k=1 or k=2) are incorrect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_34",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_34@1",
            "content": "Finally, the weighted hit rate is much higher than the vanilla hit rate, suggesting that PLMs generate wrong predictions with high confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_34",
            "start": 138,
            "end": 279,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_35@0",
            "content": "Results for MWR",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_35",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_36@0",
            "content": "The results of the MWR task are summarised in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_36",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_36@1",
            "content": "The three characteristics found in the MKR-NQ task are also observed in the MWR task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_36",
            "start": 55,
            "end": 139,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_36@2",
            "content": "Also, we found the following additional patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_36",
            "start": 141,
            "end": 189,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@0",
            "content": "In general, the hit rates are extremely high compared to the MKR-NQ task in all the PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@1",
            "content": "Analysing their predictions, we find that PLMs generate incorrect predictions primarily in antonym-asking queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 90,
            "end": 203,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@2",
            "content": "Specifically, the average HR@1 of the antonymasking queries is 41.9%, while that of the synonymasking queries is only 1.4%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 205,
            "end": 327,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@3",
            "content": "A leading cause is that PLMs simply replicate the word presented in the input query.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 329,
            "end": 412,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@4",
            "content": "Table 2 shows the ratio of instances where each PLM reproduces the same word in a question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 414,
            "end": 504,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@5",
            "content": "While the values are quite high for both synonym-asking and antonym-asking queries, the problem is more severe in the latter case, because the generated predictions are definitely incorrect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 506,
            "end": 695,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@6",
            "content": "Based on our results, we conclude that PLMs' contextualised representations lack lexical semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 697,
            "end": 806,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_37@7",
            "content": "Our conclusion is in line with the findings of Liu et al. (2019a) showing that encoderfixed PLMs are not suitable to deal with tasks that require fine-grained linguistic knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_37",
            "start": 808,
            "end": 987,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_38@0",
            "content": "Issues are more severe with nouns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_38",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_39@0",
            "content": "We observe that the hit rates are higher when a word in a question is a noun.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_39",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_39@1",
            "content": "Specifically, the average HR@1 values of nouns, adjectives, and adverbs are 35.1%, 27.4%, and 11.8%, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_39",
            "start": 78,
            "end": 191,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_39@2",
            "content": "Interestingly, PLMs have a high error rate when dealing with nouns even though they are trained with a large written English corpus, where nouns form the greatest portion (at least 37%) of all POS tags (Hudson, 1994;Liang and Liu, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_39",
            "start": 193,
            "end": 429,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_40@0",
            "content": "Results for SAR",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_40",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_41@0",
            "content": "As part of the comparison, we fine-tune each PLM on the SAR task, i.e., train the entire set of parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_41",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_41@1",
            "content": "The results are summarised in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_41",
            "start": 108,
            "end": 145,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_41@2",
            "content": "We observe a huge gap between the performance of fine-tuned models and that of encoder-fixed models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_41",
            "start": 147,
            "end": 246,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_42@0",
            "content": "In contrast to the fine-tuned models that produce a high accuracy, encoder-fixed models fall short of expectations, even recording almost a random guess performance in BERT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_42",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_42@1",
            "content": "Also, just as a common belief, large models' performance is greatly improved when fine-tuned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_42",
            "start": 181,
            "end": 273,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_42@2",
            "content": "However, the difference between the large and small encoderfixed models is insignificant, except for the ELEC-TRA models that exhibit only a marginal improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_42",
            "start": 275,
            "end": 436,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_42@3",
            "content": "The two phenomenons suggest that PLMs' outstanding performance is predicated on updating many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_42",
            "start": 438,
            "end": 734,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_43@0",
            "content": "Intermediate Training on Meaning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_43",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_44@0",
            "content": "Matching Task: IM 2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_44",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_45@0",
            "content": "Issue of PLMs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_45",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_46@0",
            "content": "Through the previous experiments, we observe that PLMs contain little information about negation and especially lexical semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_46",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_46@1",
            "content": "We hypothesise a leading cause lies in the training objective of PLMs: the language modelling (LM) objective, which is a backbone pre-training task of almost all PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_46",
            "start": 131,
            "end": 297,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_47@0",
            "content": "In the LM objective, words are generated based on given contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_47",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_47@1",
            "content": "The distributional hypothesis (Harris, 1954), which assumes that semantically related or similar words will appear in similar contexts (Mrk\u0161i\u0107 et al., 2016), is the underpinning assumption of the LM objective (Sinha et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_47",
            "start": 66,
            "end": 296,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_47@2",
            "content": "Under this assumption, a model learns the meaning of texts based on their correlation to others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_47",
            "start": 298,
            "end": 393,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_47@3",
            "content": "This is a great benefit, because a model can learn the meaning of texts using only the text form, allowing unsupervised training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_47",
            "start": 395,
            "end": 523,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_47@4",
            "content": "Based on this advantage, many unsupervised representations, such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), and current PLMs, have been developed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_47",
            "start": 525,
            "end": 696,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@0",
            "content": "However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@1",
            "content": "For instance, consider the two words \"boy\" and \"girl\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 227,
            "end": 280,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@2",
            "content": "We can readily imagine sentences in which the two words appear in the same context, e.g., \"the little boy/girl cuddled the teddy bear closely\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 282,
            "end": 424,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@3",
            "content": "As a result, a model can learn their common functional meanings, i.e., young human beings, and the vector representations would be very similar if they were trained based on the distributional hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 426,
            "end": 629,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@4",
            "content": "However, the representation hardly captures their semantic antonomy, e.g., gender.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 631,
            "end": 712,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@5",
            "content": "Similarly, negated sentences have almost identical contexts to their original forms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 714,
            "end": 797,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_48@6",
            "content": "As a result, models cannot effectively learn the semantic meaning of words and negation expressions, provided they leverage only the text forms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_48",
            "start": 799,
            "end": 942,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_49@0",
            "content": "Meaning-Matching Task",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_49",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_50@0",
            "content": "In the light of meaning-text theory, there is a correspondence between linguistic expressions (text) and semantic contents (meaning) (Mel'\u010duk and \u017dolkovskij, 1970;Mili\u0107evi\u0107, 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_50",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_50@1",
            "content": "Instead of solely relying on the distributional hypothesis, we propose the new meaning-matching task, which can directly learn the correspondence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_50",
            "start": 181,
            "end": 326,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_50@2",
            "content": "Specifically, meaning-matching is a classification that takes a word and a sentence as input and determines whether the sentence defines the word correctly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_50",
            "start": 328,
            "end": 483,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_50@3",
            "content": "Through this task, a model can learn both meaning-text correspondences and correlations between a word and other words in a definition, which is rarely found in general corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_50",
            "start": 485,
            "end": 661,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_51@0",
            "content": "For training PLMs on our new task, we apply the intermediate-training technique (Phang et al., 2018;Wang et al., 2019a;Liu et al., 2019a;Pruksachatkun et al., 2020;Vu et al., 2020), which first fine-tunes PLMs on an intermediate task, and then fine-tunes the model again on target tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_51",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_51@1",
            "content": "It has been shown that training on intermediate tasks that require high-level linguistic knowledge and inference ability could improve performance (Liu et al., 2019a;Pruksachatkun et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_51",
            "start": 288,
            "end": 481,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_51@2",
            "content": "Furthermore, it is more efficient in time and resources than pretraining models on large corpora (e.g., BERTNOT model (Hosseini et al., 2021)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_51",
            "start": 483,
            "end": 625,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_52@0",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_52",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_52@1",
            "content": "We collect about 150K free-text definitions that depict the meaning of English words from WordNet (Miller, 1995) and the English Word, Meaning, and Usage Examples dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_52",
            "start": 9,
            "end": 179,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_52@2",
            "content": "3 In cases when a word appears in both datasets, we concatenate the word's definitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_52",
            "start": 181,
            "end": 267,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_52@3",
            "content": "Several examples of our data are presented in Table 12 in Appendix A. We use publicly available English datasets for convenience, but our approach is easily adaptable to other languages, since most of them have their own dictionaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_52",
            "start": 269,
            "end": 502,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@0",
            "content": "Model MKR-NQ MWR HR@1 HR@3 WHR@3 HR@5 WHR@5 HR@1 HR@3 WHR@3 HR@5 WHR@5 BERT-large 13.33 7.70 11.17 Table 4: Results of BERT-large and RoBERTa-large after applying the IM 2 approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@1",
            "content": "We multiply 100 to each value for a better readability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 182,
            "end": 236,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@2",
            "content": "Note that the lower the values the better.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 238,
            "end": 279,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@3",
            "content": "Table 5: PLMs' accuracy change in the SAR task when we apply IM 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 281,
            "end": 347,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@4",
            "content": "We record the average across 5 runs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 349,
            "end": 384,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_53@5",
            "content": "Our models show a statistically significant difference with p-value < 0.05 (*) compared to the baseline results in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_53",
            "start": 386,
            "end": 508,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@0",
            "content": "It is necessary to generate false word-definition pairs to train PLMs on the meaning-matching task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@1",
            "content": "To achieve this, we use a negative sampling technique.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 100,
            "end": 153,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@2",
            "content": "We investigate the proper k in the range of 3, 5, 10, and 20.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 155,
            "end": 215,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@3",
            "content": "For a hyperparameter search, the performance of the RoBERTa-base model on the SAR task is used as a criterion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 217,
            "end": 326,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@4",
            "content": "Figure 2 illustrates the SAR performance of the RoBERTa-base model with different k values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 328,
            "end": 418,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@5",
            "content": "Intuitively, a large k value will lead the model to a better performance by investigating more wordmeaning combinations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 420,
            "end": 539,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@6",
            "content": "However, we observe that the model performs the best when k is 10, and the performance decreases if k is too large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 541,
            "end": 655,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@7",
            "content": "We conjecture that a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 657,
            "end": 788,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_54@8",
            "content": "As a result, large k values can increase the possibility of recognising the meaning of such similar words as different.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_54",
            "start": 790,
            "end": 908,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_55@0",
            "content": "To avoid the class-imbalance issue in a batch, we duplicate the correct word-definition pairs k times when we construct the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_55",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_55@1",
            "content": "For training, the AdamW optimiser is used with a learning rate of 5e \u22126 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_55",
            "start": 139,
            "end": 211,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_55@2",
            "content": "We use 5% of data points for validation and train the models for 15 epochs with a batch size of 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_55",
            "start": 213,
            "end": 311,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_55@3",
            "content": "The early stopping technique is used to prevent overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_55",
            "start": 313,
            "end": 372,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_56@0",
            "content": "Experiments and Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_56",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_57@0",
            "content": "We conduct the same probing tasks after the intermediate training on the meaning-matching task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_57",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_57@1",
            "content": "4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_57",
            "start": 96,
            "end": 96,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_58@0",
            "content": "SAR Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_58",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@0",
            "content": "We first focus on the SAR task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@1",
            "content": "After the intermediate training, all models are fine-tuned on the SAR task with the same hyperparameters described in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 32,
            "end": 159,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@2",
            "content": "The results are summarised in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 161,
            "end": 198,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@3",
            "content": "Improved lexical semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 200,
            "end": 237,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@4",
            "content": "We generally observe marginal or no significant improvements when fine-tuning the whole parameters, especially for large-size PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 239,
            "end": 369,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@5",
            "content": "However, with fixed encoder, the performance is significantly improved for PLMs with more than 100M parameters, and the improvements are more significant for large PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 371,
            "end": 539,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_59@6",
            "content": "Our results show that the proposed approach assists PLMs to learn enhanced representations with more abundant lexical semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_59",
            "start": 541,
            "end": 679,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_60@0",
            "content": "We find that small PLMs, such as ELECTRA-small and ALBERT models, show no significant increase in performance or are negatively impacted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_60",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_60@1",
            "content": "Because all PLMs achieve a comparable performance on the meaning-matching task, we hypothesise that a leading cause is catastrophic forgetting (Pruksachatkun et al., 2020;Wallat et al., 2020), where the model forgets previous knowledge learned through pretraining to accept new information from the intermediate task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_60",
            "start": 138,
            "end": 454,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_60@2",
            "content": "To verify this, we measure the change of parameter values after IM 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_60",
            "start": 456,
            "end": 525,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_60@3",
            "content": "Concretely, let M i and M mm i denote the parameter of i-th layer before and after IM 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_60",
            "start": 527,
            "end": 615,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_60@4",
            "content": "We calculate the average Frobenius norm for each layer:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_60",
            "start": 617,
            "end": 671,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_61@0",
            "content": "F i = 1 |M i | |M i \u2212 M mm i | F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_61",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_62@0",
            "content": "Figure 3 shows the boxplots of F i for each PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_62",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_62@1",
            "content": "We observe that the parameters of the ELECTRA-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_62",
            "start": 50,
            "end": 219,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_62@2",
            "content": "The results suggest that the size of PLMs is an important property to prevent the catastrophic forgetting issue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_62",
            "start": 221,
            "end": 332,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_63@0",
            "content": "MKR-NQ and MWR Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_63",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_64@0",
            "content": "Next, we perform the MKR-NQ and MWR tasks after applying the IM 2 method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_64",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_64@1",
            "content": "Since our models are not trained with the MLM objective, we replace the encoder of original PLMs with that of the models after fine-tuning on the meaning-matching task and reuse the MLM classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_64",
            "start": 74,
            "end": 270,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_64@2",
            "content": "For the experiments, we use BERT-large and RoBERTa-large, because they are pre-trained based on the MLM objective, and parameters are hardly changed after applying the IM 2 method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_64",
            "start": 272,
            "end": 451,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_64@3",
            "content": "The results are summarised in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_64",
            "start": 453,
            "end": 490,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_65@0",
            "content": "We observe substantial decreases in the hit rates of incorrect predictions in both PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_65",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_65@1",
            "content": "For the MWR task, we find that the issue of regenerating a word in a given query is greatly relieved after applying the IM 2 method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_65",
            "start": 89,
            "end": 220,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_65@2",
            "content": "Specifically, the percentage of such instances drops from 40.3% to 19.6% and from 33.8% to 25.2% for BERT-large and RoBERTa-large, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_65",
            "start": 222,
            "end": 365,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_65@3",
            "content": "Several examples of the predicted results are presented in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_65",
            "start": 367,
            "end": 433,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_65@4",
            "content": "The results lend support to our claim that the IM 2 approach is of benefit to learning lexical semantic information and the meaning of negated expressions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_65",
            "start": 435,
            "end": 589,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_66@0",
            "content": "Fine-Tuning on the GLUE Benchmark",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_66",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@0",
            "content": "A critical drawback of intermediate training is that the target task performance could be negatively impacted if the intermediate task is not related to the target task (Liu et al., 2019a;Pruksachatkun et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@1",
            "content": "To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and ELECTRA-large on 7 GLUE benchmark datasets (Wang et al., 2018) with their IM 2 counterparts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 217,
            "end": 394,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@2",
            "content": "We train the models for 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 396,
            "end": 523,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@3",
            "content": "It is observed that the training is generally finished within 8 epochs for all the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 525,
            "end": 614,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@4",
            "content": "The batch size per GPU and learning rates used for each dataset are described in Table 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 616,
            "end": 704,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_67@5",
            "content": "Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_67",
            "start": 706,
            "end": 808,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@0",
            "content": "The results are presented in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@1",
            "content": "We find no significant difference in performance for tasks with large datasets, such as MNLI, QNLI, QQP, and SST2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 38,
            "end": 151,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@2",
            "content": "On the contrary, tasks with small datasets, like MRPC and RTE, are slightly improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 153,
            "end": 237,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@3",
            "content": "The result is consistent with Pruksachatkun et al. (2020) and Vu et al. (2020), which showed that smaller tasks benefit much more from the intermediate training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 239,
            "end": 399,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@4",
            "content": "Furthermore, unlike the previous studies that observed a negative transfer with the COLA dataset (Phang et al., 2018;Pruksachatkun et al., 2020), the performance is improved in our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 401,
            "end": 590,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_68@5",
            "content": "The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_68",
            "start": 592,
            "end": 723,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_69@0",
            "content": "Experiments on the NegNLI Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_69",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_70@0",
            "content": "Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020), where negation plays an important role for NLI tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_70",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_70@1",
            "content": "As a baseline, we compare the reported performance of BERTNOT (Hosseini et al., 2021), which is a recently proposed remedy to improve PLMs' ability to understand negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_70",
            "start": 142,
            "end": 312,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_70@2",
            "content": "Since Hosseini et al. (2021) used BERT-base as a backbone model, we also apply the IM 2 method to BERT-base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_70",
            "start": 314,
            "end": 421,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_70@3",
            "content": "The results are summarised in Table 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_70",
            "start": 423,
            "end": 460,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_71@0",
            "content": "For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in the NegNLI datasets, while yielding a comparable performance in the original development datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_71",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_71@1",
            "content": "It is interesting that our approach improves the understanding of negation in both MKR-NQ and NegNLI tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_71",
            "start": 175,
            "end": 281,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_71@2",
            "content": "We conjecture that a leading cause is that the definitions of the meaning-matching dataset contain many negation expressions, which enables a model to learn their proposed meaning (see Table 12).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_71",
            "start": 283,
            "end": 477,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_71@3",
            "content": "The results suggest that our proposed approach is more efficient than BERTNOT, because the IM 2 method leverages less time and resources for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_71",
            "start": 479,
            "end": 628,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_72@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_72",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_73@0",
            "content": "PLMs are at the core of many success stories in natural language processing (NLP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_73",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_73@1",
            "content": "However, it remains unclear to what extent PLMs understand the syntactic and semantic properties of the human language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_73",
            "start": 83,
            "end": 201,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_73@2",
            "content": "A series of probing tasks have been conducted on PLMs and have found them lacking or falling short on some language properties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_73",
            "start": 203,
            "end": 329,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_73@3",
            "content": "Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2021;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_73",
            "start": 331,
            "end": 752,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@0",
            "content": "In addition to the above faulty behaviours, Ettinger (2020) and Kassner and Sch\u00fctze (2020) show that PLMs fail to comprehend negation, which is an important property of language in many natural language understanding (NLU) tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@1",
            "content": "Ettinger (2020) check the ability of PLMs to understand the meaning of negation in given contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 230,
            "end": 327,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@2",
            "content": "In their work, they check whether models are sensitive in their completions of sentences that either include negation or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 329,
            "end": 453,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@3",
            "content": "Under normal circumstances, the completions are expected to vary in truth depending on the presence or absence of negation in given sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 455,
            "end": 596,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@4",
            "content": "Their results show that PLMs are insensitive to the impacts of negations when completing sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 598,
            "end": 696,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@5",
            "content": "Kassner and Sch\u00fctze (2020) construct the negated LAMA dataset by inserting negation elements (e.g., \"not\") in the LAMA cloze questions (Petroni et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 698,
            "end": 855,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@6",
            "content": "They use negated and original question pairs to query PLMs and establish that models are equally prone to make the same predictions for both the original and negated questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 857,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@7",
            "content": "In a well-informed setting, it is expected that PLMs should make different predictions for the original and negated questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 1034,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_74@8",
            "content": "This shows that PLMs struggle to comprehend negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_74",
            "start": 1161,
            "end": 1213,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@0",
            "content": "In light of the highlighted faulty behaviours of PLMs, especially their struggle to comprehend negation, Hosseini et al. (2021) propose a remedy to alleviate the problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@1",
            "content": "In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2020) based on negated sentences from the training corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 171,
            "end": 339,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@2",
            "content": "They use a syntactic augmentation method to generate negated sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 341,
            "end": 411,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@3",
            "content": "In this method, the dependency parse of the sentences, POS tags, and morphological information of each word are taken as input, and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 413,
            "end": 677,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@4",
            "content": "During training, they replace objects in negated sentences with [MASK] tokens and use unlikelihood training to make the masked-out tokens unlikely under the PLM distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 679,
            "end": 852,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_75@5",
            "content": "To ensure that negated sentences are factually false, they use the corresponding positive sentences as context for the unlikelihood prediction task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_75",
            "start": 854,
            "end": 1001,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@0",
            "content": "Previous studies (e.g., Kassner and Sch\u00fctze (2020)) have mostly limited the scope of the logical negation property only to the negation expressions (e.g., \"no\" and \"not\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@1",
            "content": "However, the core spirit of this property is the opposite meaning, which is not only limited to the negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 172,
            "end": 280,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@2",
            "content": "Welleck et al. (2020) consider negating sentences using dependency tree regular expression patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 282,
            "end": 381,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@3",
            "content": "This widens the scope of negation, as it is not only limited to the negation expressions \"no\" and \"not\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 383,
            "end": 486,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@4",
            "content": "However, their approach relies on other components, such as Semgrex, and dependency and POS parsers, which could impact the quality of the data, hence impact the models' performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 488,
            "end": 669,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_76@5",
            "content": "In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_76",
            "start": 671,
            "end": 970,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_77@0",
            "content": "Summary and Outlook",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_77",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_78@0",
            "content": "In this work, we investigated PLMs' LNP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_78",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_78@1",
            "content": "Compared to previous works that only examine negation expressions, we expanded the boundary of LNP to lexical semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_78",
            "start": 41,
            "end": 160,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_78@2",
            "content": "We confirmed that PLMs are likely to violate LNP through extensive experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_78",
            "start": 162,
            "end": 240,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_79@0",
            "content": "We hypothesise that the distributional hypothesis is an insufficient basis for understanding the semantic meaning of texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_79",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_79@1",
            "content": "To alleviate the issue, we proposed a novel intermediate task: meaning-matching.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_79",
            "start": 124,
            "end": 203,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_79@2",
            "content": "Via experiments, we verified that meaningmatching is a stable intermediate task that substantially improves PLMs' understanding of negation and lexical semantic information while guaranteeing a positive transfer with multiple downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_79",
            "start": 205,
            "end": 447,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_79@3",
            "content": "Also, our approach produces a better performance on the negated NLI datasets compared to the unlikelihood training-based method, which leverages much more time and resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_79",
            "start": 449,
            "end": 622,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_79@4",
            "content": "Our work suggests that it is time to move beyond the distributional hypothesis to develop logically consistent and stable language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_79",
            "start": 624,
            "end": 761,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_80@0",
            "content": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg, Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 2017, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_80",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_81@0",
            "content": "Laura Aina, Raffaella Bernardi, Raquel Fern\u00e1ndez, A distributional study of negated adjectives and antonyms, 2018, Proceedings of the 5th Italian Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_81",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_82@0",
            "content": "Yonatan Belinkov, James Glass, Analysis methods in neural language processing: A survey, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_82",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_83@0",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_83",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_84@0",
            "content": "UNKNOWN, None, , Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot In Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_84",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_85@0",
            "content": "Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom, Make up your mind! Adversarial generation of inconsistent natural language explanations, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_85",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_86@0",
            "content": "Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill Maccartney, Marie-Catherine De Marneffe, Daniel Ramage, Eric Yeh, Christopher Manning, Learning alignments and leveraging natural logic, 2007, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_86",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_87@0",
            "content": "UNKNOWN, None, 2020, ELECTRA: Pretraining text encoders as discriminators rather than generators. International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_87",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_88@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_88",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_89@0",
            "content": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, Yoav Goldberg, Measuring and improving consistency in pretrained language models, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_89",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_90@0",
            "content": "Allyson Ettinger, What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_90",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_91@0",
            "content": "Ashim Gupta, Giorgi Kvernadze, Vivek Srikumar, Bert & family eat word salad: Experiments with text understanding, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_91",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_92@0",
            "content": "UNKNOWN, None, 1954, Distributional structure. Word, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_92",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_93@0",
            "content": "Matthew Honnibal, Mark Johnson, An improved non-monotonic transition system for dependency parsing, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_93",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_94@0",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_94",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_95@0",
            "content": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Hjelm, Alessandro Sordoni, Aaron Courville, Understanding by understanding not: Modeling negation in language models, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_95",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_96@0",
            "content": "Richard Hudson, About 37% of word-tokens are nouns, 1994, Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_96",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_97@0",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_97",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_98@0",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_98",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_99@0",
            "content": "Junying Liang, Haitao Liu, Noun distribution in natural languages, 2013, Pozna\u0144 Studies in Contemporary Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_99",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_100@0",
            "content": "Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna,  Ren, Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pretrained language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_100",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_101@0",
            "content": "Nelson Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, Noah Smith, Linguistic knowledge and transferability of contextual 2039 representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_101",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_102@0",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_102",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_103@0",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_103",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_104@0",
            "content": "Tom Mccoy, Ellie Pavlick, Tal Linzen, Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_104",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_105@0",
            "content": "I , A \u017dolkovskij, Towards a functioning 'meaning-text' model of language, 1970, Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_105",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_106@0",
            "content": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeff Dean, Distributed representations of words and phrases and their compositionality, 2013, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_106",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_107@0",
            "content": "Jasmina Mili\u0107evi\u0107, A short guide to the meaningtext linguistic theory, 2006, Journal of Koralex, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_107",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_108@0",
            "content": "George Miller, WordNet: A lexical database for English, 1995, Communications of the ACM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_108",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_109@0",
            "content": "Nikola Mrk\u0161i\u0107, \u00d3 Diarmuid, Blaise S\u00e9aghdha, Milica Thomson, Lina Ga\u0161i\u0107, Pei-Hao Rojas-Barahona, David Su, Tsung-Hsien Vandyke, Steve Wen,  Young, Counter-fitting word vectors to linguistic constraints, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_109",
            "start": 0,
            "end": 352,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_110@0",
            "content": "Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, Stress test evaluation for natural language inference, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_110",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_111@0",
            "content": "Timothy Niven, Hung-Yu Kao, Probing neural network comprehension of natural language arguments, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_111",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_112@0",
            "content": "UNKNOWN, None, 2021, Investigating the limitations of transformers with simple arithmetic tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_112",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_113@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, GloVe: Global vectors for word representation, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_113",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_114@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Language models as knowledge bases?, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_114",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_115@0",
            "content": "Thang Pham, Trung Bui, Long Mai, Anh Nguyen, Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_115",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_116@0",
            "content": "UNKNOWN, None, 2018, Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_116",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_117@0",
            "content": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Xiaoyi Phu Mon Htut, Richard Zhang, Clara Pang, Katharina Vania, Samuel Kann, Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?, , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_117",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_118@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_118",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_119@0",
            "content": "Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung, On the systematicity of probing contextualized word representations: The case of hypernymy in BERT, 2020, Proceedings of the 9th Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_119",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_120@0",
            "content": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_120",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_121@0",
            "content": "Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams, UnNatural Language Inference, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_121",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_122@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, ConceptNet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the 31st AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_122",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_123@0",
            "content": "Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer, Exploring and predicting transferability across NLP tasks, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_123",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_124@0",
            "content": "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, Do NLP models know numbers? probing numeracy in embeddings, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_124",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_125@0",
            "content": "Jonas Wallat, Jaspreet Singh, Avishek Anand, BERTnesia: Investigating the capture and forgetting of knowledge in BERT, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_125",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_126@0",
            "content": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R Mccoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, Samuel Bowman, Can you tell me how to get past Sesame Street? Sentence-level pretraining beyond language modeling, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_126",
            "start": 0,
            "end": 417,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_127@0",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_127",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_128@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_128",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "110-ARR_v2_129@0",
            "content": "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, Neural text generation with unlikelihood training, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "110-ARR_v2_129",
            "start": 0,
            "end": 197,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_1",
            "tgt_ix": "110-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_1",
            "tgt_ix": "110-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_4",
            "tgt_ix": "110-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_5",
            "tgt_ix": "110-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_7",
            "tgt_ix": "110-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_10",
            "tgt_ix": "110-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_12",
            "tgt_ix": "110-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_14",
            "tgt_ix": "110-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_13",
            "tgt_ix": "110-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_13",
            "tgt_ix": "110-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_13",
            "tgt_ix": "110-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_17",
            "tgt_ix": "110-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_18",
            "tgt_ix": "110-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_16",
            "tgt_ix": "110-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_16",
            "tgt_ix": "110-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_16",
            "tgt_ix": "110-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_16",
            "tgt_ix": "110-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_20",
            "tgt_ix": "110-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_20",
            "tgt_ix": "110-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_23",
            "tgt_ix": "110-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_24",
            "tgt_ix": "110-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_25",
            "tgt_ix": "110-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_26",
            "tgt_ix": "110-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_27",
            "tgt_ix": "110-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_28",
            "tgt_ix": "110-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_32",
            "tgt_ix": "110-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_33",
            "tgt_ix": "110-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_31",
            "tgt_ix": "110-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_31",
            "tgt_ix": "110-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_31",
            "tgt_ix": "110-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_31",
            "tgt_ix": "110-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_34",
            "tgt_ix": "110-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_38",
            "tgt_ix": "110-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_36",
            "tgt_ix": "110-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_39",
            "tgt_ix": "110-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_41",
            "tgt_ix": "110-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_40",
            "tgt_ix": "110-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_40",
            "tgt_ix": "110-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_40",
            "tgt_ix": "110-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_42",
            "tgt_ix": "110-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_43",
            "tgt_ix": "110-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_43",
            "tgt_ix": "110-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_43",
            "tgt_ix": "110-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_44",
            "tgt_ix": "110-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_46",
            "tgt_ix": "110-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_45",
            "tgt_ix": "110-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_45",
            "tgt_ix": "110-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_45",
            "tgt_ix": "110-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_45",
            "tgt_ix": "110-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_43",
            "tgt_ix": "110-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_50",
            "tgt_ix": "110-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_51",
            "tgt_ix": "110-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_52",
            "tgt_ix": "110-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_55",
            "tgt_ix": "110-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_57",
            "tgt_ix": "110-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_61",
            "tgt_ix": "110-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_62",
            "tgt_ix": "110-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_64",
            "tgt_ix": "110-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_63",
            "tgt_ix": "110-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_63",
            "tgt_ix": "110-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_63",
            "tgt_ix": "110-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_66",
            "tgt_ix": "110-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_66",
            "tgt_ix": "110-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_66",
            "tgt_ix": "110-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_70",
            "tgt_ix": "110-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_69",
            "tgt_ix": "110-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_69",
            "tgt_ix": "110-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_69",
            "tgt_ix": "110-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_71",
            "tgt_ix": "110-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_73",
            "tgt_ix": "110-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_78",
            "tgt_ix": "110-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_77",
            "tgt_ix": "110-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_77",
            "tgt_ix": "110-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_77",
            "tgt_ix": "110-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "110-ARR_v2_0",
            "tgt_ix": "110-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_1",
            "tgt_ix": "110-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_2",
            "tgt_ix": "110-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_3",
            "tgt_ix": "110-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_4",
            "tgt_ix": "110-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_5",
            "tgt_ix": "110-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_5",
            "tgt_ix": "110-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_5",
            "tgt_ix": "110-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_6",
            "tgt_ix": "110-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_7",
            "tgt_ix": "110-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_7",
            "tgt_ix": "110-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_7",
            "tgt_ix": "110-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_8",
            "tgt_ix": "110-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_9",
            "tgt_ix": "110-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_10",
            "tgt_ix": "110-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_11",
            "tgt_ix": "110-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_12",
            "tgt_ix": "110-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_12",
            "tgt_ix": "110-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_13",
            "tgt_ix": "110-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_14",
            "tgt_ix": "110-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_14",
            "tgt_ix": "110-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_14",
            "tgt_ix": "110-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_15",
            "tgt_ix": "110-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_16",
            "tgt_ix": "110-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_17",
            "tgt_ix": "110-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_18",
            "tgt_ix": "110-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_18",
            "tgt_ix": "110-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_18",
            "tgt_ix": "110-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_18",
            "tgt_ix": "110-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_19",
            "tgt_ix": "110-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_20",
            "tgt_ix": "110-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_21",
            "tgt_ix": "110-ARR_v2_21@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_22",
            "tgt_ix": "110-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_23",
            "tgt_ix": "110-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_23",
            "tgt_ix": "110-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_23",
            "tgt_ix": "110-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_24",
            "tgt_ix": "110-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_25",
            "tgt_ix": "110-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_26",
            "tgt_ix": "110-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_26",
            "tgt_ix": "110-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_26",
            "tgt_ix": "110-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_27",
            "tgt_ix": "110-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_28",
            "tgt_ix": "110-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_29",
            "tgt_ix": "110-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_30",
            "tgt_ix": "110-ARR_v2_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_31",
            "tgt_ix": "110-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_32",
            "tgt_ix": "110-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_32",
            "tgt_ix": "110-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_32",
            "tgt_ix": "110-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_33",
            "tgt_ix": "110-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_33",
            "tgt_ix": "110-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_33",
            "tgt_ix": "110-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_34",
            "tgt_ix": "110-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_34",
            "tgt_ix": "110-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_35",
            "tgt_ix": "110-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_36",
            "tgt_ix": "110-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_36",
            "tgt_ix": "110-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_36",
            "tgt_ix": "110-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_37",
            "tgt_ix": "110-ARR_v2_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_38",
            "tgt_ix": "110-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_39",
            "tgt_ix": "110-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_39",
            "tgt_ix": "110-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_39",
            "tgt_ix": "110-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_40",
            "tgt_ix": "110-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_41",
            "tgt_ix": "110-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_41",
            "tgt_ix": "110-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_41",
            "tgt_ix": "110-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_42",
            "tgt_ix": "110-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_42",
            "tgt_ix": "110-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_42",
            "tgt_ix": "110-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_42",
            "tgt_ix": "110-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_43",
            "tgt_ix": "110-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_44",
            "tgt_ix": "110-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_45",
            "tgt_ix": "110-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_46",
            "tgt_ix": "110-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_46",
            "tgt_ix": "110-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_47",
            "tgt_ix": "110-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_48",
            "tgt_ix": "110-ARR_v2_48@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_49",
            "tgt_ix": "110-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_50",
            "tgt_ix": "110-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_50",
            "tgt_ix": "110-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_50",
            "tgt_ix": "110-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_50",
            "tgt_ix": "110-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_51",
            "tgt_ix": "110-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_51",
            "tgt_ix": "110-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_51",
            "tgt_ix": "110-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_52",
            "tgt_ix": "110-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_52",
            "tgt_ix": "110-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_52",
            "tgt_ix": "110-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_52",
            "tgt_ix": "110-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_53",
            "tgt_ix": "110-ARR_v2_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_54",
            "tgt_ix": "110-ARR_v2_54@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_55",
            "tgt_ix": "110-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_55",
            "tgt_ix": "110-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_55",
            "tgt_ix": "110-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_55",
            "tgt_ix": "110-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_56",
            "tgt_ix": "110-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_57",
            "tgt_ix": "110-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_57",
            "tgt_ix": "110-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_58",
            "tgt_ix": "110-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_59",
            "tgt_ix": "110-ARR_v2_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_60",
            "tgt_ix": "110-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_61",
            "tgt_ix": "110-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_62",
            "tgt_ix": "110-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_62",
            "tgt_ix": "110-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_62",
            "tgt_ix": "110-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_63",
            "tgt_ix": "110-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_64",
            "tgt_ix": "110-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_64",
            "tgt_ix": "110-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_64",
            "tgt_ix": "110-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_64",
            "tgt_ix": "110-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_65",
            "tgt_ix": "110-ARR_v2_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_66",
            "tgt_ix": "110-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_67",
            "tgt_ix": "110-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_68",
            "tgt_ix": "110-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_69",
            "tgt_ix": "110-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_70",
            "tgt_ix": "110-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_70",
            "tgt_ix": "110-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_70",
            "tgt_ix": "110-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_70",
            "tgt_ix": "110-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_71",
            "tgt_ix": "110-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_71",
            "tgt_ix": "110-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_71",
            "tgt_ix": "110-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_71",
            "tgt_ix": "110-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_72",
            "tgt_ix": "110-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_73",
            "tgt_ix": "110-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_73",
            "tgt_ix": "110-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_73",
            "tgt_ix": "110-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_73",
            "tgt_ix": "110-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_74",
            "tgt_ix": "110-ARR_v2_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_75",
            "tgt_ix": "110-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_76",
            "tgt_ix": "110-ARR_v2_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_77",
            "tgt_ix": "110-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_78",
            "tgt_ix": "110-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_78",
            "tgt_ix": "110-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_78",
            "tgt_ix": "110-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_79",
            "tgt_ix": "110-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_79",
            "tgt_ix": "110-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_79",
            "tgt_ix": "110-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_79",
            "tgt_ix": "110-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_79",
            "tgt_ix": "110-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_80",
            "tgt_ix": "110-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_81",
            "tgt_ix": "110-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_82",
            "tgt_ix": "110-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_83",
            "tgt_ix": "110-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_84",
            "tgt_ix": "110-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_85",
            "tgt_ix": "110-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_86",
            "tgt_ix": "110-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_87",
            "tgt_ix": "110-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_88",
            "tgt_ix": "110-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_89",
            "tgt_ix": "110-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_90",
            "tgt_ix": "110-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_91",
            "tgt_ix": "110-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_92",
            "tgt_ix": "110-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_93",
            "tgt_ix": "110-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_94",
            "tgt_ix": "110-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_95",
            "tgt_ix": "110-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_96",
            "tgt_ix": "110-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_97",
            "tgt_ix": "110-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_98",
            "tgt_ix": "110-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_99",
            "tgt_ix": "110-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_100",
            "tgt_ix": "110-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_101",
            "tgt_ix": "110-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_102",
            "tgt_ix": "110-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_103",
            "tgt_ix": "110-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_104",
            "tgt_ix": "110-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_105",
            "tgt_ix": "110-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_106",
            "tgt_ix": "110-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_107",
            "tgt_ix": "110-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_108",
            "tgt_ix": "110-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_109",
            "tgt_ix": "110-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_110",
            "tgt_ix": "110-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_111",
            "tgt_ix": "110-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_112",
            "tgt_ix": "110-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_113",
            "tgt_ix": "110-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_114",
            "tgt_ix": "110-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_115",
            "tgt_ix": "110-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_116",
            "tgt_ix": "110-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_117",
            "tgt_ix": "110-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_118",
            "tgt_ix": "110-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_119",
            "tgt_ix": "110-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_120",
            "tgt_ix": "110-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_121",
            "tgt_ix": "110-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_122",
            "tgt_ix": "110-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_123",
            "tgt_ix": "110-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_124",
            "tgt_ix": "110-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_125",
            "tgt_ix": "110-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_126",
            "tgt_ix": "110-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_127",
            "tgt_ix": "110-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_128",
            "tgt_ix": "110-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "110-ARR_v2_129",
            "tgt_ix": "110-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 964,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "110-ARR",
        "version": 2
    }
}