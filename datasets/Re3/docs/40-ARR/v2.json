{
    "nodes": [
        {
            "ix": "40-ARR_v2_0",
            "content": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_2",
            "content": "Learned self-attention functions in state-of-theart NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts. Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lowerentropy attention vectors are more faithful.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "40-ARR_v2_4",
            "content": "The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Zhang and Zhang, 2019;Klerke and Plank, 2019). In this paper, we evaluate how well attention flow (Abnar and Zuidema, 2020) in large language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), aligns with human eye fixations during task-specific reading, compared to other shallow sequence labeling models (Lecun and Bengio, 1995;Vaswani et al., 2017) and a classic, heuristic model of human reading (Reichle et al., 2003). We compare the learned attention functions and the heuristic model across two taskspecific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available dataset with eye-tracking recordings of native speakers of English .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_5",
            "content": "We compare human and model attention patterns on both sentiment reading and relation extraction tasks. In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995;Vaswani et al., 2017), as well as to a frequency baseline and a heuristic, cognitively inspired model of human reading called the E-Z Reader (Reichle et al., 2003). We find that the heuristic model correlates well with human reading, as has been reported in Sood et al. (2020b). However when we apply attention flow (Abnar and Zuidema, 2020), the pre-trained Transformer models also reach comparable levels of correlation strength. Further fine-tuning experiments on BERT did not result in increased correlation to human fixations. To understand what drives the differences between models, we perform an in-depth analysis of the effect of word predictability and POS tags on correlation strength. It reveals that Transformer models do not accurately capture tail phenomena for hard-to-predict words (in contrast to the E-Z Reader) and that Transformer attention flow shows comparably weak correlation on (proper) nouns while the E-Z Reader predicts importance of these more accurately, especially on the sentiment reading task. In addition, we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns. We test faithfulness of these different attention patterns to produce the correct classification via an input reduction experiment on task-tuned BERT models. Our results highlight the trade-off between model faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors tend to be less faithful with respect to model predictions. Our code is available at github.com/ oeberle/task_gaze_transformers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_6",
            "content": "Pre-trained Language Models vs",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "40-ARR_v2_7",
            "content": "Cognitive Models Church and Liberman (2021) discuss how NLP has historically benefited from rationalist and empiricist methodologies, something that holds for cognitive modeling in general. The vast majority of application-oriented work in NLP today relies on pre-trained language models or other largescale data-driven models, but in cognitive modeling, most approaches remain heuristic and rulebased, or hybrid, e.g., relying on probabilistic language models to quantify surprisal (Rayner and Reichle, 2010;Milledge and Blythe, 2019). This is for good reasons: Cognitive modeling values interpretability (even) more, often suffers from data scarcity, and is less concerned with model reusability across different contexts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_8",
            "content": "This paper presents a head-to-head comparison of the E-Z Reader and pre-trained Transformerbased language models. We are not the first to evaluate pre-trained language models and largescale data-driven models as if they were cognitive models. Chrupa\u0142a and Alishahi (2019), for example, use representational similarity analysis to correlate sentence encodings in pre-trained language models with fMRI signals; Abdou et al. (2019) correlate sentence encodings with gaze-derived representations. More generally, it has been argued that cognitive evaluations are in some cases practically superior to standard evaluation methodologies in NLP (S\u00f8gaard, 2016;Hollenstein et al., 2019). We return to this in the Discussion and Conclusion \u00a76.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_9",
            "content": "Commonly, pre-trained language models are disregarded as cognitive models, since they are most often implemented as computationally demanding batch learning algorithms, processing data \"at once\". G\u00fcnther et al. (2019) points out that this is an artefact of their implementation, and online learning of pre-trained language models is possible, yet impractical. Generally, several researchers have argued for taking pre-trained language models seriously as cognitive models (Rogers and Wolmetz, 2016;Mandera et al., 2017;G\u00fcnther et al., 2019). In the last section, \u00a76, we discuss some of the implications of comparisons of pre-trained language models and cognitive models -for cognitive modeling, as well as for NLP. In our experiments, we focus on Transformer architectures that are currently the dominating pre-trained language models and a de facto baseline for modern NLP research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_10",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "40-ARR_v2_11",
            "content": "Data",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "40-ARR_v2_12",
            "content": "The ZuCo dataset contains eye-tracking data for 12 participants (all English native speakers) performing natural reading and relation extraction on 300 and 407 English sentences from the Wikipedia relation extraction corpus (Culotta et al., 2006) respectively and sentiment reading on 400 samples of the Stanford Sentiment Treebank (SST) (Socher et al., 2013). For our analysis, we extract and average word-based total fixation times across participants and focus on the task-specific relation extraction and sentiment reading samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_13",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "40-ARR_v2_14",
            "content": "Below we briefly describe our used models and refer to Appendix A for more details.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_15",
            "content": "Transformers The superior performance of Transformer architectures across broad sets of NLP tasks raises the question of how task-related attention patterns really are. In our experiments, we focus on comparing task-modulated human fixations to attention patterns extracted from the following commonly used models: (a) We use both pre-trained uncased BERT-base and large models (Devlin et al., 2019) as well as fine-tuned BERT models on the respective tasks. BERT was originally pre-trained on the English Wikipedia and the BookCorpus. (b) The RoBERTa model has the same architecture as BERT and demonstrates better performance on downstream tasks using an improved pre-training scheme and the use of additional news article data (Liu et al., 2019). (c) The Text-to-Text Transfer Transformer (T5) uses an encoder-decoder structure to enable parallel tasktraining and has demonstrated state-of-the-art performance over several transfer tasks including sentiment analysis and natural language inference (Raffel et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_16",
            "content": "We evaluate different ways of extracting tokenlevel importance scores: We collect attention representations and compute the mean attention vector over the final layer heads to capture the mixing of information in Transformer self-attention modules as in and present this as mean for all aforementioned Transformers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_17",
            "content": "To capture the layer-wise structure of deep Transformer models we compute attention flow (Abnar and Zuidema, 2020). This approach considers the attention matrices as a graph, where tokens are represented as nodes and attention scores as edges between consecutive layers. The edge values define the maximal flow possible between a pair of nodes. Flow between edges is thus (i) limited to the maximal attention between any two consecutive layers for this token and (ii) conserved such that the sum of incoming flow must be equal to the sum of outgoing flow. We denote the attention flow propagated back from layer L as flow L.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_18",
            "content": "We ground our analysis on Transformers by comparing them to relatively shallow models that were trained from-scratch and evaluate how well they coincide with human fixation. We train a standard CNN (Kim, 2014) network with multiple filter sizes on pre-trained GloVe embeddings (Pennington et al., 2014). Importance scores over tokens are extracted using Layerwise Relevance Propagation (LRP) (Arras et al., 2016(Arras et al., , 2017 which has been demonstrated to produce robust explanations by iterating over layers and redistributing relevance from outer layers towards the input (Bach et al., 2015;Samek et al., 2021). In parallel, we use a shallow multi-head self-attention network (Lin et al., 2017) on GloVe vectors with a linear read-out layer for which we compute token relevance scores using LRP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_19",
            "content": "As a cognitive model for human reading, we compute task-neutral fixation times using the E-Z Reader (Reichle et al., 1998) model. The E-Z Reader is a multi-stage, hybrid model, which relies on an n-gram model and several heuristics, based, for example, on theoretical assumptions about the role of predictability and average saccade length. Additionally, we compare to a frequency baseline using word statistics of the BNC (British National Corpus, Kilgarriff (1995)) 1 as proposed by Barrett et al. (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_20",
            "content": "Optimization",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "40-ARR_v2_21",
            "content": "For training models on the different tasks we remove all sentences that overlap between ZuCo and the original SST and Wikipedia datasets. Models are then trained on the remaining train-split data until early stopping is reached and we report results over five runs. We provide further details on the optimization and model task performance in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_22",
            "content": "Metric",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "40-ARR_v2_23",
            "content": "To compare models with human attention, we compute Spearman correlation between human and model-based importance vectors after concatenation of individual sentences as well as on a tokenlevel, see . This enables us to distinguish unrelated effects caused by varying sentence length from token-level importance. As described before, we extract human attention from gaze (ZuCo), simulated gaze (E-Z Reader), raw attentions (BERT, RoBERTa, T5), relevance scores (CNN, self-attention) and inverse token probability scores (BNC). 2 We use ZuCo tokens to align sentences across tokenizers and apply max-pooling of scores when bins are merged.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_24",
            "content": "Main result",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "40-ARR_v2_25",
            "content": "To evaluate how well model and human attention patterns for sentiment reading and relation extraction align, we compute pair-wise correlation scores as displayed in Figure 1. Reported correlations are statistically significant with p < 0.01 if not indicated otherwise (ns: not significant). After ranking based on the correlations on sentence-level, we observe clear differences between sentiment reading on SST and relation extraction on Wikipedia for the different models. For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < 0.05 ). For relation extraction, we see the highest correlation for BERTbase attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < 0.05). On the lower end, computing means over BERT attentions across the last layer shows weak to no correlations for both tasks. 3 The shallow architectures result in low to moderate correlations with a distinctive gap to attention flow. Focusing on flow values for Transformers, BNC and E-Z Reader, correlations are stable across word and sentence length. Correlations grouped by sentence length shows stable values around 0.6 (SST) and 0.4 \u2212 0.6 (Wikipedia) except for shorter sentences where correlations fluctuate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_26",
            "content": "To check the linear relationship between human and model attention patterns we additionally compute token-and sentence-level Pearson correlations which can be found in Appendix B. Results confirm that Spearman and Pearson correlation coefficients as well as rankings hardly differ -which suggests a linear relationship -and that correlation strength is in line with .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_27",
            "content": "Analyses",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "40-ARR_v2_28",
            "content": "In addition to our main result -that pre-trained language models are competitive to heuristic cognitive models in predicting human eye fixations during reading -we present a detailed analysis, investigating what our main results depend on, where pre-trained language models improve on cognitive models, and where they are still challenged.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_29",
            "content": "Fine-tuning BERT does not change correlations to human attention We find that fine-tuning base and large BERT models on either task does not significantly change correlations and are of similar strength to untuned models. This observation can be embedded into findings that Transformers are equipped with overcomplete sets of attention functions that hardly change until the later layers, if at all, during fine-tuning and that this change is also dependent on the tuning task itself (Kovaleva et al., 2019;Zhao and Bethard, 2020). In addition, we observe that Transformer flows propagated back from early, medium and final layers do not considerably change correlations to human attention. This can be explained by attention flow filtering the path of minimal value at each layer as discussed in Abnar and Zuidema (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_30",
            "content": "Attention flow is important The correlation analysis emphasizes that we need to capture the layered propagation structure in Transformer models, e.g., by using attention flow, in order to extract importance scores that are competitive with cognitive models. Interestingly, selecting the highest correlating head for the last attention layer produces generally weaker correlation than attention flows. 3 This offers additional evidence that raw attention weights do not reliably correspond to token relevance (Serrano and Smith, 2019;Abnar and Zuidema, 2020) and, thus, are of limited use to compare task attention to human gaze.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_31",
            "content": "Differences between language models BERT, RoBERTa and T5 are large-scale pretrained language models based on Transformers, but they also differ in various ways. One key difference is that BERT and RoBERTa use absolute position encodings, while T5 uses relative encodings. BERT and RoBERTa differ in that (i) BERT has a next-sentence-prediction auxiliary objective; (ii) RoBERTa and T5 were trained on more data; (iii) RoBERTa uses dynamic masking and trains with larger mini-batches and learning rates, while T5 uses multi-word masking; (iv) RoBERTa uses byte pair encoding for subword segmentation. We leave it as an open question whether the superior attention flows of BERT, compared to RoBERTa and T5, has to do with training data, next sentence prediction, or fortunate hyper-parameter settings, but note that BERT is also known to have higher alignment with human-generated explanations than other large-scale pre-trained language models (Prasad et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_32",
            "content": "E-Z Reader is less sensitive to hard-to-predict words and POS We compare correlations to human fixations with attention flow values for Transformer models in the last layer, the E-Z Reader and the BNC baseline for different word predictability scores computed with a 5-gram Kneser-Ney language model (Kneser and Ney, 1995;Chelba et al., 2013). Figure 3 shows the results on SST and Wikipedia for equally sized bins of word predictability scores. We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (0.3 \u2212 0.4 for both, SST and Wikipedia). In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability > 0.03).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_33",
            "content": "We also compare correlations to human fixations Input reduction When comparing machines to humans we typically regard the psychophysical data as the gold standard. We will now take the model perspective and test fidelity of both human and model attention patterns in task-tuned models. By this we aim to test how effective the exact token ranking based on attention scores is at producing the correct output probability. We perform such an input reduction analysis (Feng et al., 2018) our analysis, we observe -as to be expected -that adding tokens according to token probability (BNC prob) performs even worse than randomly adding tokens. From-scratch trained models (CNN and self-attention) are most effective in selecting taskrelevant tokens, and even more so than using any Transformer attention flow. Adding tokens based on human attention is as effective for the sentiment task as the E-Z Reader. Interestingly, for the relation extraction task, human attention vectors provide the most effective flipping order after the relevance-based shallow methods. All Transformerbased flows perform comparably in both tasks. To better understand what drives these effects we extract the fraction of POS tags for the first added token (see Figure 4 and full results in the Appendix Figure 5). Natural reading versus task-specific reading A unique feature of the ZuCo dataset is that it contains a subset of sentences that were presented to participants both in a task-specific (relation extraction) and a natural reading setting. This allows for a direct comparison of how correlation strength influenced by the task. In correlations of human gaze-based attention with model attentions are shown. The highest correlation can be observed when comparing human attention for task-specific and natural reading (0.72). The remaining model correlations correspond to the ranking and correlation strength observed in the main result (see Figure 1). We observe lower correlation scores for the task-specific reading as compared to normal reading among attention flow, the E-Z Reader and BNC. This suggests that these models capture the statistics of natural reading -as is expected for a cognitive model designed to the natural reading paradigm -and that task-related changes in human fixation patterns are not reflected in Transformer attention flows. Interestingly, averaged last layer attention heads show a reverse effect (but at much weaker correlation strength). This might suggest that pre-training in Transformer models induces specificity of later layer attention heads to tasksolving instead of general natural reading patterns.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_34",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "40-ARR_v2_35",
            "content": "Saliency modeling Early computational models of visual attention have used bottom-up approaches to model the neural circuitry representing pre-attentive selection processes from visual input (Koch and Ullman, 1985) and later the central idea of a saliency map was introduced (Niebur and Koch, 1996). A central hypothesis studying eye movements under task conditions is known as Yarbus theorem stating that a task can be directly decoded from fixation patterns (Yarbus, 1967) which has found varying support (Greene et al., 2012;Henderson et al., 2013;Borji and Itti, 2014). More recently, extracting features from deep pre-trained filters in combination with readout networks has boosted performance on the saliency task (K\u00fcmmerer et al., 2016). This progress has enabled modeling of more complex gaze patterns, e.g. vision-language tasks such as image captioning (Sugano and Bulling, 2016), visual question answering (Das et al., 2016) or text-guided object detection (Vasudevan et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_36",
            "content": "Predicting text gaze patterns has been studied extensively, often in the context of probabilistic (Feng, 2006;Hara et al., 2012;Matthies and S\u00f8gaard, 2013;Hahn and Keller, 2016) or token transition models (Nilsson and Nivre, 2009;Haji-Abolhassani and Clark, 2014;Coutrot et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_37",
            "content": "More recently deep language features have been used as feature extractors in modeling text saliency (Sood et al., 2020a; opening the question of their cognitive plausibility.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_38",
            "content": "Eye-tracking signals for NLP Augmenting machine learning models using human gaze information has been shown to improve performance for a number of different settings: Human attention patterns as regularization during model training have resulted in comparable or improved task performance in tagging part-of-speech (Barrett and S\u00f8gaard, 2015a,b;Barrett et al., 2018), sentence compression (Klerke et al., 2016), detecting sentiment (Mishra et al., 2016(Mishra et al., , 2017 or reading comprehension (Malmaud et al., 2020). In these works, general free-viewing gaze data is used without consideration of the specific training task which opens the question of task-modulation in human reading.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_39",
            "content": "From natural to task-specific reading Recent work on reading often analyses eye-tracking data in combination with neuroimaging techniques such as EEG (Wenzel et al., 2017) and f-MRI (Hillen et al., 2013;Choi et al., 2014). Research questions thereby focus either on detecting relevant parts in text (Loboda et al., 2011;Wenzel et al., 2017) or the difference between natural and pseudo-reading, i.e., text without syntax/semantics (Hillen et al., 2013) or pseudo-words (Choi et al., 2014). To the best of our knowledge there has not been any work on comparing fixations between natural reading and task-specific reading on classical NLP tasks such as relation extraction or sentiment classification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_40",
            "content": "Discussion and Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "40-ARR_v2_41",
            "content": "In this paper, we have compared attention and relevance mechanisms of a wide range of models to human gaze patterns when solving sentiment classification on SST movie reviews and relation extraction on Wikipedia articles. We generally found that Transformer architectures are competitive with the E-Z Reader, but only when computing attention flow scores. We generally saw weaker correlations for relation extraction on Wikpedia, presumably due to simpler sentence structures and the occurrence of polarity words. In the following, we discuss implications of our findings on NLP and Cognitive Science in more detail.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_42",
            "content": "Lessons for NLP One implication of the above for NLP follows from the importance of attention flow in our experiments: Using human gaze or supervise attention weights has proven effective in previous work ( \u00a75), but we observed that correlations with task-specific human attention increase significantly by using layer-dependent attention flow compared to using raw attention weights. This insight motivates going beyond regularizing raw attention weights or directly injecting human attention vectors during training, to instead optimize for correlation between attention flow and human attention. Jointly modeling language and human gaze has recently shown to yield competitive performance on paraphrase generation and sentence compression while resulting in more taskspecific attention heads (Sood et al., 2020b). For this study natural gaze patterns were also simulated using the E-Z Reader.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_43",
            "content": "Another potential implication concerns interpretability. It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide meaningful explanations for model predictions. Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022). A successful explanation of a machine learning model should be faithful, human-interpretable and practical to apply (Samek et al., 2021). Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity. By contrast, judging human-interpretability typically requires costly experiments in well-controlled settings and obtaining human gold-standards for interpretability remain difficult (Miller, 2019;Schmidt and Bie\u00dfmann, 2019). Using gaze data to evaluate the faithfulness and trustworthiness of machine learning models is a promising approach to increase model transparency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_44",
            "content": "Lessons for Cognitive Science Attention flow in Transformers, especially for BERT models, correlates surprisingly well with human task-specific reading, but what does this tell us about the shortcomings of our cognitive models? We know that word frequency and semantic relationships between words influence word fixation times (Rayner, 1998).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_45",
            "content": "In our experiments, we see relatively high correlation between human fixations and the inverse word probability baseline which raises the question to what extent reading gaze is driven by low-level patterns such as word frequency or syntactic structure in contrast to more high-level semantic context or wrap-up effects.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_46",
            "content": "In computer vision, cognitively inspired bottomup models, e.g., using intensity and contrast features, are able to explain at most half of the gaze fixation information in comparison to the human gold standard (K\u00fcmmerer et al., 2017). The robustness of the E-Z Reader on movie reviews is likely due to its explicit modeling of low-level properties such as word frequency or sentence length. BERT was recently shown to be primarily modeling higherorder word co-occurrence statistics (Sinha et al., 2021). We argue that while Transformers are limited, e.g., in not capturing the dependency of human gaze on word length (Kliegl et al., 2004), cognitive models seem to underestimate the role of word co-occurrence statistics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_47",
            "content": "During reading, humans are faced with a tradeoff between the precision of reading comprehension and reading speed, by avoiding unnecessary fixations (Hahn and Keller, 2016). This trade-off is related to the input reduction experiments performed in Section 4. Here, we observe that shallow methods score well at being sparse and effective in changing model output towards the correct class, but produce only weak correlation to human reading patterns when compared to layered language models. In comparison, extracted attention flow from pre-trained Transformer models correlates much better with human attention, but offers less sparse token attention. In other words, our results show that task-specific reading is sub-optimal relative to solving tasks and heavily regularized by natural reading patterns (see also our comparison of task-specific and natural reading in Section 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_48",
            "content": "In our experiments, we first and foremost found that Transformers, and especially BERT models, are competitive to the E-Z Reader in terms of explaining human attention in taskspecific reading. For this to be the case, computing attention flow scores (rather than raw attention weights) is important. Even so, the E-Z Reader remains better at hard-to-predict words and is less sensitive to part of speech. While Transformers thus have some limitations compared to the E-Z Reader, our results indicate that cognitive models have placed too little weight on high-level word cooccurrence statistics. Generally, Transformers and the E-Z Reader correlate much better with human attention than other, shallow from-scratch trained sequence labeling architectures. Our input reduction suggest that in a sense, trained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_49",
            "content": "In the following we present details for all modes and describe the training details used for tasktunning. Model performance over five runs is reported in Table 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_50",
            "content": "The CNN models use 300-dimensional pre-trained GloVe_840B (Pennington et al., 2014) embeddings. Input sentences are tokenized using the SpaCy tokenizer (Honnibal et al., 2020). We use 150 convolutional filters of filter sizes s = [3, 4, 5] with ReLu activation, followed by a max-pooling-layer and apply dropout of p = 0.5 of the linear classification layer during training. For training we use a batchsize of bs = 50 and train all model parameters using the Adam optimizer with a learning rate of lr = 1e \u2212 4 for a maximum number of T = 20 epochs. For all model trainings, we apply early stopping to avoid overfitting during training and stop optimization as soon as the validation loss begins to increase. To compute LRP relevances we use the general formulation of LRP propagation rules with \u03b3 = 0. for the linear readout layers (Montavon et al., 2019). We take absolute values over resulting relevance scores since we find they correlate best with human attention in comparison to raw and rectified processing. For propagation through the max-pooling layer we apply the winner-take-all principle and for convolutional layers we use the LRP-\u03b3 redistribution rule and select \u03b3 = 0.5 after a search over \u03b3 = [0., 0.25, 0.5, 0.75, 1.0] resulting in largest correlations to human attention.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_51",
            "content": "For the multi-head self-attention model again use 300-dimensional pre-trained GloVe_840B embeddings and tokenized via SpaCy. The architecture consists of a set of k = 3 self-attention heads for the SR task and k = 8 for REL. The resulting sentence representation is then fed into a linear classification readout layer with \u03b3 = 0. and which we also use for the propagation to input embeddings. During optimization we use lr = 1e \u2212 4, bs = 50 and T = 50.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_52",
            "content": "We use standard BERT-base/large-uncased architectures and tokenizers as provided by the huggingface library (Wolf et al., 2020). For BERTbase fine-tuning we use lr = 1e \u2212 5 for REL and lr = 1e \u2212 6 for SR, bs = 32 and T = 50 for both tasks. For BERT-large we use lr = 1e \u2212 5 for REL and lr = 5e \u2212 7 for SR, bs = 16 and T = 50. For RoBERTa and T5 we use the RoBERTa-base and T5-base checkpoints and respective tokenizers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_53",
            "content": "We use version 10.2 of the E-Z Reader with default parameters and 1000 repetitions. Cloze scores, i.e. word predictability scores, were therefore computed using a 5-gram Kneser-Ney language model (Kneser and Ney, 1995) as provided by the SRI Language Modeling Toolkit (Stolcke, 2002) and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "40-ARR_v2_54",
            "content": "Mostafa Abdou, Artur Kulmizev, Felix Hill, Daniel Low, Anders S\u00f8gaard, Higher-order comparisons of sentence encoder representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Mostafa Abdou",
                    "Artur Kulmizev",
                    "Felix Hill",
                    "Daniel Low",
                    "Anders S\u00f8gaard"
                ],
                "title": "Higher-order comparisons of sentence encoder representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_55",
            "content": "Samira Abnar, Willem Zuidema, Quantifying attention flow in transformers, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Samira Abnar",
                    "Willem Zuidema"
                ],
                "title": "Quantifying attention flow in transformers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_56",
            "content": "UNKNOWN, None, , Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, and Lior Wolf. 2022. XAI for transformers: Better explanations through conservative propagation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, and Lior Wolf. 2022. XAI for transformers: Better explanations through conservative propagation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "40-ARR_v2_57",
            "content": "Leila Arras, Franziska Horn, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Wojciech Samek, Explaining predictions of non-linear classifiers in NLP, 2016, Proceedings of the 1st Workshop on Representation Learning for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Leila Arras",
                    "Franziska Horn",
                    "Gr\u00e9goire Montavon",
                    "Klaus-Robert M\u00fcller",
                    "Wojciech Samek"
                ],
                "title": "Explaining predictions of non-linear classifiers in NLP",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 1st Workshop on Representation Learning for NLP",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_58",
            "content": "Leila Arras, Franziska Horn, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Wojciech Samek, What is relevant in a text document?\": An interpretable machine learning approach, 2017, PLOS ONE, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Leila Arras",
                    "Franziska Horn",
                    "Gr\u00e9goire Montavon",
                    "Klaus-Robert M\u00fcller",
                    "Wojciech Samek"
                ],
                "title": "What is relevant in a text document?\": An interpretable machine learning approach",
                "pub_date": "2017",
                "pub_title": "PLOS ONE",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_59",
            "content": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, Wojciech Samek, On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation, 2015, PLoS ONE, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Sebastian Bach",
                    "Alexander Binder",
                    "Gr\u00e9goire Montavon",
                    "Frederick Klauschen",
                    "Klaus-Robert M\u00fcller",
                    "Wojciech Samek"
                ],
                "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
                "pub_date": "2015",
                "pub_title": "PLoS ONE",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_60",
            "content": "Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, Anders S\u00f8gaard, Sequence classification with human attention, 2018, Proceedings of the 22nd Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Maria Barrett",
                    "Joachim Bingel",
                    "Nora Hollenstein",
                    "Marek Rei",
                    "Anders S\u00f8gaard"
                ],
                "title": "Sequence classification with human attention",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 22nd Conference on Computational Natural Language Learning",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_61",
            "content": "Maria Barrett, Anders S\u00f8gaard, Reading behavior predicts syntactic categories, 2015, Proceedings of the Nineteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Maria Barrett",
                    "Anders S\u00f8gaard"
                ],
                "title": "Reading behavior predicts syntactic categories",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_62",
            "content": "Maria Barrett, Anders S\u00f8gaard, Using reading behavior to predict grammatical functions, 2015, Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Maria Barrett",
                    "Anders S\u00f8gaard"
                ],
                "title": "Using reading behavior to predict grammatical functions",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_63",
            "content": "Ali Borji, Laurent Itti, Defending yarbus: eye movements reveal observers' task, 2014, Journal of vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ali Borji",
                    "Laurent Itti"
                ],
                "title": "Defending yarbus: eye movements reveal observers' task",
                "pub_date": "2014",
                "pub_title": "Journal of vision",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_64",
            "content": "Hila Chefer, Shir Gur, Lior Wolf, Generic attention-model explainability for interpreting bimodal and encoder-decoder transformers, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Hila Chefer",
                    "Shir Gur",
                    "Lior Wolf"
                ],
                "title": "Generic attention-model explainability for interpreting bimodal and encoder-decoder transformers",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_65",
            "content": "Hila Chefer, Shir Gur, Lior Wolf, Transformer interpretability beyond attention visualization, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Hila Chefer",
                    "Shir Gur",
                    "Lior Wolf"
                ],
                "title": "Transformer interpretability beyond attention visualization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_66",
            "content": "UNKNOWN, None, 2013, One billion word benchmark for measuring progress in statistical language modeling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "One billion word benchmark for measuring progress in statistical language modeling",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_67",
            "content": "Wonil Choi, H Rutvik, John M Desai,  Henderson, The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri, 2014, Frontiers in human neuroscience, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Wonil Choi",
                    "H Rutvik",
                    "John M Desai",
                    " Henderson"
                ],
                "title": "The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri",
                "pub_date": "2014",
                "pub_title": "Frontiers in human neuroscience",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_68",
            "content": "Grzegorz Chrupa\u0142a, Afra Alishahi, Correlating neural and symbolic representations of language, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Grzegorz Chrupa\u0142a",
                    "Afra Alishahi"
                ],
                "title": "Correlating neural and symbolic representations of language",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_69",
            "content": "Kenneth Church, Mark Liberman, The future of computational linguistics: On beyond alchemy, 2021, Frontiers in Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Kenneth Church",
                    "Mark Liberman"
                ],
                "title": "The future of computational linguistics: On beyond alchemy",
                "pub_date": "2021",
                "pub_title": "Frontiers in Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_70",
            "content": "Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher Manning, What does BERT look at? an analysis of BERT's attention, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Italy. Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Kevin Clark",
                    "Urvashi Khandelwal",
                    "Omer Levy",
                    "Christopher Manning"
                ],
                "title": "What does BERT look at? an analysis of BERT's attention",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Italy. Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_71",
            "content": "UNKNOWN, None, 2017, Scanpath modeling and classification with hidden markov models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Scanpath modeling and classification with hidden markov models",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_72",
            "content": "Aron Culotta, Andrew Mccallum, Jonathan Betz, Integrating probabilistic extraction models and data mining to discover relations and patterns in text, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Aron Culotta",
                    "Andrew Mccallum",
                    "Jonathan Betz"
                ],
                "title": "Integrating probabilistic extraction models and data mining to discover relations and patterns in text",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_73",
            "content": "Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, Dhruv Batra, Human attention in visual question answering: Do humans and deep networks look at the same regions?, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Abhishek Das",
                    "Harsh Agrawal",
                    "Larry Zitnick",
                    "Devi Parikh",
                    "Dhruv Batra"
                ],
                "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions?",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_74",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_75",
            "content": "Gary Feng, Eye movements as time-series random variables: A stochastic model of eye movement control in reading, 2006, Cognitive Systems Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Gary Feng"
                ],
                "title": "Eye movements as time-series random variables: A stochastic model of eye movement control in reading",
                "pub_date": "2006",
                "pub_title": "Cognitive Systems Research",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_76",
            "content": "Eric Shi Feng, Alvin Wallace, I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Pathologies of neural models make interpretations difficult, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Eric Shi Feng",
                    "Alvin Wallace",
                    "I Grissom",
                    "Mohit Iyyer",
                    "Pedro Rodriguez",
                    "Jordan Boyd-Graber"
                ],
                "title": "Pathologies of neural models make interpretations difficult",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_77",
            "content": "Michelle Greene, Tommy Liu, Jeremy Wolfe, Reconsidering yarbus: A failure to predict observers' task from eye movement patterns, 2012, Vision research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Michelle Greene",
                    "Tommy Liu",
                    "Jeremy Wolfe"
                ],
                "title": "Reconsidering yarbus: A failure to predict observers' task from eye movement patterns",
                "pub_date": "2012",
                "pub_title": "Vision research",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_78",
            "content": "Fritz G\u00fcnther, Luca Rinaldi, Marco Marelli, Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions, 2019, Perspectives on Psychological Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Fritz G\u00fcnther",
                    "Luca Rinaldi",
                    "Marco Marelli"
                ],
                "title": "Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions",
                "pub_date": "2019",
                "pub_title": "Perspectives on Psychological Science",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_79",
            "content": "Michael Hahn, Frank Keller, Modeling human reading with neural attention, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Michael Hahn",
                    "Frank Keller"
                ],
                "title": "Modeling human reading with neural attention",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_80",
            "content": "Amin Haji, - Abolhassani, James Clark, An inverse yarbus process: Predicting observers' task from eye movement patterns, 2014, Vision Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Amin Haji",
                    "- Abolhassani",
                    "James Clark"
                ],
                "title": "An inverse yarbus process: Predicting observers' task from eye movement patterns",
                "pub_date": "2014",
                "pub_title": "Vision Research",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_81",
            "content": "Tadayoshi Hara, Daichi Mochihashi, Yoshinobu Kano, Akiko Aizawa, Predicting word fixations in text with a CRF model for capturing general reading strategies among readers, 2012, Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Tadayoshi Hara",
                    "Daichi Mochihashi",
                    "Yoshinobu Kano",
                    "Akiko Aizawa"
                ],
                "title": "Predicting word fixations in text with a CRF model for capturing general reading strategies among readers",
                "pub_date": "2012",
                "pub_title": "Proceedings of the First Workshop on Eye-tracking and Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_82",
            "content": "John Henderson, Svetlana Shinkareva, Jing Wang, Steven Luke, Jenn Olejarczyk, Predicting cognitive state from eye movements, 2013, PloS one, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "John Henderson",
                    "Svetlana Shinkareva",
                    "Jing Wang",
                    "Steven Luke",
                    "Jenn Olejarczyk"
                ],
                "title": "Predicting cognitive state from eye movements",
                "pub_date": "2013",
                "pub_title": "PloS one",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_83",
            "content": "Rebekka Hillen, Thomas G\u00fcnther, Claudia Kohlen, Cornelia Eckers, Muna Van Ermingen-Marbach, Katharina Sass, Wolfgang Scharke, Josefine Vollmar, Ralph Radach, Stefan Heim, Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm, 2013, Frontiers in human neuroscience, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Rebekka Hillen",
                    "Thomas G\u00fcnther",
                    "Claudia Kohlen",
                    "Cornelia Eckers",
                    "Muna Van Ermingen-Marbach",
                    "Katharina Sass",
                    "Wolfgang Scharke",
                    "Josefine Vollmar",
                    "Ralph Radach",
                    "Stefan Heim"
                ],
                "title": "Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm",
                "pub_date": "2013",
                "pub_title": "Frontiers in human neuroscience",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_84",
            "content": "Nora Hollenstein, Lisa Beinborn, Relative importance in sentence processing, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Nora Hollenstein",
                    "Lisa Beinborn"
                ],
                "title": "Relative importance in sentence processing",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_85",
            "content": "Nora Hollenstein, Antonio De La Torre, Nicolas Langer, Ce Zhang, CogniVal: A framework for cognitive word embedding evaluation, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Nora Hollenstein",
                    "Antonio De La Torre",
                    "Nicolas Langer",
                    "Ce Zhang"
                ],
                "title": "CogniVal: A framework for cognitive word embedding evaluation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_86",
            "content": "UNKNOWN, None, 2021, Multilingual language models predict human reading behavior, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Multilingual language models predict human reading behavior",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_87",
            "content": "Nora Hollenstein, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, Nicolas Langer, Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading, 2018, Scientific data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Nora Hollenstein",
                    "Jonathan Rotsztejn",
                    "Marius Troendle",
                    "Andreas Pedroni",
                    "Ce Zhang",
                    "Nicolas Langer"
                ],
                "title": "Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading",
                "pub_date": "2018",
                "pub_title": "Scientific data",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_88",
            "content": "UNKNOWN, None, , Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_89",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Sarthak Jain",
                    "Byron Wallace"
                ],
                "title": "Attention is not Explanation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_90",
            "content": "Reinhold Kliegl, Ellen Grabner, Martin Rolfs, Ralf Engbert, Length, frequency, and predictability effects of words on eye movements in reading, 2004, European Journal of Cognitive Psychology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Reinhold Kliegl",
                    "Ellen Grabner",
                    "Martin Rolfs",
                    "Ralf Engbert"
                ],
                "title": "Length, frequency, and predictability effects of words on eye movements in reading",
                "pub_date": "2004",
                "pub_title": "European Journal of Cognitive Psychology",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_91",
            "content": "Reinhard Kneser, Hermann Ney, Improved backing-off for m-gram language modeling, 1995, 1995 international conference on acoustics, speech, and signal processing, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Reinhard Kneser",
                    "Hermann Ney"
                ],
                "title": "Improved backing-off for m-gram language modeling",
                "pub_date": "1995",
                "pub_title": "1995 international conference on acoustics, speech, and signal processing",
                "pub": "IEEE"
            }
        },
        {
            "ix": "40-ARR_v2_92",
            "content": "Christof Koch, Shimon Ullman, Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry, 1985, Human Neurobiology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Christof Koch",
                    "Shimon Ullman"
                ],
                "title": "Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry",
                "pub_date": "1985",
                "pub_title": "Human Neurobiology",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_93",
            "content": "UNKNOWN, None, 2019, Revealing the dark secrets of bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Revealing the dark secrets of bert",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_94",
            "content": "UNKNOWN, None, 2016, DeepGaze II: Reading fixations from deep features trained on object recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "DeepGaze II: Reading fixations from deep features trained on object recognition",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_95",
            "content": "Matthias K\u00fcmmerer, S Thomas, Leon Wallis, Matthias Gatys,  Bethge, Understanding low-and high-level contributions to fixation prediction, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Matthias K\u00fcmmerer",
                    "S Thomas",
                    "Leon Wallis",
                    "Matthias Gatys",
                    " Bethge"
                ],
                "title": "Understanding low-and high-level contributions to fixation prediction",
                "pub_date": "2017",
                "pub_title": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_96",
            "content": "UNKNOWN, None, 1995, Convolutional Networks for Images, Speech and Time Series, The MIT Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "1995",
                "pub_title": "Convolutional Networks for Images, Speech and Time Series",
                "pub": "The MIT Press"
            }
        },
        {
            "ix": "40-ARR_v2_97",
            "content": "UNKNOWN, None, 2017, A structured self-attentive sentence embedding, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A structured self-attentive sentence embedding",
                "pub": "CoRR"
            }
        },
        {
            "ix": "40-ARR_v2_98",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_99",
            "content": "D Tomasz, Peter Loboda, J\u00f6erg Brusilovsky,  Brunstein, Inferring word relevance from eyemovements of readers, 2011, Proceedings of the 16th international conference on Intelligent user interfaces, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "D Tomasz",
                    "Peter Loboda",
                    "J\u00f6erg Brusilovsky",
                    " Brunstein"
                ],
                "title": "Inferring word relevance from eyemovements of readers",
                "pub_date": "2011",
                "pub_title": "Proceedings of the 16th international conference on Intelligent user interfaces",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_100",
            "content": "Jonathan Malmaud, Roger Levy, Yevgeni Berzak, Bridging information-seeking human gaze and machine reading comprehension, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Jonathan Malmaud",
                    "Roger Levy",
                    "Yevgeni Berzak"
                ],
                "title": "Bridging information-seeking human gaze and machine reading comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 24th Conference on Computational Natural Language Learning",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_101",
            "content": "Pawel Mandera, Emmanuel Keuleers, Marc Brysbaert, Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation, 2017, Journal of Memory and Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Pawel Mandera",
                    "Emmanuel Keuleers",
                    "Marc Brysbaert"
                ],
                "title": "Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation",
                "pub_date": "2017",
                "pub_title": "Journal of Memory and Language",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_102",
            "content": "Franz Matthies, Anders S\u00f8gaard, With blinkers on: Robust prediction of eye movements across readers, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Franz Matthies",
                    "Anders S\u00f8gaard"
                ],
                "title": "With blinkers on: Robust prediction of eye movements across readers",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_103",
            "content": "UNKNOWN, None, 2019, The changing role of phonology in reading development, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "The changing role of phonology in reading development",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_104",
            "content": "Tim Miller, Explanation in artificial intelligence: Insights from the social sciences, 2019, Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Tim Miller"
                ],
                "title": "Explanation in artificial intelligence: Insights from the social sciences",
                "pub_date": "2019",
                "pub_title": "Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_105",
            "content": "Abhijit Mishra, Kuntal Dey, Pushpak Bhattacharyya, Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Abhijit Mishra",
                    "Kuntal Dey",
                    "Pushpak Bhattacharyya"
                ],
                "title": "Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_106",
            "content": "Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey, Pushpak Bhattacharyya, Leveraging cognitive features for sentiment analysis, 2016, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Abhijit Mishra",
                    "Diptesh Kanojia",
                    "Seema Nagar",
                    "Kuntal Dey",
                    "Pushpak Bhattacharyya"
                ],
                "title": "Leveraging cognitive features for sentiment analysis",
                "pub_date": "2016",
                "pub_title": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_107",
            "content": "UNKNOWN, None, 2019, Layer-Wise Relevance Propagation: An Overview, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Layer-Wise Relevance Propagation: An Overview",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "40-ARR_v2_108",
            "content": "E Niebur, C Koch, Control attention: Modeling the \"where\" pathway, 1996, Advances in Neural Information Processing Systems, MIT Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "E Niebur",
                    "C Koch"
                ],
                "title": "Control attention: Modeling the \"where\" pathway",
                "pub_date": "1996",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "MIT Press"
            }
        },
        {
            "ix": "40-ARR_v2_109",
            "content": "Mattias Nilsson, Joakim Nivre, Learning where to look: Modeling eye movements in reading, 2009-06-04, Proceedings of the Thirteenth Conference on Computational Natural Language Learning, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Mattias Nilsson",
                    "Joakim Nivre"
                ],
                "title": "Learning where to look: Modeling eye movements in reading",
                "pub_date": "2009-06-04",
                "pub_title": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning",
                "pub": "ACL"
            }
        },
        {
            "ix": "40-ARR_v2_110",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, Glove: Global vectors for word representation, 2014, Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "Glove: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_111",
            "content": "Grusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia, Douwe Kiela, Adina Williams, To what extent do human explanations of model behavior align with actual model behavior?, 2021, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Grusha Prasad",
                    "Yixin Nie",
                    "Mohit Bansal",
                    "Robin Jia",
                    "Douwe Kiela",
                    "Adina Williams"
                ],
                "title": "To what extent do human explanations of model behavior align with actual model behavior?",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_112",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_113",
            "content": "Keith Rayner, Eye movements in reading and information processing: 20 years of research, 1998, Psychological bulletin, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Keith Rayner"
                ],
                "title": "Eye movements in reading and information processing: 20 years of research",
                "pub_date": "1998",
                "pub_title": "Psychological bulletin",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_114",
            "content": "Keith Rayner, Susan Duffy, Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity, 1986, Memory amp; cognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Keith Rayner",
                    "Susan Duffy"
                ],
                "title": "Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity",
                "pub_date": "1986",
                "pub_title": "Memory amp; cognition",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_115",
            "content": "Keith Rayner, Erik Reichle, Models of the reading process, 2010, WIREs Cognitive Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Keith Rayner",
                    "Erik Reichle"
                ],
                "title": "Models of the reading process",
                "pub_date": "2010",
                "pub_title": "WIREs Cognitive Science",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_116",
            "content": "Alexander Erik D Reichle,  Pollatsek, L Donald, Keith Fisher,  Rayner, Toward a model of eye movement control in reading, 1998, Psychological review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Alexander Erik D Reichle",
                    " Pollatsek",
                    "L Donald",
                    "Keith Fisher",
                    " Rayner"
                ],
                "title": "Toward a model of eye movement control in reading",
                "pub_date": "1998",
                "pub_title": "Psychological review",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_117",
            "content": "UNKNOWN, None, 2003, The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": "The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_118",
            "content": "Timothy Rogers, Michael Wolmetz, Conceptual knowledge representation: A cross-section of current research, 2016, Cognitive Neuropsychology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Timothy Rogers",
                    "Michael Wolmetz"
                ],
                "title": "Conceptual knowledge representation: A cross-section of current research",
                "pub_date": "2016",
                "pub_title": "Cognitive Neuropsychology",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_119",
            "content": "UNKNOWN, None, 2021, Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_120",
            "content": "Philipp Schmidt, Felix Bie\u00dfmann, Quantifying interpretability and trust in machine learning systems, 2019, CoRR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Philipp Schmidt",
                    "Felix Bie\u00dfmann"
                ],
                "title": "Quantifying interpretability and trust in machine learning systems",
                "pub_date": "2019",
                "pub_title": "CoRR",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_121",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": [
                    "Sofia Serrano",
                    "Noah Smith"
                ],
                "title": "Is attention interpretable?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_122",
            "content": "UNKNOWN, None, 2021, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, .",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_123",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher,  Manning, Y Andrew, Christopher Ng,  Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 conference on empirical methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "D Christopher",
                    " Manning",
                    "Y Andrew",
                    "Christopher Ng",
                    " Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 conference on empirical methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_124",
            "content": "Anders S\u00f8gaard, Evaluating word embeddings with fMRI and eye-tracking, 2016, Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": [
                    "Anders S\u00f8gaard"
                ],
                "title": "Evaluating word embeddings with fMRI and eye-tracking",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_125",
            "content": "Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling, Ngoc Vu, Interpreting attention models with human visual attention in machine reading comprehension, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Ekta Sood",
                    "Simon Tannert",
                    "Diego Frassinelli",
                    "Andreas Bulling",
                    "Ngoc Vu"
                ],
                "title": "Interpreting attention models with human visual attention in machine reading comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 24th Conference on Computational Natural Language Learning",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_126",
            "content": "Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling, Improving natural language processing tasks with human gaze-guided neural attention, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b72",
                "authors": [
                    "Ekta Sood",
                    "Simon Tannert",
                    "Philipp Mueller",
                    "Andreas Bulling"
                ],
                "title": "Improving natural language processing tasks with human gaze-guided neural attention",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "40-ARR_v2_127",
            "content": "Andreas Stolcke, Srilm -an extensible language modeling toolkit, 2002, Proceedings of the 7th International Conference on Spoken Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b73",
                "authors": [
                    "Andreas Stolcke"
                ],
                "title": "Srilm -an extensible language modeling toolkit",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 7th International Conference on Spoken Language Processing",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_128",
            "content": "UNKNOWN, None, 2016, Seeing with humans: Gaze-assisted neural image captioning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b74",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Seeing with humans: Gaze-assisted neural image captioning",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_129",
            "content": "Dengxin Arun Balajee Vasudevan, Luc Dai,  Van Gool, Object referring in videos with language and human gaze, 2018-06-18, 2018 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b75",
                "authors": [
                    "Dengxin Arun Balajee Vasudevan",
                    "Luc Dai",
                    " Van Gool"
                ],
                "title": "Object referring in videos with language and human gaze",
                "pub_date": "2018-06-18",
                "pub_title": "2018 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "40-ARR_v2_130",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information cessing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b76",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information cessing Systems",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_131",
            "content": "Markus Bogojeski, Benjamin Blankertz, Real-time inference of word relevance from electroencephalogram and eye gaze, 2017, Journal of neural engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b77",
                "authors": [
                    "Markus Bogojeski",
                    "Benjamin Blankertz"
                ],
                "title": "Real-time inference of word relevance from electroencephalogram and eye gaze",
                "pub_date": "2017",
                "pub_title": "Journal of neural engineering",
                "pub": null
            }
        },
        {
            "ix": "40-ARR_v2_132",
            "content": "Sarah Wiegreffe, Yuval Pinter, Attention is not not explanation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b78",
                "authors": [
                    "Sarah Wiegreffe",
                    "Yuval Pinter"
                ],
                "title": "Attention is not not explanation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_133",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b79",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "Remi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "40-ARR_v2_134",
            "content": "UNKNOWN, None, 1967, Eye Movements and Vision. Plenum, .",
            "ntype": "ref",
            "meta": {
                "xid": "b80",
                "authors": null,
                "title": null,
                "pub_date": "1967",
                "pub_title": "Eye Movements and Vision. Plenum",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "40-ARR_v2_0@0",
            "content": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_0",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@0",
            "content": "Learned self-attention functions in state-of-theart NLP models often correlate with human attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@1",
            "content": "We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 101,
            "end": 298,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@2",
            "content": "We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 300,
            "end": 419,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@3",
            "content": "We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 421,
            "end": 586,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@4",
            "content": "Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 588,
            "end": 705,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_2@5",
            "content": "Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lowerentropy attention vectors are more faithful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_2",
            "start": 707,
            "end": 877,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_4@0",
            "content": "The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Zhang and Zhang, 2019;Klerke and Plank, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_4",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_4@1",
            "content": "In this paper, we evaluate how well attention flow (Abnar and Zuidema, 2020) in large language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), aligns with human eye fixations during task-specific reading, compared to other shallow sequence labeling models (Lecun and Bengio, 1995;Vaswani et al., 2017) and a classic, heuristic model of human reading (Reichle et al., 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_4",
            "start": 219,
            "end": 643,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_4@2",
            "content": "We compare the learned attention functions and the heuristic model across two taskspecific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available dataset with eye-tracking recordings of native speakers of English .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_4",
            "start": 645,
            "end": 964,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@0",
            "content": "We compare human and model attention patterns on both sentiment reading and relation extraction tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@1",
            "content": "In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995;Vaswani et al., 2017), as well as to a frequency baseline and a heuristic, cognitively inspired model of human reading called the E-Z Reader (Reichle et al., 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 103,
            "end": 455,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@2",
            "content": "We find that the heuristic model correlates well with human reading, as has been reported in Sood et al. (2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 457,
            "end": 569,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@3",
            "content": "However when we apply attention flow (Abnar and Zuidema, 2020), the pre-trained Transformer models also reach comparable levels of correlation strength.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 571,
            "end": 722,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@4",
            "content": "Further fine-tuning experiments on BERT did not result in increased correlation to human fixations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 724,
            "end": 822,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@5",
            "content": "To understand what drives the differences between models, we perform an in-depth analysis of the effect of word predictability and POS tags on correlation strength.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 824,
            "end": 987,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@6",
            "content": "It reveals that Transformer models do not accurately capture tail phenomena for hard-to-predict words (in contrast to the E-Z Reader) and that Transformer attention flow shows comparably weak correlation on (proper) nouns while the E-Z Reader predicts importance of these more accurately, especially on the sentiment reading task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 989,
            "end": 1318,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@7",
            "content": "In addition, we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 1320,
            "end": 1519,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@8",
            "content": "We test faithfulness of these different attention patterns to produce the correct classification via an input reduction experiment on task-tuned BERT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 1521,
            "end": 1677,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@9",
            "content": "Our results highlight the trade-off between model faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors tend to be less faithful with respect to model predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 1679,
            "end": 1919,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_5@10",
            "content": "Our code is available at github.com/ oeberle/task_gaze_transformers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_5",
            "start": 1921,
            "end": 1988,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_6@0",
            "content": "Pre-trained Language Models vs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_6",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_7@0",
            "content": "Cognitive Models Church and Liberman (2021) discuss how NLP has historically benefited from rationalist and empiricist methodologies, something that holds for cognitive modeling in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_7",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_7@1",
            "content": "The vast majority of application-oriented work in NLP today relies on pre-trained language models or other largescale data-driven models, but in cognitive modeling, most approaches remain heuristic and rulebased, or hybrid, e.g., relying on probabilistic language models to quantify surprisal (Rayner and Reichle, 2010;Milledge and Blythe, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_7",
            "start": 190,
            "end": 535,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_7@2",
            "content": "This is for good reasons: Cognitive modeling values interpretability (even) more, often suffers from data scarcity, and is less concerned with model reusability across different contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_7",
            "start": 537,
            "end": 723,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_8@0",
            "content": "This paper presents a head-to-head comparison of the E-Z Reader and pre-trained Transformerbased language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_8",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_8@1",
            "content": "We are not the first to evaluate pre-trained language models and largescale data-driven models as if they were cognitive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_8",
            "start": 114,
            "end": 241,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_8@2",
            "content": "Chrupa\u0142a and Alishahi (2019), for example, use representational similarity analysis to correlate sentence encodings in pre-trained language models with fMRI signals; Abdou et al. (2019) correlate sentence encodings with gaze-derived representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_8",
            "start": 243,
            "end": 491,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_8@3",
            "content": "More generally, it has been argued that cognitive evaluations are in some cases practically superior to standard evaluation methodologies in NLP (S\u00f8gaard, 2016;Hollenstein et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_8",
            "start": 493,
            "end": 678,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_8@4",
            "content": "We return to this in the Discussion and Conclusion \u00a76.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_8",
            "start": 680,
            "end": 733,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_9@0",
            "content": "Commonly, pre-trained language models are disregarded as cognitive models, since they are most often implemented as computationally demanding batch learning algorithms, processing data \"at once\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_9",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_9@1",
            "content": "G\u00fcnther et al. (2019) points out that this is an artefact of their implementation, and online learning of pre-trained language models is possible, yet impractical.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_9",
            "start": 196,
            "end": 358,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_9@2",
            "content": "Generally, several researchers have argued for taking pre-trained language models seriously as cognitive models (Rogers and Wolmetz, 2016;Mandera et al., 2017;G\u00fcnther et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_9",
            "start": 360,
            "end": 540,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_9@3",
            "content": "In the last section, \u00a76, we discuss some of the implications of comparisons of pre-trained language models and cognitive models -for cognitive modeling, as well as for NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_9",
            "start": 542,
            "end": 713,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_9@4",
            "content": "In our experiments, we focus on Transformer architectures that are currently the dominating pre-trained language models and a de facto baseline for modern NLP research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_9",
            "start": 715,
            "end": 882,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_10@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_10",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_11@0",
            "content": "Data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_11",
            "start": 0,
            "end": 3,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_12@0",
            "content": "The ZuCo dataset contains eye-tracking data for 12 participants (all English native speakers) performing natural reading and relation extraction on 300 and 407 English sentences from the Wikipedia relation extraction corpus (Culotta et al., 2006) respectively and sentiment reading on 400 samples of the Stanford Sentiment Treebank (SST) (Socher et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_12",
            "start": 0,
            "end": 359,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_12@1",
            "content": "For our analysis, we extract and average word-based total fixation times across participants and focus on the task-specific relation extraction and sentiment reading samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_12",
            "start": 361,
            "end": 534,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_13@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_13",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_14@0",
            "content": "Below we briefly describe our used models and refer to Appendix A for more details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_14",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_15@0",
            "content": "Transformers The superior performance of Transformer architectures across broad sets of NLP tasks raises the question of how task-related attention patterns really are.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_15",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_15@1",
            "content": "In our experiments, we focus on comparing task-modulated human fixations to attention patterns extracted from the following commonly used models: (a) We use both pre-trained uncased BERT-base and large models (Devlin et al., 2019) as well as fine-tuned BERT models on the respective tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_15",
            "start": 169,
            "end": 457,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_15@2",
            "content": "BERT was originally pre-trained on the English Wikipedia and the BookCorpus. (b) The RoBERTa model has the same architecture as BERT and demonstrates better performance on downstream tasks using an improved pre-training scheme and the use of additional news article data (Liu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_15",
            "start": 459,
            "end": 748,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_15@3",
            "content": "(c) The Text-to-Text Transfer Transformer (T5) uses an encoder-decoder structure to enable parallel tasktraining and has demonstrated state-of-the-art performance over several transfer tasks including sentiment analysis and natural language inference (Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_15",
            "start": 750,
            "end": 1022,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_16@0",
            "content": "We evaluate different ways of extracting tokenlevel importance scores: We collect attention representations and compute the mean attention vector over the final layer heads to capture the mixing of information in Transformer self-attention modules as in and present this as mean for all aforementioned Transformers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_16",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_17@0",
            "content": "To capture the layer-wise structure of deep Transformer models we compute attention flow (Abnar and Zuidema, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_17",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_17@1",
            "content": "This approach considers the attention matrices as a graph, where tokens are represented as nodes and attention scores as edges between consecutive layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_17",
            "start": 116,
            "end": 269,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_17@2",
            "content": "The edge values define the maximal flow possible between a pair of nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_17",
            "start": 271,
            "end": 343,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_17@3",
            "content": "Flow between edges is thus (i) limited to the maximal attention between any two consecutive layers for this token and (ii) conserved such that the sum of incoming flow must be equal to the sum of outgoing flow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_17",
            "start": 345,
            "end": 554,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_17@4",
            "content": "We denote the attention flow propagated back from layer L as flow L.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_17",
            "start": 556,
            "end": 623,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_18@0",
            "content": "We ground our analysis on Transformers by comparing them to relatively shallow models that were trained from-scratch and evaluate how well they coincide with human fixation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_18",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_18@1",
            "content": "We train a standard CNN (Kim, 2014) network with multiple filter sizes on pre-trained GloVe embeddings (Pennington et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_18",
            "start": 174,
            "end": 302,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_18@2",
            "content": "Importance scores over tokens are extracted using Layerwise Relevance Propagation (LRP) (Arras et al., 2016(Arras et al., , 2017 which has been demonstrated to produce robust explanations by iterating over layers and redistributing relevance from outer layers towards the input (Bach et al., 2015;Samek et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_18",
            "start": 304,
            "end": 620,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_18@3",
            "content": "In parallel, we use a shallow multi-head self-attention network (Lin et al., 2017) on GloVe vectors with a linear read-out layer for which we compute token relevance scores using LRP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_18",
            "start": 622,
            "end": 804,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_19@0",
            "content": "As a cognitive model for human reading, we compute task-neutral fixation times using the E-Z Reader (Reichle et al., 1998) model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_19",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_19@1",
            "content": "The E-Z Reader is a multi-stage, hybrid model, which relies on an n-gram model and several heuristics, based, for example, on theoretical assumptions about the role of predictability and average saccade length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_19",
            "start": 130,
            "end": 339,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_19@2",
            "content": "Additionally, we compare to a frequency baseline using word statistics of the BNC (British National Corpus, Kilgarriff (1995)) 1 as proposed by Barrett et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_19",
            "start": 341,
            "end": 506,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_20@0",
            "content": "Optimization",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_20",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_21@0",
            "content": "For training models on the different tasks we remove all sentences that overlap between ZuCo and the original SST and Wikipedia datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_21",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_21@1",
            "content": "Models are then trained on the remaining train-split data until early stopping is reached and we report results over five runs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_21",
            "start": 138,
            "end": 264,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_21@2",
            "content": "We provide further details on the optimization and model task performance in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_21",
            "start": 266,
            "end": 353,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_22@0",
            "content": "Metric",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_22",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_23@0",
            "content": "To compare models with human attention, we compute Spearman correlation between human and model-based importance vectors after concatenation of individual sentences as well as on a tokenlevel, see .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_23",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_23@1",
            "content": "This enables us to distinguish unrelated effects caused by varying sentence length from token-level importance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_23",
            "start": 199,
            "end": 309,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_23@2",
            "content": "As described before, we extract human attention from gaze (ZuCo), simulated gaze (E-Z Reader), raw attentions (BERT, RoBERTa, T5), relevance scores (CNN, self-attention) and inverse token probability scores (BNC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_23",
            "start": 311,
            "end": 523,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_23@3",
            "content": "2 We use ZuCo tokens to align sentences across tokenizers and apply max-pooling of scores when bins are merged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_23",
            "start": 525,
            "end": 635,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_24@0",
            "content": "Main result",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_24",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@0",
            "content": "To evaluate how well model and human attention patterns for sentiment reading and relation extraction align, we compute pair-wise correlation scores as displayed in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@1",
            "content": "Reported correlations are statistically significant with p < 0.01 if not indicated otherwise (ns: not significant).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 175,
            "end": 289,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@2",
            "content": "After ranking based on the correlations on sentence-level, we observe clear differences between sentiment reading on SST and relation extraction on Wikipedia for the different models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 291,
            "end": 473,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@3",
            "content": "For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < 0.05 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 475,
            "end": 680,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@4",
            "content": "For relation extraction, we see the highest correlation for BERTbase attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < 0.05).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 682,
            "end": 876,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@5",
            "content": "On the lower end, computing means over BERT attentions across the last layer shows weak to no correlations for both tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 878,
            "end": 999,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@6",
            "content": "3 The shallow architectures result in low to moderate correlations with a distinctive gap to attention flow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 1001,
            "end": 1108,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@7",
            "content": "Focusing on flow values for Transformers, BNC and E-Z Reader, correlations are stable across word and sentence length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 1110,
            "end": 1227,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_25@8",
            "content": "Correlations grouped by sentence length shows stable values around 0.6 (SST) and 0.4 \u2212 0.6 (Wikipedia) except for shorter sentences where correlations fluctuate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_25",
            "start": 1229,
            "end": 1389,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_26@0",
            "content": "To check the linear relationship between human and model attention patterns we additionally compute token-and sentence-level Pearson correlations which can be found in Appendix B. Results confirm that Spearman and Pearson correlation coefficients as well as rankings hardly differ -which suggests a linear relationship -and that correlation strength is in line with .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_26",
            "start": 0,
            "end": 366,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_27@0",
            "content": "Analyses",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_27",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_28@0",
            "content": "In addition to our main result -that pre-trained language models are competitive to heuristic cognitive models in predicting human eye fixations during reading -we present a detailed analysis, investigating what our main results depend on, where pre-trained language models improve on cognitive models, and where they are still challenged.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_28",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_29@0",
            "content": "Fine-tuning BERT does not change correlations to human attention We find that fine-tuning base and large BERT models on either task does not significantly change correlations and are of similar strength to untuned models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_29",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_29@1",
            "content": "This observation can be embedded into findings that Transformers are equipped with overcomplete sets of attention functions that hardly change until the later layers, if at all, during fine-tuning and that this change is also dependent on the tuning task itself (Kovaleva et al., 2019;Zhao and Bethard, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_29",
            "start": 222,
            "end": 530,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_29@2",
            "content": "In addition, we observe that Transformer flows propagated back from early, medium and final layers do not considerably change correlations to human attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_29",
            "start": 532,
            "end": 689,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_29@3",
            "content": "This can be explained by attention flow filtering the path of minimal value at each layer as discussed in Abnar and Zuidema (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_29",
            "start": 691,
            "end": 821,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_30@0",
            "content": "Attention flow is important The correlation analysis emphasizes that we need to capture the layered propagation structure in Transformer models, e.g., by using attention flow, in order to extract importance scores that are competitive with cognitive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_30",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_30@1",
            "content": "Interestingly, selecting the highest correlating head for the last attention layer produces generally weaker correlation than attention flows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_30",
            "start": 258,
            "end": 399,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_30@2",
            "content": "3 This offers additional evidence that raw attention weights do not reliably correspond to token relevance (Serrano and Smith, 2019;Abnar and Zuidema, 2020) and, thus, are of limited use to compare task attention to human gaze.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_30",
            "start": 401,
            "end": 627,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_31@0",
            "content": "Differences between language models BERT, RoBERTa and T5 are large-scale pretrained language models based on Transformers, but they also differ in various ways.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_31",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_31@1",
            "content": "One key difference is that BERT and RoBERTa use absolute position encodings, while T5 uses relative encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_31",
            "start": 161,
            "end": 270,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_31@2",
            "content": "BERT and RoBERTa differ in that (i) BERT has a next-sentence-prediction auxiliary objective; (ii) RoBERTa and T5 were trained on more data; (iii) RoBERTa uses dynamic masking and trains with larger mini-batches and learning rates, while T5 uses multi-word masking; (iv) RoBERTa uses byte pair encoding for subword segmentation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_31",
            "start": 272,
            "end": 598,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_31@3",
            "content": "We leave it as an open question whether the superior attention flows of BERT, compared to RoBERTa and T5, has to do with training data, next sentence prediction, or fortunate hyper-parameter settings, but note that BERT is also known to have higher alignment with human-generated explanations than other large-scale pre-trained language models (Prasad et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_31",
            "start": 600,
            "end": 965,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_32@0",
            "content": "E-Z Reader is less sensitive to hard-to-predict words and POS We compare correlations to human fixations with attention flow values for Transformer models in the last layer, the E-Z Reader and the BNC baseline for different word predictability scores computed with a 5-gram Kneser-Ney language model (Kneser and Ney, 1995;Chelba et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_32",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_32@1",
            "content": "Figure 3 shows the results on SST and Wikipedia for equally sized bins of word predictability scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_32",
            "start": 344,
            "end": 444,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_32@2",
            "content": "We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (0.3 \u2212 0.4 for both, SST and Wikipedia).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_32",
            "start": 446,
            "end": 721,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_32@3",
            "content": "In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability > 0.03).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_32",
            "start": 723,
            "end": 837,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@0",
            "content": "We also compare correlations to human fixations Input reduction When comparing machines to humans we typically regard the psychophysical data as the gold standard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@1",
            "content": "We will now take the model perspective and test fidelity of both human and model attention patterns in task-tuned models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 164,
            "end": 284,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@2",
            "content": "By this we aim to test how effective the exact token ranking based on attention scores is at producing the correct output probability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 286,
            "end": 419,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@3",
            "content": "We perform such an input reduction analysis (Feng et al., 2018) our analysis, we observe -as to be expected -that adding tokens according to token probability (BNC prob) performs even worse than randomly adding tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 421,
            "end": 638,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@4",
            "content": "From-scratch trained models (CNN and self-attention) are most effective in selecting taskrelevant tokens, and even more so than using any Transformer attention flow.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 640,
            "end": 804,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@5",
            "content": "Adding tokens based on human attention is as effective for the sentiment task as the E-Z Reader.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 806,
            "end": 901,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@6",
            "content": "Interestingly, for the relation extraction task, human attention vectors provide the most effective flipping order after the relevance-based shallow methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 903,
            "end": 1059,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@7",
            "content": "All Transformerbased flows perform comparably in both tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1061,
            "end": 1120,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@8",
            "content": "To better understand what drives these effects we extract the fraction of POS tags for the first added token (see Figure 4 and full results in the Appendix Figure 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1122,
            "end": 1287,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@9",
            "content": "Natural reading versus task-specific reading A unique feature of the ZuCo dataset is that it contains a subset of sentences that were presented to participants both in a task-specific (relation extraction) and a natural reading setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1289,
            "end": 1524,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@10",
            "content": "This allows for a direct comparison of how correlation strength influenced by the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1526,
            "end": 1612,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@11",
            "content": "In correlations of human gaze-based attention with model attentions are shown.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1614,
            "end": 1691,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@12",
            "content": "The highest correlation can be observed when comparing human attention for task-specific and natural reading (0.72).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1693,
            "end": 1808,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@13",
            "content": "The remaining model correlations correspond to the ranking and correlation strength observed in the main result (see Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1810,
            "end": 1936,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@14",
            "content": "We observe lower correlation scores for the task-specific reading as compared to normal reading among attention flow, the E-Z Reader and BNC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 1938,
            "end": 2078,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@15",
            "content": "This suggests that these models capture the statistics of natural reading -as is expected for a cognitive model designed to the natural reading paradigm -and that task-related changes in human fixation patterns are not reflected in Transformer attention flows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 2080,
            "end": 2339,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@16",
            "content": "Interestingly, averaged last layer attention heads show a reverse effect (but at much weaker correlation strength).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 2341,
            "end": 2455,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_33@17",
            "content": "This might suggest that pre-training in Transformer models induces specificity of later layer attention heads to tasksolving instead of general natural reading patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_33",
            "start": 2457,
            "end": 2625,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_34@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_34",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_35@0",
            "content": "Saliency modeling Early computational models of visual attention have used bottom-up approaches to model the neural circuitry representing pre-attentive selection processes from visual input (Koch and Ullman, 1985) and later the central idea of a saliency map was introduced (Niebur and Koch, 1996).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_35",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_35@1",
            "content": "A central hypothesis studying eye movements under task conditions is known as Yarbus theorem stating that a task can be directly decoded from fixation patterns (Yarbus, 1967) which has found varying support (Greene et al., 2012;Henderson et al., 2013;Borji and Itti, 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_35",
            "start": 300,
            "end": 572,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_35@2",
            "content": "More recently, extracting features from deep pre-trained filters in combination with readout networks has boosted performance on the saliency task (K\u00fcmmerer et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_35",
            "start": 574,
            "end": 744,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_35@3",
            "content": "This progress has enabled modeling of more complex gaze patterns, e.g. vision-language tasks such as image captioning (Sugano and Bulling, 2016), visual question answering (Das et al., 2016) or text-guided object detection (Vasudevan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_35",
            "start": 746,
            "end": 993,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_36@0",
            "content": "Predicting text gaze patterns has been studied extensively, often in the context of probabilistic (Feng, 2006;Hara et al., 2012;Matthies and S\u00f8gaard, 2013;Hahn and Keller, 2016) or token transition models (Nilsson and Nivre, 2009;Haji-Abolhassani and Clark, 2014;Coutrot et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_36",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_37@0",
            "content": "More recently deep language features have been used as feature extractors in modeling text saliency (Sood et al., 2020a; opening the question of their cognitive plausibility.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_37",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_38@0",
            "content": "Eye-tracking signals for NLP Augmenting machine learning models using human gaze information has been shown to improve performance for a number of different settings: Human attention patterns as regularization during model training have resulted in comparable or improved task performance in tagging part-of-speech (Barrett and S\u00f8gaard, 2015a,b;Barrett et al., 2018), sentence compression (Klerke et al., 2016), detecting sentiment (Mishra et al., 2016(Mishra et al., , 2017 or reading comprehension (Malmaud et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_38",
            "start": 0,
            "end": 522,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_38@1",
            "content": "In these works, general free-viewing gaze data is used without consideration of the specific training task which opens the question of task-modulation in human reading.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_38",
            "start": 524,
            "end": 691,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_39@0",
            "content": "From natural to task-specific reading Recent work on reading often analyses eye-tracking data in combination with neuroimaging techniques such as EEG (Wenzel et al., 2017) and f-MRI (Hillen et al., 2013;Choi et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_39",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_39@1",
            "content": "Research questions thereby focus either on detecting relevant parts in text (Loboda et al., 2011;Wenzel et al., 2017) or the difference between natural and pseudo-reading, i.e., text without syntax/semantics (Hillen et al., 2013) or pseudo-words (Choi et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_39",
            "start": 223,
            "end": 488,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_39@2",
            "content": "To the best of our knowledge there has not been any work on comparing fixations between natural reading and task-specific reading on classical NLP tasks such as relation extraction or sentiment classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_39",
            "start": 490,
            "end": 698,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_40@0",
            "content": "Discussion and Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_40",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_41@0",
            "content": "In this paper, we have compared attention and relevance mechanisms of a wide range of models to human gaze patterns when solving sentiment classification on SST movie reviews and relation extraction on Wikipedia articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_41",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_41@1",
            "content": "We generally found that Transformer architectures are competitive with the E-Z Reader, but only when computing attention flow scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_41",
            "start": 222,
            "end": 354,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_41@2",
            "content": "We generally saw weaker correlations for relation extraction on Wikpedia, presumably due to simpler sentence structures and the occurrence of polarity words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_41",
            "start": 356,
            "end": 512,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_41@3",
            "content": "In the following, we discuss implications of our findings on NLP and Cognitive Science in more detail.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_41",
            "start": 514,
            "end": 615,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_42@0",
            "content": "Lessons for NLP One implication of the above for NLP follows from the importance of attention flow in our experiments: Using human gaze or supervise attention weights has proven effective in previous work ( \u00a75), but we observed that correlations with task-specific human attention increase significantly by using layer-dependent attention flow compared to using raw attention weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_42",
            "start": 0,
            "end": 383,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_42@1",
            "content": "This insight motivates going beyond regularizing raw attention weights or directly injecting human attention vectors during training, to instead optimize for correlation between attention flow and human attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_42",
            "start": 385,
            "end": 597,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_42@2",
            "content": "Jointly modeling language and human gaze has recently shown to yield competitive performance on paraphrase generation and sentence compression while resulting in more taskspecific attention heads (Sood et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_42",
            "start": 599,
            "end": 815,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_42@3",
            "content": "For this study natural gaze patterns were also simulated using the E-Z Reader.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_42",
            "start": 817,
            "end": 894,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@0",
            "content": "Another potential implication concerns interpretability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@1",
            "content": "It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide meaningful explanations for model predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 57,
            "end": 252,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@2",
            "content": "Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 254,
            "end": 413,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@3",
            "content": "A successful explanation of a machine learning model should be faithful, human-interpretable and practical to apply (Samek et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 415,
            "end": 551,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@4",
            "content": "Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 553,
            "end": 703,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@5",
            "content": "By contrast, judging human-interpretability typically requires costly experiments in well-controlled settings and obtaining human gold-standards for interpretability remain difficult (Miller, 2019;Schmidt and Bie\u00dfmann, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 705,
            "end": 929,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_43@6",
            "content": "Using gaze data to evaluate the faithfulness and trustworthiness of machine learning models is a promising approach to increase model transparency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_43",
            "start": 931,
            "end": 1077,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_44@0",
            "content": "Lessons for Cognitive Science Attention flow in Transformers, especially for BERT models, correlates surprisingly well with human task-specific reading, but what does this tell us about the shortcomings of our cognitive models?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_44",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_44@1",
            "content": "We know that word frequency and semantic relationships between words influence word fixation times (Rayner, 1998).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_44",
            "start": 228,
            "end": 341,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_45@0",
            "content": "In our experiments, we see relatively high correlation between human fixations and the inverse word probability baseline which raises the question to what extent reading gaze is driven by low-level patterns such as word frequency or syntactic structure in contrast to more high-level semantic context or wrap-up effects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_45",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_46@0",
            "content": "In computer vision, cognitively inspired bottomup models, e.g., using intensity and contrast features, are able to explain at most half of the gaze fixation information in comparison to the human gold standard (K\u00fcmmerer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_46",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_46@1",
            "content": "The robustness of the E-Z Reader on movie reviews is likely due to its explicit modeling of low-level properties such as word frequency or sentence length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_46",
            "start": 235,
            "end": 389,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_46@2",
            "content": "BERT was recently shown to be primarily modeling higherorder word co-occurrence statistics (Sinha et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_46",
            "start": 391,
            "end": 502,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_46@3",
            "content": "We argue that while Transformers are limited, e.g., in not capturing the dependency of human gaze on word length (Kliegl et al., 2004), cognitive models seem to underestimate the role of word co-occurrence statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_46",
            "start": 504,
            "end": 720,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_47@0",
            "content": "During reading, humans are faced with a tradeoff between the precision of reading comprehension and reading speed, by avoiding unnecessary fixations (Hahn and Keller, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_47",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_47@1",
            "content": "This trade-off is related to the input reduction experiments performed in Section 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_47",
            "start": 174,
            "end": 257,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_47@2",
            "content": "Here, we observe that shallow methods score well at being sparse and effective in changing model output towards the correct class, but produce only weak correlation to human reading patterns when compared to layered language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_47",
            "start": 259,
            "end": 490,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_47@3",
            "content": "In comparison, extracted attention flow from pre-trained Transformer models correlates much better with human attention, but offers less sparse token attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_47",
            "start": 492,
            "end": 651,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_47@4",
            "content": "In other words, our results show that task-specific reading is sub-optimal relative to solving tasks and heavily regularized by natural reading patterns (see also our comparison of task-specific and natural reading in Section 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_47",
            "start": 653,
            "end": 881,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@0",
            "content": "In our experiments, we first and foremost found that Transformers, and especially BERT models, are competitive to the E-Z Reader in terms of explaining human attention in taskspecific reading.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@1",
            "content": "For this to be the case, computing attention flow scores (rather than raw attention weights) is important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 193,
            "end": 298,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@2",
            "content": "Even so, the E-Z Reader remains better at hard-to-predict words and is less sensitive to part of speech.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 300,
            "end": 403,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@3",
            "content": "While Transformers thus have some limitations compared to the E-Z Reader, our results indicate that cognitive models have placed too little weight on high-level word cooccurrence statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 405,
            "end": 594,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@4",
            "content": "Generally, Transformers and the E-Z Reader correlate much better with human attention than other, shallow from-scratch trained sequence labeling architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 596,
            "end": 754,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_48@5",
            "content": "Our input reduction suggest that in a sense, trained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_48",
            "start": 756,
            "end": 971,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_49@0",
            "content": "In the following we present details for all modes and describe the training details used for tasktunning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_49",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_49@1",
            "content": "Model performance over five runs is reported in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_49",
            "start": 106,
            "end": 161,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@0",
            "content": "The CNN models use 300-dimensional pre-trained GloVe_840B (Pennington et al., 2014) embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@1",
            "content": "Input sentences are tokenized using the SpaCy tokenizer (Honnibal et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 96,
            "end": 175,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@2",
            "content": "We use 150 convolutional filters of filter sizes s = [3, 4, 5] with ReLu activation, followed by a max-pooling-layer and apply dropout of p = 0.5 of the linear classification layer during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 177,
            "end": 373,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@3",
            "content": "For training we use a batchsize of bs = 50 and train all model parameters using the Adam optimizer with a learning rate of lr = 1e \u2212 4 for a maximum number of T = 20 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 375,
            "end": 547,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@4",
            "content": "For all model trainings, we apply early stopping to avoid overfitting during training and stop optimization as soon as the validation loss begins to increase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 549,
            "end": 706,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@5",
            "content": "To compute LRP relevances we use the general formulation of LRP propagation rules with \u03b3 = 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 708,
            "end": 800,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@6",
            "content": "for the linear readout layers (Montavon et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 802,
            "end": 855,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@7",
            "content": "We take absolute values over resulting relevance scores since we find they correlate best with human attention in comparison to raw and rectified processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 857,
            "end": 1013,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_50@8",
            "content": "For propagation through the max-pooling layer we apply the winner-take-all principle and for convolutional layers we use the LRP-\u03b3 redistribution rule and select \u03b3 = 0.5 after a search over \u03b3 = [0., 0.25, 0.5, 0.75, 1.0] resulting in largest correlations to human attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_50",
            "start": 1015,
            "end": 1288,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_51@0",
            "content": "For the multi-head self-attention model again use 300-dimensional pre-trained GloVe_840B embeddings and tokenized via SpaCy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_51",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_51@1",
            "content": "The architecture consists of a set of k = 3 self-attention heads for the SR task and k = 8 for REL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_51",
            "start": 125,
            "end": 223,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_51@2",
            "content": "The resulting sentence representation is then fed into a linear classification readout layer with \u03b3 = 0. and which we also use for the propagation to input embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_51",
            "start": 225,
            "end": 391,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_51@3",
            "content": "During optimization we use lr = 1e \u2212 4, bs = 50 and T = 50.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_51",
            "start": 393,
            "end": 451,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_52@0",
            "content": "We use standard BERT-base/large-uncased architectures and tokenizers as provided by the huggingface library (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_52",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_52@1",
            "content": "For BERTbase fine-tuning we use lr = 1e \u2212 5 for REL and lr = 1e \u2212 6 for SR, bs = 32 and T = 50 for both tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_52",
            "start": 129,
            "end": 238,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_52@2",
            "content": "For BERT-large we use lr = 1e \u2212 5 for REL and lr = 5e \u2212 7 for SR, bs = 16 and T = 50.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_52",
            "start": 240,
            "end": 324,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_52@3",
            "content": "For RoBERTa and T5 we use the RoBERTa-base and T5-base checkpoints and respective tokenizers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_52",
            "start": 326,
            "end": 418,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_53@0",
            "content": "We use version 10.2 of the E-Z Reader with default parameters and 1000 repetitions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_53",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_53@1",
            "content": "Cloze scores, i.e. word predictability scores, were therefore computed using a 5-gram Kneser-Ney language model (Kneser and Ney, 1995) as provided by the SRI Language Modeling Toolkit (Stolcke, 2002) and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_53",
            "start": 84,
            "end": 286,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_54@0",
            "content": "Mostafa Abdou, Artur Kulmizev, Felix Hill, Daniel Low, Anders S\u00f8gaard, Higher-order comparisons of sentence encoder representations, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_54",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_55@0",
            "content": "Samira Abnar, Willem Zuidema, Quantifying attention flow in transformers, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_55",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_56@0",
            "content": "UNKNOWN, None, , Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, and Lior Wolf. 2022. XAI for transformers: Better explanations through conservative propagation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_56",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_57@0",
            "content": "Leila Arras, Franziska Horn, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Wojciech Samek, Explaining predictions of non-linear classifiers in NLP, 2016, Proceedings of the 1st Workshop on Representation Learning for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_57",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_58@0",
            "content": "Leila Arras, Franziska Horn, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Wojciech Samek, What is relevant in a text document?\": An interpretable machine learning approach, 2017, PLOS ONE, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_58",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_59@0",
            "content": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, Wojciech Samek, On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation, 2015, PLoS ONE, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_59",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_60@0",
            "content": "Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, Anders S\u00f8gaard, Sequence classification with human attention, 2018, Proceedings of the 22nd Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_60",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_61@0",
            "content": "Maria Barrett, Anders S\u00f8gaard, Reading behavior predicts syntactic categories, 2015, Proceedings of the Nineteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_61",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_62@0",
            "content": "Maria Barrett, Anders S\u00f8gaard, Using reading behavior to predict grammatical functions, 2015, Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_62",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_63@0",
            "content": "Ali Borji, Laurent Itti, Defending yarbus: eye movements reveal observers' task, 2014, Journal of vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_63",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_64@0",
            "content": "Hila Chefer, Shir Gur, Lior Wolf, Generic attention-model explainability for interpreting bimodal and encoder-decoder transformers, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_64",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_65@0",
            "content": "Hila Chefer, Shir Gur, Lior Wolf, Transformer interpretability beyond attention visualization, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_65",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_66@0",
            "content": "UNKNOWN, None, 2013, One billion word benchmark for measuring progress in statistical language modeling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_66",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_67@0",
            "content": "Wonil Choi, H Rutvik, John M Desai,  Henderson, The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri, 2014, Frontiers in human neuroscience, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_67",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_68@0",
            "content": "Grzegorz Chrupa\u0142a, Afra Alishahi, Correlating neural and symbolic representations of language, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_68",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_69@0",
            "content": "Kenneth Church, Mark Liberman, The future of computational linguistics: On beyond alchemy, 2021, Frontiers in Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_69",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_70@0",
            "content": "Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher Manning, What does BERT look at? an analysis of BERT's attention, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Italy. Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_70",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_71@0",
            "content": "UNKNOWN, None, 2017, Scanpath modeling and classification with hidden markov models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_71",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_72@0",
            "content": "Aron Culotta, Andrew Mccallum, Jonathan Betz, Integrating probabilistic extraction models and data mining to discover relations and patterns in text, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_72",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_73@0",
            "content": "Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, Dhruv Batra, Human attention in visual question answering: Do humans and deep networks look at the same regions?, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_73",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_74@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_74",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_75@0",
            "content": "Gary Feng, Eye movements as time-series random variables: A stochastic model of eye movement control in reading, 2006, Cognitive Systems Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_75",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_76@0",
            "content": "Eric Shi Feng, Alvin Wallace, I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Pathologies of neural models make interpretations difficult, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_76",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_77@0",
            "content": "Michelle Greene, Tommy Liu, Jeremy Wolfe, Reconsidering yarbus: A failure to predict observers' task from eye movement patterns, 2012, Vision research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_77",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_78@0",
            "content": "Fritz G\u00fcnther, Luca Rinaldi, Marco Marelli, Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions, 2019, Perspectives on Psychological Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_78",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_79@0",
            "content": "Michael Hahn, Frank Keller, Modeling human reading with neural attention, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_79",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_80@0",
            "content": "Amin Haji, - Abolhassani, James Clark, An inverse yarbus process: Predicting observers' task from eye movement patterns, 2014, Vision Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_80",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_81@0",
            "content": "Tadayoshi Hara, Daichi Mochihashi, Yoshinobu Kano, Akiko Aizawa, Predicting word fixations in text with a CRF model for capturing general reading strategies among readers, 2012, Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_81",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_82@0",
            "content": "John Henderson, Svetlana Shinkareva, Jing Wang, Steven Luke, Jenn Olejarczyk, Predicting cognitive state from eye movements, 2013, PloS one, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_82",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_83@0",
            "content": "Rebekka Hillen, Thomas G\u00fcnther, Claudia Kohlen, Cornelia Eckers, Muna Van Ermingen-Marbach, Katharina Sass, Wolfgang Scharke, Josefine Vollmar, Ralph Radach, Stefan Heim, Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm, 2013, Frontiers in human neuroscience, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_83",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_84@0",
            "content": "Nora Hollenstein, Lisa Beinborn, Relative importance in sentence processing, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_84",
            "start": 0,
            "end": 296,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_85@0",
            "content": "Nora Hollenstein, Antonio De La Torre, Nicolas Langer, Ce Zhang, CogniVal: A framework for cognitive word embedding evaluation, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_85",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2021, Multilingual language models predict human reading behavior, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_86",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_87@0",
            "content": "Nora Hollenstein, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, Nicolas Langer, Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading, 2018, Scientific data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_87",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_88@0",
            "content": "UNKNOWN, None, , Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_88",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_89@0",
            "content": "Sarthak Jain, Byron Wallace, Attention is not Explanation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_89",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_90@0",
            "content": "Reinhold Kliegl, Ellen Grabner, Martin Rolfs, Ralf Engbert, Length, frequency, and predictability effects of words on eye movements in reading, 2004, European Journal of Cognitive Psychology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_90",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_91@0",
            "content": "Reinhard Kneser, Hermann Ney, Improved backing-off for m-gram language modeling, 1995, 1995 international conference on acoustics, speech, and signal processing, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_91",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_92@0",
            "content": "Christof Koch, Shimon Ullman, Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry, 1985, Human Neurobiology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_92",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_93@0",
            "content": "UNKNOWN, None, 2019, Revealing the dark secrets of bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_93",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2016, DeepGaze II: Reading fixations from deep features trained on object recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_94",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_95@0",
            "content": "Matthias K\u00fcmmerer, S Thomas, Leon Wallis, Matthias Gatys,  Bethge, Understanding low-and high-level contributions to fixation prediction, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_95",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_96@0",
            "content": "UNKNOWN, None, 1995, Convolutional Networks for Images, Speech and Time Series, The MIT Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_96",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_97@0",
            "content": "UNKNOWN, None, 2017, A structured self-attentive sentence embedding, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_97",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_98",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_99@0",
            "content": "D Tomasz, Peter Loboda, J\u00f6erg Brusilovsky,  Brunstein, Inferring word relevance from eyemovements of readers, 2011, Proceedings of the 16th international conference on Intelligent user interfaces, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_99",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_100@0",
            "content": "Jonathan Malmaud, Roger Levy, Yevgeni Berzak, Bridging information-seeking human gaze and machine reading comprehension, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_100",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_101@0",
            "content": "Pawel Mandera, Emmanuel Keuleers, Marc Brysbaert, Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation, 2017, Journal of Memory and Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_101",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_102@0",
            "content": "Franz Matthies, Anders S\u00f8gaard, With blinkers on: Robust prediction of eye movements across readers, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_102",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_103@0",
            "content": "UNKNOWN, None, 2019, The changing role of phonology in reading development, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_103",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_104@0",
            "content": "Tim Miller, Explanation in artificial intelligence: Insights from the social sciences, 2019, Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_104",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_105@0",
            "content": "Abhijit Mishra, Kuntal Dey, Pushpak Bhattacharyya, Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_105",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_106@0",
            "content": "Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey, Pushpak Bhattacharyya, Leveraging cognitive features for sentiment analysis, 2016, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_106",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2019, Layer-Wise Relevance Propagation: An Overview, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_107",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_108@0",
            "content": "E Niebur, C Koch, Control attention: Modeling the \"where\" pathway, 1996, Advances in Neural Information Processing Systems, MIT Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_108",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_109@0",
            "content": "Mattias Nilsson, Joakim Nivre, Learning where to look: Modeling eye movements in reading, 2009-06-04, Proceedings of the Thirteenth Conference on Computational Natural Language Learning, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_109",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_110@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, Glove: Global vectors for word representation, 2014, Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_110",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_111@0",
            "content": "Grusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia, Douwe Kiela, Adina Williams, To what extent do human explanations of model behavior align with actual model behavior?, 2021, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_111",
            "start": 0,
            "end": 319,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_112@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_112",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_113@0",
            "content": "Keith Rayner, Eye movements in reading and information processing: 20 years of research, 1998, Psychological bulletin, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_113",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_114@0",
            "content": "Keith Rayner, Susan Duffy, Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity, 1986, Memory amp; cognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_114",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_115@0",
            "content": "Keith Rayner, Erik Reichle, Models of the reading process, 2010, WIREs Cognitive Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_115",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_116@0",
            "content": "Alexander Erik D Reichle,  Pollatsek, L Donald, Keith Fisher,  Rayner, Toward a model of eye movement control in reading, 1998, Psychological review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_116",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_117@0",
            "content": "UNKNOWN, None, 2003, The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_117",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_118@0",
            "content": "Timothy Rogers, Michael Wolmetz, Conceptual knowledge representation: A cross-section of current research, 2016, Cognitive Neuropsychology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_118",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2021, Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_119",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_120@0",
            "content": "Philipp Schmidt, Felix Bie\u00dfmann, Quantifying interpretability and trust in machine learning systems, 2019, CoRR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_120",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_121@0",
            "content": "Sofia Serrano, Noah Smith, Is attention interpretable?, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_121",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_122@0",
            "content": "UNKNOWN, None, 2021, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_122",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_123@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher,  Manning, Y Andrew, Christopher Ng,  Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 conference on empirical methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_123",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_124@0",
            "content": "Anders S\u00f8gaard, Evaluating word embeddings with fMRI and eye-tracking, 2016, Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_124",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_125@0",
            "content": "Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling, Ngoc Vu, Interpreting attention models with human visual attention in machine reading comprehension, 2020, Proceedings of the 24th Conference on Computational Natural Language Learning, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_125",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_126@0",
            "content": "Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling, Improving natural language processing tasks with human gaze-guided neural attention, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_126",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_127@0",
            "content": "Andreas Stolcke, Srilm -an extensible language modeling toolkit, 2002, Proceedings of the 7th International Conference on Spoken Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_127",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_128@0",
            "content": "UNKNOWN, None, 2016, Seeing with humans: Gaze-assisted neural image captioning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_128",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_129@0",
            "content": "Dengxin Arun Balajee Vasudevan, Luc Dai,  Van Gool, Object referring in videos with language and human gaze, 2018-06-18, 2018 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_129",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_130@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in Neural Information cessing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_130",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_131@0",
            "content": "Markus Bogojeski, Benjamin Blankertz, Real-time inference of word relevance from electroencephalogram and eye gaze, 2017, Journal of neural engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_131",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_132@0",
            "content": "Sarah Wiegreffe, Yuval Pinter, Attention is not not explanation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_132",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_133@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_133",
            "start": 0,
            "end": 536,
            "label": {}
        },
        {
            "ix": "40-ARR_v2_134@0",
            "content": "UNKNOWN, None, 1967, Eye Movements and Vision. Plenum, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "40-ARR_v2_134",
            "start": 0,
            "end": 55,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_1",
            "tgt_ix": "40-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_1",
            "tgt_ix": "40-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_3",
            "tgt_ix": "40-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_3",
            "tgt_ix": "40-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_3",
            "tgt_ix": "40-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_4",
            "tgt_ix": "40-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_7",
            "tgt_ix": "40-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_6",
            "tgt_ix": "40-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_6",
            "tgt_ix": "40-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_6",
            "tgt_ix": "40-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_6",
            "tgt_ix": "40-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_11",
            "tgt_ix": "40-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_11",
            "tgt_ix": "40-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_12",
            "tgt_ix": "40-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_14",
            "tgt_ix": "40-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_15",
            "tgt_ix": "40-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_16",
            "tgt_ix": "40-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_18",
            "tgt_ix": "40-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_19",
            "tgt_ix": "40-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_20",
            "tgt_ix": "40-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_20",
            "tgt_ix": "40-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_21",
            "tgt_ix": "40-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_22",
            "tgt_ix": "40-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_22",
            "tgt_ix": "40-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_23",
            "tgt_ix": "40-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_24",
            "tgt_ix": "40-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_24",
            "tgt_ix": "40-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_24",
            "tgt_ix": "40-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_26",
            "tgt_ix": "40-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_28",
            "tgt_ix": "40-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_29",
            "tgt_ix": "40-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_30",
            "tgt_ix": "40-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_31",
            "tgt_ix": "40-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_32",
            "tgt_ix": "40-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_35",
            "tgt_ix": "40-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_36",
            "tgt_ix": "40-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_37",
            "tgt_ix": "40-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_38",
            "tgt_ix": "40-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_39",
            "tgt_ix": "40-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_41",
            "tgt_ix": "40-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_42",
            "tgt_ix": "40-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_44",
            "tgt_ix": "40-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_45",
            "tgt_ix": "40-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_46",
            "tgt_ix": "40-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_49",
            "tgt_ix": "40-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_51",
            "tgt_ix": "40-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_52",
            "tgt_ix": "40-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "40-ARR_v2_0",
            "tgt_ix": "40-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_1",
            "tgt_ix": "40-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_2",
            "tgt_ix": "40-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_3",
            "tgt_ix": "40-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_4",
            "tgt_ix": "40-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_4",
            "tgt_ix": "40-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_4",
            "tgt_ix": "40-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_5",
            "tgt_ix": "40-ARR_v2_5@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_6",
            "tgt_ix": "40-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_7",
            "tgt_ix": "40-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_7",
            "tgt_ix": "40-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_7",
            "tgt_ix": "40-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_8",
            "tgt_ix": "40-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_9",
            "tgt_ix": "40-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_10",
            "tgt_ix": "40-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_11",
            "tgt_ix": "40-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_12",
            "tgt_ix": "40-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_12",
            "tgt_ix": "40-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_13",
            "tgt_ix": "40-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_14",
            "tgt_ix": "40-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_15",
            "tgt_ix": "40-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_15",
            "tgt_ix": "40-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_15",
            "tgt_ix": "40-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_15",
            "tgt_ix": "40-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_16",
            "tgt_ix": "40-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_17",
            "tgt_ix": "40-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_18",
            "tgt_ix": "40-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_18",
            "tgt_ix": "40-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_18",
            "tgt_ix": "40-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_18",
            "tgt_ix": "40-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_19",
            "tgt_ix": "40-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_19",
            "tgt_ix": "40-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_19",
            "tgt_ix": "40-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_20",
            "tgt_ix": "40-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_21",
            "tgt_ix": "40-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_21",
            "tgt_ix": "40-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_21",
            "tgt_ix": "40-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_22",
            "tgt_ix": "40-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_23",
            "tgt_ix": "40-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_23",
            "tgt_ix": "40-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_23",
            "tgt_ix": "40-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_23",
            "tgt_ix": "40-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_24",
            "tgt_ix": "40-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_25",
            "tgt_ix": "40-ARR_v2_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_26",
            "tgt_ix": "40-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_27",
            "tgt_ix": "40-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_28",
            "tgt_ix": "40-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_29",
            "tgt_ix": "40-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_29",
            "tgt_ix": "40-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_29",
            "tgt_ix": "40-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_29",
            "tgt_ix": "40-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_30",
            "tgt_ix": "40-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_30",
            "tgt_ix": "40-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_30",
            "tgt_ix": "40-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_31",
            "tgt_ix": "40-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_31",
            "tgt_ix": "40-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_31",
            "tgt_ix": "40-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_31",
            "tgt_ix": "40-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_32",
            "tgt_ix": "40-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_32",
            "tgt_ix": "40-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_32",
            "tgt_ix": "40-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_32",
            "tgt_ix": "40-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_33",
            "tgt_ix": "40-ARR_v2_33@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_34",
            "tgt_ix": "40-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_35",
            "tgt_ix": "40-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_35",
            "tgt_ix": "40-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_35",
            "tgt_ix": "40-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_35",
            "tgt_ix": "40-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_36",
            "tgt_ix": "40-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_37",
            "tgt_ix": "40-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_38",
            "tgt_ix": "40-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_38",
            "tgt_ix": "40-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_39",
            "tgt_ix": "40-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_39",
            "tgt_ix": "40-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_39",
            "tgt_ix": "40-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_40",
            "tgt_ix": "40-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_41",
            "tgt_ix": "40-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_41",
            "tgt_ix": "40-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_41",
            "tgt_ix": "40-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_41",
            "tgt_ix": "40-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_42",
            "tgt_ix": "40-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_42",
            "tgt_ix": "40-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_42",
            "tgt_ix": "40-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_42",
            "tgt_ix": "40-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_43",
            "tgt_ix": "40-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_44",
            "tgt_ix": "40-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_44",
            "tgt_ix": "40-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_45",
            "tgt_ix": "40-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_46",
            "tgt_ix": "40-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_46",
            "tgt_ix": "40-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_46",
            "tgt_ix": "40-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_46",
            "tgt_ix": "40-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_47",
            "tgt_ix": "40-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_48",
            "tgt_ix": "40-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_49",
            "tgt_ix": "40-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_49",
            "tgt_ix": "40-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_50",
            "tgt_ix": "40-ARR_v2_50@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_51",
            "tgt_ix": "40-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_51",
            "tgt_ix": "40-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_51",
            "tgt_ix": "40-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_51",
            "tgt_ix": "40-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_52",
            "tgt_ix": "40-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_52",
            "tgt_ix": "40-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_52",
            "tgt_ix": "40-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_52",
            "tgt_ix": "40-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_53",
            "tgt_ix": "40-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_53",
            "tgt_ix": "40-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_54",
            "tgt_ix": "40-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_55",
            "tgt_ix": "40-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_56",
            "tgt_ix": "40-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_57",
            "tgt_ix": "40-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_58",
            "tgt_ix": "40-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_59",
            "tgt_ix": "40-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_60",
            "tgt_ix": "40-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_61",
            "tgt_ix": "40-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_62",
            "tgt_ix": "40-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_63",
            "tgt_ix": "40-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_64",
            "tgt_ix": "40-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_65",
            "tgt_ix": "40-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_66",
            "tgt_ix": "40-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_67",
            "tgt_ix": "40-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_68",
            "tgt_ix": "40-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_69",
            "tgt_ix": "40-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_70",
            "tgt_ix": "40-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_71",
            "tgt_ix": "40-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_72",
            "tgt_ix": "40-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_73",
            "tgt_ix": "40-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_74",
            "tgt_ix": "40-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_75",
            "tgt_ix": "40-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_76",
            "tgt_ix": "40-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_77",
            "tgt_ix": "40-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_78",
            "tgt_ix": "40-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_79",
            "tgt_ix": "40-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_80",
            "tgt_ix": "40-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_81",
            "tgt_ix": "40-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_82",
            "tgt_ix": "40-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_83",
            "tgt_ix": "40-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_84",
            "tgt_ix": "40-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_85",
            "tgt_ix": "40-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_86",
            "tgt_ix": "40-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_87",
            "tgt_ix": "40-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_88",
            "tgt_ix": "40-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_89",
            "tgt_ix": "40-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_90",
            "tgt_ix": "40-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_91",
            "tgt_ix": "40-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_92",
            "tgt_ix": "40-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_93",
            "tgt_ix": "40-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_94",
            "tgt_ix": "40-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_95",
            "tgt_ix": "40-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_96",
            "tgt_ix": "40-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_97",
            "tgt_ix": "40-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_98",
            "tgt_ix": "40-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_99",
            "tgt_ix": "40-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_100",
            "tgt_ix": "40-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_101",
            "tgt_ix": "40-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_102",
            "tgt_ix": "40-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_103",
            "tgt_ix": "40-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_104",
            "tgt_ix": "40-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_105",
            "tgt_ix": "40-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_106",
            "tgt_ix": "40-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_107",
            "tgt_ix": "40-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_108",
            "tgt_ix": "40-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_109",
            "tgt_ix": "40-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_110",
            "tgt_ix": "40-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_111",
            "tgt_ix": "40-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_112",
            "tgt_ix": "40-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_113",
            "tgt_ix": "40-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_114",
            "tgt_ix": "40-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_115",
            "tgt_ix": "40-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_116",
            "tgt_ix": "40-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_117",
            "tgt_ix": "40-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_118",
            "tgt_ix": "40-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_119",
            "tgt_ix": "40-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_120",
            "tgt_ix": "40-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_121",
            "tgt_ix": "40-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_122",
            "tgt_ix": "40-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_123",
            "tgt_ix": "40-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_124",
            "tgt_ix": "40-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_125",
            "tgt_ix": "40-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_126",
            "tgt_ix": "40-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_127",
            "tgt_ix": "40-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_128",
            "tgt_ix": "40-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_129",
            "tgt_ix": "40-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_130",
            "tgt_ix": "40-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_131",
            "tgt_ix": "40-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_132",
            "tgt_ix": "40-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_133",
            "tgt_ix": "40-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "40-ARR_v2_134",
            "tgt_ix": "40-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 847,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "40-ARR",
        "version": 2
    }
}